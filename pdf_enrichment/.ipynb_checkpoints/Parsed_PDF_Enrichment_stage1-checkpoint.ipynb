{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Python venv: search_agent_poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4375a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This module will take the serialized dictionary got out of PDF Parsing ann try to extract\n",
    "##Semantic Knowldge like identifying \n",
    "## 1.Important Objects/Entities\n",
    "## 2.Deduplicate Entities\n",
    "## 3.Extracting Relations\n",
    "## 4.Extract the main Ideas/Topics around Each Page\n",
    "## 5.Link the different topics via diffrent entities/Objects\n",
    "## 6.Break down the document by pages instead of Chunks .\n",
    "## 7.If a page does not fit a chunk then chunk them extract information and then deduplicate the information across\n",
    "## the page.\n",
    "\n",
    "#Next Steps:\n",
    "## 5.Try to Seggregate the BigPDF on Sections.\n",
    "## 8.Try To Find Common Objects or ideas that link these sections.\n",
    "## 9.Try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15191ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "#rag_test_input_path='/home/matrix4284/MY_GEN_AI_PROJECTS/RAG/GraphRAG/graphrag-local-ollama/ragtest/input/'+file_name\n",
    "import os\n",
    "# importing shutil module\n",
    "import shutil\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import XMLOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01aaca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"llama3.1\"\n",
    "book_text=''\n",
    "page_text=''\n",
    "parsed_pdf_path='../pdf_parsing/parsed_pdf_ouput_pickle'\n",
    "file_name='Kubernetes_in_action_text_only'\n",
    "extension='.txt'\n",
    "start_page_idx=565\n",
    "end_page_index=566\n",
    "#full_filename=file_name+'_'+str(page_idx)+extension\n",
    "#full_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbcded8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Model for Prompt Tuning\n",
    "llm = ChatOllama(base_url=\"http://192.168.50.100:11434\",model=model_name)\n",
    "\n",
    "#embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",model_kwargs = model_kwargs)\n",
    "\n",
    "##Define Vectorstore\n",
    "vectorstore = Chroma(embedding_function=embeddings, persist_directory=\"./chroma_kubernetes_in_action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a648bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialize)\n",
    "with open(parsed_pdf_path+'/'+'pdf_content_dict_stage1.pickle', 'rb') as handle:\n",
    "    document_dict_deserialized = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da4eb885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page': 2,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'Kubernetes resources covered in the book\\n* Cluster-level resource (not namespaced)\\n** Also in other API versions; listed version is the one used in this book\\n(continues on inside back cover)\\nResource (abbr.) [API version]\\nDescription\\nSection\\nNamespace* (ns) [v1]\\nEnables organizing resources into non-overlapping \\ngroups (for example, per tenant)\\n3.7\\nDeploying workloads\\nPod (po) [v1]\\nThe basic deployable unit containing one or more \\nprocesses in co-located containers\\n3.1\\nReplicaSet (rs) [apps/v1beta2**]\\nKeeps one or more pod replicas running\\n4.3\\nReplicationController (rc) [v1]\\nThe older, less-powerful equivalent of a \\nReplicaSet\\n4.2\\nJob [batch/v1]\\nRuns pods that perform a completable task\\n4.5\\nCronJob [batch/v1beta1]\\nRuns a scheduled job once or periodically\\n4.6\\nDaemonSet (ds) [apps/v1beta2**]\\nRuns one pod replica per node (on all nodes or \\nonly on those matching a node selector)\\n4.4\\nStatefulSet (sts) [apps/v1beta1**]\\nRuns stateful pods with a stable identity\\n10.2\\nDeployment (deploy) [apps/v1beta1**]\\nDeclarative deployment and updates of pods\\n9.3\\nServices\\nService (svc) [v1]\\nExposes one or more pods at a single and stable \\nIP address and port pair\\n5.1\\nEndpoints (ep) [v1]\\nDefines which pods (or other servers) are \\nexposed through a service\\n5.2.1\\nIngress (ing) [extensions/v1beta1]\\nExposes one or more services to external clients \\nthrough a single externally reachable IP address\\n5.4\\nConfig\\nConfigMap (cm) [v1]\\nA key-value map for storing non-sensitive config \\noptions for apps and exposing it to them\\n7.4\\nSecret [v1]\\nLike a ConfigMap, but for sensitive data\\n7.5\\nStorage\\nPersistentVolume* (pv) [v1]\\nPoints to persistent storage that can be mounted \\ninto a pod through a PersistentVolumeClaim\\n6.5\\nPersistentVolumeClaim (pvc) [v1]\\nA request for and claim to a PersistentVolume\\n6.5\\nStorageClass* (sc) [storage.k8s.io/v1]\\nDefines the type of dynamically-provisioned stor-\\nage claimable in a PersistentVolumeClaim\\n6.6\\n \\nwww.allitebooks.com\\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [                  Col0                     Resource (abbr.) [API version]  \\\n",
       "   0                                                    Namespace* (ns) [v1]   \n",
       "   1  Deploying workloads  Pod (po) [v1]\\nReplicaSet (rs) [apps/v1beta2**...   \n",
       "   2             Services  Service (svc) [v1]\\nEndpoints (ep) [v1]\\nIngre...   \n",
       "   3               Config                   ConfigMap (cm) [v1]\\nSecret [v1]   \n",
       "   4              Storage  PersistentVolume* (pv) [v1]\\nPersistentVolumeC...   \n",
       "   \n",
       "                                            Description  \\\n",
       "   0  Enables organizing resources into non-overlapp...   \n",
       "   1  The basic deployable unit containing one or mo...   \n",
       "   2  Exposes one or more pods at a single and stabl...   \n",
       "   3  A key-value map for storing non-sensitive conf...   \n",
       "   4  Points to persistent storage that can be mount...   \n",
       "   \n",
       "                                      Section  \n",
       "   0                                      3.7  \n",
       "   1  3.1\\n4.3\\n4.2\\n4.5\\n4.6\\n4.4\\n10.2\\n9.3  \n",
       "   2                          5.1\\n5.2.1\\n5.4  \n",
       "   3                                 7.4\\n7.5  \n",
       "   4                            6.5\\n6.5\\n6.6  ]},\n",
       " {'page': 3,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'Kubernetes in Action\\n \\nwww.allitebooks.com\\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 4,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': ' \\nwww.allitebooks.com\\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 5,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'Kubernetes\\nin Action\\nMARKO LUKŠA\\nM A N N I N G\\nSHELTER ISLAND\\n \\nwww.allitebooks.com\\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 6,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'For online information and ordering of this and other Manning books, please visit\\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \\nFor more information, please contact\\nSpecial Sales Department\\nManning Publications Co.\\n20 Baldwin Road\\nPO Box 761\\nShelter Island, NY 11964\\nEmail: orders@manning.com\\n©2018 by Manning Publications Co. All rights reserved.\\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \\nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \\npermission of the publisher.\\nMany of the designations used by manufacturers and sellers to distinguish their products are \\nclaimed as trademarks. Where those designations appear in the book, and Manning \\nPublications was aware of a trademark claim, the designations have been printed in initial caps \\nor all caps.\\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \\nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \\nRecognizing also our responsibility to conserve the resources of our planet, Manning books\\nare printed on paper that is at least 15 percent recycled and processed without the use of \\nelemental chlorine.\\nManning Publications Co.\\nDevelopment editor: Elesha Hyde\\n20 Baldwin Road\\nReview editor: Aleksandar Dragosavljevic\\n´\\nPO Box 761\\nTechnical development editor: Jeanne Boyarsky\\nShelter Island, NY 11964\\nProject editor: Kevin Sullivan\\nCopyeditor: Katie Petito\\nProofreader: Melody Dolab\\nTechnical proofreader: Antonio Magnaghi\\nIllustrator: Chuck Larson\\nTypesetter: Dennis Dalinnik\\nCover designer: Marija Tudor\\nISBN: 9781617293726\\nPrinted in the United States of America\\n1 2 3 4 5 6 7 8 9 10 – EBM – 22 21 20 19 18 17\\n \\nwww.allitebooks.com\\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 7,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': ' To my parents, \\nwho have always put their children’s needs above their own\\n \\nwww.allitebooks.com\\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 8,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': ' \\n \\nwww.allitebooks.com\\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 9,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'vii\\nbrief contents\\nPART 1\\nOVERVIEW\\n1\\n■\\nIntroducing Kubernetes\\n1\\n2\\n■\\nFirst steps with Docker and Kubernetes\\n25\\nPART 2\\nCORE CONCEPTS\\n3\\n■\\nPods: running containers in Kubernetes\\n55\\n4\\n■\\nReplication and other controllers: deploying \\nmanaged pods\\n84\\n5\\n■\\nServices: enabling clients to discover and talk \\nto pods\\n120\\n6\\n■\\nVolumes: attaching disk storage to containers\\n159\\n7\\n■\\nConfigMaps and Secrets: configuring applications\\n191\\n8\\n■\\nAccessing pod metadata and other resources from \\napplications\\n225\\n9\\n■\\nDeployments: updating applications declaratively \\n250\\n10\\n■\\nStatefulSets: deploying replicated stateful \\napplications\\n280\\n \\nwww.allitebooks.com\\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 10,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'BRIEF CONTENTS\\nviii\\nPART 3\\nBEYOND THE BASICS\\n11\\n■\\nUnderstanding Kubernetes internals\\n309\\n12\\n■\\nSecuring the Kubernetes API server\\n346\\n13\\n■\\nSecuring cluster nodes and the network\\n375\\n14\\n■\\nManaging pods’ computational resources\\n404\\n15\\n■\\nAutomatic scaling of pods and cluster nodes\\n437\\n16\\n■\\nAdvanced scheduling\\n457\\n17\\n■\\nBest practices for developing apps\\n477\\n18\\n■\\nExtending Kubernetes\\n508\\n \\nwww.allitebooks.com\\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 11,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'ix\\ncontents\\npreface\\nxxi\\nacknowledgments\\nxxiii\\nabout this book\\nxxv\\nabout the author\\nxxix\\nabout the cover illustration\\nxxx\\nPART 1\\nOVERVIEW\\n1 \\nIntroducing Kubernetes\\n1\\n1.1\\nUnderstanding the need for a system like Kubernetes\\n2\\nMoving from monolithic apps to microservices\\n3\\n■Providing a \\nconsistent environment to applications\\n6\\n■Moving to continuous \\ndelivery: DevOps and NoOps\\n6\\n1.2\\nIntroducing container technologies\\n7\\nUnderstanding what containers are\\n8\\n■Introducing the Docker \\ncontainer platform\\n12\\n■Introducing rkt—an alternative to Docker\\n15\\n1.3\\nIntroducing Kubernetes\\n16\\nUnderstanding its origins\\n16\\n■Looking at Kubernetes from the \\ntop of a mountain\\n16\\n■Understanding the architecture of a \\nKubernetes cluster\\n18\\n■Running an application in Kubernetes\\n19\\nUnderstanding the benefits of using Kubernetes\\n21\\n1.4\\nSummary\\n23\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 12,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'CONTENTS\\nx\\n2 \\nFirst steps with Docker and Kubernetes\\n25\\n2.1\\nCreating, running, and sharing a container image\\n26\\nInstalling Docker and running a Hello World container\\n26\\nCreating a trivial Node.js app\\n28\\n■Creating a Dockerfile \\nfor the image\\n29\\n■Building the container image\\n29\\nRunning the container image\\n32\\n■Exploring the inside \\nof a running container\\n33\\n■Stopping and removing a \\ncontainer\\n34\\n■Pushing the image to an image registry\\n35\\n2.2\\nSetting up a Kubernetes cluster\\n36\\nRunning a local single-node Kubernetes cluster with Minikube\\n37\\nUsing a hosted Kubernetes cluster with Google Kubernetes \\nEngine\\n38\\n■Setting up an alias and command-line completion \\nfor kubectl\\n41\\n2.3\\nRunning your first app on Kubernetes\\n42\\nDeploying your Node.js app\\n42\\n■Accessing your web \\napplication\\n45\\n■The logical parts of your system\\n47\\nHorizontally scaling the application\\n48\\n■Examining what \\nnodes your app is running on\\n51\\n■Introducing the \\nKubernetes dashboard\\n52\\n2.4\\nSummary\\n53\\nPART 2\\nCORE CONCEPTS\\n3 \\nPods: running containers in Kubernetes\\n55\\n3.1\\nIntroducing pods\\n56\\nUnderstanding why we need pods\\n56\\n■Understanding pods\\n57\\nOrganizing containers across pods properly\\n58\\n3.2\\nCreating pods from YAML or JSON descriptors\\n61\\nExamining a YAML descriptor of an existing pod\\n61\\n■Creating a \\nsimple YAML descriptor for a pod\\n63\\n■Using kubectl create to \\ncreate the pod\\n65\\n■Viewing application logs\\n65\\n■Sending \\nrequests to the pod\\n66\\n3.3\\nOrganizing pods with labels\\n67\\nIntroducing labels\\n68\\n■Specifying labels when creating a pod\\n69\\nModifying labels of existing pods\\n70\\n3.4\\nListing subsets of pods through label selectors\\n71\\nListing pods using a label selector\\n71\\n■Using multiple conditions \\nin a label selector\\n72\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 13,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'CONTENTS\\nxi\\n3.5\\nUsing labels and selectors to constrain pod \\nscheduling\\n73\\nUsing labels for categorizing worker nodes\\n74\\n■Scheduling pods to \\nspecific nodes\\n74\\n■Scheduling to one specific node\\n75\\n3.6\\nAnnotating pods\\n75\\nLooking up an object’s annotations\\n75\\n■Adding and modifying \\nannotations\\n76\\n3.7\\nUsing namespaces to group resources\\n76\\nUnderstanding the need for namespaces\\n77\\n■Discovering other \\nnamespaces and their pods\\n77\\n■Creating a namespace\\n78\\nManaging objects in other namespaces\\n79\\n■Understanding \\nthe isolation provided by namespaces\\n79\\n3.8\\nStopping and removing pods\\n80\\nDeleting a pod by name\\n80\\n■Deleting pods using label \\nselectors\\n80\\n■Deleting pods by deleting the whole \\nnamespace\\n80\\n■Deleting all pods in a namespace, \\nwhile keeping the namespace\\n81\\n■Deleting (almost) \\nall resources in a namespace\\n82\\n3.9\\nSummary\\n82\\n4 \\nReplication and other controllers: deploying managed pods\\n84\\n4.1\\nKeeping pods healthy\\n85\\nIntroducing liveness probes\\n85\\n■Creating an HTTP-based \\nliveness probe\\n86\\n■Seeing a liveness probe in action\\n87\\nConfiguring additional properties of the liveness probe\\n88\\nCreating effective liveness probes\\n89\\n4.2\\nIntroducing ReplicationControllers\\n90\\nThe operation of a ReplicationController\\n91\\n■Creating a \\nReplicationController\\n93\\n■Seeing the ReplicationController \\nin action\\n94\\n■Moving pods in and out of the scope of a \\nReplicationController\\n98\\n■Changing the pod template\\n101\\nHorizontally scaling pods\\n102\\n■Deleting a \\nReplicationController\\n103\\n4.3\\nUsing ReplicaSets instead of ReplicationControllers\\n104\\nComparing a ReplicaSet to a ReplicationController\\n105\\nDefining a ReplicaSet\\n105\\n■Creating and examining a \\nReplicaSet\\n106\\n■Using the ReplicaSet’s more expressive \\nlabel selectors\\n107\\n■Wrapping up ReplicaSets\\n108\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 14,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'CONTENTS\\nxii\\n4.4\\nRunning exactly one pod on each node with \\nDaemonSets\\n108\\nUsing a DaemonSet to run a pod on every node\\n109\\nUsing a DaemonSet to run pods only on certain nodes\\n109\\n4.5\\nRunning pods that perform a single completable \\ntask\\n112\\nIntroducing the Job resource\\n112\\n■Defining a Job resource\\n113\\nSeeing a Job run a pod\\n114\\n■Running multiple pod instances \\nin a Job\\n114\\n■Limiting the time allowed for a Job pod to \\ncomplete\\n116\\n4.6\\nScheduling Jobs to run periodically or once \\nin the future\\n116\\nCreating a CronJob\\n116\\n■Understanding how scheduled \\njobs are run\\n117\\n4.7\\nSummary\\n118\\n5 \\nServices: enabling clients to discover and talk to pods\\n120\\n5.1\\nIntroducing services\\n121\\nCreating services\\n122\\n■Discovering services\\n128\\n5.2\\nConnecting to services living outside the cluster\\n131\\nIntroducing service endpoints\\n131\\n■Manually configuring \\nservice endpoints\\n132\\n■Creating an alias for an external \\nservice\\n134\\n5.3\\nExposing services to external clients\\n134\\nUsing a NodePort service\\n135\\n■Exposing a service through \\nan external load balancer\\n138\\n■Understanding the peculiarities \\nof external connections\\n141\\n5.4\\nExposing services externally through an Ingress \\nresource\\n142\\nCreating an Ingress resource\\n144\\n■Accessing the service \\nthrough the Ingress\\n145\\n■Exposing multiple services \\nthrough the same Ingress\\n146\\n■Configuring Ingress to \\nhandle TLS traffic\\n147\\n5.5\\nSignaling when a pod is ready to accept connections\\n149\\nIntroducing readiness probes\\n149\\n■Adding a readiness probe \\nto a pod\\n151\\n■Understanding what real-world readiness \\nprobes should do\\n153\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 15,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'CONTENTS\\nxiii\\n5.6\\nUsing a headless service for discovering individual \\npods\\n154\\nCreating a headless service\\n154\\n■Discovering pods \\nthrough DNS\\n155\\n■Discovering all pods—even those \\nthat aren’t ready\\n156\\n5.7\\nTroubleshooting services\\n156\\n5.8\\nSummary\\n157\\n6 \\nVolumes: attaching disk storage to containers\\n159\\n6.1\\nIntroducing volumes\\n160\\nExplaining volumes in an example\\n160\\n■Introducing available \\nvolume types\\n162\\n6.2\\nUsing volumes to share data between containers\\n163\\nUsing an emptyDir volume\\n163\\n■Using a Git repository as the \\nstarting point for a volume\\n166\\n6.3\\nAccessing files on the worker node’s filesystem\\n169\\nIntroducing the hostPath volume\\n169\\n■Examining system pods \\nthat use hostPath volumes\\n170\\n6.4\\nUsing persistent storage\\n171\\nUsing a GCE Persistent Disk in a pod volume\\n171\\n■Using other \\ntypes of volumes with underlying persistent storage\\n174\\n6.5\\nDecoupling pods from the underlying storage \\ntechnology\\n176\\nIntroducing PersistentVolumes and PersistentVolumeClaims\\n176\\nCreating a PersistentVolume\\n177\\n■Claiming a PersistentVolume \\nby creating a PersistentVolumeClaim\\n179\\n■Using a \\nPersistentVolumeClaim in a pod\\n181\\n■Understanding the \\nbenefits of using PersistentVolumes and claims\\n182\\n■Recycling \\nPersistentVolumes\\n183\\n6.6\\nDynamic provisioning of PersistentVolumes\\n184\\nDefining the available storage types through StorageClass \\nresources\\n185\\n■Requesting the storage class in a \\nPersistentVolumeClaim\\n185\\n■Dynamic provisioning \\nwithout specifying a storage class\\n187\\n6.7\\nSummary\\n190\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 16,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'CONTENTS\\nxiv\\n7 \\nConfigMaps and Secrets: configuring applications\\n191\\n7.1\\nConfiguring containerized applications\\n191\\n7.2\\nPassing command-line arguments to containers\\n192\\nDefining the command and arguments in Docker\\n193\\nOverriding the command and arguments in Kubernetes\\n195\\n7.3\\nSetting environment variables for a container\\n196\\nSpecifying environment variables in a container definition\\n197\\nReferring to other environment variables in a variable’s value\\n198\\nUnderstanding the drawback of hardcoding environment \\nvariables\\n198\\n7.4\\nDecoupling configuration with a ConfigMap\\n198\\nIntroducing ConfigMaps\\n198\\n■Creating a ConfigMap\\n200\\nPassing a ConfigMap entry to a container as an environment \\nvariable\\n202\\n■Passing all entries of a ConfigMap as environment \\nvariables at once\\n204\\n■Passing a ConfigMap entry as a \\ncommand-line argument\\n204\\n■Using a configMap volume to \\nexpose ConfigMap entries as files\\n205\\n■Updating an app’s config \\nwithout having to restart the app\\n211\\n7.5\\nUsing Secrets to pass sensitive data to containers\\n213\\nIntroducing Secrets\\n214\\n■Introducing the default token \\nSecret\\n214\\n■Creating a Secret\\n216\\n■Comparing ConfigMaps \\nand Secrets\\n217\\n■Using the Secret in a pod\\n218\\nUnderstanding image pull Secrets\\n222\\n7.6\\nSummary\\n224\\n8 \\nAccessing pod metadata and other resources from \\napplications\\n225\\n8.1\\nPassing metadata through the Downward API\\n226\\nUnderstanding the available metadata\\n226\\n■Exposing metadata \\nthrough environment variables\\n227\\n■Passing metadata through \\nfiles in a downwardAPI volume\\n230\\n8.2\\nTalking to the Kubernetes API server\\n233\\nExploring the Kubernetes REST API\\n234\\n■Talking to the API \\nserver from within a pod\\n238\\n■Simplifying API server \\ncommunication with ambassador containers\\n243\\n■Using client \\nlibraries to talk to the API server\\n246\\n8.3\\nSummary\\n249\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 17,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'CONTENTS\\nxv\\n9 \\nDeployments: updating applications declaratively\\n250\\n9.1\\nUpdating applications running in pods\\n251\\nDeleting old pods and replacing them with new ones\\n252\\nSpinning up new pods and then deleting the old ones\\n252\\n9.2\\nPerforming an automatic rolling update with a \\nReplicationController\\n254\\nRunning the initial version of the app\\n254\\n■Performing a rolling \\nupdate with kubectl\\n256\\n■Understanding why kubectl rolling-\\nupdate is now obsolete\\n260\\n9.3\\nUsing Deployments for updating apps declaratively\\n261\\nCreating a Deployment\\n262\\n■Updating a Deployment\\n264\\nRolling back a deployment\\n268\\n■Controlling the rate of the \\nrollout\\n271\\n■Pausing the rollout process\\n273\\n■Blocking \\nrollouts of bad versions\\n274\\n9.4\\nSummary\\n279\\n10 \\nStatefulSets: deploying replicated stateful applications\\n280\\n10.1\\nReplicating stateful pods\\n281\\nRunning multiple replicas with separate storage for each\\n281\\nProviding a stable identity for each pod\\n282\\n10.2\\nUnderstanding StatefulSets\\n284\\nComparing StatefulSets with ReplicaSets\\n284\\n■Providing a \\nstable network identity\\n285\\n■Providing stable dedicated storage \\nto each stateful instance\\n287\\n■Understanding StatefulSet \\nguarantees\\n289\\n10.3\\nUsing a StatefulSet\\n290\\nCreating the app and container image\\n290\\n■Deploying the app \\nthrough a StatefulSet\\n291\\n■Playing with your pods\\n295\\n10.4\\nDiscovering peers in a StatefulSet\\n299\\nImplementing peer discovery through DNS\\n301\\n■Updating a \\nStatefulSet\\n302\\n■Trying out your clustered data store\\n303\\n10.5\\nUnderstanding how StatefulSets deal with node \\nfailures\\n304\\nSimulating a node’s disconnection from the network\\n304\\nDeleting the pod manually\\n306\\n10.6\\nSummary\\n307\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 18,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'CONTENTS\\nxvi\\nPART 3\\nBEYOND THE BASICS\\n11 \\nUnderstanding Kubernetes internals\\n309\\n11.1\\nUnderstanding the architecture\\n310\\nThe distributed nature of Kubernetes components\\n310\\nHow Kubernetes uses etcd\\n312\\n■What the API server does\\n316\\nUnderstanding how the API server notifies clients of resource \\nchanges\\n318\\n■Understanding the Scheduler\\n319\\nIntroducing the controllers running in the Controller Manager\\n321\\nWhat the Kubelet does\\n326\\n■The role of the Kubernetes Service \\nProxy\\n327\\n■Introducing Kubernetes add-ons\\n328\\n■Bringing it \\nall together\\n330\\n11.2\\nHow controllers cooperate\\n330\\nUnderstanding which components are involved\\n330\\n■The chain \\nof events\\n331\\n■Observing cluster events\\n332\\n11.3\\nUnderstanding what a running pod is\\n333\\n11.4\\nInter-pod networking\\n335\\nWhat the network must be like\\n335\\n■Diving deeper into \\nhow networking works\\n336\\n■Introducing the Container \\nNetwork Interface\\n338\\n11.5\\nHow services are implemented\\n338\\nIntroducing the kube-proxy\\n339\\n■How kube-proxy uses iptables\\n339\\n11.6\\nRunning highly available clusters\\n341\\nMaking your apps highly available\\n341\\n■Making Kubernetes \\nControl Plane components highly available\\n342\\n11.7\\nSummary\\n345\\n12 \\nSecuring the Kubernetes API server\\n346\\n12.1\\nUnderstanding authentication\\n346\\nUsers and groups\\n347\\n■Introducing ServiceAccounts\\n348\\nCreating ServiceAccounts\\n349\\n■Assigning a ServiceAccount \\nto a pod\\n351\\n12.2\\nSecuring the cluster with role-based access control\\n353\\nIntroducing the RBAC authorization plugin\\n353\\n■Introducing \\nRBAC resources\\n355\\n■Using Roles and RoleBindings\\n358\\nUsing ClusterRoles and ClusterRoleBindings\\n362\\nUnderstanding default ClusterRoles and ClusterRoleBindings\\n371\\nGranting authorization permissions wisely\\n373\\n12.3\\nSummary\\n373\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 19,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'CONTENTS\\nxvii\\n13 \\nSecuring cluster nodes and the network\\n375\\n13.1\\nUsing the host node’s namespaces in a pod\\n376\\nUsing the node’s network namespace in a pod\\n376\\n■Binding to \\na host port without using the host’s network namespace\\n377\\nUsing the node’s PID and IPC namespaces\\n379\\n13.2\\nConfiguring the container’s security context\\n380\\nRunning a container as a specific user\\n381\\n■Preventing a \\ncontainer from running as root\\n382\\n■Running pods in \\nprivileged mode\\n382\\n■Adding individual kernel capabilities \\nto a container\\n384\\n■Dropping capabilities from a container\\n385\\nPreventing processes from writing to the container’s filesystem\\n386\\nSharing volumes when containers run as different users\\n387\\n13.3\\nRestricting the use of security-related features \\nin pods\\n389\\nIntroducing the PodSecurityPolicy resource\\n389\\n■Understanding \\nrunAsUser, fsGroup, and supplementalGroups policies\\n392\\nConfiguring allowed, default, and disallowed capabilities\\n394\\nConstraining the types of volumes pods can use\\n395\\n■Assigning \\ndifferent PodSecurityPolicies to different users and groups\\n396\\n13.4\\nIsolating the pod network\\n399\\nEnabling network isolation in a namespace\\n399\\n■Allowing \\nonly some pods in the namespace to connect to a server pod\\n400\\nIsolating the network between Kubernetes namespaces\\n401\\nIsolating using CIDR notation\\n402\\n■Limiting the outbound \\ntraffic of a set of pods\\n403\\n13.5\\nSummary\\n403\\n14 \\nManaging pods’ computational resources\\n404\\n14.1\\nRequesting resources for a pod’s containers\\n405\\nCreating pods with resource requests\\n405\\n■Understanding how \\nresource requests affect scheduling\\n406\\n■Understanding how CPU \\nrequests affect CPU time sharing\\n411\\n■Defining and requesting \\ncustom resources\\n411\\n14.2\\nLimiting resources available to a container\\n412\\nSetting a hard limit for the amount of resources a container \\ncan use\\n412\\n■Exceeding the limits\\n414\\n■Understanding \\nhow apps in containers see limits\\n415\\n14.3\\nUnderstanding pod QoS classes\\n417\\nDefining the QoS class for a pod\\n417\\n■Understanding which \\nprocess gets killed when memory is low\\n420\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 20,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'CONTENTS\\nxviii\\n14.4\\nSetting default requests and limits for pods per \\nnamespace\\n421\\nIntroducing the LimitRange resource\\n421\\n■Creating a \\nLimitRange object\\n422\\n■Enforcing the limits\\n423\\nApplying default resource requests and limits\\n424\\n14.5\\nLimiting the total resources available in \\na namespace\\n425\\nIntroducing the ResourceQuota object\\n425\\n■Specifying a quota \\nfor persistent storage\\n427\\n■Limiting the number of objects that can \\nbe created\\n427\\n■Specifying quotas for specific pod states and/or \\nQoS classes\\n429\\n14.6\\nMonitoring pod resource usage\\n430\\nCollecting and retrieving actual resource usages\\n430\\n■Storing \\nand analyzing historical resource consumption statistics\\n432\\n14.7\\nSummary\\n435\\n15 \\nAutomatic scaling of pods and cluster nodes\\n437\\n15.1\\nHorizontal pod autoscaling\\n438\\nUnderstanding the autoscaling process\\n438\\n■Scaling based \\non CPU utilization\\n441\\n■Scaling based on memory \\nconsumption\\n448\\n■Scaling based on other and custom \\nmetrics\\n448\\n■Determining which metrics are appropriate for \\nautoscaling\\n450\\n■Scaling down to zero replicas\\n450\\n15.2\\nVertical pod autoscaling\\n451\\nAutomatically configuring resource requests\\n451\\n■Modifying \\nresource requests while a pod is running\\n451\\n15.3\\nHorizontal scaling of cluster nodes\\n452\\nIntroducing the Cluster Autoscaler\\n452\\n■Enabling the Cluster \\nAutoscaler\\n454\\n■Limiting service disruption during cluster \\nscale-down\\n454\\n15.4\\nSummary\\n456\\n16 \\nAdvanced scheduling\\n457\\n16.1\\nUsing taints and tolerations to repel pods from certain \\nnodes\\n457\\nIntroducing taints and tolerations\\n458\\n■Adding custom taints to \\na node\\n460\\n■Adding tolerations to pods\\n460\\n■Understanding \\nwhat taints and tolerations can be used for\\n461\\n \\nwww.allitebooks.com\\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 21,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'CONTENTS\\nxix\\n16.2\\nUsing node affinity to attract pods to certain nodes\\n462\\nSpecifying hard node affinity rules\\n463\\n■Prioritizing nodes when \\nscheduling a pod\\n465\\n16.3\\nCo-locating pods with pod affinity and anti-affinity\\n468\\nUsing inter-pod affinity to deploy pods on the same node\\n468\\nDeploying pods in the same rack, availability zone, or geographic \\nregion\\n471\\n■Expressing pod affinity preferences instead of hard \\nrequirements\\n472\\n■Scheduling pods away from each other with \\npod anti-affinity\\n474\\n16.4\\nSummary\\n476\\n17 \\nBest practices for developing apps\\n477\\n17.1\\nBringing everything together\\n478\\n17.2\\nUnderstanding the pod’s lifecycle\\n479\\nApplications must expect to be killed and relocated\\n479\\nRescheduling of dead or partially dead pods\\n482\\n■Starting \\npods in a specific order\\n483\\n■Adding lifecycle hooks\\n485\\nUnderstanding pod shutdown\\n489\\n17.3\\nEnsuring all client requests are handled properly\\n492\\nPreventing broken client connections when a pod is starting up\\n492\\nPreventing broken connections during pod shut-down\\n493\\n17.4\\nMaking your apps easy to run and manage in \\nKubernetes\\n497\\nMaking manageable container images\\n497\\n■Properly \\ntagging your images and using imagePullPolicy wisely\\n497\\nUsing multi-dimensional instead of single-dimensional labels\\n498\\nDescribing each resource through annotations\\n498\\n■Providing \\ninformation on why the process terminated\\n498\\n■Handling \\napplication logs\\n500\\n17.5\\nBest practices for development and testing\\n502\\nRunning apps outside of Kubernetes during development\\n502\\nUsing Minikube in development\\n503\\n■Versioning and auto-\\ndeploying resource manifests\\n504\\n■Introducing Ksonnet as an \\nalternative to writing YAML/JSON manifests\\n505\\n■Employing \\nContinuous Integration and Continuous Delivery (CI/CD)\\n506\\n17.6\\nSummary\\n506\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 22,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'CONTENTS\\nxx\\n18 \\nExtending Kubernetes\\n508\\n18.1\\nDefining custom API objects\\n508\\nIntroducing CustomResourceDefinitions\\n509\\n■Automating \\ncustom resources with custom controllers\\n513\\n■Validating \\ncustom objects\\n517\\n■Providing a custom API server for your \\ncustom objects\\n518\\n18.2\\nExtending Kubernetes with the Kubernetes Service \\nCatalog\\n519\\nIntroducing the Service Catalog\\n520\\n■Introducing the \\nService Catalog API server and Controller Manager\\n521\\nIntroducing Service Brokers and the OpenServiceBroker API\\n522\\nProvisioning and using a service\\n524\\n■Unbinding and \\ndeprovisioning\\n526\\n■Understanding what the Service \\nCatalog brings\\n526\\n18.3\\nPlatforms built on top of Kubernetes\\n527\\nRed Hat OpenShift Container Platform\\n527\\n■Deis Workflow \\nand Helm\\n530\\n18.4\\nSummary\\n533\\nappendix A\\nUsing kubectl with multiple clusters\\n534\\nappendix B\\nSetting up a multi-node cluster with kubeadm\\n539\\nappendix C\\nUsing other container runtimes\\n552\\nappendix D\\nCluster Federation\\n556\\nindex 561\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 23,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'xxi\\npreface\\nAfter working at Red Hat for a few years, in late 2014 I was assigned to a newly-\\nestablished team called Cloud Enablement. Our task was to bring the company’s\\nrange of middleware products to the OpenShift Container Platform, which was then\\nbeing developed on top of Kubernetes. At that time, Kubernetes was still in its\\ninfancy—version 1.0 hadn’t even been released yet.\\n Our team had to get to know the ins and outs of Kubernetes quickly to set a proper\\ndirection for our software and take advantage of everything Kubernetes had to offer.\\nWhen faced with a problem, it was hard for us to tell if we were doing things wrong or\\nmerely hitting one of the early Kubernetes bugs. \\n Both Kubernetes and my understanding of it have come a long way since then.\\nWhen I first started using it, most people hadn’t even heard of Kubernetes. Now, virtu-\\nally every software engineer knows about it, and it has become one of the fastest-\\ngrowing and most-widely-adopted ways of running applications in both the cloud and\\non-premises datacenters. \\n In my first month of dealing with Kubernetes, I wrote a two-part blog post about\\nhow to run a JBoss WildFly application server cluster in OpenShift/Kubernetes. At the\\ntime, I never could have imagined that a simple blog post would ultimately lead the\\npeople at Manning to contact me about whether I would like to write a book about\\nKubernetes. Of course, I couldn’t say no to such an offer, even though I was sure\\nthey’d approached other people as well and would ultimately pick someone else.\\n And yet, here we are. After more than a year and a half of writing and researching,\\nthe book is done. It’s been an awesome journey. Writing a book about a technology is\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 24,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'PREFACE\\nxxii\\nabsolutely the best way to get to know it in much greater detail than you’d learn as just\\na user. As my knowledge of Kubernetes has expanded during the process and Kuber-\\nnetes itself has evolved, I’ve constantly gone back to previous chapters I’ve written and\\nadded additional information. I’m a perfectionist, so I’ll never really be absolutely sat-\\nisfied with the book, but I’m happy to hear that a lot of readers of the Manning Early\\nAccess Program (MEAP) have found it to be a great guide to Kubernetes.\\n My aim is to get the reader to understand the technology itself and teach them\\nhow to use the tooling to effectively and efficiently develop and deploy apps to Kuber-\\nnetes clusters. In the book, I don’t put much emphasis on how to actually set up and\\nmaintain a proper highly available Kubernetes cluster, but the last part should give\\nreaders a very solid understanding of what such a cluster consists of and should allow\\nthem to easily comprehend additional resources that deal with this subject.\\n I hope you’ll enjoy reading it, and that it teaches you how to get the most out of\\nthe awesome system that is Kubernetes.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 25,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'xxiii\\nacknowledgments\\nBefore I started writing this book, I had no clue how many people would be involved\\nin bringing it from a rough manuscript to a published piece of work. This means\\nthere are a lot of people to thank.\\n First, I’d like to thank Erin Twohey for approaching me about writing this book,\\nand Michael Stephens from Manning, who had full confidence in my ability to write it\\nfrom day one. His words of encouragement early on really motivated me and kept me\\nmotivated throughout the last year and a half. \\n I would also like to thank my initial development editor Andrew Warren, who\\nhelped me get my first chapter out the door, and Elesha Hyde, who took over from\\nAndrew and worked with me all the way to the last chapter. Thank you for bearing\\nwith me, even though I’m a difficult person to deal with, as I tend to drop off the\\nradar fairly regularly. \\n I would also like to thank Jeanne Boyarsky, who was the first reviewer to read and\\ncomment on my chapters while I was writing them. Jeanne and Elesha were instrumen-\\ntal in making the book as nice as it hopefully is. Without their comments, the book\\ncould never have received such good reviews from external reviewers and readers.\\n I’d like to thank my technical proofreader, Antonio Magnaghi, and of course all\\nmy external reviewers: Al Krinker, Alessandro Campeis, Alexander Myltsev, Csaba Sari,\\nDavid DiMaria, Elias Rangel, Erisk Zelenka, Fabrizio Cucci, Jared Duncan, Keith\\nDonaldson, Michael Bright, Paolo Antinori, Peter Perlepes, and Tiklu Ganguly. Their\\npositive comments kept me going at times when I worried my writing was utterly awful\\nand completely useless. On the other hand, their constructive criticism helped improve\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 26,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'ACKNOWLEDGMENTS\\nxxiv\\nsections that I’d quickly thrown together without enough effort. Thank you for point-\\ning out the hard-to-understand sections and suggesting ways of improving the book.\\nAlso, thank you for asking the right questions, which made me realize I was wrong\\nabout two or three things in the initial versions of the manuscript.\\n I also need to thank readers who bought the early version of the book through\\nManning’s MEAP program and voiced their comments in the online forum or reached\\nout to me directly—especially Vimal Kansal, Paolo Patierno, and Roland Huß, who\\nnoticed quite a few inconsistencies and other mistakes. And I would like to thank\\neveryone at Manning who has been involved in getting this book published. Before I\\nfinish, I also need to thank my colleague and high school friend Aleš Justin, who\\nbrought me to Red Hat, and my wonderful colleagues from the Cloud Enablement\\nteam. If I hadn’t been at Red Hat or in the team, I wouldn’t have been the one to write\\nthis book.\\n Lastly, I would like to thank my wife and my son, who were way too understanding\\nand supportive over the last 18 months, while I was locked in my office instead of\\nspending time with them.\\n Thank you all!\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 27,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'xxv\\nabout this book\\nKubernetes in Action aims to make you a proficient user of Kubernetes. It teaches you\\nvirtually all the concepts you need to understand to effectively develop and run appli-\\ncations in a Kubernetes environment. \\n Before diving into Kubernetes, the book gives an overview of container technolo-\\ngies like Docker, including how to build containers, so that even readers who haven’t\\nused these technologies before can get up and running.  It then slowly guides you\\nthrough most of what you need to know about Kubernetes—from basic concepts to\\nthings hidden below the surface.\\nWho should read this book\\nThe book focuses primarily on application developers, but it also provides an overview\\nof managing applications from the operational perspective. It’s meant for anyone\\ninterested in running and managing containerized applications on more than just a\\nsingle server.\\n Both beginner and advanced software engineers who want to learn about con-\\ntainer technologies and orchestrating multiple related containers at scale will gain the\\nexpertise necessary to develop, containerize, and run their applications in a Kuberne-\\ntes environment. \\n No previous exposure to either container technologies or Kubernetes is required.\\nThe book explains the subject matter in a progressively detailed manner, and doesn’t\\nuse any application source code that would be too hard for non-expert developers to\\nunderstand. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 28,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'ABOUT THIS BOOK\\nxxvi\\n Readers, however, should have at least a basic knowledge of programming, com-\\nputer networking, and running basic commands in Linux, and an understanding of\\nwell-known computer protocols like HTTP. \\nHow this book is organized: a roadmap\\nThis book has three parts that cover 18 chapters.\\n Part 1 gives a short introduction to Docker and Kubernetes, how to set up a Kuber-\\nnetes cluster, and how to run a simple application in it. It contains two chapters:\\n■\\nChapter 1 explains what Kubernetes is, how it came to be, and how it helps to\\nsolve today’s problems of managing applications at scale.\\n■\\nChapter 2 is a hands-on tutorial on how to build a container image and run it in\\na Kubernetes cluster. It also explains how to run a local single-node Kubernetes\\ncluster and a proper multi-node cluster in the cloud.\\nPart 2 introduces the key concepts you must understand to run applications in Kuber-\\nnetes. The chapters are as follows:\\n■\\nChapter 3 introduces the fundamental building block in Kubernetes—the pod—\\nand explains how to organize pods and other Kubernetes objects through labels. \\n■\\nChapter 4 teaches you how Kubernetes keeps applications healthy by automati-\\ncally restarting containers. It also shows how to properly run managed pods,\\nhorizontally scale them, make them resistant to failures of cluster nodes, and\\nrun them at a predefined time in the future or periodically.\\n■\\nChapter 5 shows how pods can expose the service they provide to clients run-\\nning both inside and outside the cluster. It also shows how pods running in the\\ncluster can discover and access services, regardless of whether they live in or out\\nof the cluster. \\n■\\nChapter 6 explains how multiple containers running in the same pod can share\\nfiles and how you can manage persistent storage and make it accessible to pods. \\n■\\nChapter 7 shows how to pass configuration data and sensitive information like\\ncredentials to apps running inside pods.\\n■\\nChapter 8 describes how applications can get information about the Kuberne-\\ntes environment they’re running in and how they can talk to Kubernetes to\\nalter the state of the cluster.\\n■\\nChapter 9 introduces the concept of a Deployment and explains the proper way\\nof running and updating applications in a Kubernetes environment.\\n■\\nChapter 10 introduces a dedicated way of running stateful applications, which\\nusually require a stable identity and state.\\nPart 3 dives deep into the internals of a Kubernetes cluster, introduces some addi-\\ntional concepts, and reviews everything you’ve learned in the first two parts from a\\nhigher perspective. This is the last group of chapters:\\n■\\nChapter 11 goes beneath the surface of Kubernetes and explains all the compo-\\nnents that make up a Kubernetes cluster and what each of them does. It also\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 29,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'ABOUT THIS BOOK\\nxxvii\\nexplains how pods communicate through the network and how services per-\\nform load balancing across multiple pods.\\n■\\nChapter 12 explains how to secure your Kubernetes API server, and by exten-\\nsion the cluster, using authentication and authorization. \\n■\\nChapter 13 teaches you how pods can access the node’s resources and how a\\ncluster administrator can prevent pods from doing that.\\n■\\nChapter 14 dives into constraining the computational resources each applica-\\ntion is allowed to consume, configuring the applications’ Quality of Service\\nguarantees, and monitoring the resource usage of individual applications. It\\nalso teaches you how to prevent users from consuming too many resources.\\n■\\nChapter 15 discusses how Kubernetes can be configured to automatically scale\\nthe number of running replicas of your application, and how it can also increase\\nthe size of your cluster when your current number of cluster nodes can’t accept\\nany additional applications. \\n■\\nChapter 16 shows how to ensure pods are scheduled only to certain nodes or\\nhow to prevent them from being scheduled to others. It also shows how to make\\nsure pods are scheduled together or how to prevent that from happening.\\n■\\nChapter 17 teaches you how you should develop your applications to make them\\ngood citizens of your cluster. It also gives you a few pointers on how to set up your\\ndevelopment and testing workflows to reduce friction during development.\\n■\\nChapter 18 shows you how you can extend Kubernetes with your own custom\\nobjects and how others have done it and created enterprise-class application\\nplatforms.\\nAs you progress through these chapters, you’ll not only learn about the individual\\nKubernetes building blocks, but also progressively improve your knowledge of using\\nthe kubectl command-line tool.\\nAbout the code\\nWhile this book doesn’t contain a lot of actual source code, it does contain a lot of\\nmanifests of Kubernetes resources in YAML format and shell commands along with\\ntheir outputs. All of this is formatted in a fixed-width font like this to separate it\\nfrom ordinary text. \\n Shell commands are mostly in bold, to clearly separate them from their output, but\\nsometimes only the most important parts of the command or parts of the command’s\\noutput are in bold for emphasis. In most cases, the command output has been reformat-\\nted to make it fit into the limited space in the book. Also, because the Kubernetes CLI\\ntool kubectl is constantly evolving, newer versions may print out more information\\nthan what’s shown in the book. Don’t be confused if they don’t match exactly. \\n Listings sometimes include a line-continuation marker (➥) to show that a line of\\ntext wraps to the next line. They also include annotations, which highlight and explain\\nthe most important parts. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 30,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'ABOUT THIS BOOK\\nxxviii\\n Within text paragraphs, some very common elements such as Pod, Replication-\\nController, ReplicaSet, DaemonSet, and so forth are set in regular font to avoid over-\\nproliferation of code font and help readability. In some places, “Pod” is capitalized\\nto refer to the Pod resource, and lowercased to refer to the actual group of running\\ncontainers.\\n All the samples in the book have been tested with Kubernetes version 1.8 running\\nin Google Kubernetes Engine and in a local cluster run with Minikube. The complete\\nsource code and YAML manifests can be found at https:/\\n/github.com/luksa/kubernetes-\\nin-action or downloaded from the publisher’s website at www.manning.com/books/\\nkubernetes-in-action.\\nBook forum\\nPurchase of Kubernetes in Action includes free access to a private web forum run by\\nManning Publications where you can make comments about the book, ask technical\\nquestions, and receive help from the author and from other users. To access the\\nforum, go to https:/\\n/forums.manning.com/forums/kubernetes-in-action. You can also\\nlearn more about Manning’s forums and the rules of conduct at https:/\\n/forums\\n.manning.com/forums/about.\\n Manning’s commitment to our readers is to provide a venue where a meaningful\\ndialogue between individual readers and between readers and the author can take\\nplace. It is not a commitment to any specific amount of participation on the part of\\nthe author, whose contribution to the forum remains voluntary (and unpaid). We sug-\\ngest you try asking the author some challenging questions lest his interest stray! The\\nforum and the archives of previous discussions will be accessible from the publisher’s\\nwebsite as long as the book is in print.\\nOther online resources\\nYou can find a wide range of additional Kubernetes resources at the following locations:\\n■\\nThe Kubernetes website at https:/\\n/kubernetes.io\\n■\\nThe Kubernetes Blog, which regularly posts interesting info (http:/\\n/blog.kuber-\\nnetes.io)\\n■\\nThe Kubernetes community’s Slack channel at http:/\\n/slack.k8s.io\\n■\\nThe Kubernetes and Cloud Native Computing Foundation’s YouTube channels:\\n– https:/\\n/www.youtube.com/channel/UCZ2bu0qutTOM0tHYa_jkIwg \\n– https:/\\n/www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA \\nTo gain a deeper understanding of individual topics or even to help contribute to\\nKubernetes, you can also check out any of the Kubernetes Special Interest Groups (SIGs)\\nat https:/\\n/github.com/kubernetes/kubernetes/wiki/Special-Interest-Groups-(SIGs).\\n And, finally, as Kubernetes is open source, there’s a wealth of information available\\nin the Kubernetes source code itself. You’ll find it at https:/\\n/github.com/kubernetes/\\nkubernetes and related repositories. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 31,\n",
       "  'img_cnt': 1,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'xxix\\nabout the author\\nMarko Lukša is a software engineer with more than 20 years of\\nprofessional experience developing everything from simple\\nweb applications to full ERP systems, frameworks, and middle-\\nware software. He took his first steps in programming back in\\n1985, at the age of six, on a second-hand ZX Spectrum com-\\nputer his father had bought for him. In primary school, he was\\nthe national champion in the Logo programming competition\\nand attended summer coding camps, where he learned to pro-\\ngram in Pascal. Since then, he has developed software in a\\nwide range of programming languages.\\n   In high school, he started building dynamic websites when\\nthe web was still relatively young. He then moved on to developing software for the\\nhealthcare and telecommunications industries at a local company, while studying\\ncomputer science at the University of Ljubljana, Slovenia. Eventually, he ended up\\nworking for Red Hat, initially developing an open source implementation of the Goo-\\ngle App Engine API, which utilized Red Hat’s JBoss middleware products underneath.\\nHe also worked in or contributed to projects like CDI/Weld, Infinispan/JBoss Data-\\nGrid, and others.\\n Since late 2014, he has been part of Red Hat’s Cloud Enablement team, where his\\nresponsibilities include staying up-to-date on new developments in Kubernetes and\\nrelated technologies and ensuring the company’s middleware software utilizes the fea-\\ntures of Kubernetes and OpenShift to their full potential.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 32,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'xxx\\nabout the cover illustration\\nThe figure on the cover of Kubernetes in Action is a “Member of the Divan,” the Turkish\\nCouncil of State or governing body. The illustration is taken from a collection of cos-\\ntumes of the Ottoman Empire published on January 1, 1802, by William Miller of Old\\nBond Street, London. The title page is missing from the collection and we have been\\nunable to track it down to date. The book’s table of contents identifies the figures in\\nboth English and French, and each illustration bears the names of two artists who\\nworked on it, both of whom would no doubt be surprised to find their art gracing the\\nfront cover of a computer programming book ... 200 years later.\\n The collection was purchased by a Manning editor at an antiquarian flea market in\\nthe “Garage” on West 26th Street in Manhattan. The seller was an American based in\\nAnkara, Turkey, and the transaction took place just as he was packing up his stand for\\nthe day. The Manning editor didn’t have on his person the substantial amount of cash\\nthat was required for the purchase, and a credit card and check were both politely\\nturned down. With the seller flying back to Ankara that evening, the situation was get-\\nting hopeless. What was the solution? It turned out to be nothing more than an old-\\nfashioned verbal agreement sealed with a handshake. The seller proposed that the\\nmoney be transferred to him by wire, and the editor walked out with the bank infor-\\nmation on a piece of paper and the portfolio of images under his arm. Needless to say,\\nwe transferred the funds the next day, and we remain grateful and impressed by this\\nunknown person’s trust in one of us. It recalls something that might have happened a\\nlong time ago. We at Manning celebrate the inventiveness, the initiative, and, yes, the\\nfun of the computer business with book covers based on the rich diversity of regional\\nlife of two centuries ago‚ brought back to life by the pictures from this collection.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 33,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '1\\nIntroducing Kubernetes\\nYears ago, most software applications were big monoliths, running either as a single\\nprocess or as a small number of processes spread across a handful of servers. These\\nlegacy systems are still widespread today. They have slow release cycles and are\\nupdated relatively infrequently. At the end of every release cycle, developers pack-\\nage up the whole system and hand it over to the ops team, who then deploys and\\nmonitors it. In case of hardware failures, the ops team manually migrates it to the\\nremaining healthy servers. \\n Today, these big monolithic legacy applications are slowly being broken down\\ninto smaller, independently running components called microservices. Because\\nThis chapter covers\\n\\uf0a1Understanding how software development and \\ndeployment has changed over recent years\\n\\uf0a1Isolating applications and reducing environment \\ndifferences using containers\\n\\uf0a1Understanding how containers and Docker are \\nused by Kubernetes\\n\\uf0a1Making developers’ and sysadmins’ jobs easier \\nwith Kubernetes\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 34,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '2\\nCHAPTER 1\\nIntroducing Kubernetes\\nmicroservices are decoupled from each other, they can be developed, deployed, updated,\\nand scaled individually. This enables you to change components quickly and as often as\\nnecessary to keep up with today’s rapidly changing business requirements. \\n But with bigger numbers of deployable components and increasingly larger data-\\ncenters, it becomes increasingly difficult to configure, manage, and keep the whole\\nsystem running smoothly. It’s much harder to figure out where to put each of those\\ncomponents to achieve high resource utilization and thereby keep the hardware costs\\ndown. Doing all this manually is hard work. We need automation, which includes\\nautomatic scheduling of those components to our servers, automatic configuration,\\nsupervision, and failure-handling. This is where Kubernetes comes in.\\n Kubernetes enables developers to deploy their applications themselves and as\\noften as they want, without requiring any assistance from the operations (ops) team.\\nBut Kubernetes doesn’t benefit only developers. It also helps the ops team by automat-\\nically monitoring and rescheduling those apps in the event of a hardware failure. The\\nfocus for system administrators (sysadmins) shifts from supervising individual apps to\\nmostly supervising and managing Kubernetes and the rest of the infrastructure, while\\nKubernetes itself takes care of the apps. \\nNOTE\\nKubernetes is Greek for pilot or helmsman (the person holding the\\nship’s steering wheel). People pronounce Kubernetes in a few different ways.\\nMany pronounce it as Koo-ber-nay-tace, while others pronounce it more like\\nKoo-ber-netties. No matter which form you use, people will understand what\\nyou mean.\\nKubernetes abstracts away the hardware infrastructure and exposes your whole data-\\ncenter as a single enormous computational resource. It allows you to deploy and run\\nyour software components without having to know about the actual servers under-\\nneath. When deploying a multi-component application through Kubernetes, it selects\\na server for each component, deploys it, and enables it to easily find and communi-\\ncate with all the other components of your application. \\n This makes Kubernetes great for most on-premises datacenters, but where it starts\\nto shine is when it’s used in the largest datacenters, such as the ones built and oper-\\nated by cloud providers. Kubernetes allows them to offer developers a simple platform\\nfor deploying and running any type of application, while not requiring the cloud pro-\\nvider’s own sysadmins to know anything about the tens of thousands of apps running\\non their hardware. \\n With more and more big companies accepting the Kubernetes model as the best\\nway to run apps, it’s becoming the standard way of running distributed apps both in\\nthe cloud, as well as on local on-premises infrastructure. \\n1.1\\nUnderstanding the need for a system like Kubernetes \\nBefore you start getting to know Kubernetes in detail, let’s take a quick look at how\\nthe development and deployment of applications has changed in recent years. This\\nchange is both a consequence of splitting big monolithic apps into smaller microservices\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 35,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '3\\nUnderstanding the need for a system like Kubernetes\\nand of the changes in the infrastructure that runs those apps. Understanding these\\nchanges will help you better see the benefits of using Kubernetes and container tech-\\nnologies such as Docker.\\n1.1.1\\nMoving from monolithic apps to microservices\\nMonolithic applications consist of components that are all tightly coupled together and\\nhave to be developed, deployed, and managed as one entity, because they all run as a sin-\\ngle OS process. Changes to one part of the application require a redeployment of the\\nwhole application, and over time the lack of hard boundaries between the parts results\\nin the increase of complexity and consequential deterioration of the quality of the whole\\nsystem because of the unconstrained growth of inter-dependencies between these parts. \\n Running a monolithic application usually requires a small number of powerful\\nservers that can provide enough resources for running the application. To deal with\\nincreasing loads on the system, you then either have to vertically scale the servers (also\\nknown as scaling up) by adding more CPUs, memory, and other server components,\\nor scale the whole system horizontally, by setting up additional servers and running\\nmultiple copies (or replicas) of an application (scaling out). While scaling up usually\\ndoesn’t require any changes to the app, it gets expensive relatively quickly and in prac-\\ntice always has an upper limit. Scaling out, on the other hand, is relatively cheap hard-\\nware-wise, but may require big changes in the application code and isn’t always\\npossible—certain parts of an application are extremely hard or next to impossible to\\nscale horizontally (relational databases, for example). If any part of a monolithic\\napplication isn’t scalable, the whole application becomes unscalable, unless you can\\nsplit up the monolith somehow.\\nSPLITTING APPS INTO MICROSERVICES\\nThese and other problems have forced us to start splitting complex monolithic appli-\\ncations into smaller independently deployable components called microservices. Each\\nmicroservice runs as an independent process (see figure 1.1) and communicates with\\nother microservices through simple, well-defined interfaces (APIs).\\nServer 1\\nMonolithic application\\nSingle process\\nServer 1\\nProcess 1.1\\nProcess 1.2\\nMicroservices-based application\\nServer 2\\nProcess 2.1\\nProcess 2.2\\nFigure 1.1\\nComponents inside a monolithic application vs. standalone microservices\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Monolithic application Microservices-based application\\nServer 1 Server 1 Server 2\\nProcess 1.1 Process 2.1\\nSingle process Process 1.2 Process 2.2  \\\n",
       "   0                           Server 1\\nSingle process                                                                                                    \n",
       "   1                                               None                                                                                                    \n",
       "   2                                               None                                                                                                    \n",
       "   \n",
       "                                    Col1  Col2  \\\n",
       "   0  Server 1\\nProcess 1.1\\nProcess 1.2  None   \n",
       "   1                                None         \n",
       "   2                                None         \n",
       "   \n",
       "                                    Col3  \n",
       "   0  Server 2\\nProcess 2.1\\nProcess 2.2  \n",
       "   1                                None  \n",
       "   2                                None  ]},\n",
       " {'page': 36,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '4\\nCHAPTER 1\\nIntroducing Kubernetes\\nMicroservices communicate through synchronous protocols such as HTTP, over which\\nthey usually expose RESTful (REpresentational State Transfer) APIs, or through asyn-\\nchronous protocols such as AMQP (Advanced Message Queueing Protocol). These\\nprotocols are simple, well understood by most developers, and not tied to any specific\\nprogramming language. Each microservice can be written in the language that’s most\\nappropriate for implementing that specific microservice.\\n Because each microservice is a standalone process with a relatively static external\\nAPI, it’s possible to develop and deploy each microservice separately. A change to one\\nof them doesn’t require changes or redeployment of any other service, provided that\\nthe API doesn’t change or changes only in a backward-compatible way. \\nSCALING MICROSERVICES\\nScaling microservices, unlike monolithic systems, where you need to scale the system as\\na whole, is done on a per-service basis, which means you have the option of scaling only\\nthose services that require more resources, while leaving others at their original scale.\\nFigure 1.2 shows an example. Certain components are replicated and run as multiple\\nprocesses deployed on different servers, while others run as a single application process.\\nWhen a monolithic application can’t be scaled out because one of its parts is unscal-\\nable, splitting the app into microservices allows you to horizontally scale the parts that\\nallow scaling out, and scale the parts that don’t, vertically instead of horizontally.\\nServer 1\\nProcess 1.1\\nProcess 1.2\\nProcess 1.3\\nServer 2\\nProcess 2.1\\nProcess 2.2\\nServer 3\\nProcess 3.1\\nProcess 3.2\\nProcess 3.3\\nServer 4\\nProcess 4.1\\nProcess 4.2\\nProcess 2.3\\nSingle instance\\n(possibly not scalable)\\nThree instances of\\nthe same component\\nFigure 1.2\\nEach microservice can be scaled individually.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 37,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '5\\nUnderstanding the need for a system like Kubernetes\\nDEPLOYING MICROSERVICES\\nAs always, microservices also have drawbacks. When your system consists of only a\\nsmall number of deployable components, managing those components is easy. It’s\\ntrivial to decide where to deploy each component, because there aren’t that many\\nchoices. When the number of those components increases, deployment-related deci-\\nsions become increasingly difficult because not only does the number of deployment\\ncombinations increase, but the number of inter-dependencies between the compo-\\nnents increases by an even greater factor. \\n Microservices perform their work together as a team, so they need to find and talk\\nto each other. When deploying them, someone or something needs to configure all of\\nthem properly to enable them to work together as a single system. With increasing\\nnumbers of microservices, this becomes tedious and error-prone, especially when you\\nconsider what the ops/sysadmin teams need to do when a server fails. \\n Microservices also bring other problems, such as making it hard to debug and trace\\nexecution calls, because they span multiple processes and machines. Luckily, these\\nproblems are now being addressed with distributed tracing systems such as Zipkin. \\nUNDERSTANDING THE DIVERGENCE OF ENVIRONMENT REQUIREMENTS\\nAs I’ve already mentioned, components in a microservices architecture aren’t only\\ndeployed independently, but are also developed that way. Because of their indepen-\\ndence and the fact that it’s common to have separate teams developing each compo-\\nnent, nothing impedes each team from using different libraries and replacing them\\nwhenever the need arises. The divergence of dependencies between application com-\\nponents, like the one shown in figure 1.3, where applications require different ver-\\nsions of the same libraries, is inevitable.\\nServer running a monolithic app\\nMonolithic app\\nLibrary B\\nv2.4\\nLibrary C\\nv1.1\\nLibrary A\\nv1.0\\nLibrary Y\\nv3.2\\nLibrary X\\nv1.4\\nServer running multiple apps\\nLibrary B\\nv2.4\\nLibrary C\\nv1.1\\nLibrary C\\nv2.0\\nLibrary A\\nv1.0\\nLibrary A\\nv2.2\\nLibrary Y\\nv4.0\\nLibrary Y\\nv3.2\\nLibrary X\\nv2.3\\nLibrary X\\nv1.4\\nApp 1\\nApp 2\\nApp 3\\nApp 4\\nRequires libraries\\nRequires libraries\\nFigure 1.3\\nMultiple applications running on the same host may have conflicting dependencies.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Server running a monolithic app Server running multiple apps\\nMonolithic app App 1 App 2 App 3 App 4\\nRequires libraries\\nLibrary A\\nLibrary C\\nRequires libraries v1.0 v1.1\\nLibrary B\\nv2.4\\nLibrary A Library B Library C Library A Library C\\nv1.0 v2.4 v1.1 v2.2 v2.0\\nLibrary Y\\nLibrary X v3.2\\nv1.4\\nLibrary X Library Y Library X Library Y\\nv1.4 v3.2 v2.3 v4.0  \\\n",
       "   0  Monolithic app\\nRequires libraries\\nLibrary A ...                                                                                                                                                                                                                                                                                                                          \n",
       "   \n",
       "                                                   Col1  \n",
       "   0  App 1 App 2 App 3 App 4\\nRequires libraries\\nL...  ,\n",
       "     Library C\\nv2.0             Col1\n",
       "   0            None  Library Y\\nv3.2,\n",
       "   Empty DataFrame\n",
       "   Columns: [Library X\n",
       "   v1.4, Library Y\n",
       "   v3.2]\n",
       "   Index: []]},\n",
       " {'page': 38,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '6\\nCHAPTER 1\\nIntroducing Kubernetes\\nDeploying dynamically linked applications that require different versions of shared\\nlibraries, and/or require other environment specifics, can quickly become a night-\\nmare for the ops team who deploys and manages them on production servers. The\\nbigger the number of components you need to deploy on the same host, the harder it\\nwill be to manage all their dependencies to satisfy all their requirements. \\n1.1.2\\nProviding a consistent environment to applications\\nRegardless of how many individual components you’re developing and deploying,\\none of the biggest problems that developers and operations teams always have to deal\\nwith is the differences in the environments they run their apps in. Not only is there a\\nhuge difference between development and production environments, differences\\neven exist between individual production machines. Another unavoidable fact is that\\nthe environment of a single production machine will change over time. \\n These differences range from hardware to the operating system to the libraries\\nthat are available on each machine. Production environments are managed by the\\noperations team, while developers often take care of their development laptops on\\ntheir own. The difference is how much these two groups of people know about sys-\\ntem administration, and this understandably leads to relatively big differences\\nbetween those two systems, not to mention that system administrators give much more\\nemphasis on keeping the system up to date with the latest security patches, while a lot\\nof developers don’t care about that as much. \\n Also, production systems can run applications from multiple developers or devel-\\nopment teams, which isn’t necessarily true for developers’ computers. A production\\nsystem must provide the proper environment to all applications it hosts, even though\\nthey may require different, even conflicting, versions of libraries.\\n To reduce the number of problems that only show up in production, it would be\\nideal if applications could run in the exact same environment during development\\nand in production so they have the exact same operating system, libraries, system con-\\nfiguration, networking environment, and everything else. You also don’t want this\\nenvironment to change too much over time, if at all. Also, if possible, you want the\\nability to add applications to the same server without affecting any of the existing\\napplications on that server. \\n1.1.3\\nMoving to continuous delivery: DevOps and NoOps\\nIn the last few years, we’ve also seen a shift in the whole application development pro-\\ncess and how applications are taken care of in production. In the past, the develop-\\nment team’s job was to create the application and hand it off to the operations team,\\nwho then deployed it, tended to it, and kept it running. But now, organizations are\\nrealizing it’s better to have the same team that develops the application also take part\\nin deploying it and taking care of it over its whole lifetime. This means the developer,\\nQA, and operations teams now need to collaborate throughout the whole process.\\nThis practice is called DevOps.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 39,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '7\\nIntroducing container technologies\\nUNDERSTANDING THE BENEFITS\\nHaving the developers more involved in running the application in production leads\\nto them having a better understanding of both the users’ needs and issues and the\\nproblems faced by the ops team while maintaining the app. Application developers\\nare now also much more inclined to give users the app earlier and then use their feed-\\nback to steer further development of the app. \\n To release newer versions of applications more often, you need to streamline the\\ndeployment process. Ideally, you want developers to deploy the applications them-\\nselves without having to wait for the ops people. But deploying an application often\\nrequires an understanding of the underlying infrastructure and the organization of\\nthe hardware in the datacenter. Developers don’t always know those details and, most\\nof the time, don’t even want to know about them. \\nLETTING DEVELOPERS AND SYSADMINS DO WHAT THEY DO BEST\\nEven though developers and system administrators both work toward achieving the\\nsame goal of running a successful software application as a service to its customers, they\\nhave different individual goals and motivating factors. Developers love creating new fea-\\ntures and improving the user experience. They don’t normally want to be the ones mak-\\ning sure that the underlying operating system is up to date with all the security patches\\nand things like that. They prefer to leave that up to the system administrators. \\n The ops team is in charge of the production deployments and the hardware infra-\\nstructure they run on. They care about system security, utilization, and other aspects\\nthat aren’t a high priority for developers. The ops people don’t want to deal with the\\nimplicit interdependencies of all the application components and don’t want to think\\nabout how changes to either the underlying operating system or the infrastructure\\ncan affect the operation of the application as a whole, but they must.\\n Ideally, you want the developers to deploy applications themselves without know-\\ning anything about the hardware infrastructure and without dealing with the ops\\nteam. This is referred to as NoOps. Obviously, you still need someone to take care of\\nthe hardware infrastructure, but ideally, without having to deal with peculiarities of\\neach application running on it. \\n As you’ll see, Kubernetes enables us to achieve all of this. By abstracting away the\\nactual hardware and exposing it as a single platform for deploying and running apps,\\nit allows developers to configure and deploy their applications without any help from\\nthe sysadmins and allows the sysadmins to focus on keeping the underlying infrastruc-\\nture up and running, while not having to know anything about the actual applications\\nrunning on top of it.\\n1.2\\nIntroducing container technologies\\nIn section 1.1 I presented a non-comprehensive list of problems facing today’s devel-\\nopment and ops teams. While you have many ways of dealing with them, this book will\\nfocus on how they’re solved with Kubernetes. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 40,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '8\\nCHAPTER 1\\nIntroducing Kubernetes\\n Kubernetes uses Linux container technologies to provide isolation of running\\napplications, so before we dig into Kubernetes itself, you need to become familiar\\nwith the basics of containers to understand what Kubernetes does itself, and what it\\noffloads to container technologies like Docker or rkt (pronounced “rock-it”).\\n1.2.1\\nUnderstanding what containers are\\nIn section 1.1.1 we saw how different software components running on the same\\nmachine will require different, possibly conflicting, versions of dependent libraries or\\nhave other different environment requirements in general. \\n When an application is composed of only smaller numbers of large components,\\nit’s completely acceptable to give a dedicated Virtual Machine (VM) to each compo-\\nnent and isolate their environments by providing each of them with their own operat-\\ning system instance. But when these components start getting smaller and their\\nnumbers start to grow, you can’t give each of them their own VM if you don’t want to\\nwaste hardware resources and keep your hardware costs down. But it’s not only about\\nwasting hardware resources. Because each VM usually needs to be configured and\\nmanaged individually, rising numbers of VMs also lead to wasting human resources,\\nbecause they increase the system administrators’ workload considerably.\\nISOLATING COMPONENTS WITH LINUX CONTAINER TECHNOLOGIES\\nInstead of using virtual machines to isolate the environments of each microservice (or\\nsoftware processes in general), developers are turning to Linux container technolo-\\ngies. They allow you to run multiple services on the same host machine, while not only\\nexposing a different environment to each of them, but also isolating them from each\\nother, similarly to VMs, but with much less overhead.\\n A process running in a container runs inside the host’s operating system, like all\\nthe other processes (unlike VMs, where processes run in separate operating sys-\\ntems). But the process in the container is still isolated from other processes. To the\\nprocess itself, it looks like it’s the only one running on the machine and in its oper-\\nating system. \\nCOMPARING VIRTUAL MACHINES TO CONTAINERS\\nCompared to VMs, containers are much more lightweight, which allows you to run\\nhigher numbers of software components on the same hardware, mainly because each\\nVM needs to run its own set of system processes, which requires additional compute\\nresources in addition to those consumed by the component’s own process. A con-\\ntainer, on the other hand, is nothing more than a single isolated process running in\\nthe host OS, consuming only the resources that the app consumes and without the\\noverhead of any additional processes. \\n Because of the overhead of VMs, you often end up grouping multiple applications\\ninto each VM because you don’t have enough resources to dedicate a whole VM to\\neach app. When using containers, you can (and should) have one container for each\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 41,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '9\\nIntroducing container technologies\\napplication, as shown in figure 1.4. The end-result is that you can fit many more appli-\\ncations on the same bare-metal machine.\\nWhen you run three VMs on a host, you have three completely separate operating sys-\\ntems running on and sharing the same bare-metal hardware. Underneath those VMs\\nis the host’s OS and a hypervisor, which divides the physical hardware resources into\\nsmaller sets of virtual resources that can be used by the operating system inside each\\nVM. Applications running inside those VMs perform system calls to the guest OS’ ker-\\nnel in the VM, and the kernel then performs x86 instructions on the host’s physical\\nCPU through the hypervisor. \\nNOTE\\nTwo types of hypervisors exist. Type 1 hypervisors don’t use a host OS,\\nwhile Type 2 do.\\nContainers, on the other hand, all perform system calls on the exact same kernel run-\\nning in the host OS. This single kernel is the only one performing x86 instructions on\\nthe host’s CPU. The CPU doesn’t need to do any kind of virtualization the way it does\\nwith VMs (see figure 1.5).\\n The main benefit of virtual machines is the full isolation they provide, because\\neach VM runs its own Linux kernel, while containers all call out to the same kernel,\\nwhich can clearly pose a security risk. If you have a limited amount of hardware\\nresources, VMs may only be an option when you have a small number of processes that\\nApps running in three VMs\\n(on a single machine)\\nBare-metal machine\\nVM 1\\nVM 2\\nVM 3\\nApp A\\nApp B\\nApp C\\nApp D\\nApp E\\nApp F\\nGuest OS\\nGuest OS\\nGuest OS\\nBare-metal machine\\nHost OS\\nHypervisor\\nApps running in\\nisolated containers\\nContainer 1\\nContainer 2\\nContainer 3\\nApp A\\nApp B\\nApp C\\nContainer 4\\nContainer 5\\nContainer 6\\nApp D\\nApp E\\nApp F\\nContainer 7\\nContainer 8\\nContainer 9\\nApp ...\\nApp ...\\nApp ...\\nHost OS\\nFigure 1.4\\nUsing VMs to isolate groups of applications vs. isolating individual apps with containers\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Apps running in three VMs Apps running in\\n(on a single machine) isolated containers\\nVM 1 VM 2 VM 3 Container 1 Container 2 Container 3\\nApp A App C App E App A App B App C\\nApp B App D App F Container 4 Container 5 Container 6\\nApp D App E App F\\nGuest OS Guest OS Guest OS\\nContainer 7 Container 8 Container 9\\nApp ... App ... App ...\\nHypervisor\\nHost OS Host OS\\nBare-metal machine Bare-metal machine  \\\n",
       "   0  VM 1 VM 2 VM 3\\nApp A App C App E\\nApp B App D...                                                                                                                                                                                                                                                                                                                                                                      \n",
       "   \n",
       "                                                   Col1  \n",
       "   0  Container 1 Container 2 Container 3\\nApp A App...  ]},\n",
       " {'page': 42,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '10\\nCHAPTER 1\\nIntroducing Kubernetes\\nyou want to isolate. To run greater numbers of isolated processes on the same\\nmachine, containers are a much better choice because of their low overhead. Remem-\\nber, each VM runs its own set of system services, while containers don’t, because they\\nall run in the same OS. That also means that to run a container, nothing needs to be\\nbooted up, as is the case in VMs. A process run in a container starts up immediately.\\nApps running in multiple VMs\\nVM 1\\nApp\\nA\\nApp\\nB\\nKernel\\nVirtual CPU\\nHypervisor\\nPhysical CPU\\nKernel\\nPhysical CPU\\nVM 2\\nApp\\nD\\nKernel\\nVirtual CPU\\nApp\\nC\\nApp\\nE\\nVM 3\\nApp\\nF\\nKernel\\nVirtual CPU\\nApps running in isolated containers\\nContainer\\nA\\nContainer\\nB\\nContainer\\nC\\nContainer\\nD\\nContainer\\nE\\nContainer\\nF\\nApp\\nA\\nApp\\nB\\nApp\\nD\\nApp\\nE\\nApp\\nF\\nApp\\nC\\nFigure 1.5\\nThe difference between \\nhow apps in VMs use the CPU vs. how \\nthey use them in containers\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [          VM 1    Col1         VM 2    Col3    Col4 VM 3\\nApp\\nF\n",
       "   0       App\\nA  App\\nB       App\\nC  App\\nD  App\\nE       App\\nF\n",
       "   1                 None                 None    None         None\n",
       "   2       Kernel    None       Kernel    None    None       Kernel\n",
       "   3                 None                 None    None             \n",
       "   4  Virtual CPU    None  Virtual CPU    None    None  Virtual CPU\n",
       "   5   Hypervisor    None         None    None    None         None,\n",
       "   Empty DataFrame\n",
       "   Columns: [App\n",
       "   A, App\n",
       "   B]\n",
       "   Index: []]},\n",
       " {'page': 43,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '11\\nIntroducing container technologies\\nINTRODUCING THE MECHANISMS THAT MAKE CONTAINER ISOLATION POSSIBLE \\nBy this point, you’re probably wondering how exactly containers can isolate processes\\nif they’re running on the same operating system. Two mechanisms make this possible.\\nThe first one, Linux Namespaces, makes sure each process sees its own personal view of\\nthe system (files, processes, network interfaces, hostname, and so on). The second\\none is Linux Control Groups (cgroups), which limit the amount of resources the process\\ncan consume (CPU, memory, network bandwidth, and so on).\\nISOLATING PROCESSES WITH LINUX NAMESPACES\\nBy default, each Linux system initially has one single namespace. All system resources,\\nsuch as filesystems, process IDs, user IDs, network interfaces, and others, belong to the\\nsingle namespace. But you can create additional namespaces and organize resources\\nacross them. When running a process, you run it inside one of those namespaces. The\\nprocess will only see resources that are inside the same namespace. Well, multiple\\nkinds of namespaces exist, so a process doesn’t belong to one namespace, but to one\\nnamespace of each kind. \\n The following kinds of namespaces exist:\\n\\uf0a1Mount (mnt)\\n\\uf0a1Process ID (pid)\\n\\uf0a1Network (net)\\n\\uf0a1Inter-process communication (ipc)\\n\\uf0a1UTS\\n\\uf0a1User ID (user)\\nEach namespace kind is used to isolate a certain group of resources. For example, the\\nUTS namespace determines what hostname and domain name the process running\\ninside that namespace sees. By assigning two different UTS namespaces to a pair of\\nprocesses, you can make them see different local hostnames. In other words, to the\\ntwo processes, it will appear as though they are running on two different machines (at\\nleast as far as the hostname is concerned). \\n Likewise, what Network namespace a process belongs to determines which net-\\nwork interfaces the application running inside the process sees. Each network inter-\\nface belongs to exactly one namespace, but can be moved from one namespace to\\nanother. Each container uses its own Network namespace, and therefore each con-\\ntainer sees its own set of network interfaces.\\n This should give you a basic idea of how namespaces are used to isolate applica-\\ntions running in containers from each other. \\nLIMITING RESOURCES AVAILABLE TO A PROCESS\\nThe other half of container isolation deals with limiting the amount of system\\nresources a container can consume. This is achieved with cgroups, a Linux kernel fea-\\nture that limits the resource usage of a process (or a group of processes). A process\\ncan’t use more than the configured amount of CPU, memory, network bandwidth,\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 44,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '12\\nCHAPTER 1\\nIntroducing Kubernetes\\nand so on. This way, processes cannot hog resources reserved for other processes,\\nwhich is similar to when each process runs on a separate machine.\\n1.2.2\\nIntroducing the Docker container platform\\nWhile container technologies have been around for a long time, they’ve become\\nmore widely known with the rise of the Docker container platform. Docker was the\\nfirst container system that made containers easily portable across different machines.\\nIt simplified the process of packaging up not only the application but also all its\\nlibraries and other dependencies, even the whole OS file system, into a simple, por-\\ntable package that can be used to provision the application to any other machine\\nrunning Docker. \\n When you run an application packaged with Docker, it sees the exact filesystem\\ncontents that you’ve bundled with it. It sees the same files whether it’s running on\\nyour development machine or a production machine, even if it the production server\\nis running a completely different Linux OS. The application won’t see anything from\\nthe server it’s running on, so it doesn’t matter if the server has a completely different\\nset of installed libraries compared to your development machine. \\n For example, if you’ve packaged up your application with the files of the whole\\nRed Hat Enterprise Linux (RHEL) operating system, the application will believe it’s\\nrunning inside RHEL, both when you run it on your development computer that runs\\nFedora and when you run it on a server running Debian or some other Linux distribu-\\ntion. Only the kernel may be different.\\n This is similar to creating a VM image by installing an operating system into a VM,\\ninstalling the app inside it, and then distributing the whole VM image around and\\nrunning it. Docker achieves the same effect, but instead of using VMs to achieve app\\nisolation, it uses Linux container technologies mentioned in the previous section to\\nprovide (almost) the same level of isolation that VMs do. Instead of using big mono-\\nlithic VM images, it uses container images, which are usually smaller.\\n A big difference between Docker-based container images and VM images is that\\ncontainer images are composed of layers, which can be shared and reused across mul-\\ntiple images. This means only certain layers of an image need to be downloaded if the\\nother layers were already downloaded previously when running a different container\\nimage that also contains the same layers.\\nUNDERSTANDING DOCKER CONCEPTS\\nDocker is a platform for packaging, distributing, and running applications. As we’ve\\nalready stated, it allows you to package your application together with its whole envi-\\nronment. This can be either a few libraries that the app requires or even all the files\\nthat are usually available on the filesystem of an installed operating system. Docker\\nmakes it possible to transfer this package to a central repository from which it can\\nthen be transferred to any computer running Docker and executed there (for the\\nmost part, but not always, as we’ll soon explain).\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 45,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '13\\nIntroducing container technologies\\n Three main concepts in Docker comprise this scenario:\\n\\uf0a1Images—A Docker-based container image is something you package your appli-\\ncation and its environment into. It contains the filesystem that will be available\\nto the application and other metadata, such as the path to the executable that\\nshould be executed when the image is run. \\n\\uf0a1Registries—A Docker Registry is a repository that stores your Docker images and\\nfacilitates easy sharing of those images between different people and comput-\\ners. When you build your image, you can either run it on the computer you’ve\\nbuilt it on, or you can push (upload) the image to a registry and then pull\\n(download) it on another computer and run it there. Certain registries are pub-\\nlic, allowing anyone to pull images from it, while others are private, only accessi-\\nble to certain people or machines.\\n\\uf0a1Containers—A Docker-based container is a regular Linux container created from\\na Docker-based container image. A running container is a process running on\\nthe host running Docker, but it’s completely isolated from both the host and all\\nother processes running on it. The process is also resource-constrained, mean-\\ning it can only access and use the amount of resources (CPU, RAM, and so on)\\nthat are allocated to it. \\nBUILDING, DISTRIBUTING, AND RUNNING A DOCKER IMAGE\\nFigure 1.6 shows all three concepts and how they relate to each other. The developer\\nfirst builds an image and then pushes it to a registry. The image is thus available to\\nanyone who can access the registry. They can then pull the image to any other\\nmachine running Docker and run the image. Docker creates an isolated container\\nbased on the image and runs the binary executable specified as part of the image.\\nDocker\\nImage\\nContainer\\nImage registry\\nImage\\nDocker\\nImage\\nDevelopment machine\\nProduction machine\\n1. Developer tells\\nDocker to build\\nand push image\\n2. Docker\\nbuilds image\\n4. Developer tells\\nDocker on production\\nmachine to run image\\n3. Docker\\npushes image\\nto registry\\n5. Docker pulls\\nimage from\\nregistry\\n6. Docker runs\\ncontainer from\\nimage\\nDeveloper\\nFigure 1.6\\nDocker images, registries, and containers\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 46,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '14\\nCHAPTER 1\\nIntroducing Kubernetes\\nCOMPARING VIRTUAL MACHINES AND DOCKER CONTAINERS\\nI’ve explained how Linux containers are generally like virtual machines, but much\\nmore lightweight. Now let’s look at how Docker containers specifically compare to vir-\\ntual machines (and how Docker images compare to VM images). Figure 1.7 again shows\\nthe same six applications running both in VMs and as Docker containers.\\nYou’ll notice that apps A and B have access to the same binaries and libraries both\\nwhen running in a VM and when running as two separate containers. In the VM, this\\nis obvious, because both apps see the same filesystem (that of the VM). But we said\\nHost running multiple VMs\\nVM 1\\nApp\\nA\\nApp\\nB\\nBinaries and\\nlibraries\\n(Filesystem)\\nGuest OS kernel\\nHypervisor\\nHost OS\\nHost OS\\nVM 2\\nApp\\nD\\nGuest OS kernel\\nApp\\nC\\nApp\\nE\\nVM 3\\nApp\\nF\\nGuest OS kernel\\nHost running multiple Docker containers\\nContainer 1\\nContainer 2\\nContainer 3\\nContainer 4\\nContainer 5\\nContainer 6\\nApp\\nD\\nApp\\nE\\nApp\\nF\\nApp\\nC\\nApp\\nA\\nApp\\nB\\nBinaries and\\nlibraries\\n(Filesystem)\\nBinaries and\\nlibraries\\n(Filesystem)\\nBinaries and\\nlibraries\\n(Filesystem)\\nBinaries and\\nlibraries\\n(Filesystem)\\nBinaries and\\nlibraries\\n(Filesystem)\\nDocker\\nFigure 1.7\\nRunning six apps on \\nthree VMs vs. running them in \\nDocker containers\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [                                    VM 1    Col1  \\\n",
       "   0                                 App\\nA  App\\nB   \n",
       "   1  Binaries and\\nlibraries\\n(Filesystem)    None   \n",
       "   2                        Guest OS kernel    None   \n",
       "   3                             Hypervisor    None   \n",
       "   4                                Host OS    None   \n",
       "   \n",
       "                               VM 3\\nApp\\nF  \n",
       "   0                                 App\\nF  \n",
       "   1  Binaries and\\nlibraries\\n(Filesystem)  \n",
       "   2                        Guest OS kernel  \n",
       "   3                                   None  \n",
       "   4                                   None  ,\n",
       "                                       VM 2    Col1    Col2\n",
       "   0                                 App\\nC  App\\nD  App\\nE\n",
       "   1  Binaries and\\nlibraries\\n(Filesystem)    None    None\n",
       "   2                        Guest OS kernel    None    None,\n",
       "                                Container 1 Container 2  \\\n",
       "   0                                 App\\nA      App\\nB   \n",
       "   1                                   None        None   \n",
       "   2  Binaries and\\nlibraries\\n(Filesystem)        None   \n",
       "   3                                Host OS        None   \n",
       "   \n",
       "                                Container 3 Container 4 Container 5  \\\n",
       "   0                                 App\\nC      App\\nD      App\\nE   \n",
       "   1                                   None        None        None   \n",
       "   2  Binaries and\\nlibraries\\n(Filesystem)        None        None   \n",
       "   3                                   None        None        None   \n",
       "   \n",
       "                                Container 6    Col6  \n",
       "   0                                 App\\nF    None  \n",
       "   1                                   None  Docker  \n",
       "   2  Binaries and\\nlibraries\\n(Filesystem)    None  \n",
       "   3                                   None    None  ]},\n",
       " {'page': 47,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '15\\nIntroducing container technologies\\nthat each container has its own isolated filesystem. How can both app A and app B\\nshare the same files?\\nUNDERSTANDING IMAGE LAYERS\\nI’ve already said that Docker images are composed of layers. Different images can con-\\ntain the exact same layers because every Docker image is built on top of another\\nimage and two different images can both use the same parent image as their base.\\nThis speeds up the distribution of images across the network, because layers that have\\nalready been transferred as part of the first image don’t need to be transferred again\\nwhen transferring the other image. \\n But layers don’t only make distribution more efficient, they also help reduce the\\nstorage footprint of images. Each layer is only stored once. Two containers created\\nfrom two images based on the same base layers can therefore read the same files, but\\nif one of them writes over those files, the other one doesn’t see those changes. There-\\nfore, even if they share files, they’re still isolated from each other. This works because\\ncontainer image layers are read-only. When a container is run, a new writable layer is\\ncreated on top of the layers in the image. When the process in the container writes to\\na file located in one of the underlying layers, a copy of the whole file is created in the\\ntop-most layer and the process writes to the copy. \\nUNDERSTANDING THE PORTABILITY LIMITATIONS OF CONTAINER IMAGES\\nIn theory, a container image can be run on any Linux machine running Docker, but\\none small caveat exists—one related to the fact that all containers running on a host use\\nthe host’s Linux kernel. If a containerized application requires a specific kernel version,\\nit may not work on every machine. If a machine runs a different version of the Linux\\nkernel or doesn’t have the same kernel modules available, the app can’t run on it.\\n While containers are much more lightweight compared to VMs, they impose cer-\\ntain constraints on the apps running inside them. VMs have no such constraints,\\nbecause each VM runs its own kernel. \\n And it’s not only about the kernel. It should also be clear that a containerized app\\nbuilt for a specific hardware architecture can only run on other machines that have\\nthe same architecture. You can’t containerize an application built for the x86 architec-\\nture and expect it to run on an ARM-based machine because it also runs Docker. You\\nstill need a VM for that.\\n1.2.3\\nIntroducing rkt—an alternative to Docker\\nDocker was the first container platform that made containers mainstream. I hope I’ve\\nmade it clear that Docker itself doesn’t provide process isolation. The actual isolation\\nof containers is done at the Linux kernel level using kernel features such as Linux\\nNamespaces and cgroups. Docker only makes it easy to use those features.\\n After the success of Docker, the Open Container Initiative (OCI) was born to cre-\\nate open industry standards around container formats and runtime. Docker is part\\nof that initiative, as is rkt (pronounced “rock-it”), which is another Linux container\\nengine. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 48,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '16\\nCHAPTER 1\\nIntroducing Kubernetes\\n Like Docker, rkt is a platform for running containers. It puts a strong emphasis on\\nsecurity, composability, and conforming to open standards. It uses the OCI container\\nimage format and can even run regular Docker container images. \\n This book focuses on using Docker as the container runtime for Kubernetes,\\nbecause it was initially the only one supported by Kubernetes. Recently, Kubernetes\\nhas also started supporting rkt, as well as others, as the container runtime. \\n The reason I mention rkt at this point is so you don’t make the mistake of thinking\\nKubernetes is a container orchestration system made specifically for Docker-based\\ncontainers. In fact, over the course of this book, you’ll realize that the essence of\\nKubernetes isn’t orchestrating containers. It’s much more. Containers happen to be\\nthe best way to run apps on different cluster nodes. With that in mind, let’s finally dive\\ninto the core of what this book is all about—Kubernetes.\\n1.3\\nIntroducing Kubernetes\\nWe’ve already shown that as the number of deployable application components in\\nyour system grows, it becomes harder to manage them all. Google was probably the\\nfirst company that realized it needed a much better way of deploying and managing\\ntheir software components and their infrastructure to scale globally. It’s one of only a\\nfew companies in the world that runs hundreds of thousands of servers and has had to\\ndeal with managing deployments on such a massive scale. This has forced them to\\ndevelop solutions for making the development and deployment of thousands of soft-\\nware components manageable and cost-efficient.\\n1.3.1\\nUnderstanding its origins\\nThrough the years, Google developed an internal system called Borg (and later a new\\nsystem called Omega), that helped both application developers and system administra-\\ntors manage those thousands of applications and services. In addition to simplifying\\nthe development and management, it also helped them achieve a much higher utiliza-\\ntion of their infrastructure, which is important when your organization is that large.\\nWhen you run hundreds of thousands of machines, even tiny improvements in utiliza-\\ntion mean savings in the millions of dollars, so the incentives for developing such a\\nsystem are clear.\\n After having kept Borg and Omega secret for a whole decade, in 2014 Google\\nintroduced Kubernetes, an open-source system based on the experience gained\\nthrough Borg, Omega, and other internal Google systems. \\n1.3.2\\nLooking at Kubernetes from the top of a mountain\\nKubernetes is a software system that allows you to easily deploy and manage container-\\nized applications on top of it. It relies on the features of Linux containers to run het-\\nerogeneous applications without having to know any internal details of these\\napplications and without having to manually deploy these applications on each host.\\nBecause these apps run in containers, they don’t affect other apps running on the\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 49,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '17\\nIntroducing Kubernetes\\nsame server, which is critical when you run applications for completely different orga-\\nnizations on the same hardware. This is of paramount importance for cloud provid-\\ners, because they strive for the best possible utilization of their hardware while still\\nhaving to maintain complete isolation of hosted applications.\\n Kubernetes enables you to run your software applications on thousands of com-\\nputer nodes as if all those nodes were a single, enormous computer. It abstracts away\\nthe underlying infrastructure and, by doing so, simplifies development, deployment,\\nand management for both development and the operations teams. \\n Deploying applications through Kubernetes is always the same, whether your clus-\\nter contains only a couple of nodes or thousands of them. The size of the cluster\\nmakes no difference at all. Additional cluster nodes simply represent an additional\\namount of resources available to deployed apps.\\nUNDERSTANDING THE CORE OF WHAT KUBERNETES DOES\\nFigure 1.8 shows the simplest possible view of a Kubernetes system. The system is com-\\nposed of a master node and any number of worker nodes. When the developer sub-\\nmits a list of apps to the master, Kubernetes deploys them to the cluster of worker\\nnodes. What node a component lands on doesn’t (and shouldn’t) matter—neither to\\nthe developer nor to the system administrator.\\nThe developer can specify that certain apps must run together and Kubernetes will\\ndeploy them on the same worker node. Others will be spread around the cluster, but\\nthey can talk to each other in the same way, regardless of where they’re deployed.\\nHELPING DEVELOPERS FOCUS ON THE CORE APP FEATURES\\nKubernetes can be thought of as an operating system for the cluster. It relieves appli-\\ncation developers from having to implement certain infrastructure-related services\\ninto their apps; instead they rely on Kubernetes to provide these services. This includes\\nthings such as service discovery, scaling, load-balancing, self-healing, and even leader\\nKubernetes\\nmaster\\nTens or thousands of worker nodes exposed\\nas a single deployment platform\\n1x\\nApp descriptor\\n5x\\n2x\\nDeveloper\\nFigure 1.8\\nKubernetes exposes the whole datacenter as a single deployment platform.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 50,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '18\\nCHAPTER 1\\nIntroducing Kubernetes\\nelection. Application developers can therefore focus on implementing the actual fea-\\ntures of the applications and not waste time figuring out how to integrate them with\\nthe infrastructure.\\nHELPING OPS TEAMS ACHIEVE BETTER RESOURCE UTILIZATION\\nKubernetes will run your containerized app somewhere in the cluster, provide infor-\\nmation to its components on how to find each other, and keep all of them running.\\nBecause your application doesn’t care which node it’s running on, Kubernetes can\\nrelocate the app at any time, and by mixing and matching apps, achieve far better\\nresource utilization than is possible with manual scheduling.\\n1.3.3\\nUnderstanding the architecture of a Kubernetes cluster\\nWe’ve seen a bird’s-eye view of Kubernetes’ architecture. Now let’s take a closer look at\\nwhat a Kubernetes cluster is composed of. At the hardware level, a Kubernetes cluster\\nis composed of many nodes, which can be split into two types: \\n\\uf0a1The master node, which hosts the Kubernetes Control Plane that controls and man-\\nages the whole Kubernetes system\\n\\uf0a1Worker nodes that run the actual applications you deploy\\nFigure 1.9 shows the components running on these two sets of nodes. I’ll explain\\nthem next.\\nTHE CONTROL PLANE\\nThe Control Plane is what controls the cluster and makes it function. It consists of\\nmultiple components that can run on a single master node or be split across multiple\\nnodes and replicated to ensure high availability. These components are\\n\\uf0a1The Kubernetes API Server, which you and the other Control Plane components\\ncommunicate with\\nControl Plane (master)\\netcd\\nAPI server\\nkube-proxy\\nWorker node(s)\\nKubelet\\nContainer Runtime\\nScheduler\\nController\\nManager\\nFigure 1.9\\nThe components that make up a Kubernetes cluster\\n \\nwww.allitebooks.com\\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [                                                Col0  Col1  \\\n",
       "   0  Worker node(s)\\nKubelet kube-proxy\\nContainer ...         \n",
       "   1                                               None  None   \n",
       "   2                                               None  None   \n",
       "   \n",
       "     Worker node(s)\\nKubelet kube-proxy\\nContainer Runtime  Col3  Col4  \n",
       "   0  Worker node(s)\\nKubelet kube-proxy\\nContainer ...     None  None  \n",
       "   1                                  Container Runtime     None  None  \n",
       "   2                                               None           None  ]},\n",
       " {'page': 51,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '19\\nIntroducing Kubernetes\\n\\uf0a1The Scheduler, which schedules your apps (assigns a worker node to each deploy-\\nable component of your application) \\n\\uf0a1The Controller Manager, which performs cluster-level functions, such as repli-\\ncating components, keeping track of worker nodes, handling node failures,\\nand so on\\n\\uf0a1etcd, a reliable distributed data store that persistently stores the cluster\\nconfiguration.\\nThe components of the Control Plane hold and control the state of the cluster, but\\nthey don’t run your applications. This is done by the (worker) nodes.\\nTHE NODES\\nThe worker nodes are the machines that run your containerized applications. The\\ntask of running, monitoring, and providing services to your applications is done by\\nthe following components:\\n\\uf0a1Docker, rkt, or another container runtime, which runs your containers\\n\\uf0a1The Kubelet, which talks to the API server and manages containers on its node\\n\\uf0a1The Kubernetes Service Proxy (kube-proxy), which load-balances network traffic\\nbetween application components\\nWe’ll explain all these components in detail in chapter 11. I’m not a fan of explaining\\nhow things work before first explaining what something does and teaching people to\\nuse it. It’s like learning to drive a car. You don’t want to know what’s under the hood.\\nYou first want to learn how to drive it from point A to point B. Only after you learn\\nhow to do that do you become interested in how a car makes that possible. After all,\\nknowing what’s under the hood may someday help you get the car moving again after\\nit breaks down and leaves you stranded at the side of the road.\\n1.3.4\\nRunning an application in Kubernetes\\nTo run an application in Kubernetes, you first need to package it up into one or more\\ncontainer images, push those images to an image registry, and then post a description\\nof your app to the Kubernetes API server. \\n The description includes information such as the container image or images that\\ncontain your application components, how those components are related to each\\nother, and which ones need to be run co-located (together on the same node) and\\nwhich don’t. For each component, you can also specify how many copies (or replicas)\\nyou want to run. Additionally, the description also includes which of those compo-\\nnents provide a service to either internal or external clients and should be exposed\\nthrough a single IP address and made discoverable to the other components. \\nUNDERSTANDING HOW THE DESCRIPTION RESULTS IN A RUNNING CONTAINER\\nWhen the API server processes your app’s description, the Scheduler schedules the\\nspecified groups of containers onto the available worker nodes based on computa-\\ntional resources required by each group and the unallocated resources on each node\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 52,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '20\\nCHAPTER 1\\nIntroducing Kubernetes\\nat that moment. The Kubelet on those nodes then instructs the Container Runtime\\n(Docker, for example) to pull the required container images and run the containers. \\n Examine figure 1.10 to gain a better understanding of how applications are\\ndeployed in Kubernetes. The app descriptor lists four containers, grouped into three\\nsets (these sets are called pods; we’ll explain what they are in chapter 3). The first two\\npods each contain only a single container, whereas the last one contains two. That\\nmeans both containers need to run co-located and shouldn’t be isolated from each\\nother. Next to each pod, you also see a number representing the number of replicas\\nof each pod that need to run in parallel. After submitting the descriptor to Kuberne-\\ntes, it will schedule the specified number of replicas of each pod to the available\\nworker nodes. The Kubelets on the nodes will then tell Docker to pull the container\\nimages from the image registry and run the containers.\\nKEEPING THE CONTAINERS RUNNING\\nOnce the application is running, Kubernetes continuously makes sure that the deployed\\nstate of the application always matches the description you provided. For example, if\\n1x\\nApp descriptor\\nLegend:\\nContainer image\\nMultiple containers\\nrunning “together”\\n(not fully isolated)\\n5x\\n2x\\nControl Plane\\n(master)\\nImage registry\\nWorker nodes\\n...\\nkube-proxy\\nDocker\\nKubelet\\nkube-proxy\\nDocker\\nKubelet\\nContainer\\n...\\nkube-proxy\\nDocker\\nKubelet\\nkube-proxy\\nDocker\\nKubelet\\n...\\nkube-proxy\\nDocker\\nKubelet\\nkube-proxy\\nDocker\\nKubelet\\nFigure 1.10\\nA basic overview of the Kubernetes architecture and an application running on top of it\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Kubelet, kube-proxy]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [Kubelet, kube-proxy]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [Kubelet, kube-proxy]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [Kubelet, kube-proxy]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [Kubelet, kube-proxy]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [Kubelet, kube-proxy]\n",
       "   Index: []]},\n",
       " {'page': 53,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '21\\nIntroducing Kubernetes\\nyou specify that you always want five instances of a web server running, Kubernetes will\\nalways keep exactly five instances running. If one of those instances stops working\\nproperly, like when its process crashes or when it stops responding, Kubernetes will\\nrestart it automatically. \\n Similarly, if a whole worker node dies or becomes inaccessible, Kubernetes will\\nselect new nodes for all the containers that were running on the node and run them\\non the newly selected nodes.\\nSCALING THE NUMBER OF COPIES\\nWhile the application is running, you can decide you want to increase or decrease the\\nnumber of copies, and Kubernetes will spin up additional ones or stop the excess\\nones, respectively. You can even leave the job of deciding the optimal number of cop-\\nies to Kubernetes. It can automatically keep adjusting the number, based on real-time\\nmetrics, such as CPU load, memory consumption, queries per second, or any other\\nmetric your app exposes. \\nHITTING A MOVING TARGET\\nWe’ve said that Kubernetes may need to move your containers around the cluster.\\nThis can occur when the node they were running on has failed or because they were\\nevicted from a node to make room for other containers. If the container is providing a\\nservice to external clients or other containers running in the cluster, how can they use\\nthe container properly if it’s constantly moving around the cluster? And how can cli-\\nents connect to containers providing a service when those containers are replicated\\nand spread across the whole cluster?\\n To allow clients to easily find containers that provide a specific service, you can tell\\nKubernetes which containers provide the same service and Kubernetes will expose all\\nof them at a single static IP address and expose that address to all applications run-\\nning in the cluster. This is done through environment variables, but clients can also\\nlook up the service IP through good old DNS. The kube-proxy will make sure connec-\\ntions to the service are load balanced across all the containers that provide the service.\\nThe IP address of the service stays constant, so clients can always connect to its con-\\ntainers, even when they’re moved around the cluster.\\n1.3.5\\nUnderstanding the benefits of using Kubernetes\\nIf you have Kubernetes deployed on all your servers, the ops team doesn’t need to\\ndeal with deploying your apps anymore. Because a containerized application already\\ncontains all it needs to run, the system administrators don’t need to install anything to\\ndeploy and run the app. On any node where Kubernetes is deployed, Kubernetes can\\nrun the app immediately without any help from the sysadmins. \\nSIMPLIFYING APPLICATION DEPLOYMENT\\nBecause Kubernetes exposes all its worker nodes as a single deployment platform,\\napplication developers can start deploying applications on their own and don’t need\\nto know anything about the servers that make up the cluster. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 54,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '22\\nCHAPTER 1\\nIntroducing Kubernetes\\n In essence, all the nodes are now a single bunch of computational resources that\\nare waiting for applications to consume them. A developer doesn’t usually care what\\nkind of server the application is running on, as long as the server can provide the\\napplication with adequate system resources. \\n Certain cases do exist where the developer does care what kind of hardware the\\napplication should run on. If the nodes are heterogeneous, you’ll find cases when you\\nwant certain apps to run on nodes with certain capabilities and run other apps on oth-\\ners. For example, one of your apps may require being run on a system with SSDs\\ninstead of HDDs, while other apps run fine on HDDs. In such cases, you obviously\\nwant to ensure that particular app is always scheduled to a node with an SSD.\\n Without using Kubernetes, the sysadmin would select one specific node that has an\\nSSD and deploy the app there. But when using Kubernetes, instead of selecting a spe-\\ncific node where your app should be run, it’s more appropriate to tell Kubernetes to\\nonly choose among nodes with an SSD. You’ll learn how to do that in chapter 3.\\nACHIEVING BETTER UTILIZATION OF HARDWARE\\nBy setting up Kubernetes on your servers and using it to run your apps instead of run-\\nning them manually, you’ve decoupled your app from the infrastructure. When you\\ntell Kubernetes to run your application, you’re letting it choose the most appropriate\\nnode to run your application on based on the description of the application’s\\nresource requirements and the available resources on each node. \\n By using containers and not tying the app down to a specific node in your cluster,\\nyou’re allowing the app to freely move around the cluster at any time, so the different\\napp components running on the cluster can be mixed and matched to be packed\\ntightly onto the cluster nodes. This ensures the node’s hardware resources are utilized\\nas best as possible.\\n The ability to move applications around the cluster at any time allows Kubernetes\\nto utilize the infrastructure much better than what you can achieve manually. Humans\\naren’t good at finding optimal combinations, especially when the number of all possi-\\nble options is huge, such as when you have many application components and many\\nserver nodes they can be deployed on. Computers can obviously perform this work\\nmuch better and faster than humans. \\nHEALTH CHECKING AND SELF-HEALING\\nHaving a system that allows moving an application across the cluster at any time is also\\nvaluable in the event of server failures. As your cluster size increases, you’ll deal with\\nfailing computer components ever more frequently. \\n Kubernetes monitors your app components and the nodes they run on and auto-\\nmatically reschedules them to other nodes in the event of a node failure. This frees\\nthe ops team from having to migrate app components manually and allows the team\\nto immediately focus on fixing the node itself and returning it to the pool of available\\nhardware resources instead of focusing on relocating the app.\\n If your infrastructure has enough spare resources to allow normal system opera-\\ntion even without the failed node, the ops team doesn’t even need to react to the failure\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 55,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '23\\nSummary\\nimmediately, such as at 3 a.m. They can sleep tight and deal with the failed node\\nduring regular work hours.\\nAUTOMATIC SCALING\\nUsing Kubernetes to manage your deployed applications also means the ops team\\ndoesn’t need to constantly monitor the load of individual applications to react to sud-\\nden load spikes. As previously mentioned, Kubernetes can be told to monitor the\\nresources used by each application and to keep adjusting the number of running\\ninstances of each application. \\n If Kubernetes is running on cloud infrastructure, where adding additional nodes is\\nas easy as requesting them through the cloud provider’s API, Kubernetes can even\\nautomatically scale the whole cluster size up or down based on the needs of the\\ndeployed applications.\\nSIMPLIFYING APPLICATION DEVELOPMENT\\nThe features described in the previous section mostly benefit the operations team. But\\nwhat about the developers? Does Kubernetes bring anything to their table? It defi-\\nnitely does.\\n If you turn back to the fact that apps run in the same environment both during\\ndevelopment and in production, this has a big effect on when bugs are discovered. We\\nall agree the sooner you discover a bug, the easier it is to fix it, and fixing it requires\\nless work. It’s the developers who do the fixing, so this means less work for them. \\n Then there’s the fact that developers don’t need to implement features that they\\nwould usually implement. This includes discovery of services and/or peers in a clustered\\napplication. Kubernetes does this instead of the app. Usually, the app only needs to look\\nup certain environment variables or perform a DNS lookup. If that’s not enough, the\\napplication can query the Kubernetes API server directly to get that and/or other infor-\\nmation. Querying the Kubernetes API server like that can even save developers from\\nhaving to implement complicated mechanisms such as leader election.\\n As a final example of what Kubernetes brings to the table, you also need to con-\\nsider the increase in confidence developers will feel knowing that when a new version\\nof their app is going to be rolled out, Kubernetes can automatically detect if the new\\nversion is bad and stop its rollout immediately. This increase in confidence usually\\naccelerates the continuous delivery of apps, which benefits the whole organization.\\n1.4\\nSummary\\nIn this introductory chapter, you’ve seen how applications have changed in recent\\nyears and how they can now be harder to deploy and manage. We’ve introduced\\nKubernetes and shown how it, together with Docker and other container platforms,\\nhelps deploy and manage applications and the infrastructure they run on. You’ve\\nlearned that\\n\\uf0a1Monolithic apps are easier to deploy, but harder to maintain over time and\\nsometimes impossible to scale.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 56,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '24\\nCHAPTER 1\\nIntroducing Kubernetes\\n\\uf0a1Microservices-based application architectures allow easier development of each\\ncomponent, but are harder to deploy and configure to work as a single system.\\n\\uf0a1Linux containers provide much the same benefits as virtual machines, but are\\nfar more lightweight and allow for much better hardware utilization.\\n\\uf0a1Docker improved on existing Linux container technologies by allowing easier and\\nfaster provisioning of containerized apps together with their OS environments.\\n\\uf0a1Kubernetes exposes the whole datacenter as a single computational resource\\nfor running applications.\\n\\uf0a1Developers can deploy apps through Kubernetes without assistance from\\nsysadmins.\\n\\uf0a1Sysadmins can sleep better by having Kubernetes deal with failed nodes auto-\\nmatically.\\nIn the next chapter, you’ll get your hands dirty by building an app and running it in\\nDocker and then in Kubernetes.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 57,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '25\\nFirst steps with Docker\\nand Kubernetes\\nBefore you start learning about Kubernetes concepts in detail, let’s see how to cre-\\nate a simple application, package it into a container image, and run it in a managed\\nKubernetes cluster (in Google Kubernetes Engine) or in a local single-node cluster.\\nThis should give you a slightly better overview of the whole Kubernetes system and\\nwill make it easier to follow the next few chapters, where we’ll go over the basic\\nbuilding blocks and concepts in Kubernetes.\\nThis chapter covers\\n\\uf0a1Creating, running, and sharing a container image \\nwith Docker\\n\\uf0a1Running a single-node Kubernetes cluster locally\\n\\uf0a1Setting up a Kubernetes cluster on Google \\nKubernetes Engine\\n\\uf0a1Setting up and using the kubectl command-line \\nclient\\n\\uf0a1Deploying an app on Kubernetes and scaling it \\nhorizontally\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 58,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '26\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\n2.1\\nCreating, running, and sharing a container image\\nAs you’ve already learned in the previous chapter, running applications in Kubernetes\\nrequires them to be packaged into container images. We’ll do a basic introduction to\\nusing Docker in case you haven’t used it yet. In the next few sections you’ll\\n1\\nInstall Docker and run your first “Hello world” container \\n2\\nCreate a trivial Node.js app that you’ll later deploy in Kubernetes\\n3\\nPackage the app into a container image so you can then run it as an isolated\\ncontainer\\n4\\nRun a container based on the image\\n5\\nPush the image to Docker Hub so that anyone anywhere can run it\\n2.1.1\\nInstalling Docker and running a Hello World container\\nFirst, you’ll need to install Docker on your Linux machine. If you don’t use Linux,\\nyou’ll need to start a Linux virtual machine (VM) and run Docker inside that VM. If\\nyou’re using a Mac or Windows and install Docker per instructions, Docker will set up\\na VM for you and run the Docker daemon inside that VM. The Docker client execut-\\nable will be available on your host OS, and will communicate with the daemon inside\\nthe VM. \\n To install Docker, follow the instructions at http:/\\n/docs.docker.com/engine/\\ninstallation/ for your specific operating system. After completing the installation, you\\ncan use the Docker client executable to run various Docker commands. For example,\\nyou could try pulling and running an existing image from Docker Hub, the public\\nDocker registry, which contains ready-to-use container images for many well-known\\nsoftware packages. One of them is the busybox image, which you’ll use to run a simple\\necho \"Hello world\" command. \\nRUNNING A HELLO WORLD CONTAINER\\nIf you’re unfamiliar with busybox, it’s a single executable that combines many of the\\nstandard UNIX command-line tools, such as echo, ls, gzip, and so on. Instead of the\\nbusybox image, you could also use any other full-fledged OS container image such as\\nFedora, Ubuntu, or other similar images, as long as it includes the echo executable.\\n How do you run the busybox image? You don’t need to download or install any-\\nthing. Use the docker run command and specify what image to download and run\\nand (optionally) what command to execute, as shown in the following listing.\\n$ docker run busybox echo \"Hello world\"\\nUnable to find image \\'busybox:latest\\' locally\\nlatest: Pulling from docker.io/busybox\\n9a163e0b8d13: Pull complete \\nfef924a0204a: Pull complete\\nDigest: sha256:97473e34e311e6c1b3f61f2a721d038d1e5eef17d98d1353a513007cf46ca6bd\\nStatus: Downloaded newer image for docker.io/busybox:latest\\nHello world\\nListing 2.1\\nRunning a Hello world container with Docker\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 59,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '27\\nCreating, running, and sharing a container image\\nThis doesn’t look that impressive, but when you consider that the whole “app” was\\ndownloaded and executed with a single command, without you having to install that\\napp or anything else, you’ll agree it’s awesome. In your case, the app was a single execut-\\nable (busybox), but it might as well have been an incredibly complex app with many\\ndependencies. The whole process of setting up and running the app would have been\\nexactly the same. What’s also important is that the app was executed inside a container,\\ncompletely isolated from all the other processes running on your machine.\\nUNDERSTANDING WHAT HAPPENS BEHIND THE SCENES\\nFigure 2.1 shows exactly what happened when you performed the docker run com-\\nmand. First, Docker checked to see if the busybox:latest image was already present\\non your local machine. It wasn’t, so Docker pulled it from the Docker Hub registry at\\nhttp:/\\n/docker.io. After the image was downloaded to your machine, Docker created a\\ncontainer from that image and ran the command inside it. The echo command\\nprinted the text to STDOUT and then the process terminated and the container\\nstopped.\\nRUNNING OTHER IMAGES\\nRunning other existing container images is much the same as how you ran the busybox\\nimage. In fact, it’s often even simpler, because you usually don’t need to specify what\\ncommand to execute, the way you did in the example (echo \"Hello world\"). The\\ncommand that should be executed is usually baked into the image itself, but you can\\noverride it if you want. After searching or browsing through the publicly available\\nimages on http:/\\n/hub.docker.com or another public registry, you tell Docker to run\\nthe image like this:\\n$ docker run <image>\\nFigure 2.1\\nRunning echo “Hello world” in a container based on the busybox container image\\nLocal machine\\nDocker Hub\\n1. docker run busybox\\necho \"Hello world\"\\n3. Docker pulls\\nbusybox image\\nfrom registry (if not\\navailable locally)\\n2. Docker checks if busybox\\nimage is already stored locally\\n4. Docker runs\\necho \"Hello world\"\\nin isolated container\\nbusybox\\nDocker\\nbusybox\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 60,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '28\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\nVERSIONING CONTAINER IMAGES\\nAll software packages get updated, so more than a single version of a package usually\\nexists. Docker supports having multiple versions or variants of the same image under\\nthe same name. Each variant must have a unique tag. When referring to images with-\\nout explicitly specifying the tag, Docker will assume you’re referring to the so-called\\nlatest tag. To run a different version of the image, you may specify the tag along with\\nthe image name like this:\\n$ docker run <image>:<tag>\\n2.1.2\\nCreating a trivial Node.js app\\nNow that you have a working Docker setup, you’re going to create an app. You’ll build\\na trivial Node.js web application and package it into a container image. The applica-\\ntion will accept HTTP requests and respond with the hostname of the machine it’s\\nrunning in. This way, you’ll see that an app running inside a container sees its own\\nhostname and not that of the host machine, even though it’s running on the host like\\nany other process. This will be useful later, when you deploy the app on Kubernetes\\nand scale it out (scale it horizontally; that is, run multiple instances of the app). You’ll\\nsee your HTTP requests hitting different instances of the app.\\n Your app will consist of a single file called app.js with the contents shown in the fol-\\nlowing listing.\\nconst http = require(\\'http\\');\\nconst os = require(\\'os\\');\\nconsole.log(\"Kubia server starting...\");\\nvar handler = function(request, response) {\\n  console.log(\"Received request from \" + request.connection.remoteAddress);\\n  response.writeHead(200);\\n  response.end(\"You\\'ve hit \" + os.hostname() + \"\\\\n\");\\n};\\nvar www = http.createServer(handler);\\nwww.listen(8080);\\nIt should be clear what this code does. It starts up an HTTP server on port 8080. The\\nserver responds with an HTTP response status code 200 OK and the text \"You’ve hit\\n<hostname>\" to every request. The request handler also logs the client’s IP address to\\nthe standard output, which you’ll need later.\\nNOTE\\nThe returned hostname is the server’s actual hostname, not the one\\nthe client sends in the HTTP request’s Host header.\\nYou could now download and install Node.js and test your app directly, but this isn’t\\nnecessary, because you’ll use Docker to package the app into a container image and\\nListing 2.2\\nA simple Node.js app: app.js\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 61,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '29\\nCreating, running, and sharing a container image\\nenable it to be run anywhere without having to download or install anything (except\\nDocker, which does need to be installed on the machine you want to run the image on).\\n2.1.3\\nCreating a Dockerfile for the image\\nTo package your app into an image, you first need to create a file called Dockerfile,\\nwhich will contain a list of instructions that Docker will perform when building the\\nimage. The Dockerfile needs to be in the same directory as the app.js file and should\\ncontain the commands shown in the following listing.\\nFROM node:7\\nADD app.js /app.js\\nENTRYPOINT [\"node\", \"app.js\"]\\nThe FROM line defines the container image you’ll use as a starting point (the base\\nimage you’re building on top of). In your case, you’re using the node container image,\\ntag 7. In the second line, you’re adding your app.js file from your local directory into\\nthe root directory in the image, under the same name (app.js). Finally, in the third\\nline, you’re defining what command should be executed when somebody runs the\\nimage. In your case, the command is node app.js.\\n2.1.4\\nBuilding the container image\\nNow that you have your Dockerfile and the app.js file, you have everything you need\\nto build your image. To build it, run the following Docker command:\\n$ docker build -t kubia .\\nFigure 2.2 shows what happens during the build process. You’re telling Docker to\\nbuild an image called kubia based on the contents of the current directory (note the\\ndot at the end of the build command). Docker will look for the Dockerfile in the direc-\\ntory and build the image based on the instructions in the file.\\nListing 2.3\\nA Dockerfile for building a container image for your app\\nChoosing a base image\\nYou may wonder why we chose this specific image as your base. Because your app\\nis a Node.js app, you need your image to contain the node binary executable to run\\nthe app. You could have used any image that contains that binary, or you could have\\neven used a Linux distro base image such as fedora or ubuntu and installed\\nNode.js into the container at image build time. But because the node image is made\\nspecifically for running Node.js apps, and includes everything you need to run your\\napp, you’ll use that as the base image.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 62,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '30\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\nUNDERSTANDING HOW AN IMAGE IS BUILT\\nThe build process isn’t performed by the Docker client. Instead, the contents of the\\nwhole directory are uploaded to the Docker daemon and the image is built there.\\nThe client and daemon don’t need to be on the same machine at all. If you’re using\\nDocker on a non-Linux OS, the client is on your host OS, but the daemon runs\\ninside a VM. Because all the files in the build directory are uploaded to the daemon,\\nif it contains many large files and the daemon isn’t running locally, the upload may\\ntake longer. \\nTIP\\nDon’t include any unnecessary files in the build directory, because they’ll\\nslow down the build process—especially when the Docker daemon is on a\\nremote machine. \\nDuring the build process, Docker will first pull the base image (node:7) from the pub-\\nlic image repository (Docker Hub), unless the image has already been pulled and is\\nstored on your machine. \\nUNDERSTANDING IMAGE LAYERS\\nAn image isn’t a single, big, binary blob, but is composed of multiple layers, which you\\nmay have already noticed when running the busybox example (there were multiple\\nPull complete lines—one for each layer). Different images may share several layers,\\nFigure 2.2\\nBuilding a new container image from a Dockerfile\\nLocal machine\\nDocker Hub\\n1. docker build\\nkubia .\\n3. Docker pulls image\\nnode:7.0 if it isn’t\\nstored locally yet\\n4. Build new\\nimage\\n2. Docker client uploads\\ndirectory contents to daemon\\nDockerﬁle\\nDocker client\\nDocker daemon\\napp.js\\nnode:7.0\\nnode:7.0\\nkubia:latest\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 63,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '31\\nCreating, running, and sharing a container image\\nwhich makes storing and transferring images much more efficient. For example, if\\nyou create multiple images based on the same base image (such as node:7 in the exam-\\nple), all the layers comprising the base image will be stored only once. Also, when pull-\\ning an image, Docker will download each layer individually. Several layers may already\\nbe stored on your machine, so Docker will only download those that aren’t.\\n You may think that each Dockerfile creates only a single new layer, but that’s not\\nthe case. When building an image, a new layer is created for each individual command\\nin the Dockerfile. During the build of your image, after pulling all the layers of the base\\nimage, Docker will create a new layer on top of them and add the app.js file into it.\\nThen it will create yet another layer that will specify the command that should be exe-\\ncuted when the image is run. This last layer will then be tagged as kubia:latest. This is\\nshown in figure 2.3, which also shows how a different image called other:latest would\\nuse the same layers of the Node.js image as your own image does.\\nWhen the build process completes, you have a new image stored locally. You can see it\\nby telling Docker to list all locally stored images, as shown in the following listing.\\n$ docker images\\nREPOSITORY   TAG      IMAGE ID           CREATED             VIRTUAL SIZE\\nkubia        latest   d30ecc7419e7       1 minute ago        637.1 MB\\n...\\nCOMPARING BUILDING IMAGES WITH A DOCKERFILE VS. MANUALLY\\nDockerfiles are the usual way of building container images with Docker, but you could\\nalso build the image manually by running a container from an existing image, execut-\\ning commands in the container, exiting the container, and committing the final state\\nas a new image. This is exactly what happens when you build from a Dockerfile, but\\nit’s performed automatically and is repeatable, which allows you to make changes to\\nListing 2.4\\nListing locally stored images\\nFigure 2.3\\nContainer images are composed of layers that can be shared among different images.\\nRUN curl ...\\nCMD node\\nADD app.js/app.js\\n...\\nCMD node app.js\\nkubia:latest image\\n...\\n...\\nRUN apt-get ...\\nother:latest image\\n...\\nnode:0.12 image\\nbuildpack-deps:jessie image\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [CMD node app.js\n",
       "   ADD app.js/app.js\n",
       "   CMD node\n",
       "   RUN curl ..\n",
       "   kubia:latest image\n",
       "   ...\n",
       "   RUN apt-get\n",
       "   ..., ...\n",
       "   ... other:latest image\n",
       "   .\n",
       "   node:0.12 image\n",
       "   ...\n",
       "   buildpack-deps:jessie image]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [CMD node, Col1]\n",
       "   Index: [],\n",
       "         CMD node Col1\n",
       "   0  RUN curl ..    .\n",
       "   1          ...     ,\n",
       "   Empty DataFrame\n",
       "   Columns: [RUN apt-get, ...]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [..., Col1]\n",
       "   Index: []]},\n",
       " {'page': 64,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '32\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\nthe Dockerfile and rebuild the image any time, without having to manually retype all\\nthe commands again.\\n2.1.5\\nRunning the container image\\nYou can now run your image with the following command:\\n$ docker run --name kubia-container -p 8080:8080 -d kubia\\nThis tells Docker to run a new container called kubia-container from the kubia\\nimage. The container will be detached from the console (-d flag), which means it will\\nrun in the background. Port 8080 on the local machine will be mapped to port 8080\\ninside the container (-p 8080:8080 option), so you can access the app through\\nhttp:/\\n/localhost:8080. \\n If you’re not running the Docker daemon on your local machine (if you’re using a\\nMac or Windows, the daemon is running inside a VM), you’ll need to use the host-\\nname or IP of the VM running the daemon instead of localhost. You can look it up\\nthrough the DOCKER_HOST environment variable.\\nACCESSING YOUR APP\\nNow try to access your application at http:/\\n/localhost:8080 (be sure to replace local-\\nhost with the hostname or IP of the Docker host if necessary): \\n$ curl localhost:8080\\nYou’ve hit 44d76963e8e1   \\nThat’s the response from your app. Your tiny application is now running inside a con-\\ntainer, isolated from everything else. As you can see, it’s returning 44d76963e8e1 as its\\nhostname, and not the actual hostname of your host machine. The hexadecimal num-\\nber is the ID of the Docker container. \\nLISTING ALL RUNNING CONTAINERS\\nLet’s list all running containers in the following listing, so you can examine the list\\n(I’ve edited the output to make it more readable—imagine the last two lines as the\\ncontinuation of the first two).\\n$ docker ps\\nCONTAINER ID  IMAGE         COMMAND               CREATED        ...\\n44d76963e8e1  kubia:latest  \"/bin/sh -c \\'node ap  6 minutes ago  ...\\n...  STATUS              PORTS                    NAMES\\n...  Up 6 minutes        0.0.0.0:8080->8080/tcp   kubia-container\\nA single container is running. For each container, Docker prints out its ID and name,\\nthe image used to run the container, and the command that’s executing inside the\\ncontainer. \\nListing 2.5\\nListing running containers\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 65,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '33\\nCreating, running, and sharing a container image\\nGETTING ADDITIONAL INFORMATION ABOUT A CONTAINER\\nThe docker ps command only shows the most basic information about the containers.\\nTo see additional information, you can use docker inspect:\\n$ docker inspect kubia-container\\nDocker will print out a long JSON containing low-level information about the con-\\ntainer. \\n2.1.6\\nExploring the inside of a running container\\nWhat if you want to see what the environment is like inside the container? Because\\nmultiple processes can run inside the same container, you can always run an addi-\\ntional process in it to see what’s inside. You can even run a shell, provided that the\\nshell’s binary executable is available in the image. \\nRUNNING A SHELL INSIDE AN EXISTING CONTAINER\\nThe Node.js image on which you’ve based your image contains the bash shell, so you\\ncan run the shell inside the container like this:\\n$ docker exec -it kubia-container bash\\nThis will run bash inside the existing kubia-container container. The bash process\\nwill have the same Linux namespaces as the main container process. This allows you\\nto explore the container from within and see how Node.js and your app see the system\\nwhen running inside the container. The -it option is shorthand for two options: \\n\\uf0a1\\n-i, which makes sure STDIN is kept open. You need this for entering com-\\nmands into the shell. \\n\\uf0a1\\n-t, which allocates a pseudo terminal (TTY).\\nYou need both if you want the use the shell like you’re used to. (If you leave out the\\nfirst one, you can’t type any commands, and if you leave out the second one, the com-\\nmand prompt won’t be displayed and some commands will complain about the TERM\\nvariable not being set.)\\nEXPLORING THE CONTAINER FROM WITHIN\\nLet’s see how to use the shell in the following listing to see the processes running in\\nthe container.\\nroot@44d76963e8e1:/# ps aux\\nUSER  PID %CPU %MEM    VSZ   RSS TTY STAT START TIME COMMAND\\nroot    1  0.0  0.1 676380 16504 ?   Sl   12:31 0:00 node app.js\\nroot   10  0.0  0.0  20216  1924 ?   Ss   12:31 0:00 bash\\nroot   19  0.0  0.0  17492  1136 ?   R+   12:38 0:00 ps aux\\nYou see only three processes. You don’t see any other processes from the host OS. \\nListing 2.6\\nListing processes from inside a container\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 66,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '34\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\nUNDERSTANDING THAT PROCESSES IN A CONTAINER RUN IN THE HOST OPERATING SYSTEM\\nIf you now open another terminal and list the processes on the host OS itself, you will,\\namong all other host processes, also see the processes running in the container, as\\nshown in listing 2.7. \\nNOTE\\nIf you’re using a Mac or Windows, you’ll need to log into the VM where\\nthe Docker daemon is running to see these processes.\\n$ ps aux | grep app.js\\nUSER  PID %CPU %MEM    VSZ   RSS TTY STAT START TIME COMMAND\\nroot  382  0.0  0.1 676380 16504 ?   Sl   12:31 0:00 node app.js\\nThis proves that processes running in the container are running in the host OS. If you\\nhave a keen eye, you may have noticed that the processes have different IDs inside the\\ncontainer vs. on the host. The container is using its own PID Linux namespace and\\nhas a completely isolated process tree, with its own sequence of numbers. \\nTHE CONTAINER’S FILESYSTEM IS ALSO ISOLATED\\nLike having an isolated process tree, each container also has an isolated filesystem.\\nListing the contents of the root directory inside the container will only show the files\\nin the container and will include all the files that are in the image plus any files that\\nare created while the container is running (log files and similar), as shown in the fol-\\nlowing listing.\\nroot@44d76963e8e1:/# ls /\\napp.js  boot  etc   lib    media  opt   root  sbin  sys  usr\\nbin     dev   home  lib64  mnt    proc  run   srv   tmp  var\\nIt contains the app.js file and other system directories that are part of the node:7 base\\nimage you’re using. To exit the container, you exit the shell by running the exit com-\\nmand and you’ll be returned to your host machine (like logging out of an ssh session,\\nfor example).\\nTIP\\nEntering a running container like this is useful when debugging an app\\nrunning in a container. When something’s wrong, the first thing you’ll want\\nto explore is the actual state of the system your application sees. Keep in mind\\nthat an application will not only see its own unique filesystem, but also pro-\\ncesses, users, hostname, and network interfaces.\\n2.1.7\\nStopping and removing a container\\nTo stop your app, you tell Docker to stop the kubia-container container:\\n$ docker stop kubia-container\\nListing 2.7\\nA container’s processes run in the host OS\\nListing 2.8\\nA container has its own complete filesystem\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 67,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '35\\nCreating, running, and sharing a container image\\nThis will stop the main process running in the container and consequently stop the\\ncontainer, because no other processes are running inside the container. The con-\\ntainer itself still exists and you can see it with docker ps -a. The -a option prints out\\nall the containers, those running and those that have been stopped. To truly remove a\\ncontainer, you need to remove it with the docker rm command:\\n$ docker rm kubia-container\\nThis deletes the container. All its contents are removed and it can’t be started again.\\n2.1.8\\nPushing the image to an image registry\\nThe image you’ve built has so far only been available on your local machine. To allow\\nyou to run it on any other machine, you need to push the image to an external image\\nregistry. For the sake of simplicity, you won’t set up a private image registry and will\\ninstead push the image to Docker Hub (http:/\\n/hub.docker.com), which is one of the\\npublicly available registries. Other widely used such registries are Quay.io and the\\nGoogle Container Registry.\\n Before you do that, you need to re-tag your image according to Docker Hub’s\\nrules. Docker Hub will allow you to push an image if the image’s repository name\\nstarts with your Docker Hub ID. You create your Docker Hub ID by registering at\\nhttp:/\\n/hub.docker.com. I’ll use my own ID (luksa) in the following examples. Please\\nchange every occurrence with your own ID.\\nTAGGING AN IMAGE UNDER AN ADDITIONAL TAG\\nOnce you know your ID, you’re ready to rename your image, currently tagged as\\nkubia, to luksa/kubia (replace luksa with your own Docker Hub ID):\\n$ docker tag kubia luksa/kubia\\nThis doesn’t rename the tag; it creates an additional tag for the same image. You can\\nconfirm this by listing the images stored on your system with the docker images com-\\nmand, as shown in the following listing.\\n$ docker images | head\\nREPOSITORY        TAG      IMAGE ID        CREATED             VIRTUAL SIZE\\nluksa/kubia       latest   d30ecc7419e7    About an hour ago   654.5 MB\\nkubia             latest   d30ecc7419e7    About an hour ago   654.5 MB\\ndocker.io/node    7.0      04c0ca2a8dad    2 days ago          654.5 MB\\n...\\nAs you can see, both kubia and luksa/kubia point to the same image ID, so they’re in\\nfact one single image with two tags. \\nListing 2.9\\nA container image can have multiple tags\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 68,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '36\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\nPUSHING THE IMAGE TO DOCKER HUB\\nBefore you can push the image to Docker Hub, you need to log in under your user ID\\nwith the docker login command. Once you’re logged in, you can finally push the\\nyourid/kubia image to Docker Hub like this:\\n$ docker push luksa/kubia\\nRUNNING THE IMAGE ON A DIFFERENT MACHINE\\nAfter the push to Docker Hub is complete, the image will be available to everyone.\\nYou can now run the image on any machine running Docker by executing the follow-\\ning command:\\n$ docker run -p 8080:8080 -d luksa/kubia\\nIt doesn’t get much simpler than that. And the best thing about this is that your appli-\\ncation will have the exact same environment every time and everywhere it’s run. If it\\nran fine on your machine, it should run as well on every other Linux machine. No\\nneed to worry about whether the host machine has Node.js installed or not. In fact,\\neven if it does, your app won’t use it, because it will use the one installed inside the\\nimage.\\n2.2\\nSetting up a Kubernetes cluster\\nNow that you have your app packaged inside a container image and made available\\nthrough Docker Hub, you can deploy it in a Kubernetes cluster instead of running it\\nin Docker directly. But first, you need to set up the cluster itself. \\n Setting up a full-fledged, multi-node Kubernetes cluster isn’t a simple task, espe-\\ncially if you’re not well-versed in Linux and networking administration. A proper\\nKubernetes install spans multiple physical or virtual machines and requires the net-\\nworking to be set up properly, so that all the containers running inside the Kuberne-\\ntes cluster can connect to each other through the same flat networking space. \\n A long list of methods exists for installing a Kubernetes cluster. These methods are\\ndescribed in detail in the documentation at http:/\\n/kubernetes.io. We’re not going to\\nlist all of them here, because the list keeps evolving, but Kubernetes can be run on\\nyour local development machine, your own organization’s cluster of machines, on\\ncloud providers providing virtual machines (Google Compute Engine, Amazon EC2,\\nMicrosoft Azure, and so on), or by using a managed Kubernetes cluster such as Goo-\\ngle Kubernetes Engine (previously known as Google Container Engine). \\n In this chapter, we’ll cover two simple options for getting your hands on a running\\nKubernetes cluster. You’ll see how to run a single-node Kubernetes cluster on your\\nlocal machine and how to get access to a hosted cluster running on Google Kuberne-\\ntes Engine (GKE). \\n A third option, which covers installing a cluster with the kubeadm tool, is explained\\nin appendix B. The instructions there show you how to set up a three-node Kubernetes\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 69,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '37\\nSetting up a Kubernetes cluster\\ncluster using virtual machines, but I suggest you try it only after reading the first 11\\nchapters of the book.\\n Another option is to install Kubernetes on Amazon’s AWS (Amazon Web Services).\\nFor this, you can look at the kops tool, which is built on top of kubeadm mentioned in\\nthe previous paragraph, and is available at http:/\\n/github.com/kubernetes/kops. It\\nhelps you deploy production-grade, highly available Kubernetes clusters on AWS and\\nwill eventually support other platforms as well (Google Kubernetes Engine, VMware,\\nvSphere, and so on).\\n2.2.1\\nRunning a local single-node Kubernetes cluster with Minikube\\nThe simplest and quickest path to a fully functioning Kubernetes cluster is by using\\nMinikube. Minikube is a tool that sets up a single-node cluster that’s great for both\\ntesting Kubernetes and developing apps locally. \\n Although we can’t show certain Kubernetes features related to managing apps on\\nmultiple nodes, the single-node cluster should be enough for exploring most topics\\ndiscussed in this book. \\nINSTALLING MINIKUBE\\nMinikube is a single binary that needs to be downloaded and put onto your path. It’s\\navailable for OSX, Linux, and Windows. To install it, the best place to start is to go to\\nthe Minikube repository on GitHub (http:/\\n/github.com/kubernetes/minikube) and\\nfollow the instructions there.\\n For example, on OSX and Linux, Minikube can be downloaded and set up with a\\nsingle command. For OSX, this is what the command looks like:\\n$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/\\n➥ v0.23.0/minikube-darwin-amd64 && chmod +x minikube && sudo mv minikube \\n➥ /usr/local/bin/\\nOn Linux, you download a different release (replace “darwin” with “linux” in the\\nURL). On Windows, you can download the file manually, rename it to minikube.exe,\\nand put it onto your path. Minikube runs Kubernetes inside a VM run through either\\nVirtualBox or KVM, so you also need to install one of them before you can start the\\nMinikube cluster.\\nSTARTING A KUBERNETES CLUSTER WITH MINIKUBE\\nOnce you have Minikube installed locally, you can immediately start up the Kuberne-\\ntes cluster with the command in the following listing.\\n$ minikube start\\nStarting local Kubernetes cluster...\\nStarting VM...\\nSSH-ing files into VM...\\n...\\nKubectl is now configured to use the cluster. \\nListing 2.10\\nStarting a Minikube virtual machine\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 70,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '38\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\nStarting the cluster takes more than a minute, so don’t interrupt the command before\\nit completes. \\nINSTALLING THE KUBERNETES CLIENT (KUBECTL)\\nTo interact with Kubernetes, you also need the kubectl CLI client. Again, all you need\\nto do is download it and put it on your path. The latest stable release for OSX, for\\nexample, can be downloaded and installed with the following command:\\n$ curl -LO https://storage.googleapis.com/kubernetes-release/release\\n➥  /$(curl -s https://storage.googleapis.com/kubernetes-release/release\\n➥  /stable.txt)/bin/darwin/amd64/kubectl \\n➥  && chmod +x kubectl \\n➥  && sudo mv kubectl /usr/local/bin/\\nTo download kubectl for Linux or Windows, replace darwin in the URL with either\\nlinux or windows.\\nNOTE\\nIf you’ll be using multiple Kubernetes clusters (for example, both\\nMinikube and GKE), refer to appendix A for information on how to set up\\nand switch between different kubectl contexts.\\nCHECKING TO SEE THE CLUSTER IS UP AND KUBECTL CAN TALK TO IT\\nTo verify your cluster is working, you can use the kubectl cluster-info command\\nshown in the following listing.\\n$ kubectl cluster-info\\nKubernetes master is running at https://192.168.99.100:8443\\nKubeDNS is running at https://192.168.99.100:8443/api/v1/proxy/...\\nkubernetes-dashboard is running at https://192.168.99.100:8443/api/v1/...\\nThis shows the cluster is up. It shows the URLs of the various Kubernetes components,\\nincluding the API server and the web console. \\nTIP\\nYou can run minikube ssh to log into the Minikube VM and explore it\\nfrom the inside. For example, you may want to see what processes are run-\\nning on the node.\\n2.2.2\\nUsing a hosted Kubernetes cluster with Google Kubernetes Engine\\nIf you want to explore a full-fledged multi-node Kubernetes cluster instead, you can\\nuse a managed Google Kubernetes Engine (GKE) cluster. This way, you don’t need to\\nmanually set up all the cluster nodes and networking, which is usually too much for\\nsomeone making their first steps with Kubernetes. Using a managed solution such as\\nGKE makes sure you don’t end up with a misconfigured, non-working, or partially work-\\ning cluster.\\nListing 2.11\\nDisplaying cluster information\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 71,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '39\\nSetting up a Kubernetes cluster\\nSETTING UP A GOOGLE CLOUD PROJECT AND DOWNLOADING THE NECESSARY CLIENT BINARIES\\nBefore you can set up a new Kubernetes cluster, you need to set up your GKE environ-\\nment. Because the process may change, I’m not listing the exact instructions here. To\\nget started, please follow the instructions at https:/\\n/cloud.google.com/container-\\nengine/docs/before-you-begin.\\n Roughly, the whole procedure includes\\n1\\nSigning up for a Google account, in the unlikely case you don’t have one\\nalready.\\n2\\nCreating a project in the Google Cloud Platform Console. \\n3\\nEnabling billing. This does require your credit card info, but Google provides a\\n12-month free trial. And they’re nice enough to not start charging automati-\\ncally after the free trial is over.)\\n4\\nEnabling the Kubernetes Engine API.\\n5\\nDownloading and installing Google Cloud SDK. (This includes the gcloud\\ncommand-line tool, which you’ll need to create a Kubernetes cluster.)\\n6\\nInstalling the kubectl command-line tool with gcloud components install\\nkubectl.\\nNOTE\\nCertain operations (the one in step 2, for example) may take a few\\nminutes to complete, so relax and grab a coffee in the meantime.\\nCREATING A KUBERNETES CLUSTER WITH THREE NODES\\nAfter completing the installation, you can create a Kubernetes cluster with three\\nworker nodes using the command shown in the following listing.\\n$ gcloud container clusters create kubia --num-nodes 3 \\n➥ --machine-type f1-micro\\nCreating cluster kubia...done.\\nCreated [https://container.googleapis.com/v1/projects/kubia1-\\n1227/zones/europe-west1-d/clusters/kubia].\\nkubeconfig entry generated for kubia.\\nNAME   ZONE   MST_VER MASTER_IP     TYPE     NODE_VER NUM_NODES STATUS\\nkubia  eu-w1d 1.5.3   104.155.92.30 f1-micro 1.5.3    3         RUNNING\\nYou should now have a running Kubernetes cluster with three worker nodes as shown\\nin figure 2.4. You’re using three nodes to help better demonstrate features that apply\\nto multiple nodes. You can use a smaller number of nodes, if you want. \\nGETTING AN OVERVIEW OF YOUR CLUSTER\\nTo give you a basic idea of what your cluster looks like and how to interact with it, see\\nfigure 2.4. Each node runs Docker, the Kubelet and the kube-proxy. You’ll interact\\nwith the cluster through the kubectl command line client, which issues REST requests\\nto the Kubernetes API server running on the master node.\\nListing 2.12\\nCreating a three-node cluster in GKE\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 72,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '40\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\nCHECKING IF THE CLUSTER IS UP BY LISTING CLUSTER NODES\\nYou’ll use the kubectl command now to list all the nodes in your cluster, as shown in\\nthe following listing.\\n$ kubectl get nodes\\nNAME                      STATUS  AGE  VERSION\\ngke-kubia-85f6-node-0rrx  Ready   1m    v1.5.3\\ngke-kubia-85f6-node-heo1  Ready   1m    v1.5.3\\ngke-kubia-85f6-node-vs9f  Ready   1m    v1.5.3\\nThe kubectl get command can list all kinds of Kubernetes objects. You’ll use it con-\\nstantly, but it usually shows only the most basic information for the listed objects. \\nTIP\\nYou can log into one of the nodes with gcloud compute ssh <node-name>\\nto explore what’s running on the node.\\nListing 2.13\\nListing cluster nodes with kubectl\\nFigure 2.4\\nHow you’re interacting with your three-node Kubernetes cluster \\nLocal dev machine\\nREST call\\nkubectl\\nREST API server\\nMaster node\\n(IP 104.155.92.30)\\nDocker\\nKubelet\\nkube-proxy\\ngke-kubia-85f6-node-heo1\\nDocker\\nKubelet\\nkube-proxy\\ngke-kubia-85f6-node-vs9f\\nDocker\\nWorker nodes\\nKubernetes cluster\\nKubelet\\nkube-proxy\\ngke-kubia-85f6-node-0rrx\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [    Docker        Col1\n",
       "   0  Kubelet  kube-proxy,\n",
       "       Docker        Col1\n",
       "   0  Kubelet  kube-proxy,\n",
       "       Docker        Col1\n",
       "   0  Kubelet  kube-proxy]},\n",
       " {'page': 73,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '41\\nSetting up a Kubernetes cluster\\nRETRIEVING ADDITIONAL DETAILS OF AN OBJECT\\nTo see more detailed information about an object, you can use the kubectl describe\\ncommand, which shows much more: \\n$ kubectl describe node gke-kubia-85f6-node-0rrx\\nI’m omitting the actual output of the describe command, because it’s fairly wide and\\nwould be completely unreadable here in the book. The output shows the node’s sta-\\ntus, its CPU and memory data, system information, containers running on the node,\\nand much more.\\n In the previous kubectl describe example, you specified the name of the node\\nexplicitly, but you could also have performed a simple kubectl describe node without\\ntyping the node’s name and it would print out a detailed description of all the nodes.\\nTIP\\nRunning the describe and get commands without specifying the name\\nof the object comes in handy when only one object of a given type exists, so\\nyou don’t waste time typing or copy/pasting the object’s name.\\nWhile we’re talking about reducing keystrokes, let me give you additional advice on\\nhow to make working with kubectl much easier, before we move on to running your\\nfirst app in Kubernetes.\\n2.2.3\\nSetting up an alias and command-line completion for kubectl \\nYou’ll use kubectl often. You’ll soon realize that having to type the full command\\nevery time is a real pain. Before you continue, take a minute to make your life easier\\nby setting up an alias and tab completion for kubectl.\\nCREATING AN ALIAS\\nThroughout the book, I’ll always be using the full name of the kubectl executable,\\nbut you may want to add a short alias such as k, so you won’t have to type kubectl\\nevery time. If you haven’t used aliases yet, here’s how you define one. Add the follow-\\ning line to your ~/.bashrc or equivalent file:\\nalias k=kubectl\\nNOTE\\nYou may already have the k executable if you used gcloud to set up the\\ncluster.\\nCONFIGURING TAB COMPLETION FOR KUBECTL\\nEven with a short alias such as k, you’ll still need to type way more than you’d like. Luck-\\nily, the kubectl command can also output shell completion code for both the bash and\\nzsh shell. It doesn’t enable tab completion of only command names, but also of the\\nactual object names. For example, instead of having to write the whole node name in\\nthe previous example, all you’d need to type is\\n$ kubectl desc<TAB> no<TAB> gke-ku<TAB>\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 74,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '42\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\nTo enable tab completion in bash, you’ll first need to install a package called bash-\\ncompletion and then run the following command (you’ll probably also want to add it\\nto ~/.bashrc or equivalent):\\n$ source <(kubectl completion bash)\\nBut there’s one caveat. When you run the preceding command, tab completion will\\nonly work when you use the full kubectl name (it won’t work when you use the k\\nalias). To fix this, you need to transform the output of the kubectl completion com-\\nmand a bit:\\n$ source <(kubectl completion bash | sed s/kubectl/k/g)\\nNOTE\\nUnfortunately, as I’m writing this, shell completion doesn’t work for\\naliases on MacOS. You’ll have to use the full kubectl command name if you\\nwant completion to work.\\nNow you’re all set to start interacting with your cluster without having to type too\\nmuch. You can finally run your first app on Kubernetes.\\n2.3\\nRunning your first app on Kubernetes\\nBecause this may be your first time, you’ll use the simplest possible way of running an\\napp on Kubernetes. Usually, you’d prepare a JSON or YAML manifest, containing a\\ndescription of all the components you want to deploy, but because we haven’t talked\\nabout the types of components you can create in Kubernetes yet, you’ll use a simple\\none-line command to get something running.\\n2.3.1\\nDeploying your Node.js app\\nThe simplest way to deploy your app is to use the kubectl run command, which will\\ncreate all the necessary components without having to deal with JSON or YAML. This\\nway, we don’t need to dive into the structure of each object yet. Try to run the image\\nyou created and pushed to Docker Hub earlier. Here’s how to run it in Kubernetes:\\n$ kubectl run kubia --image=luksa/kubia --port=8080 --generator=run/v1\\nreplicationcontroller \"kubia\" created\\nThe --image=luksa/kubia part obviously specifies the container image you want to\\nrun, and the --port=8080 option tells Kubernetes that your app is listening on port\\n8080. The last flag (--generator) does require an explanation, though. Usually, you\\nwon’t use it, but you’re using it here so Kubernetes creates a ReplicationController\\ninstead of a Deployment. You’ll learn what ReplicationControllers are later in the chap-\\nter, but we won’t talk about Deployments until chapter 9. That’s why I don’t want\\nkubectl to create a Deployment yet.\\n As the previous command’s output shows, a ReplicationController called kubia\\nhas been created. As already mentioned, we’ll see what that is later in the chapter. For\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 75,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '43\\nRunning your first app on Kubernetes\\nnow, let’s start from the bottom and focus on the container you created (you can\\nassume a container has been created, because you specified a container image in the\\nrun command).\\nINTRODUCING PODS\\nYou may be wondering if you can see your container in a list showing all the running\\ncontainers. Maybe something such as kubectl get containers? Well, that’s not exactly\\nhow Kubernetes works. It doesn’t deal with individual containers directly. Instead, it\\nuses the concept of multiple co-located containers. This group of containers is called\\na Pod. \\n A pod is a group of one or more tightly related containers that will always run\\ntogether on the same worker node and in the same Linux namespace(s). Each pod\\nis like a separate logical machine with its own IP, hostname, processes, and so on,\\nrunning a single application. The application can be a single process, running in a\\nsingle container, or it can be a main application process and additional supporting\\nprocesses, each running in its own container. All the containers in a pod will appear\\nto be running on the same logical machine, whereas containers in other pods, even\\nif they’re running on the same worker node, will appear to be running on a differ-\\nent one. \\n To better understand the relationship between containers, pods, and nodes, exam-\\nine figure 2.5. As you can see, each pod has its own IP and contains one or more con-\\ntainers, each running an application process. Pods are spread out across different\\nworker nodes.\\nLISTING PODS\\nBecause you can’t list individual containers, since they’re not standalone Kubernetes\\nobjects, can you list pods instead? Yes, you can. Let’s see how to tell kubectl to list\\npods in the following listing.\\nFigure 2.5\\nThe relationship between containers, pods, and physical worker nodes\\nWorker node 1\\nPod 2\\nIP: 10.1.0.2\\nContainer 1\\nContainer 2\\nPod 3\\nIP: 10.1.0.3\\nContainer 1\\nPod 1\\nIP: 10.1.0.1\\nContainer\\nWorker node 2\\nPod 5\\nIP: 10.1.1.2\\nContainer 1\\nPod 6\\nIP: 10.1.1.3\\nContainer 1\\nPod 4\\nIP: 10.1.1.1\\nContainer\\nContainer 2\\nContainer 2\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 76,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '44\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\n$ kubectl get pods\\nNAME          READY     STATUS    RESTARTS   AGE\\nkubia-4jfyf   0/1       Pending   0          1m\\nThis is your pod. Its status is still Pending and the pod’s single container is shown as\\nnot ready yet (this is what the 0/1 in the READY column means). The reason why the\\npod isn’t running yet is because the worker node the pod has been assigned to is\\ndownloading the container image before it can run it. When the download is finished,\\nthe pod’s container will be created and then the pod will transition to the Running\\nstate, as shown in the following listing.\\n$ kubectl get pods\\nNAME          READY     STATUS    RESTARTS   AGE\\nkubia-4jfyf   1/1       Running   0          5m\\nTo see more information about the pod, you can also use the kubectl describe pod\\ncommand, like you did earlier for one of the worker nodes. If the pod stays stuck in\\nthe Pending status, it might be that Kubernetes can’t pull the image from the registry.\\nIf you’re using your own image, make sure it’s marked as public on Docker Hub. To\\nmake sure the image can be pulled successfully, try pulling the image manually with\\nthe docker pull command on another machine. \\nUNDERSTANDING WHAT HAPPENED BEHIND THE SCENES\\nTo help you visualize what transpired, look at figure 2.6. It shows both steps you had to\\nperform to get a container image running inside Kubernetes. First, you built the\\nimage and pushed it to Docker Hub. This was necessary because building the image\\non your local machine only makes it available on your local machine, but you needed\\nto make it accessible to the Docker daemons running on your worker nodes.\\n When you ran the kubectl command, it created a new ReplicationController\\nobject in the cluster by sending a REST HTTP request to the Kubernetes API server.\\nThe ReplicationController then created a new pod, which was then scheduled to one\\nof the worker nodes by the Scheduler. The Kubelet on that node saw that the pod was\\nscheduled to it and instructed Docker to pull the specified image from the registry\\nbecause the image wasn’t available locally. After downloading the image, Docker cre-\\nated and ran the container. \\n The other two nodes are displayed to show context. They didn’t play any role in\\nthe process, because the pod wasn’t scheduled to them.\\nDEFINITION\\nThe term scheduling means assigning the pod to a node. The\\npod is run immediately, not at a time in the future as the term might lead you\\nto believe.\\nListing 2.14\\nListing pods\\nListing 2.15\\nListing pods again to see if the pod’s status has changed\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 77,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '45\\nRunning your first app on Kubernetes\\n2.3.2\\nAccessing your web application\\nWith your pod running, how do you access it? We mentioned that each pod gets its\\nown IP address, but this address is internal to the cluster and isn’t accessible from\\noutside of it. To make the pod accessible from the outside, you’ll expose it through a\\nService object. You’ll create a special service of type LoadBalancer, because if you cre-\\nate a regular service (a ClusterIP service), like the pod, it would also only be accessi-\\nble from inside the cluster. By creating a LoadBalancer-type service, an external load\\nbalancer will be created and you can connect to the pod through the load balancer’s\\npublic IP. \\nCREATING A SERVICE OBJECT\\nTo create the service, you’ll tell Kubernetes to expose the ReplicationController you\\ncreated earlier:\\n$ kubectl expose rc kubia --type=LoadBalancer --name kubia-http\\nservice \"kubia-http\" exposed\\nFigure 2.6\\nRunning the luksa/kubia container image in Kubernetes\\nLocal dev\\nmachine\\nkubectl\\nREST API server\\nScheduler\\nMaster node(s)\\nDocker\\nKubelet\\ngke-kubia-85f6-node-0rrx\\nDocker\\nKubelet\\ngke-kubia-85f6-node-heo1\\nDocker\\nKubelet\\ngke-kubia-85f6-node-vs9f\\nDocker Hub\\n3. kubectl run kubia\\n--image=luksa/kubia\\n--port=8080\\n4.\\nissues\\nkubectl\\nREST call\\n5. Pod created\\nand scheduled\\nto a worker node\\n7. Kubelet\\ninstructs\\nDocker\\nto run the\\nimage\\n8. Docker pulls\\nand runs\\nluksa/kubia\\n6. Kubelet\\nis notiﬁed\\n1. docker push\\nluksa/kubia\\n2. Image\\nluksa/kubia\\nis pushed to\\nDocker Hub\\nDocker\\npod kubia-4jfyf\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 78,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '46\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\nNOTE\\nWe’re using the abbreviation rc instead of replicationcontroller.\\nMost resource types have an abbreviation like this so you don’t have to type\\nthe full name (for example, po for pods, svc for services, and so on).\\nLISTING SERVICES\\nThe expose command’s output mentions a service called kubia-http. Services are\\nobjects like Pods and Nodes, so you can see the newly created Service object by run-\\nning the kubectl get services command, as shown in the following listing.\\n$ kubectl get services\\nNAME         CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\\nkubernetes   10.3.240.1     <none>        443/TCP         34m\\nkubia-http   10.3.246.185   <pending>     8080:31348/TCP  4s\\nThe list shows two services. Ignore the kubernetes service for now and take a close\\nlook at the kubia-http service you created. It doesn’t have an external IP address yet,\\nbecause it takes time for the load balancer to be created by the cloud infrastructure\\nKubernetes is running on. Once the load balancer is up, the external IP address of the\\nservice should be displayed. Let’s wait a while and list the services again, as shown in\\nthe following listing.\\n$ kubectl get svc\\nNAME         CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\\nkubernetes   10.3.240.1     <none>        443/TCP         35m\\nkubia-http   10.3.246.185   104.155.74.57 8080:31348/TCP  1m\\nAha, there’s the external IP. Your application is now accessible at http:/\\n/104.155.74\\n.57:8080 from anywhere in the world. \\nNOTE\\nMinikube doesn’t support LoadBalancer services, so the service will\\nnever get an external IP. But you can access the service anyway through its\\nexternal port. How to do that is described in the next section’s tip.\\nACCESSING YOUR SERVICE THROUGH ITS EXTERNAL IP\\nYou can now send requests to your pod through the service’s external IP and port:\\n$ curl 104.155.74.57:8080\\nYou’ve hit kubia-4jfyf \\nWoohoo! Your app is now running somewhere in your three-node Kubernetes cluster\\n(or a single-node cluster if you’re using Minikube). If you don’t count the steps\\nrequired to set up the whole cluster, all it took was two simple commands to get your\\napp running and to make it accessible to users across the world.\\nListing 2.16\\nListing Services\\nListing 2.17\\nListing services again to see if an external IP has been assigned\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 79,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '47\\nRunning your first app on Kubernetes\\nTIP\\nWhen using Minikube, you can get the IP and port through which you\\ncan access the service by running minikube service kubia-http.\\nIf you look closely, you’ll see that the app is reporting the name of the pod as its host-\\nname. As already mentioned, each pod behaves like a separate independent machine\\nwith its own IP address and hostname. Even though the application is running in\\nthe worker node’s operating system, to the app it appears as though it’s running on\\na separate machine dedicated to the app itself—no other processes are running\\nalongside it.\\n2.3.3\\nThe logical parts of your system\\nUntil now, I’ve mostly explained the actual physical components of your system. You\\nhave three worker nodes, which are VMs running Docker and the Kubelet, and you\\nhave a master node that controls the whole system. Honestly, we don’t know if a single\\nmaster node is hosting all the individual components of the Kubernetes Control Plane\\nor if they’re split across multiple nodes. It doesn’t really matter, because you’re only\\ninteracting with the API server, which is accessible at a single endpoint.\\n Besides this physical view of the system, there’s also a separate, logical view of it.\\nI’ve already mentioned Pods, ReplicationControllers, and Services. All of them will be\\nexplained in the next few chapters, but let’s quickly look at how they fit together and\\nwhat roles they play in your little setup.\\nUNDERSTANDING HOW THE REPLICATIONCONTROLLER, THE POD, AND THE SERVICE FIT TOGETHER\\nAs I’ve already explained, you’re not creating and working with containers directly.\\nInstead, the basic building block in Kubernetes is the pod. But, you didn’t really cre-\\nate any pods either, at least not directly. By running the kubectl run command you\\ncreated a ReplicationController, and this ReplicationController is what created the\\nactual Pod object. To make that pod accessible from outside the cluster, you told\\nKubernetes to expose all the pods managed by that ReplicationController as a single\\nService. A rough picture of all three elements is presented in figure 2.7.\\nFigure 2.7\\nYour system consists of a ReplicationController, a Pod, and a Service.\\nPod: kubia-4jfyf\\nIP: 10.1.0.1\\nContainer\\nPort\\n8080\\nService: kubia-http\\nInternal IP: 10.3.246.185\\nExternal IP: 104.155.74.57\\nReplicationController: kubia\\nReplicas: 1\\nPort\\n8080\\nIncoming\\nrequest\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 80,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '48\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\nUNDERSTANDING THE POD AND ITS CONTAINER\\nThe main and most important component in your system is the pod. It contains only a\\nsingle container, but generally a pod can contain as many containers as you want.\\nInside the container is your Node.js process, which is bound to port 8080 and is wait-\\ning for HTTP requests. The pod has its own unique private IP address and hostname. \\nUNDERSTANDING THE ROLE OF THE REPLICATIONCONTROLLER\\nThe next component is the kubia ReplicationController. It makes sure there’s always\\nexactly one instance of your pod running. Generally, ReplicationControllers are used\\nto replicate pods (that is, create multiple copies of a pod) and keep them running. In\\nyour case, you didn’t specify how many pod replicas you want, so the Replication-\\nController created a single one. If your pod were to disappear for any reason, the\\nReplicationController would create a new pod to replace the missing one. \\nUNDERSTANDING WHY YOU NEED A SERVICE\\nThe third component of your system is the kubia-http service. To understand why\\nyou need services, you need to learn a key detail about pods. They’re ephemeral. A\\npod may disappear at any time—because the node it’s running on has failed, because\\nsomeone deleted the pod, or because the pod was evicted from an otherwise healthy\\nnode. When any of those occurs, a missing pod is replaced with a new one by the\\nReplicationController, as described previously. This new pod gets a different IP\\naddress from the pod it’s replacing. This is where services come in—to solve the prob-\\nlem of ever-changing pod IP addresses, as well as exposing multiple pods at a single\\nconstant IP and port pair. \\n When a service is created, it gets a static IP, which never changes during the lifetime of\\nthe service. Instead of connecting to pods directly, clients should connect to the service\\nthrough its constant IP address. The service makes sure one of the pods receives the con-\\nnection, regardless of where the pod is currently running (and what its IP address is). \\n Services represent a static location for a group of one or more pods that all provide\\nthe same service. Requests coming to the IP and port of the service will be forwarded\\nto the IP and port of one of the pods belonging to the service at that moment.  \\n2.3.4\\nHorizontally scaling the application\\nYou now have a running application, monitored and kept running by a Replication-\\nController and exposed to the world through a service. Now let’s make additional\\nmagic happen. \\n One of the main benefits of using Kubernetes is the simplicity with which you can\\nscale your deployments. Let’s see how easy it is to scale up the number of pods. You’ll\\nincrease the number of running instances to three. \\n Your pod is managed by a ReplicationController. Let’s see it with the kubectl get\\ncommand:\\n$ kubectl get replicationcontrollers\\nNAME        DESIRED    CURRENT   AGE\\nkubia       1          1         17m\\n \\nwww.allitebooks.com\\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 81,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '49\\nRunning your first app on Kubernetes\\nThe list shows a single ReplicationController called kubia. The DESIRED column\\nshows the number of pod replicas you want the ReplicationController to keep,\\nwhereas the CURRENT column shows the actual number of pods currently running. In\\nyour case, you wanted to have a single replica of the pod running, and exactly one\\nreplica is currently running. \\nINCREASING THE DESIRED REPLICA COUNT\\nTo scale up the number of replicas of your pod, you need to change the desired\\nreplica count on the ReplicationController like this:\\n$ kubectl scale rc kubia --replicas=3\\nreplicationcontroller \"kubia\" scaled\\nYou’ve now told Kubernetes to make sure three instances of your pod are always run-\\nning. Notice that you didn’t instruct Kubernetes what action to take. You didn’t tell it\\nto add two more pods. You only set the new desired number of instances and let\\nKubernetes determine what actions it needs to take to achieve the requested state. \\n This is one of the most fundamental Kubernetes principles. Instead of telling\\nKubernetes exactly what actions it should perform, you’re only declaratively changing\\nthe desired state of the system and letting Kubernetes examine the current actual\\nstate and reconcile it with the desired state. This is true across all of Kubernetes.\\nSEEING THE RESULTS OF THE SCALE-OUT\\nBack to your replica count increase. Let’s list the ReplicationControllers again to see\\nthe updated replica count:\\n$ kubectl get rc\\nNAME        DESIRED    CURRENT   READY   AGE\\nkubia       3          3         2       17m\\nBecause the actual number of pods has already been increased to three (as evident\\nfrom the CURRENT column), listing all the pods should now show three pods instead\\nof one:\\n$ kubectl get pods\\nNAME          READY     STATUS    RESTARTS   AGE\\nkubia-hczji   1/1       Running   0          7s\\nkubia-iq9y6   0/1       Pending   0          7s\\nkubia-4jfyf   1/1       Running   0          18m\\nListing all the resource types with kubectl get\\nYou’ve been using the same basic kubectl get command to list things in your cluster.\\nYou’ve used this command to list Node, Pod, Service and ReplicationController\\nobjects. You can get a list of all the possible object types by invoking kubectl get\\nwithout specifying the type. You can then use those types with various kubectl\\ncommands such as get, describe, and so on. The list also shows the abbreviations\\nI mentioned earlier.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 82,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '50\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\nAs you can see, three pods exist instead of one. Two are already running, one is still\\npending, but should be ready in a few moments, as soon as the container image is\\ndownloaded and the container is started. \\n As you can see, scaling an application is incredibly simple. Once your app is run-\\nning in production and a need to scale the app arises, you can add additional\\ninstances with a single command without having to install and run additional copies\\nmanually. \\n Keep in mind that the app itself needs to support being scaled horizontally. Kuber-\\nnetes doesn’t magically make your app scalable; it only makes it trivial to scale the app\\nup or down. \\nSEEING REQUESTS HIT ALL THREE PODS WHEN HITTING THE SERVICE\\nBecause you now have multiple instances of your app running, let’s see what happens\\nif you hit the service URL again. Will you always hit the same app instance or not?\\n$ curl 104.155.74.57:8080\\nYou’ve hit kubia-hczji\\n$ curl 104.155.74.57:8080\\nYou’ve hit kubia-iq9y6\\n$ curl 104.155.74.57:8080\\nYou’ve hit kubia-iq9y6\\n$ curl 104.155.74.57:8080\\nYou’ve hit kubia-4jfyf   \\nRequests are hitting different pods randomly. This is what services in Kubernetes do\\nwhen more than one pod instance backs them. They act as a load balancer standing in\\nfront of multiple pods. When there’s only one pod, services provide a static address\\nfor the single pod. Whether a service is backed by a single pod or a group of pods,\\nthose pods come and go as they’re moved around the cluster, which means their IP\\naddresses change, but the service is always there at the same address. This makes it\\neasy for clients to connect to the pods, regardless of how many exist and how often\\nthey change location.\\nVISUALIZING THE NEW STATE OF YOUR SYSTEM\\nLet’s visualize your system again to see what’s changed from before. Figure 2.8\\nshows the new state of your system. You still have a single service and a single\\nReplicationController, but you now have three instances of your pod, all managed\\nby the ReplicationController. The service no longer sends all requests to a single\\npod, but spreads them across all three pods as shown in the experiment with curl\\nin the previous section.\\n As an exercise, you can now try spinning up additional instances by increasing the\\nReplicationController’s replica count even further and then scaling back down.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 83,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '51\\nRunning your first app on Kubernetes\\n2.3.5\\nExamining what nodes your app is running on \\nYou may be wondering what nodes your pods have been scheduled to. In the Kuber-\\nnetes world, what node a pod is running on isn’t that important, as long as it gets\\nscheduled to a node that can provide the CPU and memory the pod needs to run\\nproperly. \\n Regardless of the node they’re scheduled to, all the apps running inside contain-\\ners have the same type of OS environment. Each pod has its own IP and can talk to\\nany other pod, regardless of whether that other pod is also running on the same node\\nor on a different one. Each pod is provided with the requested amount of computa-\\ntional resources, so whether those resources are provided by one node or another\\ndoesn’t make any difference. \\nDISPLAYING THE POD IP AND THE POD’S NODE WHEN LISTING PODS\\nIf you’ve been paying close attention, you probably noticed that the kubectl get pods\\ncommand doesn’t even show any information about the nodes the pods are scheduled\\nto. This is because it’s usually not an important piece of information.\\n But you can request additional columns to display using the -o wide option. When\\nlisting pods, this option shows the pod’s IP and the node the pod is running on:\\n$ kubectl get pods -o wide\\nNAME          READY   STATUS    RESTARTS   AGE   IP         NODE\\nkubia-hczji   1/1     Running   0          7s    10.1.0.2   gke-kubia-85...\\nPod: kubia-4jfyf\\nIP: 10.1.0.1\\nContainer\\nService: kubia-http\\nInternal IP: 10.3.246.185\\nExternal IP: 104.155.74.57\\nPort\\n8080\\nIncoming\\nrequest\\nPod: kubia-hczji\\nIP: 10.1.0.2\\nContainer\\nPod: kubia-iq9y6\\nIP: 10.1.0.3\\nContainer\\nReplicationController: kubia\\nReplicas: 3\\nPort\\n8080\\nPort\\n8080\\nPort\\n8080\\nFigure 2.8\\nThree instances of a pod managed by the same ReplicationController and exposed \\nthrough a single service IP and port.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 84,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '52\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\nINSPECTING OTHER DETAILS OF A POD WITH KUBECTL DESCRIBE\\nYou can also see the node by using the kubectl describe command, which shows\\nmany other details of the pod, as shown in the following listing.\\n$ kubectl describe pod kubia-hczji\\nName:        kubia-hczji\\nNamespace:   default\\nNode:        gke-kubia-85f6-node-vs9f/10.132.0.3    \\nStart Time:  Fri, 29 Apr 2016 14:12:33 +0200\\nLabels:      run=kubia\\nStatus:      Running\\nIP:          10.1.0.2\\nControllers: ReplicationController/kubia\\nContainers:  ...\\nConditions:\\n  Type       Status\\n  Ready      True \\nVolumes: ...\\nEvents: ...\\nThis shows, among other things, the node the pod has been scheduled to, the time\\nwhen it was started, the image(s) it’s running, and other useful information.\\n2.3.6\\nIntroducing the Kubernetes dashboard\\nBefore we wrap up this initial hands-on chapter, let’s look at another way of exploring\\nyour Kubernetes cluster.\\n Up to now, you’ve only been using the kubectl command-line tool. If you’re more\\ninto graphical web user interfaces, you’ll be glad to hear that Kubernetes also comes\\nwith a nice (but still evolving) web dashboard.\\n The dashboard allows you to list all the Pods, ReplicationControllers, Services, and\\nother objects deployed in your cluster, as well as to create, modify, and delete them.\\nFigure 2.9 shows the dashboard.\\n Although you won’t use the dashboard in this book, you can open it up any time to\\nquickly see a graphical view of what’s deployed in your cluster after you create or mod-\\nify objects through kubectl.\\nACCESSING THE DASHBOARD WHEN RUNNING KUBERNETES IN GKE\\nIf you’re using Google Kubernetes Engine, you can find out the URL of the dash-\\nboard through the kubectl cluster-info command, which we already introduced:\\n$ kubectl cluster-info | grep dashboard\\nkubernetes-dashboard is running at https://104.155.108.191/api/v1/proxy/\\n➥ namespaces/kube-system/services/kubernetes-dashboard\\nListing 2.18\\nDescribing a pod with kubectl describe\\nHere’s the node the pod \\nhas been scheduled to.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 85,\n",
       "  'img_cnt': 1,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '53\\nSummary\\nIf you open this URL in a browser, you’re presented with a username and password\\nprompt. You’ll find the username and password by running the following command:\\n$ gcloud container clusters describe kubia | grep -E \"(username|password):\"\\n  password: 32nENgreEJ632A12          \\n  username: admin                     \\nACCESSING THE DASHBOARD WHEN USING MINIKUBE\\nTo open the dashboard in your browser when using Minikube to run your Kubernetes\\ncluster, run the following command:\\n$ minikube dashboard\\nThe dashboard will open in your default browser. Unlike with GKE, you won’t need to\\nenter any credentials to access it.\\n2.4\\nSummary\\nHopefully, this initial hands-on chapter has shown you that Kubernetes isn’t a compli-\\ncated platform to use, and you’re ready to learn in depth about all the things it can\\nprovide. After reading this chapter, you should now know how to\\n\\uf0a1Pull and run any publicly available container image\\n\\uf0a1Package your apps into container images and make them available to anyone by\\npushing the images to a remote image registry\\nFigure 2.9\\nScreenshot of the Kubernetes web-based dashboard\\nThe username and password \\nfor the dashboard\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 86,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '54\\nCHAPTER 2\\nFirst steps with Docker and Kubernetes\\n\\uf0a1Enter a running container and inspect its environment\\n\\uf0a1Set up a multi-node Kubernetes cluster on Google Kubernetes Engine\\n\\uf0a1Configure an alias and tab completion for the kubectl command-line tool\\n\\uf0a1List and inspect Nodes, Pods, Services, and ReplicationControllers in a Kuber-\\nnetes cluster\\n\\uf0a1Run a container in Kubernetes and make it accessible from outside the cluster\\n\\uf0a1Have a basic sense of how Pods, ReplicationControllers, and Services relate to\\none another\\n\\uf0a1Scale an app horizontally by changing the ReplicationController’s replica count\\n\\uf0a1Access the web-based Kubernetes dashboard on both Minikube and GKE \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 87,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '55\\nPods: running\\ncontainers in Kubernetes\\nThe previous chapter should have given you a rough picture of the basic compo-\\nnents you create in Kubernetes and at least an outline of what they do. Now, we’ll\\nstart reviewing all types of Kubernetes objects (or resources) in greater detail, so\\nyou’ll understand when, how, and why to use each of them. We’ll start with pods,\\nbecause they’re the central, most important, concept in Kubernetes. Everything\\nelse either manages, exposes, or is used by pods. \\nThis chapter covers\\n\\uf0a1Creating, running, and stopping pods\\n\\uf0a1Organizing pods and other resources with labels\\n\\uf0a1Performing an operation on all pods with a \\nspecific label\\n\\uf0a1Using namespaces to split pods into non-\\noverlapping groups\\n\\uf0a1Scheduling pods onto specific types of worker \\nnodes\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 88,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '56\\nCHAPTER 3\\nPods: running containers in Kubernetes\\n3.1\\nIntroducing pods\\nYou’ve already learned that a pod is a co-located group of containers and represents\\nthe basic building block in Kubernetes. Instead of deploying containers individually,\\nyou always deploy and operate on a pod of containers. We’re not implying that a pod\\nalways includes more than one container—it’s common for pods to contain only a sin-\\ngle container. The key thing about pods is that when a pod does contain multiple con-\\ntainers, all of them are always run on a single worker node—it never spans multiple\\nworker nodes, as shown in figure 3.1.\\n3.1.1\\nUnderstanding why we need pods\\nBut why do we even need pods? Why can’t we use containers directly? Why would we\\neven need to run multiple containers together? Can’t we put all our processes into a\\nsingle container? We’ll answer those questions now.\\nUNDERSTANDING WHY MULTIPLE CONTAINERS ARE BETTER THAN ONE CONTAINER RUNNING \\nMULTIPLE PROCESSES\\nImagine an app consisting of multiple processes that either communicate through\\nIPC (Inter-Process Communication) or through locally stored files, which requires\\nthem to run on the same machine. Because in Kubernetes you always run processes in\\ncontainers and each container is much like an isolated machine, you may think it\\nmakes sense to run multiple processes in a single container, but you shouldn’t do that. \\n Containers are designed to run only a single process per container (unless the\\nprocess itself spawns child processes). If you run multiple unrelated processes in a\\nsingle container, it is your responsibility to keep all those processes running, man-\\nage their logs, and so on. For example, you’d have to include a mechanism for auto-\\nmatically restarting individual processes if they crash. Also, all those processes would\\nlog to the same standard output, so you’d have a hard time figuring out what pro-\\ncess logged what. \\nNode 1\\nPod 2\\nIP: 10.1.0.2\\nContainer 1\\nContainer 2\\nPod 1\\nIP: 10.1.0.1\\nContainer\\nNode 2\\nPod 4\\nIP: 10.1.1.2\\nContainer 1\\nPod 5\\nIP: 10.1.1.3\\nContainer 1\\nContainer 2\\nPod 3\\nContainer 1\\nContainer 2\\nFigure 3.1\\nAll containers of a pod run on the same node. A pod never spans two nodes.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Container Container 1\\nContainer 1\\nContainer 2\\nPod 1 Pod 2\\nIP: 10.1.0.1 IP: 10.1.0.2  \\\n",
       "   0                                               None                                        \n",
       "   \n",
       "       Col1  \\\n",
       "   0  Pod 3   \n",
       "   \n",
       "     Container 1 Container 1\\nContainer 2\\nContainer 2\\nPod 4 Pod 5\\nIP: 10.1.1.2 IP: 10.1.1.3  \n",
       "   0                                               None                                         ]},\n",
       " {'page': 89,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '57\\nIntroducing pods\\n Therefore, you need to run each process in its own container. That’s how Docker\\nand Kubernetes are meant to be used. \\n3.1.2\\nUnderstanding pods\\nBecause you’re not supposed to group multiple processes into a single container, it’s\\nobvious you need another higher-level construct that will allow you to bind containers\\ntogether and manage them as a single unit. This is the reasoning behind pods. \\n A pod of containers allows you to run closely related processes together and pro-\\nvide them with (almost) the same environment as if they were all running in a single\\ncontainer, while keeping them somewhat isolated. This way, you get the best of both\\nworlds. You can take advantage of all the features containers provide, while at the\\nsame time giving the processes the illusion of running together. \\nUNDERSTANDING THE PARTIAL ISOLATION BETWEEN CONTAINERS OF THE SAME POD\\nIn the previous chapter, you learned that containers are completely isolated from\\neach other, but now you see that you want to isolate groups of containers instead of\\nindividual ones. You want containers inside each group to share certain resources,\\nalthough not all, so that they’re not fully isolated. Kubernetes achieves this by config-\\nuring Docker to have all containers of a pod share the same set of Linux namespaces\\ninstead of each container having its own set. \\n Because all containers of a pod run under the same Network and UTS namespaces\\n(we’re talking about Linux namespaces here), they all share the same hostname and\\nnetwork interfaces. Similarly, all containers of a pod run under the same IPC namespace\\nand can communicate through IPC. In the latest Kubernetes and Docker versions, they\\ncan also share the same PID namespace, but that feature isn’t enabled by default. \\nNOTE\\nWhen containers of the same pod use separate PID namespaces, you\\nonly see the container’s own processes when running ps aux in the container.\\nBut when it comes to the filesystem, things are a little different. Because most of the\\ncontainer’s filesystem comes from the container image, by default, the filesystem of\\neach container is fully isolated from other containers. However, it’s possible to have\\nthem share file directories using a Kubernetes concept called a Volume, which we’ll\\ntalk about in chapter 6.\\nUNDERSTANDING HOW CONTAINERS SHARE THE SAME IP AND PORT SPACE\\nOne thing to stress here is that because containers in a pod run in the same Network\\nnamespace, they share the same IP address and port space. This means processes run-\\nning in containers of the same pod need to take care not to bind to the same port\\nnumbers or they’ll run into port conflicts. But this only concerns containers in the\\nsame pod. Containers of different pods can never run into port conflicts, because\\neach pod has a separate port space. All the containers in a pod also have the same\\nloopback network interface, so a container can communicate with other containers in\\nthe same pod through localhost.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Docker',\n",
       "    'description': 'Containerization technology',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Group of containers that share resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Containers',\n",
       "    'description': 'Lightweight and standalone execution environment',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Linux namespaces',\n",
       "    'description': 'Resource isolation mechanism for Linux',\n",
       "    'category': 'hardware/software'},\n",
       "   {'entity': 'Network namespace',\n",
       "    'description': 'Mechanism to isolate network resources',\n",
       "    'category': 'hardware/software'},\n",
       "   {'entity': 'UTS namespace',\n",
       "    'description': 'Mechanism to isolate hostname and network interface resources',\n",
       "    'category': 'hardware/software'},\n",
       "   {'entity': 'IPC namespace',\n",
       "    'description': 'Mechanism to allow inter-process communication between containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PID namespace',\n",
       "    'description': 'Mechanism to isolate process ID resources',\n",
       "    'category': 'hardware/software'},\n",
       "   {'entity': 'Volumes',\n",
       "    'description': 'Kubernetes concept for sharing file directories between containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'IP address',\n",
       "    'description': 'Unique identifier for a network interface',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Port space',\n",
       "    'description': 'Set of available port numbers for container communication',\n",
       "    'category': 'hardware/software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Docker\",\\n    \"description\": \"configure to share the same set of Linux namespaces instead of each container having its own\",\\n    \"destination_entity\": \"Linux namespaces\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"achieves partial isolation between containers by configuring Docker\",\\n    \"destination_entity\": \"Docker\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"run under the same Network and UTS namespaces, sharing the same hostname and network interfaces\",\\n    \"destination_entity\": \"Network namespace\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"can communicate through IPC because they run under the same IPC namespace\",\\n    \"destination_entity\": \"IPC namespace\"\\n  },\\n  {\\n    \"source_entity\": \"Docker\",\\n    \"description\": \"allows containers in a pod to share the same IP address and port space\",\\n    \"destination_entity\": \"Port space\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"allow running closely related processes together while providing them with almost the same environment as if they were all running in a single container\",\\n    \"destination_entity\": \"Containers\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"configures Docker to have containers of a pod share the same set of Linux namespaces instead of each container having its own\",\\n    \"destination_entity\": \"Linux namespaces\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"can run under separate PID namespaces, but this feature isn\\'t enabled by default\",\\n    \"destination_entity\": \"PID namespace\"\\n  },\\n  {\\n    \"source_entity\": \"Docker\",\\n    \"description\": \"allows containers in a pod to share the same IP address and port space\",\\n    \"destination_entity\": \"IP address\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"can have their filesystems fully isolated from other containers by default, but can also share file directories using Volumes\",\\n    \"destination_entity\": \"Volumes\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"concept called a Volume allows having containers in the same pod share file directories\",\\n    \"destination_entity\": \"Volumes\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"can communicate with other containers in the same pod through localhost because they have the same loopback network interface\",\\n    \"destination_entity\": \"Network namespace\"\\n  }\\n]\\n```'},\n",
       " {'page': 90,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '58\\nCHAPTER 3\\nPods: running containers in Kubernetes\\nINTRODUCING THE FLAT INTER-POD NETWORK\\nAll pods in a Kubernetes cluster reside in a single flat, shared, network-address space\\n(shown in figure 3.2), which means every pod can access every other pod at the other\\npod’s IP address. No NAT (Network Address Translation) gateways exist between them.\\nWhen two pods send network packets between each other, they’ll each see the actual\\nIP address of the other as the source IP in the packet.\\nConsequently, communication between pods is always simple. It doesn’t matter if two\\npods are scheduled onto a single or onto different worker nodes; in both cases the\\ncontainers inside those pods can communicate with each other across the flat NAT-\\nless network, much like computers on a local area network (LAN), regardless of the\\nactual inter-node network topology. Like a computer on a LAN, each pod gets its own\\nIP address and is accessible from all other pods through this network established spe-\\ncifically for pods. This is usually achieved through an additional software-defined net-\\nwork layered on top of the actual network.\\n To sum up what’s been covered in this section: pods are logical hosts and behave\\nmuch like physical hosts or VMs in the non-container world. Processes running in the\\nsame pod are like processes running on the same physical or virtual machine, except\\nthat each process is encapsulated in a container. \\n3.1.3\\nOrganizing containers across pods properly\\nYou should think of pods as separate machines, but where each one hosts only a cer-\\ntain app. Unlike the old days, when we used to cram all sorts of apps onto the same\\nhost, we don’t do that with pods. Because pods are relatively lightweight, you can have\\nas many as you need without incurring almost any overhead. Instead of stuffing every-\\nthing into a single pod, you should organize apps into multiple pods, where each one\\ncontains only tightly related components or processes.\\nNode 1\\nPod A\\nIP: 10.1.1.6\\nContainer 1\\nContainer 2\\nPod B\\nIP: 10.1.1.7\\nContainer 1\\nContainer 2\\nNode 2\\nFlat network\\nPod C\\nIP: 10.1.2.5\\nContainer 1\\nContainer 2\\nPod D\\nIP: 10.1.2.7\\nContainer 1\\nContainer 2\\nFigure 3.2\\nEach pod gets a routable IP address and all other pods see the pod under \\nthat IP address.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Container 1 Container 1\\nContainer 2 Container 2\\nPod A Pod B\\nIP: 10.1.1.6 IP: 10.1.1.7  \\\n",
       "   0                                                                                            \n",
       "   1                                       Flat network                                         \n",
       "   \n",
       "        Col1  \\\n",
       "   0  Node 1   \n",
       "   1    None   \n",
       "   \n",
       "     Container 1 Container 1\\nContainer 2 Container 2\\nPod C Pod D\\nIP: 10.1.2.5 IP: 10.1.2.7  \\\n",
       "   0                                               None                                         \n",
       "   1                                               None                                         \n",
       "   \n",
       "        Col3  Col4  \n",
       "   0  Node 2        \n",
       "   1    None  None  ],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'A container orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'The basic execution unit in a Kubernetes cluster, consisting of one or more containers.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Containers',\n",
       "    'description': 'Lightweight and standalone executable components that run in isolation from other containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Flat network',\n",
       "    'description': 'A shared, NAT-less network space where all pods can access each other at their IP addresses.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'A physical or virtual machine that hosts one or more pods and their containers.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'IP address',\n",
       "    'description': 'A unique identifier for a pod or container, used to communicate with other pods and containers on the same network.',\n",
       "    'category': 'networking'},\n",
       "   {'entity': 'Pod A',\n",
       "    'description': 'An example pod that hosts two containers and has an IP address of 10.1.1.6.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pod B',\n",
       "    'description': 'Another example pod that hosts two containers and has an IP address of 10.1.1.7.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Container 1',\n",
       "    'description': 'An example container hosted by Pod A or Pod B, with a specific process running inside it.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Container 2',\n",
       "    'description': 'Another example container hosted by Pod A or Pod B, with a different process running inside it.',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Pod A\", \"description\": \"has IP address\", \"destination_entity\": \"IP: 10.1.1.6\"},\\n  {\"source_entity\": \"Pod B\", \"description\": \"has IP address\", \"destination_entity\": \"IP: 10.1.1.7\"},\\n  {\"source_entity\": \"Pod C\", \"description\": \"has IP address\", \"destination_entity\": \"IP: 10.1.2.5\"},\\n  {\"source_entity\": \"Pod D\", \"description\": \"has IP address\", \"destination_entity\": \"IP: 10.1.2.7\"},\\n  {\"source_entity\": \"Container 1 in Pod A\", \"description\": \"communicates with\", \"destination_entity\": \"Container 2 in Pod A\"},\\n  {\"source_entity\": \"Container 1 in Pod B\", \"description\": \"communicates with\", \"destination_entity\": \"Container 2 in Pod B\"},\\n  {\"source_entity\": \"Pod B\", \"description\": \"hosts\", \"destination_entity\": \"Container 1\"},\\n  {\"source_entity\": \"Pod B\", \"description\": \"hosts\", \"destination_entity\": \"Container 2\"},\\n  {\"source_entity\": \"Node 1\", \"description\": \"hosts\", \"destination_entity\": \"Pod A\"},\\n  {\"source_entity\": \"Node 1\", \"description\": \"has IP address\", \"destination_entity\": \"10.1.1.6\"},\\n  {\"source_entity\": \"Flat network\", \"description\": \"connects\", \"destination_entity\": \"multiple pods\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"manages\", \"destination_entity\": \"pods and containers\"},\\n  {\"source_entity\": \"Pod A\", \"description\": \"is connected to\", \"destination_entity\": \"Flat network\"},\\n  {\"source_entity\": \"Pod B\", \"description\": \"is connected to\", \"destination_entity\": \"Flat network\"}\\n]\\n```\\n\\nNote that I\\'ve tried to be as specific as possible with the source and destination entities, while still making sense in the context of the document. Let me know if you\\'d like me to clarify any of these relations!'},\n",
       " {'page': 91,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '59\\nIntroducing pods\\n Having said that, do you think a multi-tier application consisting of a frontend\\napplication server and a backend database should be configured as a single pod or as\\ntwo pods?\\nSPLITTING MULTI-TIER APPS INTO MULTIPLE PODS\\nAlthough nothing is stopping you from running both the frontend server and the\\ndatabase in a single pod with two containers, it isn’t the most appropriate way. We’ve\\nsaid that all containers of the same pod always run co-located, but do the web server\\nand the database really need to run on the same machine? The answer is obviously no,\\nso you don’t want to put them into a single pod. But is it wrong to do so regardless? In\\na way, it is.\\n If both the frontend and backend are in the same pod, then both will always be\\nrun on the same machine. If you have a two-node Kubernetes cluster and only this sin-\\ngle pod, you’ll only be using a single worker node and not taking advantage of the\\ncomputational resources (CPU and memory) you have at your disposal on the second\\nnode. Splitting the pod into two would allow Kubernetes to schedule the frontend to\\none node and the backend to the other node, thereby improving the utilization of\\nyour infrastructure.\\nSPLITTING INTO MULTIPLE PODS TO ENABLE INDIVIDUAL SCALING\\nAnother reason why you shouldn’t put them both into a single pod is scaling. A pod is\\nalso the basic unit of scaling. Kubernetes can’t horizontally scale individual contain-\\ners; instead, it scales whole pods. If your pod consists of a frontend and a backend con-\\ntainer, when you scale up the number of instances of the pod to, let’s say, two, you end\\nup with two frontend containers and two backend containers. \\n Usually, frontend components have completely different scaling requirements\\nthan the backends, so we tend to scale them individually. Not to mention the fact that\\nbackends such as databases are usually much harder to scale compared to (stateless)\\nfrontend web servers. If you need to scale a container individually, this is a clear indi-\\ncation that it needs to be deployed in a separate pod. \\nUNDERSTANDING WHEN TO USE MULTIPLE CONTAINERS IN A POD\\nThe main reason to put multiple containers into a single pod is when the application\\nconsists of one main process and one or more complementary processes, as shown in\\nfigure 3.3.\\nPod\\nMain container\\nSupporting\\ncontainer 1\\nSupporting\\ncontainer 2\\nVolume\\nFigure 3.3\\nPods should contain tightly coupled \\ncontainers, usually a main container and containers \\nthat support the main one.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'pod',\n",
       "    'description': 'A pod is the basic execution unit in Kubernetes, consisting of one or more containers.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubernetes cluster',\n",
       "    'description': 'A set of worker nodes running the Kubernetes control plane and making up a Kubernetes deployment.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'worker node',\n",
       "    'description': 'A machine that runs containers for a Kubernetes cluster.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A lightweight and stand-alone execution environment for applications.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'frontend application server',\n",
       "    'description': 'The part of an application that handles user input and displays the output to the user.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'backend database',\n",
       "    'description': 'The part of an application that stores and manages data for the frontend to use.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'CPU',\n",
       "    'description': 'Central Processing Unit, a component of a computer system responsible for executing instructions.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'memory',\n",
       "    'description': 'A type of hardware storage that temporarily holds data while it is being processed or used.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'horizontal scaling',\n",
       "    'description': 'The process of increasing the number of available resources (e.g., containers, nodes) to handle a larger workload.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'vertical scaling',\n",
       "    'description': 'The process of increasing the power or capacity of individual resources (e.g., containers, nodes).',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"run co-located containers\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"frontend application server\",\\n    \"description\": \"should not be configured as a single pod with backend database\",\\n    \"destination_entity\": \"backend database\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"improve utilization of infrastructure by splitting into multiple pods\",\\n    \"destination_entity\": \"Kubernetes cluster\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"enable individual scaling by splitting into multiple containers\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"frontend application server\",\\n    \"description\": \"usually have different scaling requirements than backend database\",\\n    \"destination_entity\": \"backend database\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"contain tightly coupled containers, usually a main container and supporting containers\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes cluster\",\\n    \"description\": \"utilize worker nodes to run multiple pods\",\\n    \"destination_entity\": \"worker node\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"scale whole pods, not individual containers\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"horizontal scaling\",\\n    \"description\": \"scale individual containers by deploying in separate pod\",\\n    \"destination_entity\": \"vertical scaling\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes cluster\",\\n    \"description\": \"utilize CPU and memory of multiple worker nodes\",\\n    \"destination_entity\": \"CPU\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes cluster\",\\n    \"description\": \"utilize CPU and memory of multiple worker nodes\",\\n    \"destination_entity\": \"memory\"\\n  }\\n]\\n```'},\n",
       " {'page': 92,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '60\\nCHAPTER 3\\nPods: running containers in Kubernetes\\nFor example, the main container in a pod could be a web server that serves files from\\na certain file directory, while an additional container (a sidecar container) periodi-\\ncally downloads content from an external source and stores it in the web server’s\\ndirectory. In chapter 6 you’ll see that you need to use a Kubernetes Volume that you\\nmount into both containers. \\n Other examples of sidecar containers include log rotators and collectors, data pro-\\ncessors, communication adapters, and others.\\nDECIDING WHEN TO USE MULTIPLE CONTAINERS IN A POD\\nTo recap how containers should be grouped into pods—when deciding whether to\\nput two containers into a single pod or into two separate pods, you always need to ask\\nyourself the following questions:\\n\\uf0a1Do they need to be run together or can they run on different hosts?\\n\\uf0a1Do they represent a single whole or are they independent components?\\n\\uf0a1Must they be scaled together or individually? \\nBasically, you should always gravitate toward running containers in separate pods,\\nunless a specific reason requires them to be part of the same pod. Figure 3.4 will help\\nyou memorize this.\\nAlthough pods can contain multiple containers, to keep things simple for now, you’ll\\nonly be dealing with single-container pods in this chapter. You’ll see how multiple\\ncontainers are used in the same pod later, in chapter 6. \\nPod\\nFrontend\\nprocess\\nBackend\\nprocess\\nContainer\\nPod\\nFrontend\\nprocess\\nFrontend\\ncontainer\\nFrontend pod\\nFrontend\\nprocess\\nFrontend\\ncontainer\\nBackend pod\\nBackend\\nprocess\\nBackend\\ncontainer\\nBackend\\nprocess\\nBackend\\ncontainer\\nFigure 3.4\\nA container shouldn’t run multiple processes. A pod shouldn’t contain multiple \\ncontainers if they don’t need to run on the same machine.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'a logical host in Kubernetes that can contain one or more containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'a lightweight and standalone executable package of software, similar to a Docker container',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Process',\n",
       "    'description': 'an instance of a program running on a computer, in this case within a container or pod',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubernetes Volume',\n",
       "    'description': 'a mechanism for persisting data across restarts and rescheduling of containers',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Sidecar Container',\n",
       "    'description': 'an additional container that runs alongside the main container in a pod, often used for logging or communication purposes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Log Rotator',\n",
       "    'description': 'a process or container that periodically rotates and manages log files',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Collector',\n",
       "    'description': 'a process or container that collects data from various sources, often used in conjunction with a sidecar container',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Data Processor',\n",
       "    'description': 'a process or container that processes and manipulates data, often used to prepare data for analysis or storage',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Communication Adapter',\n",
       "    'description': 'a process or container that facilitates communication between different components or systems',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Collector\",\\n    \"description\": \"rotates and collects logs from a file directory\",\\n    \"destination_entity\": \"Log Rotator\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes Volume\",\\n    \"description\": \"is mounted into multiple containers for data sharing\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"Sidecar Container\",\\n    \"description\": \"periodically downloads content from an external source and stores it in another container\\'s directory\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"Data Processor\",\\n    \"description\": \"processes data for a Backend process\",\\n    \"destination_entity\": \"Backend process\"\\n  },\\n  {\\n    \"source_entity\": \"Communication Adapter\",\\n    \"description\": \"adapts communication between two processes\",\\n    \"destination_entity\": \"Process\"\\n  },\\n  {\\n    \"source_entity\": \"Container\",\\n    \"description\": \"runs multiple processes (e.g. Frontend and Backend)\",\\n    \"destination_entity\": \"Process\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"can contain multiple containers for data sharing or scaling\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"Frontend process\",\\n    \"description\": \"is run in a separate pod from Backend process\",\\n    \"destination_entity\": \"Backend process\"\\n  },\\n  {\\n    \"source_entity\": \"Frontend container\",\\n    \"description\": \"runs the Frontend process and is part of a single-container pod\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Backend pod\",\\n    \"description\": \"contains multiple containers for data sharing or scaling\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"Backend process\",\\n    \"description\": \"is run in a separate pod from Frontend process\",\\n    \"destination_entity\": \"Frontend process\"\\n  }\\n]\\n```\\n\\nNote that I\\'ve included all the entities listed, even if they appear multiple times or are used in different contexts. The relations have been extracted based on the text and may not be exhaustive.'},\n",
       " {'page': 93,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '61\\nCreating pods from YAML or JSON descriptors\\n3.2\\nCreating pods from YAML or JSON descriptors\\nPods and other Kubernetes resources are usually created by posting a JSON or YAML\\nmanifest to the Kubernetes REST API endpoint. Also, you can use other, simpler ways\\nof creating resources, such as the kubectl run command you used in the previous\\nchapter, but they usually only allow you to configure a limited set of properties, not\\nall. Additionally, defining all your Kubernetes objects from YAML files makes it possi-\\nble to store them in a version control system, with all the benefits it brings.\\n To configure all aspects of each type of resource, you’ll need to know and under-\\nstand the Kubernetes API object definitions. You’ll get to know most of them as you\\nlearn about each resource type throughout this book. We won’t explain every single\\nproperty, so you should also refer to the Kubernetes API reference documentation at\\nhttp:/\\n/kubernetes.io/docs/reference/ when creating objects.\\n3.2.1\\nExamining a YAML descriptor of an existing pod\\nYou already have some existing pods you created in the previous chapter, so let’s look\\nat what a YAML definition for one of those pods looks like. You’ll use the kubectl get\\ncommand with the -o yaml option to get the whole YAML definition of the pod, as\\nshown in the following listing.\\n$ kubectl get po kubia-zxzij -o yaml\\napiVersion: v1                         \\nkind: Pod                                       \\nmetadata:                                                 \\n  annotations:                                            \\n    kubernetes.io/created-by: ...                         \\n  creationTimestamp: 2016-03-18T12:37:50Z                 \\n  generateName: kubia-                                    \\n  labels:                                                 \\n    run: kubia                                            \\n  name: kubia-zxzij                                       \\n  namespace: default                                      \\n  resourceVersion: \"294\"                                  \\n  selfLink: /api/v1/namespaces/default/pods/kubia-zxzij   \\n  uid: 3a564dc0-ed06-11e5-ba3b-42010af00004               \\nspec:                                                   \\n  containers:                                           \\n  - image: luksa/kubia                                  \\n    imagePullPolicy: IfNotPresent                       \\n    name: kubia                                         \\n    ports:                                              \\n    - containerPort: 8080                               \\n      protocol: TCP                                     \\n    resources:                                          \\n      requests:                                         \\n        cpu: 100m                                       \\nListing 3.1\\nFull YAML of a deployed pod\\nKubernetes API version used \\nin this YAML descriptor\\nType of Kubernetes \\nobject/resource\\nPod metadata (name, \\nlabels, annotations, \\nand so on)\\nPod specification/\\ncontents (list of \\npod’s containers, \\nvolumes, and so on)\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command to create resources',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'JSON or YAML manifest',\n",
       "    'description': 'descriptor for creating pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes REST API endpoint',\n",
       "    'description': 'endpoint for creating resources',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'kubectl run command',\n",
       "    'description': 'simplified way of creating resources',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubernetes API object definitions',\n",
       "    'description': 'configuration options for resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl get command',\n",
       "    'description': 'command to retrieve resource definitions',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'yaml option',\n",
       "    'description': 'flag for retrieving YAML definitions',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'Kubernetes API version used in a descriptor',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'type of Kubernetes object/resource',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Pod metadata (name, labels, annotations)',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'Pod specification/contents',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'containers',\n",
       "    'description': \"list of pod's containers\",\n",
       "    'category': 'database'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'container image',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ports',\n",
       "    'description': 'list of container ports',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'protocol',\n",
       "    'description': 'TCP protocol used in a port',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'resources',\n",
       "    'description': 'requests for CPU resources',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"You\", \"description\": \"can use kubectl run command to create resources\", \"destination_entity\": \"resources\"},\\n\\n {\"source_entity\": \"you\", \"description\": \"only allow you to configure a limited set of properties\", \"destination_entity\": \"kubectl run command\"},\\n\\n {\"source_entity\": \"kubectl get command\", \"description\": \"get the whole YAML definition of the pod\", \"destination_entity\": \"YAML definition\"},\\n\\n {\"source_entity\": \"Kubernetes API object definitions\", \"description\": \"need to know and understand to configure all aspects of each type of resource\", \"destination_entity\": \"resources\"},\\n\\n {\"source_entity\": \"kubectl get command\", \"description\": \"get the YAML descriptor of an existing pod\", \"destination_entity\": \"YAML descriptor\"},\\n\\n {\"source_entity\": \"Kubernetes REST API endpoint\", \"description\": \"post a JSON or YAML manifest to create resources\", \"destination_entity\": \"resources\"},\\n\\n {\"source_entity\": \"you\", \"description\": \"can store all your Kubernetes objects from YAML files in a version control system\", \"destination_entity\": \"version control system\"},\\n\\n {\"source_entity\": \"kubectl get command with -o yaml option\", \"description\": \"get the whole YAML definition of the pod\", \"destination_entity\": \"YAML definition\"},\\n\\n {\"source_entity\": \"Kubernetes API reference documentation\", \"description\": \"should refer to when creating objects\", \"destination_entity\": \"objects\"},\\n\\n {\"source_entity\": \"kubectl run command\", \"description\": \"usually only allow you to configure a limited set of properties\", \"destination_entity\": \"properties\"},\\n\\n {\"source_entity\": \"You\", \"description\": \"need to know and understand the Kubernetes API object definitions\", \"destination_entity\": \"Kubernetes API object definitions\"}]\\n\\nNote: I\\'ve kept the \"you\" entity as is, assuming it refers to a human user interacting with the system. If you\\'d like me to change it to something else (e.g., \"user\"), let me know!'},\n",
       " {'page': 94,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '62\\nCHAPTER 3\\nPods: running containers in Kubernetes\\n    terminationMessagePath: /dev/termination-log      \\n    volumeMounts:                                     \\n    - mountPath: /var/run/secrets/k8s.io/servacc      \\n      name: default-token-kvcqa                       \\n      readOnly: true                                  \\n  dnsPolicy: ClusterFirst                             \\n  nodeName: gke-kubia-e8fe08b8-node-txje              \\n  restartPolicy: Always                               \\n  serviceAccount: default                             \\n  serviceAccountName: default                         \\n  terminationGracePeriodSeconds: 30                   \\n  volumes:                                            \\n  - name: default-token-kvcqa                         \\n    secret:                                           \\n      secretName: default-token-kvcqa                 \\nstatus:                                                   \\n  conditions:                                             \\n  - lastProbeTime: null                                   \\n    lastTransitionTime: null                              \\n    status: \"True\"                                        \\n    type: Ready                                           \\n  containerStatuses:                                      \\n  - containerID: docker://f0276994322d247ba...            \\n    image: luksa/kubia                                    \\n    imageID: docker://4c325bcc6b40c110226b89fe...         \\n    lastState: {}                                         \\n    name: kubia                                           \\n    ready: true                                           \\n    restartCount: 0                                       \\n    state:                                                \\n      running:                                            \\n        startedAt: 2016-03-18T12:46:05Z                   \\n  hostIP: 10.132.0.4                                      \\n  phase: Running                                          \\n  podIP: 10.0.2.3                                         \\n  startTime: 2016-03-18T12:44:32Z                         \\nI know this looks complicated, but it becomes simple once you understand the basics\\nand know how to distinguish between the important parts and the minor details. Also,\\nyou can take comfort in the fact that when creating a new pod, the YAML you need to\\nwrite is much shorter, as you’ll see later.\\nINTRODUCING THE MAIN PARTS OF A POD DEFINITION\\nThe pod definition consists of a few parts. First, there’s the Kubernetes API version\\nused in the YAML and the type of resource the YAML is describing. Then, three\\nimportant sections are found in almost all Kubernetes resources:\\n\\uf0a1Metadata includes the name, namespace, labels, and other information about\\nthe pod.\\n\\uf0a1Spec contains the actual description of the pod’s contents, such as the pod’s con-\\ntainers, volumes, and other data.\\nPod specification/\\ncontents (list of \\npod’s containers, \\nvolumes, and so on)\\nDetailed status \\nof the pod and \\nits containers\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': '/dev/termination-log',\n",
       "    'description': 'Path for termination message',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'default-token-kvcqa',\n",
       "    'description': 'Service account token',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubia',\n",
       "    'description': 'Container name',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'luksa/kubia',\n",
       "    'description': 'Container image',\n",
       "    'category': 'image'},\n",
       "   {'entity': '/var/run/secrets/k8s.io/servacc',\n",
       "    'description': 'Mount path for secrets',\n",
       "    'category': 'path'},\n",
       "   {'entity': 'ClusterFirst',\n",
       "    'description': 'DNS policy',\n",
       "    'category': 'networking'},\n",
       "   {'entity': 'Always',\n",
       "    'description': 'Restart policy',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Ready',\n",
       "    'description': 'Pod condition type',\n",
       "    'category': 'condition'},\n",
       "   {'entity': 'default-token-kvcqa',\n",
       "    'description': 'Secret name',\n",
       "    'category': 'database'},\n",
       "   {'entity': '10.132.0.4',\n",
       "    'description': 'Host IP address',\n",
       "    'category': 'networking'},\n",
       "   {'entity': 'Running', 'description': 'Pod phase', 'category': 'process'},\n",
       "   {'entity': '10.0.2.3',\n",
       "    'description': 'Pod IP address',\n",
       "    'category': 'networking'},\n",
       "   {'entity': '2016-03-18T12:44:32Z',\n",
       "    'description': 'StartTime timestamp',\n",
       "    'category': 'timestamp'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"uses\", \"destination_entity\": \"API version\"},\\n  {\"source_entity\": \"Kubernetes API version\", \"description\": \"is used in\", \"destination_entity\": \"YAML\"},\\n  {\"source_entity\": \"YAML\", \"description\": \"describes the type of resource\", \"destination_entity\": \"resource\"},\\n  {\"source_entity\": \"Metadata\", \"description\": \"includes the name and namespace of the pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Spec\", \"description\": \"contains the actual description of the pod\\'s contents\", \"destination_entity\": \"pod contents\"},\\n  {\"source_entity\": \"Pod specification\", \"description\": \"includes a list of the pod\\'s containers, volumes, and other data\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Container status\", \"description\": \"provides detailed status of the pod and its containers\", \"destination_entity\": \"pod and containers\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"runs a container with image luksa/kubia\", \"destination_entity\": \"container kubia\"},\\n  {\"source_entity\": \"luksa/kubia\", \"description\": \"is the image of the running container\", \"destination_entity\": \"container kubia\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"sets terminationGracePeriodSeconds to 30 seconds\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"has a status phase of Running\", \"destination_entity\": \"status phase\"},\\n  {\"source_entity\": \"10.132.0.4\", \"description\": \"is the host IP address\", \"destination_entity\": \"host\"},\\n  {\"source_entity\": \"10.0.2.3\", \"description\": \"is the pod IP address\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"2016-03-18T12:44:32Z\", \"description\": \"is the start time of the pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"ClusterFirst\", \"description\": \"is the DNS policy used by Kubernetes\", \"destination_entity\": \"Kubernetes\"},\\n  {\"source_entity\": \"Always\", \"description\": \"is the restart policy used by Kubernetes\", \"destination_entity\": \"Kubernetes\"},\\n  {\"source_entity\": \"/dev/termination-log\", \"description\": \"is the path to the termination log file\", \"destination_entity\": \"log file\"},\\n  {\"source_entity\": \"kubia\", \"description\": \"is a container running in the pod\", \"destination_entity\": \"container\"}\\n]\\n\\nNote: The entities provided as input were '},\n",
       " {'page': 95,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '63\\nCreating pods from YAML or JSON descriptors\\n\\uf0a1Status contains the current information about the running pod, such as what\\ncondition the pod is in, the description and status of each container, and the\\npod’s internal IP and other basic info.\\nListing 3.1 showed a full description of a running pod, including its status. The status\\npart contains read-only runtime data that shows the state of the resource at a given\\nmoment. When creating a new pod, you never need to provide the status part. \\n The three parts described previously show the typical structure of a Kubernetes\\nAPI object. As you’ll see throughout the book, all other objects have the same anat-\\nomy. This makes understanding new objects relatively easy.\\n Going through all the individual properties in the previous YAML doesn’t make\\nmuch sense, so, instead, let’s see what the most basic YAML for creating a pod looks\\nlike. \\n3.2.2\\nCreating a simple YAML descriptor for a pod\\nYou’re going to create a file called kubia-manual.yaml (you can create it in any\\ndirectory you want), or download the book’s code archive, where you’ll find the\\nfile inside the Chapter03 directory. The following listing shows the entire contents\\nof the file.\\napiVersion: v1         \\nkind: Pod                             \\nmetadata:     \\n  name: kubia-manual         \\nspec: \\n  containers: \\n  - image: luksa/kubia          \\n    name: kubia         \\n    ports: \\n    - containerPort: 8080     \\n      protocol: TCP\\nI’m sure you’ll agree this is much simpler than the definition in listing 3.1. Let’s exam-\\nine this descriptor in detail. It conforms to the v1 version of the Kubernetes API. The\\ntype of resource you’re describing is a pod, with the name kubia-manual. The pod\\nconsists of a single container based on the luksa/kubia image. You’ve also given a\\nname to the container and indicated that it’s listening on port 8080. \\nSPECIFYING CONTAINER PORTS\\nSpecifying ports in the pod definition is purely informational. Omitting them has no\\neffect on whether clients can connect to the pod through the port or not. If the con-\\nListing 3.2\\nA basic pod manifest: kubia-manual.yaml\\nDescriptor conforms\\nto version v1 of\\nKubernetes API\\nYou’re \\ndescribing a pod.\\nThe name \\nof the pod\\nContainer image to create \\nthe container from\\nName of the container\\nThe port the app \\nis listening on\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A Kubernetes object that represents a running application or service.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'YAML',\n",
       "    'description': 'A human-readable serialization format for data.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'JSON',\n",
       "    'description': 'A lightweight data interchange format.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes API',\n",
       "    'description': 'An interface to interact with Kubernetes objects and resources.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pod Status',\n",
       "    'description': 'Information about the running pod, including its condition, containers, and internal IP address.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'ContainerPort',\n",
       "    'description': 'A port exposed by a container that can be accessed from outside the container.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'TCP',\n",
       "    'description': 'A protocol used for communication between networked devices.',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'image: luksa/kubia',\n",
       "    'description': 'The Docker image used to create a container.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Information about the pod, including its name and namespace.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'A section in the pod definition that specifies the desired state of the pod.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'A list of containers that are part of the pod.',\n",
       "    'category': 'resource'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"You\", \"description\": \"create a new pod\", \"destination_entity\": \"Kubernetes API\"},\\n  {\"source_entity\": \"API object\", \"description\": \"have the same anatomy as other Kubernetes objects\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Listing 3.1\", \"description\": \"show a full description of a running pod, including its status\", \"destination_entity\": \"Pod Status\"},\\n  {\"source_entity\": \"Kubernetes API\", \"description\": \"specify the version of the API being used\", \"destination_entity\": \"YAML descriptor\"},\\n  {\"source_entity\": \"JSON descriptors\", \"description\": \"can be used to create pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Descriptor\", \"description\": \"conforms to version v1 of Kubernetes API\", \"destination_entity\": \"Kubernetes API\"},\\n  {\"source_entity\": \"You\", \"description\": \"are describing a pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"The name\", \"description\": \"of the pod is kubia-manual\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Container image\", \"description\": \"is created from luksa/kubia image\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"You\", \"description\": \"have given a name to the container\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"The port\", \"description\": \"the app is listening on is 8080\", \"destination_entity\": \"containerPort\"},\\n  {\"source_entity\": \"Specifying ports\", \"description\": \"has no effect on whether clients can connect to the pod through the port or not\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Listing 3.2\", \"description\": \"shows a basic pod manifest: kubia-manual.yaml\", \"destination_entity\": \"YAML descriptor\"}\\n]\\n```'},\n",
       " {'page': 96,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"64\\nCHAPTER 3\\nPods: running containers in Kubernetes\\ntainer is accepting connections through a port bound to the 0.0.0.0 address, other\\npods can always connect to it, even if the port isn’t listed in the pod spec explicitly. But\\nit makes sense to define the ports explicitly so that everyone using your cluster can\\nquickly see what ports each pod exposes. Explicitly defining ports also allows you to\\nassign a name to each port, which can come in handy, as you’ll see later in the book.\\nUsing kubectl explain to discover possible API object fields\\nWhen preparing a manifest, you can either turn to the Kubernetes reference\\ndocumentation at http:/\\n/kubernetes.io/docs/api to see which attributes are\\nsupported by each API object, or you can use the kubectl explain command.\\nFor example, when creating a pod manifest from scratch, you can start by asking\\nkubectl to explain pods:\\n$ kubectl explain pods\\nDESCRIPTION:\\nPod is a collection of containers that can run on a host. This resource \\nis created by clients and scheduled onto hosts.\\nFIELDS:\\n   kind      <string>\\n     Kind is a string value representing the REST resource this object\\n     represents...\\n   metadata  <Object>\\n     Standard object's metadata...\\n   spec      <Object>\\n     Specification of the desired behavior of the pod...\\n   status    <Object>\\n     Most recently observed status of the pod. This data may not be up to\\n     date...\\nKubectl prints out the explanation of the object and lists the attributes the object\\ncan contain. You can then drill deeper to find out more about each attribute. For\\nexample, you can examine the spec attribute like this:\\n$ kubectl explain pod.spec\\nRESOURCE: spec <Object>\\nDESCRIPTION:\\n    Specification of the desired behavior of the pod...\\n    podSpec is a description of a pod.\\nFIELDS:\\n   hostPID   <boolean>\\n     Use the host's pid namespace. Optional: Default to false.\\n   ...\\n   volumes   <[]Object>\\n     List of volumes that can be mounted by containers belonging to the\\n     pod.\\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A collection of containers that can run on a host.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A lightweight and stand-alone executable binary image used in Linux operating systems since 1999.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'host',\n",
       "    'description': 'The machine or device running the container.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'port',\n",
       "    'description': 'An endpoint for communication between different address spaces.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool to manage Kubernetes resources.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'manifest',\n",
       "    'description': 'A file that defines the desired state of a pod or other Kubernetes resource.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API object',\n",
       "    'description': 'An entity in the Kubernetes API, such as a pod or deployment.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'The specification of the desired behavior of a pod or other Kubernetes resource.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'status',\n",
       "    'description': 'The most recently observed status of a pod or other Kubernetes resource.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'volumes',\n",
       "    'description': 'A list of volumes that can be mounted by containers belonging to a pod.',\n",
       "    'category': 'database'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"container\", \"description\": \"runs on a host, accepting connections through a port bound to the 0.0.0.0 address\", \"destination_entity\": \"host\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"explains possible API object fields\", \"destination_entity\": \"API object\"},\\n  {\"source_entity\": \"pod spec\", \"description\": \"specifies the desired behavior of the pod, including volumes that can be mounted by containers\", \"destination_entity\": \"volumes\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"prints out the explanation of the object and lists the attributes the object can contain\", \"destination_entity\": \"API object\"},\\n  {\"source_entity\": \"pod spec\", \"description\": \"drills deeper to find out more about each attribute, such as hostPID and volumes\", \"destination_entity\": \"spec\"},\\n  {\"source_entity\": \"container\", \"description\": \"can be connected to by other pods even if the port isn\\'t listed in the pod spec explicitly\", \"destination_entity\": \"port\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"displays the attributes that a manifest can contain, such as kind and metadata\", \"destination_entity\": \"manifest\"},\\n  {\"source_entity\": \"pod spec\", \"description\": \"describes the behavior of the pod, including hostPID and volumes\", \"destination_entity\": \"spec\"},\\n  {\"source_entity\": \"container\", \"description\": \"is a collection of containers that can run on a host\", \"destination_entity\": \"Pod\"}\\n]\\n```'},\n",
       " {'page': 97,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '65\\nCreating pods from YAML or JSON descriptors\\n3.2.3\\nUsing kubectl create to create the pod\\nTo create the pod from your YAML file, use the kubectl create command:\\n$ kubectl create -f kubia-manual.yaml\\npod \"kubia-manual\" created\\nThe kubectl create -f command is used for creating any resource (not only pods)\\nfrom a YAML or JSON file. \\nRETRIEVING THE WHOLE DEFINITION OF A RUNNING POD\\nAfter creating the pod, you can ask Kubernetes for the full YAML of the pod. You’ll\\nsee it’s similar to the YAML you saw earlier. You’ll learn about the additional fields\\nappearing in the returned definition in the next sections. Go ahead and use the fol-\\nlowing command to see the full descriptor of the pod:\\n$ kubectl get po kubia-manual -o yaml\\nIf you’re more into JSON, you can also tell kubectl to return JSON instead of YAML\\nlike this (this works even if you used YAML to create the pod):\\n$ kubectl get po kubia-manual -o json\\nSEEING YOUR NEWLY CREATED POD IN THE LIST OF PODS\\nYour pod has been created, but how do you know if it’s running? Let’s list pods to see\\ntheir statuses:\\n$ kubectl get pods\\nNAME            READY   STATUS    RESTARTS   AGE\\nkubia-manual    1/1     Running   0          32s\\nkubia-zxzij     1/1     Running   0          1d    \\nThere’s your kubia-manual pod. Its status shows that it’s running. If you’re like me,\\nyou’ll probably want to confirm that’s true by talking to the pod. You’ll do that in a\\nminute. First, you’ll look at the app’s log to check for any errors.\\n3.2.4\\nViewing application logs\\nYour little Node.js application logs to the process’s standard output. Containerized\\napplications usually log to the standard output and standard error stream instead of\\n   Containers  <[]Object> -required-\\n     List of containers belonging to the pod. Containers cannot currently\\n     Be added or removed. There must be at least one container in a pod.\\n     Cannot be updated. More info:\\n     http://releases.k8s.io/release-1.4/docs/user-guide/containers.md\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl create',\n",
       "    'description': 'command to create resources from YAML or JSON file',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a container that provides a runtime environment for one or more applications',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'YAML',\n",
       "    'description': 'human-readable serialized format used to describe Kubernetes resources',\n",
       "    'category': 'format'},\n",
       "   {'entity': 'JSON',\n",
       "    'description': 'lightweight data interchange format used to describe Kubernetes resources',\n",
       "    'category': 'format'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'container orchestration system',\n",
       "    'category': 'platform'},\n",
       "   {'entity': 'kubectl get',\n",
       "    'description': 'command to retrieve information about Kubernetes resources',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pod list',\n",
       "    'description': 'list of all running pods in a cluster',\n",
       "    'category': 'output'},\n",
       "   {'entity': 'READY',\n",
       "    'description': 'status column in pod list showing the number of container(s) ready',\n",
       "    'category': 'column'},\n",
       "   {'entity': 'STATUS',\n",
       "    'description': 'status column in pod list showing the current status of the pod',\n",
       "    'category': 'column'},\n",
       "   {'entity': 'RESTARTS',\n",
       "    'description': 'status column in pod list showing the number of times the container has been restarted',\n",
       "    'category': 'column'},\n",
       "   {'entity': 'AGE',\n",
       "    'description': 'status column in pod list showing how long the pod has been running',\n",
       "    'category': 'column'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'list of containers belonging to a pod',\n",
       "    'category': 'component'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl create\", \"description\": \"create a pod from YAML descriptor\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl get\", \"description\": \"get the full YAML of a running pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl get\", \"description\": \"list pods to see their statuses\", \"destination_entity\": \"pod list\"},\\n  {\"source_entity\": \"kubectl get\", \"description\": \"view the status of a pod\", \"destination_entity\": \"STATUS\"},\\n  {\"source_entity\": \"kubectl get\", \"description\": \"view the AGE of a pod\", \"destination_entity\": \"AGE\"},\\n  {\"source_entity\": \"kubectl get\", \"description\": \"view the RESTARTS of a pod\", \"destination_entity\": \"RESTARTS\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"manage containers and pods\", \"destination_entity\": \"containers\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"create a pod from YAML or JSON descriptor\", \"destination_entity\": \"YAML\"},\\n  {\"source_entity\": \"JSON\", \"description\": \"view the YAML of a pod\", \"destination_entity\": \"YAML\"},\\n  {\"source_entity\": \"kubectl create\", \"description\": \"create a pod from JSON descriptor\", \"destination_entity\": \"pod\"}\\n]\\n```'},\n",
       " {'page': 98,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '66\\nCHAPTER 3\\nPods: running containers in Kubernetes\\nwriting their logs to files. This is to allow users to view logs of different applications in\\na simple, standard way. \\n The container runtime (Docker in your case) redirects those streams to files and\\nallows you to get the container’s log by running\\n$ docker logs <container id>\\nYou could use ssh to log into the node where your pod is running and retrieve its logs\\nwith docker logs, but Kubernetes provides an easier way. \\nRETRIEVING A POD’S LOG WITH KUBECTL LOGS\\nTo see your pod’s log (more precisely, the container’s log) you run the following com-\\nmand on your local machine (no need to ssh anywhere):\\n$ kubectl logs kubia-manual\\nKubia server starting...\\nYou haven’t sent any web requests to your Node.js app, so the log only shows a single\\nlog statement about the server starting up. As you can see, retrieving logs of an appli-\\ncation running in Kubernetes is incredibly simple if the pod only contains a single\\ncontainer. \\nNOTE\\nContainer logs are automatically rotated daily and every time the log file\\nreaches 10MB in size. The kubectl logs command only shows the log entries\\nfrom the last rotation.\\nSPECIFYING THE CONTAINER NAME WHEN GETTING LOGS OF A MULTI-CONTAINER POD\\nIf your pod includes multiple containers, you have to explicitly specify the container\\nname by including the -c <container name> option when running kubectl logs. In\\nyour kubia-manual pod, you set the container’s name to kubia, so if additional con-\\ntainers exist in the pod, you’d have to get its logs like this:\\n$ kubectl logs kubia-manual -c kubia\\nKubia server starting...\\nNote that you can only retrieve container logs of pods that are still in existence. When\\na pod is deleted, its logs are also deleted. To make a pod’s logs available even after the\\npod is deleted, you need to set up centralized, cluster-wide logging, which stores all\\nthe logs into a central store. Chapter 17 explains how centralized logging works.\\n3.2.5\\nSending requests to the pod\\nThe pod is now running—at least that’s what kubectl get and your app’s log say. But\\nhow do you see it in action? In the previous chapter, you used the kubectl expose\\ncommand to create a service to gain access to the pod externally. You’re not going to\\ndo that now, because a whole chapter is dedicated to services, and you have other ways\\nof connecting to a pod for testing and debugging purposes. One of them is through\\nport forwarding.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Docker',\n",
       "    'description': 'container runtime',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'running containers in Kubernetes',\n",
       "    'category': 'hardware/software'},\n",
       "   {'entity': 'container id',\n",
       "    'description': 'unique identifier for a container',\n",
       "    'category': 'hardware/software'},\n",
       "   {'entity': '$ docker logs <container id>',\n",
       "    'description': 'command to retrieve container logs',\n",
       "    'category': 'software/command'},\n",
       "   {'entity': 'ssh',\n",
       "    'description': 'remote access protocol',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'physical or virtual machine running a pod',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': '$ kubectl logs kubia-manual',\n",
       "    'description': 'command to retrieve pod logs',\n",
       "    'category': 'software/command'},\n",
       "   {'entity': 'container name',\n",
       "    'description': 'name assigned to a container within a pod',\n",
       "    'category': 'hardware/software'},\n",
       "   {'entity': '$ kubectl logs kubia-manual -c kubia',\n",
       "    'description': 'command to retrieve logs from a specific container in a pod',\n",
       "    'category': 'software/command'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'running instance of one or more containers',\n",
       "    'category': 'hardware/software'},\n",
       "   {'entity': 'service',\n",
       "    'description': 'abstraction for accessing a pod externally',\n",
       "    'category': 'hardware/software'},\n",
       "   {'entity': 'port forwarding',\n",
       "    'description': 'mechanism for connecting to a pod through an external port',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"node\",\\n    \"description\": \"redirects container streams to files for logging\",\\n    \"destination_entity\": \"Docker\"\\n  },\\n  {\\n    \"source_entity\": \"Docker\",\\n    \"description\": \"allows retrieving container\\'s log by running $ docker logs <container id>\",\\n    \"destination_entity\": \"$ docker logs <container id>\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"provides an easier way to retrieve pod\\'s log with kubectl logs command\",\\n    \"destination_entity\": \"kubectl\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"allows retrieving pod\\'s log (container\\'s log) with $ kubectl logs <pod name>\",\\n    \"destination_entity\": \"$ kubectl logs <pod name>\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"requires specifying container name when getting logs of a multi-container pod\",\\n    \"destination_entity\": \"container name\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"deletes pod and its logs when the pod is deleted\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"node\",\\n    \"description\": \"can be accessed using ssh for logging into the node\",\\n    \"destination_entity\": \"ssh\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"exposes pod to gain access externally, but not used in this chapter\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"uses port forwarding for testing and debugging purposes\",\\n    \"destination_entity\": \"port forwarding\"\\n  },\\n  {\\n    \"source_entity\": \"Docker\",\\n    \"description\": \"rotates container logs daily and when log file reaches 10MB in size\",\\n    \"destination_entity\": \"container logs\"\\n  },\\n  {\\n    \"source_entity\": \"$ kubectl logs kubia-manual\",\\n    \"description\": \"displays only the log entries from the last rotation\",\\n    \"destination_entity\": \"kubectl logs command\"\\n  }\\n]'},\n",
       " {'page': 99,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '67\\nOrganizing pods with labels\\nFORWARDING A LOCAL NETWORK PORT TO A PORT IN THE POD\\nWhen you want to talk to a specific pod without going through a service (for debug-\\nging or other reasons), Kubernetes allows you to configure port forwarding to the\\npod. This is done through the kubectl port-forward command. The following\\ncommand will forward your machine’s local port 8888 to port 8080 of your kubia-\\nmanual pod:\\n$ kubectl port-forward kubia-manual 8888:8080\\n... Forwarding from 127.0.0.1:8888 -> 8080\\n... Forwarding from [::1]:8888 -> 8080\\nThe port forwarder is running and you can now connect to your pod through the\\nlocal port. \\nCONNECTING TO THE POD THROUGH THE PORT FORWARDER\\nIn a different terminal, you can now use curl to send an HTTP request to your pod\\nthrough the kubectl port-forward proxy running on localhost:8888:\\n$ curl localhost:8888\\nYou’ve hit kubia-manual\\nFigure 3.5 shows an overly simplified view of what happens when you send the request.\\nIn reality, a couple of additional components sit between the kubectl process and the\\npod, but they aren’t relevant right now.\\nUsing port forwarding like this is an effective way to test an individual pod. You’ll\\nlearn about other similar methods throughout the book. \\n3.3\\nOrganizing pods with labels\\nAt this point, you have two pods running in your cluster. When deploying actual\\napplications, most users will end up running many more pods. As the number of\\npods increases, the need for categorizing them into subsets becomes more and\\nmore evident.\\n For example, with microservices architectures, the number of deployed microser-\\nvices can easily exceed 20 or more. Those components will probably be replicated\\nKubernetes cluster\\nPort\\n8080\\nLocal machine\\nkubectl\\nport-forward\\nprocess\\ncurl\\nPort\\n8888\\nPod:\\nkubia-manual\\nFigure 3.5\\nA simplified view of what happens when you use curl with kubectl port-forward\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Port\\n8888\\nkubectl\\ncurl port-forward\\nprocess Col1  \\\n",
       "   0                                            None        \n",
       "   \n",
       "     Port\\n8080\\nPod:\\nkubia-manual  \n",
       "   0                           None  ],\n",
       "  'entities': [{'entity': 'pod',\n",
       "    'description': 'a container running an application in Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'label',\n",
       "    'description': 'a way to categorize pods into subsets',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the command-line tool for interacting with a Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'port-forward',\n",
       "    'description': 'a method of forwarding traffic from the local machine to a pod in the cluster',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'port',\n",
       "    'description': 'an endpoint for network communication, used to connect to a pod or service',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kubectl port-forward',\n",
       "    'description': 'the command used to enable port forwarding from the local machine to a pod',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'a tool for sending HTTP requests to a server, used to test a pod through the port forwarder',\n",
       "    'category': 'tool'},\n",
       "   {'entity': 'service',\n",
       "    'description': 'an abstraction that provides a network interface to a pod or group of pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'cluster',\n",
       "    'description': 'a collection of machines (nodes) running Kubernetes, used to deploy and manage applications',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"executes port forwarding command\", \"destination_entity\": \"port-forward\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"configures port forwarding to a pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"forwards local machine port to a pod\\'s port\", \"destination_entity\": \"port\"},\\n  {\"source_entity\": \"kubectl port-forward\", \"description\": \"establishes connection between local machine and pod\", \"destination_entity\": \"port\"},\\n  {\"source_entity\": \"curl\", \"description\": \"sends HTTP request through kubectl port-forward proxy\", \"destination_entity\": \"kubectl port-forward\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"sets up proxy for curl to communicate with a pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"port-forward\", \"description\": \"forwards local machine port to a pod\\'s port\", \"destination_entity\": \"port\"},\\n  {\"source_entity\": \"service\", \"description\": \"not directly related in this context, but mentioned as an alternative to port forwarding\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"label\", \"description\": \"used for categorizing pods into subsets\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"executes command to forward a local network port to a pod\\'s port\", \"destination_entity\": \"port-forward\"}\\n]\\n```\\n\\nNote that the relations were extracted based on the context of the document and the entities provided. The description for each relation is a brief summary of what action was being carried out by the source entity on the destination entity.'},\n",
       " {'page': 100,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '68\\nCHAPTER 3\\nPods: running containers in Kubernetes\\n(multiple copies of the same component will be deployed) and multiple versions or\\nreleases (stable, beta, canary, and so on) will run concurrently. This can lead to hun-\\ndreds of pods in the system. Without a mechanism for organizing them, you end up\\nwith a big, incomprehensible mess, such as the one shown in figure 3.6. The figure\\nshows pods of multiple microservices, with several running multiple replicas, and others\\nrunning different releases of the same microservice.\\nIt’s evident you need a way of organizing them into smaller groups based on arbitrary\\ncriteria, so every developer and system administrator dealing with your system can eas-\\nily see which pod is which. And you’ll want to operate on every pod belonging to a cer-\\ntain group with a single action instead of having to perform the action for each pod\\nindividually. \\n Organizing pods and all other Kubernetes objects is done through labels.\\n3.3.1\\nIntroducing labels\\nLabels are a simple, yet incredibly powerful, Kubernetes feature for organizing not\\nonly pods, but all other Kubernetes resources. A label is an arbitrary key-value pair you\\nattach to a resource, which is then utilized when selecting resources using label selectors\\n(resources are filtered based on whether they include the label specified in the selec-\\ntor). A resource can have more than one label, as long as the keys of those labels are\\nunique within that resource. You usually attach labels to resources when you create\\nthem, but you can also add additional labels or even modify the values of existing\\nlabels later without having to recreate the resource. \\nUI pod\\nUI pod\\nUI pod\\nAccount\\nService\\npod\\nProduct\\nCatalog\\npod\\nProduct\\nCatalog\\npod\\nProduct\\nCatalog\\npod\\nShopping\\nCart\\npod\\nShopping\\nCart\\npod\\nOrder\\nService\\npod\\nUI pod\\nUI pod\\nProduct\\nCatalog\\npod\\nProduct\\nCatalog\\npod\\nOrder\\nService\\npod\\nAccount\\nService\\npod\\nProduct\\nCatalog\\npod\\nProduct\\nCatalog\\npod\\nOrder\\nService\\npod\\nFigure 3.6\\nUncategorized pods in a microservices architecture\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pods',\n",
       "    'description': 'running containers in Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'simple, yet incredibly powerful, Kubernetes feature for organizing resources',\n",
       "    'category': 'feature'},\n",
       "   {'entity': 'label selectors',\n",
       "    'description': 'resources filtered based on whether they include the label specified',\n",
       "    'category': 'selector'},\n",
       "   {'entity': 'UI pod',\n",
       "    'description': 'container running a user interface service',\n",
       "    'category': 'pod'},\n",
       "   {'entity': 'Account Service',\n",
       "    'description': 'service handling account management',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'Product Catalog',\n",
       "    'description': 'service managing product catalog information',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'Shopping Cart',\n",
       "    'description': 'service handling shopping cart functionality',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'Order Service',\n",
       "    'description': 'service handling order management',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'Kubernetes resources',\n",
       "    'description': 'all objects managed by Kubernetes',\n",
       "    'category': 'resources'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Labels\", \"description\": \"attach to a resource\", \"destination_entity\": \"Resources\"},\\n  {\"source_entity\": \"Resources\", \"description\": \"be filtered based on labels\", \"destination_entity\": \"Label selectors\"},\\n  {\"source_entity\": \"Label selectors\", \"description\": \"filter resources based on labels\", \"destination_entity\": \"Resources\"},\\n  {\"source_entity\": \"Developers and system administrators\", \"description\": \"easily see which pod is which\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Users\", \"description\": \"operate on every pod belonging to a certain group with a single action\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"be organized into smaller groups based on arbitrary criteria\", \"destination_entity\": \"Labels\"},\\n  {\"source_entity\": \"UI pod\", \"description\": \"perform some operation on\", \"destination_entity\": \"Shopping Cart\"},\\n  {\"source_entity\": \"Account Service\", \"description\": \"perform some operation on\", \"destination_entity\": \"Order Service\"},\\n  {\"source_entity\": \"Product Catalog\", \"description\": \"be related to\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Kubernetes resources\", \"description\": \"be organized by labels\", \"destination_entity\": \"Labels\"},\\n  {\"source_entity\": \"Label selectors\", \"description\": \"filter resources based on labels\", \"destination_entity\": \"Resources\"},\\n  {\"source_entity\": \"Users\", \"description\": \"use label selectors to filter resources\", \"destination_entity\": \"Label selectors\"}\\n]\\n```\\n\\nNote: I\\'ve extracted the relations as per the rules provided, and included all the entities mentioned in the document page.'},\n",
       " {'page': 101,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '69\\nOrganizing pods with labels\\n Let’s turn back to the microservices example from figure 3.6. By adding labels to\\nthose pods, you get a much-better-organized system that everyone can easily make\\nsense of. Each pod is labeled with two labels:\\n\\uf0a1\\napp, which specifies which app, component, or microservice the pod belongs to. \\n\\uf0a1\\nrel, which shows whether the application running in the pod is a stable, beta,\\nor a canary release.\\nDEFINITION\\nA canary release is when you deploy a new version of an applica-\\ntion next to the stable version, and only let a small fraction of users hit the\\nnew version to see how it behaves before rolling it out to all users. This pre-\\nvents bad releases from being exposed to too many users.\\nBy adding these two labels, you’ve essentially organized your pods into two dimen-\\nsions (horizontally by app and vertically by release), as shown in figure 3.7.\\nEvery developer or ops person with access to your cluster can now easily see the sys-\\ntem’s structure and where each pod fits in by looking at the pod’s labels.\\n3.3.2\\nSpecifying labels when creating a pod\\nNow, you’ll see labels in action by creating a new pod with two labels. Create a new file\\ncalled kubia-manual-with-labels.yaml with the contents of the following listing.\\napiVersion: v1                                         \\nkind: Pod                                              \\nmetadata:                                              \\n  name: kubia-manual-v2\\nListing 3.3\\nA pod with labels: kubia-manual-with-labels.yaml\\nUI pod\\napp: ui\\nrel: stable\\nrel=stable\\napp=ui\\nAccount\\nService\\npod\\napp: as\\nrel: stable\\napp=as\\napp: pc\\nrel: stable\\napp=pc\\napp: sc\\nrel: stable\\napp=sc\\napp: os\\nrel: stable\\napp=os\\nProduct\\nCatalog\\npod\\nShopping\\nCart\\npod\\nOrder\\nService\\npod\\nUI pod\\napp: ui\\nrel: beta\\nrel=beta\\napp: pc\\nrel: beta\\napp: os\\nrel: beta\\nProduct\\nCatalog\\npod\\nOrder\\nService\\npod\\nrel=canary\\nAccount\\nService\\npod\\napp: as\\nrel: canary\\napp: pc\\nrel: canary\\napp: os\\nrel: canary\\nProduct\\nCatalog\\npod\\nOrder\\nService\\npod\\nFigure 3.7\\nOrganizing pods in a microservices architecture with pod labels\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pods',\n",
       "    'description': 'A Pod is a logical host for one or more application containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Labels',\n",
       "    'description': 'A label is a key-value pair that can be attached to a pod, replication controller, service, persistent volume claim, or other Kubernetes API object.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'App',\n",
       "    'description': 'The name of the application or microservice the pod belongs to.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Rel',\n",
       "    'description': 'The release type of the application running in the pod (stable, beta, canary).',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Canary Release',\n",
       "    'description': 'A deployment strategy where a new version of an application is deployed alongside the existing stable version.',\n",
       "    'category': 'deployment'},\n",
       "   {'entity': 'Kubernetes API Object',\n",
       "    'description': 'An object that can be created and managed by the Kubernetes API, such as a pod, replication controller, or service.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Pod Labels',\n",
       "    'description': 'Key-value pairs attached to a pod to identify its application and release type.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Replication Controller',\n",
       "    'description': 'An object that manages the number of replicas (identical copies) of an application running in Kubernetes.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'An abstract way to expose an application running within a cluster, so that it can be accessed from outside the cluster.',\n",
       "    'category': 'networking'},\n",
       "   {'entity': 'Persistent Volume Claim',\n",
       "    'description': 'A request for storage resources in a Kubernetes cluster.',\n",
       "    'category': 'storage'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Replication Controller\",\\n    \"description\": \"specifies which app, component, or microservice a pod belongs to\",\\n    \"destination_entity\": \"App\"\\n  },\\n  {\\n    \"source_entity\": \"Labels\",\\n    \"description\": \"organizes pods into two dimensions (horizontally by app and vertically by release)\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Canary Release\",\\n    \"description\": \"deploys a new version of an application next to the stable version, exposing it to a small fraction of users\",\\n    \"destination_entity\": \"App\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes API Object\",\\n    \"description\": \"defines a pod with labels when creating a new pod\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Labels\",\\n    \"description\": \"specifies which release (stable, beta, or canary) an application running in a pod is\",\\n    \"destination_entity\": \"Rel\"\\n  },\\n  {\\n    \"source_entity\": \"App\",\\n    \"description\": \"determines the type of release (stable, beta, or canary) for a microservice\",\\n    \"destination_entity\": \"Canary Release\"\\n  },\\n  {\\n    \"source_entity\": \"Pod Labels\",\\n    \"description\": \"provides information about the structure and organization of pods in a cluster\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"provides access to pods with specific labels\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Persistent Volume Claim\",\\n    \"description\": \"is not directly related to the context, but is a Kubernetes API object\",\\n    \"destination_entity\": null\\n  }\\n]\\n```\\n\\nNote: The last relation has a destination entity of `null` because Persistent Volume Claim is not directly related to any other entity in this context.'},\n",
       " {'page': 102,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '70\\nCHAPTER 3\\nPods: running containers in Kubernetes\\n  labels:    \\n    creation_method: manual          \\n    env: prod                        \\nspec: \\n  containers: \\n  - image: luksa/kubia\\n    name: kubia\\n    ports: \\n    - containerPort: 8080\\n      protocol: TCP\\nYou’ve included the labels creation_method=manual and env=data.labels section.\\nYou’ll create this pod now:\\n$ kubectl create -f kubia-manual-with-labels.yaml\\npod \"kubia-manual-v2\" created\\nThe kubectl get pods command doesn’t list any labels by default, but you can see\\nthem by using the --show-labels switch:\\n$ kubectl get po --show-labels\\nNAME            READY  STATUS   RESTARTS  AGE LABELS\\nkubia-manual    1/1    Running  0         16m <none>\\nkubia-manual-v2 1/1    Running  0         2m  creat_method=manual,env=prod\\nkubia-zxzij     1/1    Running  0         1d  run=kubia\\nInstead of listing all labels, if you’re only interested in certain labels, you can specify\\nthem with the -L switch and have each displayed in its own column. List pods again\\nand show the columns for the two labels you’ve attached to your kubia-manual-v2 pod:\\n$ kubectl get po -L creation_method,env\\nNAME            READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV\\nkubia-manual    1/1     Running   0          16m   <none>            <none>\\nkubia-manual-v2 1/1     Running   0          2m    manual            prod\\nkubia-zxzij     1/1     Running   0          1d    <none>            <none>\\n3.3.3\\nModifying labels of existing pods\\nLabels can also be added to and modified on existing pods. Because the kubia-man-\\nual pod was also created manually, let’s add the creation_method=manual label to it: \\n$ kubectl label po kubia-manual creation_method=manual\\npod \"kubia-manual\" labeled\\nNow, let’s also change the env=prod label to env=debug on the kubia-manual-v2 pod,\\nto see how existing labels can be changed.\\nNOTE\\nYou need to use the --overwrite option when changing existing labels.\\n$ kubectl label po kubia-manual-v2 env=debug --overwrite\\npod \"kubia-manual-v2\" labeled\\nTwo labels are \\nattached to the pod.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pods',\n",
       "    'description': 'running containers in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'metadata attached to a pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'creation_method',\n",
       "    'description': 'label key for creation method',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'env',\n",
       "    'description': 'label key for environment',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'pod specification',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'list of containers in a pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'container image name',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'name', 'description': 'container name', 'category': 'software'},\n",
       "   {'entity': 'ports',\n",
       "    'description': 'list of container ports',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'containerPort',\n",
       "    'description': 'container port number',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'protocol',\n",
       "    'description': 'port protocol (e.g. TCP, UDP)',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Kubernetes command-line tool',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'create',\n",
       "    'description': 'kubectl command to create a pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': '-f',\n",
       "    'description': 'option to specify the file containing the pod definition',\n",
       "    'category': 'software'},\n",
       "   {'entity': '--show-labels',\n",
       "    'description': 'option to display labels in kubectl output',\n",
       "    'category': 'software'},\n",
       "   {'entity': '-L',\n",
       "    'description': 'option to specify columns for labels in kubectl output',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'get',\n",
       "    'description': 'kubectl command to retrieve pod information',\n",
       "    'category': 'software'},\n",
       "   {'entity': '--overwrite',\n",
       "    'description': 'option to overwrite existing labels when modifying them',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'label',\n",
       "    'description': 'kubectl command to add or modify a label on an existing pod',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"create a new pod with labels creation_method=manual and env=data.labels\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"get pods with labels creation_method,env\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl label\",\\n    \"description\": \"add the creation_method=manual label to the kubia-manual pod\",\\n    \"destination_entity\": \"kubia-manual\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl label\",\\n    \"description\": \"change the env=prod label to env=debug on the kubia-manual-v2 pod\",\\n    \"destination_entity\": \"kubia-manual-v2\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"have a container with image luksa/kubia and name kubia\",\\n    \"destination_entity\": \"containers\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"have a port with containerPort 8080 and protocol TCP\",\\n    \"destination_entity\": \"ports\"\\n  },\\n  {\\n    \"source_entity\": \"kubia-manual-v2\",\\n    \"description\": \"has labels creation_method=manual and env=prod\",\\n    \"destination_entity\": \"labels\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl get\",\\n    \"description\": \"list pods with columns for creation_method,env\",\\n    \"destination_entity\": \"Pods\"\\n  }\\n]'},\n",
       " {'page': 103,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"71\\nListing subsets of pods through label selectors\\nList the pods again to see the updated labels:\\n$ kubectl get po -L creation_method,env\\nNAME            READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV\\nkubia-manual    1/1     Running   0          16m   manual            <none>\\nkubia-manual-v2 1/1     Running   0          2m    manual            debug\\nkubia-zxzij     1/1     Running   0          1d    <none>            <none>\\nAs you can see, attaching labels to resources is trivial, and so is changing them on\\nexisting resources. It may not be evident right now, but this is an incredibly powerful\\nfeature, as you’ll see in the next chapter. But first, let’s see what you can do with these\\nlabels, in addition to displaying them when listing pods.\\n3.4\\nListing subsets of pods through label selectors\\nAttaching labels to resources so you can see the labels next to each resource when list-\\ning them isn’t that interesting. But labels go hand in hand with label selectors. Label\\nselectors allow you to select a subset of pods tagged with certain labels and perform an\\noperation on those pods. A label selector is a criterion, which filters resources based\\non whether they include a certain label with a certain value. \\n A label selector can select resources based on whether the resource\\n\\uf0a1Contains (or doesn’t contain) a label with a certain key\\n\\uf0a1Contains a label with a certain key and value\\n\\uf0a1Contains a label with a certain key, but with a value not equal to the one you\\nspecify\\n3.4.1\\nListing pods using a label selector\\nLet’s use label selectors on the pods you’ve created so far. To see all pods you created\\nmanually (you labeled them with creation_method=manual), do the following:\\n$ kubectl get po -l creation_method=manual\\nNAME              READY     STATUS    RESTARTS   AGE\\nkubia-manual      1/1       Running   0          51m\\nkubia-manual-v2   1/1       Running   0          37m\\nTo list all pods that include the env label, whatever its value is:\\n$ kubectl get po -l env\\nNAME              READY     STATUS    RESTARTS   AGE\\nkubia-manual-v2   1/1       Running   0          37m\\nAnd those that don’t have the env label:\\n$ kubectl get po -l '!env'\\nNAME           READY     STATUS    RESTARTS   AGE\\nkubia-manual   1/1       Running   0          51m\\nkubia-zxzij    1/1       Running   0          10d\\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'get',\n",
       "    'description': 'Command to retrieve information about resources',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'po',\n",
       "    'description': 'Pods resource in Kubernetes',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'label selectors',\n",
       "    'description': 'Criteria for selecting resources based on labels',\n",
       "    'category': 'feature'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'Key-value pairs attached to resources for filtering and organization',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'creation_method',\n",
       "    'description': 'Label key for tracking how pods were created',\n",
       "    'category': 'label'},\n",
       "   {'entity': 'env',\n",
       "    'description': 'Label key for tracking environment-related information',\n",
       "    'category': 'label'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Running instance of a container in Kubernetes',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'kubia-manual',\n",
       "    'description': 'Pod name',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'kubia-manual-v2',\n",
       "    'description': 'Pod name',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'kubia-zxzij',\n",
       "    'description': 'Pod name',\n",
       "    'category': 'resource'},\n",
       "   {'entity': '-L',\n",
       "    'description': 'Flag for specifying label selectors in the get command',\n",
       "    'category': 'flag'},\n",
       "   {'entity': '!env',\n",
       "    'description': 'Label selector to exclude resources with the env label',\n",
       "    'category': 'selector'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"list pods using a label selector\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"label selectors\",\\n    \"description\": \"allow you to select a subset of pods tagged with certain labels and perform an operation on those pods\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"get all pods that include the env label, whatever its value is\",\\n    \"destination_entity\": \"env\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"list all pods that don\\'t have the env label\",\\n    \"destination_entity\": \"!env\"\\n  },\\n  {\\n    \"source_entity\": \"creation_method=manual\",\\n    \"description\": \"label pods created manually\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"get pods with creation_method=manual\",\\n    \"destination_entity\": \"kubia-manual-v2\"\\n  },\\n  {\\n    \"source_entity\": \"-L env\",\\n    \"description\": \"list all pods that include the env label, whatever its value is\",\\n    \"destination_entity\": \"kubia-manual-v2\"\\n  },\\n  {\\n    \"source_entity\": \"!env\",\\n    \"description\": \"list all pods that don\\'t have the env label\",\\n    \"destination_entity\": \"kubia-manual\"\\n  },\\n  {\\n    \"source_entity\": \"-l creation_method=manual\",\\n    \"description\": \"list all pods created manually\",\\n    \"destination_entity\": \"kubia-manual\"\\n  },\\n  {\\n    \"source_entity\": \"-L !env\",\\n    \"description\": \"list all pods that don\\'t have the env label\",\\n    \"destination_entity\": \"kubia-zxzij\"\\n  }\\n]\\n```'},\n",
       " {'page': 104,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '72\\nCHAPTER 3\\nPods: running containers in Kubernetes\\nNOTE\\nMake sure to use single quotes around !env, so the bash shell doesn’t\\nevaluate the exclamation mark.\\nSimilarly, you could also match pods with the following label selectors:\\n\\uf0a1\\ncreation_method!=manual to select pods with the creation_method label with\\nany value other than manual\\n\\uf0a1\\nenv in (prod,devel) to select pods with the env label set to either prod or\\ndevel\\n\\uf0a1\\nenv notin (prod,devel) to select pods with the env label set to any value other\\nthan prod or devel\\nTurning back to the pods in the microservices-oriented architecture example, you\\ncould select all pods that are part of the product catalog microservice by using the\\napp=pc label selector (shown in the following figure).\\n3.4.2\\nUsing multiple conditions in a label selector\\nA selector can also include multiple comma-separated criteria. Resources need to\\nmatch all of them to match the selector. If, for example, you want to select only pods\\nrunning the beta release of the product catalog microservice, you’d use the following\\nselector: app=pc,rel=beta (visualized in figure 3.9).\\n Label selectors aren’t useful only for listing pods, but also for performing actions\\non a subset of all pods. For example, later in the chapter, you’ll see how to use label\\nselectors to delete multiple pods at once. But label selectors aren’t used only by\\nkubectl. They’re also used internally, as you’ll see next.\\nUI pod\\napp: ui\\nrel: stable\\nrel=stable\\napp=ui\\nAccount\\nService\\npod\\napp: as\\nrel: stable\\napp=as\\napp: pc\\nrel: stable\\napp=pc\\napp: sc\\nrel: stable\\napp=sc\\napp: os\\nrel: stable\\napp=os\\nProduct\\nCatalog\\npod\\nShopping\\nCart\\npod\\nOrder\\nService\\npod\\nUI pod\\napp: ui\\nrel: beta\\nrel=beta\\napp: pc\\nrel: beta\\napp: os\\nrel: beta\\nProduct\\nCatalog\\npod\\nOrder\\nService\\npod\\nrel=canary\\nAccount\\nService\\npod\\napp: as\\nrel: canary\\napp: pc\\nrel: canary\\napp: os\\nrel: canary\\nProduct\\nCatalog\\npod\\nOrder\\nService\\npod\\nFigure 3.8\\nSelecting the product catalog microservice pods using the “app=pc” label selector\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  app=ui app=as\\nrel=stable\\napp: ui app: as\\nAccount\\nUI pod rel: stable Service rel: stable\\npod\\napp: ui rel=beta\\nUI pod rel: beta\\nrel=canary\\napp: as\\nAccount\\nService rel: canary\\npod  \\\n",
       "   0                                               None                                                                                                                                             \n",
       "   \n",
       "                                                 app=pc  \\\n",
       "   0  app: pc\\nProduct\\nCatalog rel: stable\\npod\\nap...   \n",
       "   \n",
       "     app=sc app=os\\napp: sc app: os\\nShopping Order\\nCart rel: stable Service rel: stable\\npod pod\\napp: os\\nOrder\\nService rel: beta\\npod\\napp: os\\nOrder\\nService rel: canary\\npod  \n",
       "   0                                               None                                                                                                                               ],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': '!env',\n",
       "    'description': 'bash shell environment variable notation',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Kubernetes command-line tool',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'UI pod',\n",
       "    'description': 'User Interface container in Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'app=pc',\n",
       "    'description': 'Label selector for product catalog microservice',\n",
       "    'category': 'labelselector'},\n",
       "   {'entity': 'rel=beta',\n",
       "    'description': 'Label selector for beta release of product catalog microservice',\n",
       "    'category': 'labelselector'},\n",
       "   {'entity': 'env in (prod,devel)',\n",
       "    'description': 'Label selector for pods with env label set to prod or devel',\n",
       "    'category': 'labelselector'},\n",
       "   {'entity': 'creation_method!=manual',\n",
       "    'description': 'Label selector for pods with creation method not equal to manual',\n",
       "    'category': 'labelselector'},\n",
       "   {'entity': 'env notin (prod,devel)',\n",
       "    'description': 'Label selector for pods with env label not set to prod or devel',\n",
       "    'category': 'labelselector'},\n",
       "   {'entity': 'app=as',\n",
       "    'description': 'Label selector for account service pod',\n",
       "    'category': 'labelselector'},\n",
       "   {'entity': 'rel=stable',\n",
       "    'description': 'Label selector for stable release of microservice',\n",
       "    'category': 'labelselector'},\n",
       "   {'entity': 'app=os',\n",
       "    'description': 'Label selector for order service pod',\n",
       "    'category': 'labelselector'},\n",
       "   {'entity': 'rel=canary',\n",
       "    'description': 'Label selector for canary release of microservice',\n",
       "    'category': 'labelselector'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Kubernetes pod object',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"UI pod\", \"description\": \"has label selector app=ui and rel=stable\", \"destination_entity\": \"label selector\"}]\\n\\n{\"source_entity\": \"!env\", \"description\": \"should not be evaluated by bash shell\", \"destination_entity\": \"bash shell\"}\\n\\n{\"source_entity\": \"rel=beta\", \"description\": \"selects pods with release beta\", \"destination_entity\": \"pods\"}\\n\\n{\"source_entity\": \"UI pod\", \"description\": \"has label selector app=ui and rel=stable\", \"destination_entity\": \"label selector\"}\\n\\n{\"source_entity\": \"!env\", \"description\": \"should not be evaluated by bash shell\", \"destination_entity\": \"bash shell\"}\\n\\n{\"source_entity\": \"rel=stable\", \"description\": \"selects pods with release stable\", \"destination_entity\": \"pods\"}\\n\\n{\"source_entity\": \"app=os\", \"description\": \"labels os microservice pod as stable\", \"destination_entity\": \"pod\"}\\n\\n{\"source_entity\": \"kubectl\", \"description\": \"uses label selectors for listing and performing actions on pods\", \"destination_entity\": \"label selectors\"}\\n\\n{\"source_entity\": \"app=pc\", \"description\": \"labels product catalog microservice pod as stable\", \"destination_entity\": \"pod\"}\\n\\n{\"source_entity\": \"creation_method!=manual\", \"description\": \"selects pods created by any method other than manual\", \"destination_entity\": \"pods\"}\\n\\n{\"source_entity\": \"env notin (prod,devel)\", \"description\": \"selects pods with env label set to any value other than prod or devel\", \"destination_entity\": \"pods\"}\\n\\n{\"source_entity\": \"app=as\", \"description\": \"labels account service microservice pod as stable\", \"destination_entity\": \"pod\"}\\n\\n{\"source_entity\": \"pod\", \"description\": \"can be selected and performed actions on using label selectors\", \"destination_entity\": \"label selectors\"}\\n\\n{\"source_entity\": \"rel=canary\", \"description\": \"selects pods with release canary\", \"destination_entity\": \"pods\"}\\n\\n{\"source_entity\": \"Kubernetes\", \"description\": \"uses label selectors internally for listing and performing actions on pods\", \"destination_entity\": \"label selectors\"}'},\n",
       " {'page': 105,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '73\\nUsing labels and selectors to constrain pod scheduling\\n3.5\\nUsing labels and selectors to constrain pod scheduling\\nAll the pods you’ve created so far have been scheduled pretty much randomly across\\nyour worker nodes. As I’ve mentioned in the previous chapter, this is the proper way\\nof working in a Kubernetes cluster. Because Kubernetes exposes all the nodes in the\\ncluster as a single, large deployment platform, it shouldn’t matter to you what node a\\npod is scheduled to. Because each pod gets the exact amount of computational\\nresources it requests (CPU, memory, and so on) and its accessibility from other pods\\nisn’t at all affected by the node the pod is scheduled to, usually there shouldn’t be any\\nneed for you to tell Kubernetes exactly where to schedule your pods. \\n Certain cases exist, however, where you’ll want to have at least a little say in where\\na pod should be scheduled. A good example is when your hardware infrastructure\\nisn’t homogenous. If part of your worker nodes have spinning hard drives, whereas\\nothers have SSDs, you may want to schedule certain pods to one group of nodes and\\nthe rest to the other. Another example is when you need to schedule pods perform-\\ning intensive GPU-based computation only to nodes that provide the required GPU\\nacceleration. \\n You never want to say specifically what node a pod should be scheduled to, because\\nthat would couple the application to the infrastructure, whereas the whole idea of\\nKubernetes is hiding the actual infrastructure from the apps that run on it. But if you\\nwant to have a say in where a pod should be scheduled, instead of specifying an exact\\nnode, you should describe the node requirements and then let Kubernetes select a\\nnode that matches those requirements. This can be done through node labels and\\nnode label selectors. \\nUI pod\\napp: ui\\nrel: stable\\nrel=stable\\napp=ui\\nAccount\\nService\\npod\\napp: as\\nrel: stable\\napp=as\\napp: pc\\nrel: stable\\napp=pc\\napp: sc\\nrel: stable\\napp=sc\\napp: os\\nrel: stable\\napp=os\\nProduct\\nCatalog\\npod\\nShopping\\nCart\\npod\\nOrder\\nService\\npod\\nUI pod\\napp: ui\\nrel: beta\\nrel=beta\\napp: pc\\nrel: beta\\napp: os\\nrel: beta\\nProduct\\nCatalog\\npod\\nOrder\\nService\\npod\\nrel=canary\\nAccount\\nService\\npod\\napp: as\\nrel: canary\\napp: pc\\nrel: canary\\napp: os\\nrel: canary\\nProduct\\nCatalog\\npod\\nOrder\\nService\\npod\\nFigure 3.9\\nSelecting pods with multiple label selectors\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  app=ui app=as\\nrel=stable\\napp: ui app: as\\nAccount\\nUI pod rel: stable Service rel: stable\\npod  \\\n",
       "   0                                               None                                                 \n",
       "   1                 app: ui rel=beta\\nUI pod rel: beta                                                 \n",
       "   2  rel=canary\\napp: as\\nAccount\\nService rel: can...                                                 \n",
       "   \n",
       "                                          app=pc  \\\n",
       "   0  app: pc\\nProduct\\nCatalog rel: stable\\npod   \n",
       "   1    app: pc\\nProduct\\nCatalog rel: beta\\npod   \n",
       "   2  app: pc\\nProduct\\nCatalog rel: canary\\npod   \n",
       "   \n",
       "     app=sc app=os\\napp: sc app: os\\nShopping Order\\nCart rel: stable Service rel: stable\\npod pod  \n",
       "   0                                               None                                             \n",
       "   1             app: os\\nOrder\\nService rel: beta\\npod                                             \n",
       "   2           app: os\\nOrder\\nService rel: canary\\npod                                             ],\n",
       "  'entities': [{'entity': 'labels',\n",
       "    'description': 'A way to add metadata to Kubernetes resources, such as pods and nodes.',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'selectors',\n",
       "    'description': 'Used to constrain pod scheduling based on node labels.',\n",
       "    'category': 'Kubernetes Concept'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'The smallest deployable unit in a Kubernetes cluster.',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'A physical or virtual machine that can run containers and pods.',\n",
       "    'category': 'Kubernetes Component'},\n",
       "   {'entity': 'worker nodes',\n",
       "    'description': 'Nodes that have the capacity to run pods and are responsible for performing workloads.',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'deployment platform',\n",
       "    'description': 'A concept in Kubernetes that exposes all nodes as a single, large platform for deploying resources.',\n",
       "    'category': 'Kubernetes Concept'},\n",
       "   {'entity': 'CPU',\n",
       "    'description': 'A type of computational resource required by pods to run efficiently.',\n",
       "    'category': 'Hardware Component'},\n",
       "   {'entity': 'memory',\n",
       "    'description': 'A type of computational resource required by pods to run efficiently.',\n",
       "    'category': 'Hardware Component'},\n",
       "   {'entity': 'GPU acceleration',\n",
       "    'description': 'A feature that provides high-performance computing capabilities using graphics processing units.',\n",
       "    'category': 'Hardware Component'},\n",
       "   {'entity': 'node labels',\n",
       "    'description': 'Metadata added to nodes to describe their characteristics and capabilities.',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'node label selectors',\n",
       "    'description': 'Used to select nodes based on their labels that match specific criteria.',\n",
       "    'category': 'Kubernetes Concept'},\n",
       "   {'entity': 'app: ui',\n",
       "    'description': 'A label assigned to a pod, indicating it is the UI component of an application.',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'rel: stable',\n",
       "    'description': 'A label assigned to a pod, indicating its release status as stable.',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'app: as',\n",
       "    'description': 'A label assigned to a pod, indicating it is the Account Service component of an application.',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A resource in Kubernetes that provides a network interface for accessing pods and services.',\n",
       "    'category': 'Kubernetes Component'},\n",
       "   {'entity': 'Order Service',\n",
       "    'description': 'A service in Kubernetes that handles order-related operations.',\n",
       "    'category': 'Kubernetes Resource'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"exposes all nodes as a single deployment platform\", \"destination_entity\": \"nodes\"},\\n  {\"source_entity\": \"each pod\", \"description\": \"gets exact amount of computational resources it requests (CPU, memory, etc.)\", \"destination_entity\": \"computational resources\"},\\n  {\"source_entity\": \"pod scheduling\", \"description\": \"usually doesn\\'t matter what node a pod is scheduled to\", \"destination_entity\": \"node\"},\\n  {\"source_entity\": \"hardware infrastructure\", \"description\": \"may have spinning hard drives or SSDs\", \"destination_entity\": \"hard drives\"},\\n  {\"source_entity\": \"pod scheduling\", \"description\": \"should be scheduled on nodes with required GPU acceleration\", \"destination_entity\": \"GPU acceleration\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"hides actual infrastructure from apps that run on it\", \"destination_entity\": \"applications\"},\\n  {\"source_entity\": \"node requirements\", \"description\": \"can be described through node labels and selectors\", \"destination_entity\": \"labels and selectors\"},\\n  {\"source_entity\": \"UI pod\", \"description\": \"has label app: ui and relation rel: stable\", \"destination_entity\": \"stable relation\"},\\n  {\"source_entity\": \"Account Service pod\", \"description\": \"has label app: as and relation rel: stable\", \"destination_entity\": \"stable relation\"},\\n  {\"source_entity\": \"Product Catalog pod\", \"description\": \"has label app: pc and relation rel: stable\", \"destination_entity\": \"stable relation\"},\\n  {\"source_entity\": \"Order Service pod\", \"description\": \"has label app: os and relation rel: stable\", \"destination_entity\": \"stable relation\"},\\n  {\"source_entity\": \"UI pod\", \"description\": \"should be scheduled on nodes with GPU acceleration for beta relation\", \"destination_entity\": \"GPU acceleration\"},\\n  {\"source_entity\": \"Product Catalog pod\", \"description\": \"has label app: pc and relation rel: beta\", \"destination_entity\": \"beta relation\"},\\n  {\"source_entity\": \"Order Service pod\", \"description\": \"has label app: os and relation rel: beta\", \"destination_entity\": \"beta relation\"},\\n  {\"source_entity\": \"Account Service pod\", \"description\": \"should be scheduled on nodes with GPU acceleration for canary relation\", \"destination_entity\": \"GPU acceleration\"},\\n  {\"source_entity\": \"node labels\", \"description\": \"can be used to select pods based on multiple label selectors\", \"destination_entity\": \"pods and selectors\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"exposes all nodes as a single deployment platform for UI pod with rel: stable\", \"destination_entity\": \"nodes and labels\"},\\n  {\"source_entity\": \"node label selectors\", \"description\": \"can be used to select pods based on multiple labels\", \"destination_entity\": \"pods and labels\"}\\n]\\n```'},\n",
       " {'page': 106,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '74\\nCHAPTER 3\\nPods: running containers in Kubernetes\\n3.5.1\\nUsing labels for categorizing worker nodes\\nAs you learned earlier, pods aren’t the only Kubernetes resource type that you can\\nattach a label to. Labels can be attached to any Kubernetes object, including nodes.\\nUsually, when the ops team adds a new node to the cluster, they’ll categorize the node\\nby attaching labels specifying the type of hardware the node provides or anything else\\nthat may come in handy when scheduling pods. \\n Let’s imagine one of the nodes in your cluster contains a GPU meant to be used\\nfor general-purpose GPU computing. You want to add a label to the node showing this\\nfeature. You’re going to add the label gpu=true to one of your nodes (pick one out of\\nthe list returned by kubectl get nodes):\\n$ kubectl label node gke-kubia-85f6-node-0rrx gpu=true\\nnode \"gke-kubia-85f6-node-0rrx\" labeled\\nNow you can use a label selector when listing the nodes, like you did before with pods.\\nList only nodes that include the label gpu=true:\\n$ kubectl get nodes -l gpu=true\\nNAME                      STATUS AGE\\ngke-kubia-85f6-node-0rrx  Ready  1d\\nAs expected, only one node has this label. You can also try listing all the nodes and tell\\nkubectl to display an additional column showing the values of each node’s gpu label\\n(kubectl get nodes -L gpu).\\n3.5.2\\nScheduling pods to specific nodes\\nNow imagine you want to deploy a new pod that needs a GPU to perform its work.\\nTo ask the scheduler to only choose among the nodes that provide a GPU, you’ll\\nadd a node selector to the pod’s YAML. Create a file called kubia-gpu.yaml with the\\nfollowing listing’s contents and then use kubectl create -f kubia-gpu.yaml to cre-\\nate the pod.\\napiVersion: v1                                         \\nkind: Pod                                              \\nmetadata:                                              \\n  name: kubia-gpu\\nspec: \\n  nodeSelector:               \\n    gpu: \"true\"               \\n  containers: \\n  - image: luksa/kubia\\n    name: kubia\\nListing 3.4\\nUsing a label selector to schedule a pod to a specific node: kubia-gpu.yaml\\nnodeSelector tells Kubernetes \\nto deploy this pod only to \\nnodes containing the \\ngpu=true label.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'a virtual machine or a physical server in a Kubernetes cluster',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'label',\n",
       "    'description': 'a key-value pair used to categorize and identify objects in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'gpu',\n",
       "    'description': 'Graphics Processing Unit, a type of hardware that can be used for general-purpose computing',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'the basic execution unit in Kubernetes, equivalent to a container',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'nodeSelector',\n",
       "    'description': 'a field used in pod specification to select specific nodes for deployment',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'gpu=true',\n",
       "    'description': 'a label value indicating that a node has a GPU available',\n",
       "    'category': 'label'},\n",
       "   {'entity': 'kubectl get nodes',\n",
       "    'description': 'command used to list all nodes in a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl label node',\n",
       "    'description': 'command used to add or update labels on a node',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubia-gpu.yaml',\n",
       "    'description': 'a YAML file containing the specification for a pod that requires a GPU',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ops team\",\\n    \"description\": \"adds a new node to the cluster\",\\n    \"destination_entity\": \"cluster\"\\n  },\\n  {\\n    \"source_entity\": \"ops team\",\\n    \"description\": \"categorizes the node by attaching labels\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"ops team\",\\n    \"description\": \"attaches labels specifying hardware type\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"you\",\\n    \"description\": \"adds a label to the node showing GPU feature\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"labels a node with gpu=true\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"lists nodes with gpu=true label\",\\n    \"destination_entity\": \"nodes\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"displays an additional column showing node\\'s gpu label\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"scheduler\",\\n    \"description\": \"chooses among nodes providing a GPU\",\\n    \"destination_entity\": \"nodes\"\\n  },\\n  {\\n    \"source_entity\": \"pod spec\",\\n    \"description\": \"sets nodeSelector to gpu=true\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl create\",\\n    \"description\": \"creates the pod with kubia-gpu.yaml\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"nodeSelector\",\\n    \"description\": \"tells Kubernetes to deploy pod only to nodes containing gpu=true label\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl get nodes\",\\n    \"description\": \"lists nodes with gpu=true label\",\\n    \"destination_entity\": \"nodes\"\\n  }\\n]\\n```'},\n",
       " {'page': 107,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '75\\nAnnotating pods\\nYou’ve added a nodeSelector field under the spec section. When you create the pod,\\nthe scheduler will only choose among the nodes that contain the gpu=true label\\n(which is only a single node in your case). \\n3.5.3\\nScheduling to one specific node\\nSimilarly, you could also schedule a pod to an exact node, because each node also has\\na unique label with the key kubernetes.io/hostname and value set to the actual host-\\nname of the node. But setting the nodeSelector to a specific node by the hostname\\nlabel may lead to the pod being unschedulable if the node is offline. You shouldn’t\\nthink in terms of individual nodes. Always think about logical groups of nodes that sat-\\nisfy certain criteria specified through label selectors.\\n This was a quick demonstration of how labels and label selectors work and how\\nthey can be used to influence the operation of Kubernetes. The importance and use-\\nfulness of label selectors will become even more evident when we talk about Replication-\\nControllers and Services in the next two chapters. \\nNOTE\\nAdditional ways of influencing which node a pod is scheduled to are\\ncovered in chapter 16.\\n3.6\\nAnnotating pods\\nIn addition to labels, pods and other objects can also contain annotations. Annotations\\nare also key-value pairs, so in essence, they’re similar to labels, but they aren’t meant to\\nhold identifying information. They can’t be used to group objects the way labels can.\\nWhile objects can be selected through label selectors, there’s no such thing as an\\nannotation selector. \\n On the other hand, annotations can hold much larger pieces of information and\\nare primarily meant to be used by tools. Certain annotations are automatically added\\nto objects by Kubernetes, but others are added by users manually.\\n Annotations are also commonly used when introducing new features to Kuberne-\\ntes. Usually, alpha and beta versions of new features don’t introduce any new fields to\\nAPI objects. Annotations are used instead of fields, and then once the required API\\nchanges have become clear and been agreed upon by the Kubernetes developers, new\\nfields are introduced and the related annotations deprecated.\\n A great use of annotations is adding descriptions for each pod or other API object,\\nso that everyone using the cluster can quickly look up information about each individ-\\nual object. For example, an annotation used to specify the name of the person who\\ncreated the object can make collaboration between everyone working on the cluster\\nmuch easier.\\n3.6.1\\nLooking up an object’s annotations\\nLet’s see an example of an annotation that Kubernetes added automatically to the\\npod you created in the previous chapter. To see the annotations, you’ll need to\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'nodeSelector',\n",
       "    'description': 'A field under the spec section used to select a node for a pod',\n",
       "    'category': 'kubernetes/config'},\n",
       "   {'entity': 'gpu=true label',\n",
       "    'description': 'A label on a node indicating it has a GPU',\n",
       "    'category': 'hardware/hardware_attribute'},\n",
       "   {'entity': 'nodeSelector by hostname',\n",
       "    'description': 'Selecting a node by its hostname using the kubernetes.io/hostname label',\n",
       "    'category': 'kubernetes/config'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'Key-value pairs used to identify and group objects in Kubernetes',\n",
       "    'category': 'kubernetes/config'},\n",
       "   {'entity': 'label selectors',\n",
       "    'description': 'Used to select objects based on labels',\n",
       "    'category': 'kubernetes/config'},\n",
       "   {'entity': 'annotations',\n",
       "    'description': 'Key-value pairs used to hold larger pieces of information, primarily meant for tools',\n",
       "    'category': 'kubernetes/config'},\n",
       "   {'entity': 'annotation selector',\n",
       "    'description': 'No such thing as an annotation selector in Kubernetes',\n",
       "    'category': 'kubernetes/config'},\n",
       "   {'entity': 'Replication-Controllers',\n",
       "    'description': 'Kubernetes resources that manage sets of replicas',\n",
       "    'category': 'kubernetes/resource'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'Kubernetes resources that provide load balancing and network connectivity',\n",
       "    'category': 'kubernetes/resource'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A Kubernetes object representing a containerized application',\n",
       "    'category': 'kubernetes/resource'},\n",
       "   {'entity': 'API objects',\n",
       "    'description': 'Kubernetes resources that can be manipulated through the API',\n",
       "    'category': 'kubernetes/config'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"scheduler\",\\n    \"description\": \"choose nodes that contain the gpu=true label\",\\n    \"destination_entity\": \"nodes\"\\n  },\\n  {\\n    \"source_entity\": \"scheduler\",\\n    \"description\": \"schedule a pod to an exact node\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"labels\",\\n    \"description\": \"contain identifying information\",\\n    \"destination_entity\": \"objects\"\\n  },\\n  {\\n    \"source_entity\": \"label selectors\",\\n    \"description\": \"influence the operation of Kubernetes\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"annotations\",\\n    \"description\": \"hold larger pieces of information\",\\n    \"destination_entity\": \"objects\"\\n  },\\n  {\\n    \"source_entity\": \"tools\",\\n    \"description\": \"use annotations for new features\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"users\",\\n    \"description\": \"add annotations manually to objects\",\\n    \"destination_entity\": \"API objects\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes developers\",\\n    \"description\": \"agree on API changes and introduce new fields\",\\n    \"destination_entity\": \"API objects\"\\n  },\\n  {\\n    \"source_entity\": \"annotations\",\\n    \"description\": \"add descriptions for each pod or object\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"annotation selector\",\\n    \"description\": \"does not exist\",\\n    \"destination_entity\": \"null\"\\n  },\\n  {\\n    \"source_entity\": \"nodeSelector by hostname\",\\n    \"description\": \"may lead to the pod being unschedulable if the node is offline\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"label selectors\",\\n    \"description\": \"become even more evident when we talk about Replication-Controllers and Services\",\\n    \"destination_entity\": \"Replication-Controllers and Services\"\\n  }\\n]\\n```\\n\\nNote that the relation between `nodeSelector by hostname` and `pod` is a warning, not an action.'},\n",
       " {'page': 108,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '76\\nCHAPTER 3\\nPods: running containers in Kubernetes\\nrequest the full YAML of the pod or use the kubectl describe command. You’ll use the\\nfirst option in the following listing.\\n$ kubectl get po kubia-zxzij -o yaml\\napiVersion: v1\\nkind: pod\\nmetadata:\\n  annotations:\\n    kubernetes.io/created-by: |\\n      {\"kind\":\"SerializedReference\", \"apiVersion\":\"v1\", \\n      \"reference\":{\"kind\":\"ReplicationController\", \"namespace\":\"default\", ...\\nWithout going into too many details, as you can see, the kubernetes.io/created-by\\nannotation holds JSON data about the object that created the pod. That’s not some-\\nthing you’d want to put into a label. Labels should be short, whereas annotations can\\ncontain relatively large blobs of data (up to 256 KB in total).\\nNOTE\\nThe kubernetes.io/created-by annotations was deprecated in ver-\\nsion 1.8 and will be removed in 1.9, so you will no longer see it in the YAML.\\n3.6.2\\nAdding and modifying annotations\\nAnnotations can obviously be added to pods at creation time, the same way labels can.\\nThey can also be added to or modified on existing pods later. The simplest way to add\\nan annotation to an existing object is through the kubectl annotate command. \\n You’ll try adding an annotation to your kubia-manual pod now:\\n$ kubectl annotate pod kubia-manual mycompany.com/someannotation=\"foo bar\"\\npod \"kubia-manual\" annotated\\nYou added the annotation mycompany.com/someannotation with the value foo bar.\\nIt’s a good idea to use this format for annotation keys to prevent key collisions. When\\ndifferent tools or libraries add annotations to objects, they may accidentally override\\neach other’s annotations if they don’t use unique prefixes like you did here.\\n You can use kubectl describe to see the annotation you added:\\n$ kubectl describe pod kubia-manual\\n...\\nAnnotations:    mycompany.com/someannotation=foo bar\\n...\\n3.7\\nUsing namespaces to group resources\\nLet’s turn back to labels for a moment. We’ve seen how they organize pods and other\\nobjects into groups. Because each object can have multiple labels, those groups of\\nobjects can overlap. Plus, when working with the cluster (through kubectl for example),\\nif you don’t explicitly specify a label selector, you’ll always see all objects. \\nListing 3.5\\nA pod’s annotations\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command to interact with Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'yaml',\n",
       "    'description': 'file format for configuration data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'lightweight and ephemeral container in Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'annotations',\n",
       "    'description': 'metadata that can be attached to objects',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'key-value pairs that identify and classify objects',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl get',\n",
       "    'description': 'command to retrieve information about pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl describe',\n",
       "    'description': 'command to view detailed information about an object',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'logical grouping of resources in Kubernetes',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'object that ensures a specified number of replicas are running at any given time',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[{\"source_entity\": \"kubectl\", \"description\": \"used to request the full YAML of the pod or use the kubectl describe command.\", \"destination_entity\": \"yaml\"},\\n{\"source_entity\": \"kubectl\", \"description\": \"used to see the annotation that was added to a pod.\", \"destination_entity\": \"kubectl describe\"},\\n{\"source_entity\": \"Kubernetes\", \"description\": \"has an annotation named kubernetes.io/created-by that holds JSON data about the object that created the pod.\", \"destination_entity\": \"annotations\"},\\n{\"source_entity\": \"labels\", \"description\": \"organize pods and other objects into groups, but can overlap when working with the cluster.\", \"destination_entity\": \"Pods\"},\\n{\"source_entity\": \"kubectl get\", \"description\": \"used to get information about a pod, including its annotations.\", \"destination_entity\": \"annotations\"},\\n{\"source_entity\": \"yaml\", \"description\": \"contains data about the object that created the pod, as specified in the kubernetes.io/created-by annotation.\", \"destination_entity\": \"Kubernetes\"},\\n{\"source_entity\": \"ReplicationController\", \"description\": \"is referenced in the kubernetes.io/created-by annotation of a pod.\", \"destination_entity\": \"annotations\"},\\n{\"source_entity\": \"namespace\", \"description\": \"used to group resources in Kubernetes, but can overlap when working with the cluster.\", \"destination_entity\": \"labels\"},\\n{\"source_entity\": \"kubectl annotate\", \"description\": \"used to add an annotation to an existing object or modify one on a pod.\", \"destination_entity\": \"annotations\"},\\n{\"source_entity\": \"kubectl describe\", \"description\": \"used to see information about a pod, including its annotations.\", \"destination_entity\": \"annotations\"}]'},\n",
       " {'page': 109,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '77\\nUsing namespaces to group resources\\n But what about times when you want to split objects into separate, non-overlapping\\ngroups? You may want to only operate inside one group at a time. For this and other\\nreasons, Kubernetes also groups objects into namespaces. These aren’t the Linux\\nnamespaces we talked about in chapter 2, which are used to isolate processes from\\neach other. Kubernetes namespaces provide a scope for objects names. Instead of hav-\\ning all your resources in one single namespace, you can split them into multiple name-\\nspaces, which also allows you to use the same resource names multiple times (across\\ndifferent namespaces).\\n3.7.1\\nUnderstanding the need for namespaces\\nUsing multiple namespaces allows you to split complex systems with numerous com-\\nponents into smaller distinct groups. They can also be used for separating resources\\nin a multi-tenant environment, splitting up resources into production, development,\\nand QA environments, or in any other way you may need. Resource names only need\\nto be unique within a namespace. Two different namespaces can contain resources of\\nthe same name. But, while most types of resources are namespaced, a few aren’t. One\\nof them is the Node resource, which is global and not tied to a single namespace.\\nYou’ll learn about other cluster-level resources in later chapters.\\n Let’s see how to use namespaces now.\\n3.7.2\\nDiscovering other namespaces and their pods\\nFirst, let’s list all namespaces in your cluster:\\n$ kubectl get ns\\nNAME          LABELS    STATUS    AGE\\ndefault       <none>    Active    1h\\nkube-public   <none>    Active    1h\\nkube-system   <none>    Active    1h\\nUp to this point, you’ve operated only in the default namespace. When listing resources\\nwith the kubectl get command, you’ve never specified the namespace explicitly, so\\nkubectl always defaulted to the default namespace, showing you only the objects in\\nthat namespace. But as you can see from the list, the kube-public and the kube-system\\nnamespaces also exist. Let’s look at the pods that belong to the kube-system name-\\nspace, by telling kubectl to list pods in that namespace only:\\n$ kubectl get po --namespace kube-system\\nNAME                                 READY     STATUS    RESTARTS   AGE\\nfluentd-cloud-kubia-e8fe-node-txje   1/1       Running   0          1h\\nheapster-v11-fz1ge                   1/1       Running   0          1h\\nkube-dns-v9-p8a4t                    0/4       Pending   0          1h\\nkube-ui-v4-kdlai                     1/1       Running   0          1h\\nl7-lb-controller-v0.5.2-bue96        2/2       Running   92         1h\\nTIP\\nYou can also use -n instead of --namespace.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'namespaces',\n",
       "    'description': 'groups objects into namespaces to provide a scope for object names',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'resources',\n",
       "    'description': 'objects that can be created and managed by Kubernetes',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'Linux namespaces',\n",
       "    'description': 'used to isolate processes from each other',\n",
       "    'category': 'operating_system'},\n",
       "   {'entity': 'processes',\n",
       "    'description': 'running instances of a program',\n",
       "    'category': 'operating_system'},\n",
       "   {'entity': 'Kubectl',\n",
       "    'description': 'command-line tool for managing Kubernetes resources',\n",
       "    'category': 'kubernetes_tool'},\n",
       "   {'entity': 'get command',\n",
       "    'description': 'used to list resources in a namespace or cluster',\n",
       "    'category': 'kubernetes_command'},\n",
       "   {'entity': 'default namespace',\n",
       "    'description': 'the default namespace for resources created by users',\n",
       "    'category': 'kubernetes_namespace'},\n",
       "   {'entity': 'kube-public namespace',\n",
       "    'description': 'a built-in namespace that contains publicly readable data',\n",
       "    'category': 'kubernetes_namespace'},\n",
       "   {'entity': 'kube-system namespace',\n",
       "    'description': 'a built-in namespace that contains system-level resources',\n",
       "    'category': 'kubernetes_namespace'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'the smallest deployable unit in Kubernetes, containing one or more containers',\n",
       "    'category': 'kubernetes_resource'},\n",
       "   {'entity': '-n option',\n",
       "    'description': 'shortcut for specifying a namespace when using Kubectl commands',\n",
       "    'category': 'kubernetes_option'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"provides a scope for objects names\", \"destination_entity\": \"namespaces\"},\\n  {\"source_entity\": \"namespaces\", \"description\": \"allow resources to be unique within a namespace\", \"destination_entity\": \"resources\"},\\n  {\"source_entity\": \"kube-public namespace\", \"description\": \"exists in the cluster and is not empty\", \"destination_entity\": \"cluster\"},\\n  {\"source_entity\": \"kube-system namespace\", \"description\": \"exists in the cluster and contains pods\", \"destination_entity\": \"cluster\"},\\n  {\"source_entity\": \"Kubectl\", \"description\": \"lists all namespaces in the cluster\", \"destination_entity\": \"namespaces\"},\\n  {\"source_entity\": \"default namespace\", \"description\": \"is the default namespace for kubectl commands\", \"destination_entity\": \"kubectl\"},\\n  {\"source_entity\": \"Kubectl\", \"description\": \"defaults to the default namespace when listing resources\", \"destination_entity\": \"default namespace\"},\\n  {\"source_entity\": \"kube-system namespace\", \"description\": \"contains pods that belong to it\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"get command\", \"description\": \"is used to list resources in a specific namespace\", \"destination_entity\": \"resources\"},\\n  {\"source_entity\": \"namespaces\", \"description\": \"are not the same as Linux namespaces\", \"destination_entity\": \"Linux namespaces\"},\\n  {\"source_entity\": \"-n option\", \"description\": \"is an alternative way to specify a namespace\", \"destination_entity\": \"kubectl\"},\\n  {\"source_entity\": \"pods\", \"description\": \"exist in multiple namespaces, including kube-system and default\", \"destination_entity\": \"namespaces\"}\\n]\\n\\nNote that some of these relations may seem obvious or redundant, but I\\'ve tried to extract all possible connections between the entities mentioned in the document page.'},\n",
       " {'page': 110,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '78\\nCHAPTER 3\\nPods: running containers in Kubernetes\\nYou’ll learn about these pods later in the book (don’t worry if the pods shown here\\ndon’t match the ones on your system exactly). It’s clear from the name of the name-\\nspace that these are resources related to the Kubernetes system itself. By having\\nthem in this separate namespace, it keeps everything nicely organized. If they were\\nall in the default namespace, mixed in with the resources you create yourself, you’d\\nhave a hard time seeing what belongs where, and you might inadvertently delete sys-\\ntem resources. \\n Namespaces enable you to separate resources that don’t belong together into non-\\noverlapping groups. If several users or groups of users are using the same Kubernetes\\ncluster, and they each manage their own distinct set of resources, they should each use\\ntheir own namespace. This way, they don’t need to take any special care not to inad-\\nvertently modify or delete the other users’ resources and don’t need to concern them-\\nselves with name conflicts, because namespaces provide a scope for resource names,\\nas has already been mentioned.\\n  Besides isolating resources, namespaces are also used for allowing only certain users\\naccess to particular resources and even for limiting the amount of computational\\nresources available to individual users. You’ll learn about this in chapters 12 through 14.\\n3.7.3\\nCreating a namespace\\nA namespace is a Kubernetes resource like any other, so you can create it by posting a\\nYAML file to the Kubernetes API server. Let’s see how to do this now. \\nCREATING A NAMESPACE FROM A YAML FILE\\nFirst, create a custom-namespace.yaml file with the following listing’s contents (you’ll\\nfind the file in the book’s code archive).\\napiVersion: v1\\nkind: Namespace         \\nmetadata:\\n  name: custom-namespace  \\nNow, use kubectl to post the file to the Kubernetes API server:\\n$ kubectl create -f custom-namespace.yaml\\nnamespace \"custom-namespace\" created\\nCREATING A NAMESPACE WITH KUBECTL CREATE NAMESPACE\\nAlthough writing a file like the previous one isn’t a big deal, it’s still a hassle. Luckily,\\nyou can also create namespaces with the dedicated kubectl create namespace com-\\nmand, which is quicker than writing a YAML file. By having you create a YAML mani-\\nfest for the namespace, I wanted to reinforce the idea that everything in Kubernetes\\nListing 3.6\\nA YAML definition of a namespace: custom-namespace.yaml\\nThis says you’re \\ndefining a namespace.\\nThis is the name \\nof the namespace.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': '',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'running containers in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'namespaces',\n",
       "    'description': 'enable you to separate resources that don’t belong together into non-overlapping groups',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'posting a YAML file to the Kubernetes API server',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'YAML',\n",
       "    'description': 'a file used to create a namespace in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'a key in the YAML file for defining a namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'a key in the YAML file for specifying the type of resource (Namespace)',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'a section in the YAML file for providing metadata about the namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'the name of the namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl create namespace',\n",
       "    'description': 'a command for creating a namespace using kubectl',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'namespace', 'description': '', 'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"You\", \"description\": \"will learn about Pods later in the book\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"Namespaces\", \"description\": \"enable you to separate resources that don’t belong together into non-overlapping groups\", \"destination_entity\": \"resources\"},\\n  {\"source_entity\": \"namespaces\", \"description\": \"keep everything nicely organized by having them in a separate namespace\", \"destination_entity\": \"resources\"},\\n  {\"source_entity\": \"you\", \"description\": \"might inadvertently delete system resources if they are all in the default namespace\", \"destination_entity\": \"system resources\"},\\n  {\"source_entity\": \"users or groups of users\", \"description\": \"should each use their own namespace to manage their own distinct set of resources\", \"destination_entity\": \"resources\"},\\n  {\"source_entity\": \"namespaces\", \"description\": \"provide a scope for resource names and prevent name conflicts\", \"destination_entity\": \"resource names\"},\\n  {\"source_entity\": \"you\", \"description\": \"can create a namespace by posting a YAML file to the Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"can be used to post the YAML file to the Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"you\", \"description\": \"can also create a namespace using the dedicated kubectl create namespace command\", \"destination_entity\": \"kubectl\"},\\n  {\"source_entity\": \"metadata\", \"description\": \"contains information about the namespace, including its name\", \"destination_entity\": \"namespace\"},\\n  {\"source_entity\": \"namespaces\", \"description\": \"are used for isolating resources and allowing only certain users access to particular resources\", \"destination_entity\": \"resources\"},\\n  {\"source_entity\": \"kubectl create namespace\", \"description\": \"is a quicker way to create a namespace compared to writing a YAML file\", \"destination_entity\": \"namespace\"}\\n]\\n```\\n\\nNote that some of these relations may seem trivial or obvious, but they are all extracted based on the text and entities provided.'},\n",
       " {'page': 111,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '79\\nUsing namespaces to group resources\\nhas a corresponding API object that you can create, read, update, and delete by post-\\ning a YAML manifest to the API server.\\n You could have created the namespace like this:\\n$ kubectl create namespace custom-namespace\\nnamespace \"custom-namespace\" created\\nNOTE\\nAlthough most objects’ names must conform to the naming conven-\\ntions specified in RFC 1035 (Domain names), which means they may contain\\nonly letters, digits, dashes, and dots, namespaces (and a few others) aren’t\\nallowed to contain dots. \\n3.7.4\\nManaging objects in other namespaces\\nTo create resources in the namespace you’ve created, either add a namespace: custom-\\nnamespace entry to the metadata section, or specify the namespace when creating the\\nresource with the kubectl create command:\\n$ kubectl create -f kubia-manual.yaml -n custom-namespace\\npod \"kubia-manual\" created\\nYou now have two pods with the same name (kubia-manual). One is in the default\\nnamespace, and the other is in your custom-namespace.\\n When listing, describing, modifying, or deleting objects in other namespaces, you\\nneed to pass the --namespace (or -n) flag to kubectl. If you don’t specify the name-\\nspace, kubectl performs the action in the default namespace configured in the cur-\\nrent kubectl context. The current context’s namespace and the current context itself\\ncan be changed through kubectl config commands. To learn more about managing\\nkubectl contexts, refer to appendix A. \\nTIP\\nTo quickly switch to a different namespace, you can set up the following\\nalias: alias kcd=\\'kubectl config set-context $(kubectl config current-\\ncontext) --namespace \\'. You can then switch between namespaces using kcd\\nsome-namespace.\\n3.7.5\\nUnderstanding the isolation provided by namespaces\\nTo wrap up this section about namespaces, let me explain what namespaces don’t pro-\\nvide—at least not out of the box. Although namespaces allow you to isolate objects\\ninto distinct groups, which allows you to operate only on those belonging to the speci-\\nfied namespace, they don’t provide any kind of isolation of running objects. \\n For example, you may think that when different users deploy pods across different\\nnamespaces, those pods are isolated from each other and can’t communicate, but that’s\\nnot necessarily the case. Whether namespaces provide network isolation depends on\\nwhich networking solution is deployed with Kubernetes. When the solution doesn’t\\nprovide inter-namespace network isolation, if a pod in namespace foo knows the IP\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'namespaces',\n",
       "    'description': 'a way to group resources in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API object',\n",
       "    'description': 'an API endpoint that can be created, read, updated, and deleted by posting a YAML manifest',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the command-line tool used to interact with Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'create namespace command',\n",
       "    'description': 'a command used to create a new namespace',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'a way to group resources in Kubernetes, which is not allowed to contain dots',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata section',\n",
       "    'description': 'a part of the YAML manifest that specifies the namespace for a resource',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a basic execution unit in Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'resource',\n",
       "    'description': 'an object in Kubernetes, such as a pod or service',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'context',\n",
       "    'description': 'the current session of kubectl that specifies the namespace and other settings',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'alias',\n",
       "    'description': 'a shortcut for a command or sequence of commands',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubectl config',\n",
       "    'description': 'a command used to manage the configuration of kubectl',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'namespace flag',\n",
       "    'description': 'a flag that specifies the namespace when using kubectl',\n",
       "    'category': 'flag'},\n",
       "   {'entity': 'networking solution',\n",
       "    'description': 'a software component that manages network connectivity between pods in different namespaces',\n",
       "    'category': 'hardware/network'},\n",
       "   {'entity': 'pod isolation',\n",
       "    'description': 'the ability to isolate pods from each other based on namespace or network settings',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubectl config\",\\n    \"description\": \"Used to set the current context\\'s namespace and change the current context itself.\",\\n    \"destination_entity\": \"Context\"\\n  },\\n  {\\n    \"source_entity\": \"Namespace flag\",\\n    \"description\": \"Needed to pass when listing, describing, modifying, or deleting objects in other namespaces.\",\\n    \"destination_entity\": \"Namespace\"\\n  },\\n  {\\n    \"source_entity\": \"Networking solution\",\\n    \"description\": \"Determined whether namespaces provide network isolation.\",\\n    \"destination_entity\": \"Namespaces\"\\n  },\\n  {\\n    \"source_entity\": \"Create namespace command\",\\n    \"description\": \"Used to create resources in the specified namespace, either by adding a namespace entry to metadata or specifying the namespace with -n flag.\",\\n    \"destination_entity\": \"Namespace\"\\n  },\\n  {\\n    \"source_entity\": \"Metadata section\",\\n    \"description\": \"Where you can add a namespace entry to specify the resource\\'s namespace.\",\\n    \"destination_entity\": \"Resource\"\\n  },\\n  {\\n    \"source_entity\": \"Alias\",\\n    \"description\": \"Can be set up to quickly switch to a different namespace using kcd some-namespace.\",\\n    \"destination_entity\": \"Namespace\"\\n  },\\n  {\\n    \"source_entity\": \"Pod isolation\",\\n    \"description\": \"Not provided by namespaces out of the box, even though objects can be isolated into distinct groups.\",\\n    \"destination_entity\": \"Namespaces\"\\n  },\\n  {\\n    \"source_entity\": \"API object\",\\n    \"description\": \"Can be created, read, updated, and deleted by posting a YAML manifest to the API server.\",\\n    \"destination_entity\": \"Namespace\"\\n  },\\n  {\\n    \"source_entity\": \"Kubectl\",\\n    \"description\": \"Used to create resources in other namespaces, either by adding a namespace entry to metadata or specifying the namespace with -n flag.\",\\n    \"destination_entity\": \"Resource\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"Can be created, listed, described, modified, and deleted using kubectl commands, but needs namespace flag if not in default namespace.\",\\n    \"destination_entity\": \"Namespace\"\\n  },\\n  {\\n    \"source_entity\": \"Context\",\\n    \"description\": \"Can be changed through kubectl config commands to switch between namespaces.\",\\n    \"destination_entity\": \"Namespaces\"\\n  },\\n  {\\n    \"source_entity\": \"Resource\",\\n    \"description\": \"Needs namespace entry in metadata section when creating resources, or -n flag with kubectl create command if not in default namespace.\",\\n    \"destination_entity\": \"Namespace\"\\n  }\\n]\\n```'},\n",
       " {'page': 112,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '80\\nCHAPTER 3\\nPods: running containers in Kubernetes\\naddress of a pod in namespace bar, there is nothing preventing it from sending traffic,\\nsuch as HTTP requests, to the other pod. \\n3.8\\nStopping and removing pods\\nYou’ve created a number of pods, which should all still be running. You have four\\npods running in the default namespace and one pod in custom-namespace. You’re\\ngoing to stop all of them now, because you don’t need them anymore.\\n3.8.1\\nDeleting a pod by name\\nFirst, delete the kubia-gpu pod by name:\\n$ kubectl delete po kubia-gpu\\npod \"kubia-gpu\" deleted\\nBy deleting a pod, you’re instructing Kubernetes to terminate all the containers that are\\npart of that pod. Kubernetes sends a SIGTERM signal to the process and waits a certain\\nnumber of seconds (30 by default) for it to shut down gracefully. If it doesn’t shut down\\nin time, the process is then killed through SIGKILL. To make sure your processes are\\nalways shut down gracefully, they need to handle the SIGTERM signal properly. \\nTIP\\nYou can also delete more than one pod by specifying multiple, space-sep-\\narated names (for example, kubectl delete po pod1 pod2).\\n3.8.2\\nDeleting pods using label selectors\\nInstead of specifying each pod to delete by name, you’ll now use what you’ve learned\\nabout label selectors to stop both the kubia-manual and the kubia-manual-v2 pod.\\nBoth pods include the creation_method=manual label, so you can delete them by\\nusing a label selector:\\n$ kubectl delete po -l creation_method=manual\\npod \"kubia-manual\" deleted\\npod \"kubia-manual-v2\" deleted \\nIn the earlier microservices example, where you had tens (or possibly hundreds) of\\npods, you could, for instance, delete all canary pods at once by specifying the\\nrel=canary label selector (visualized in figure 3.10):\\n$ kubectl delete po -l rel=canary\\n3.8.3\\nDeleting pods by deleting the whole namespace\\nOkay, back to your real pods. What about the pod in the custom-namespace? You no\\nlonger need either the pods in that namespace, or the namespace itself. You can\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'a container running in Kubernetes',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'container orchestration system',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'command-line tool for interacting with Kubernetes',\n",
       "    'category': 'software/command'},\n",
       "   {'entity': 'SIGTERM',\n",
       "    'description': 'signal sent to process to shut down gracefully',\n",
       "    'category': 'process/signal'},\n",
       "   {'entity': 'SIGKILL',\n",
       "    'description': \"signal sent to process to kill if it doesn't shut down in time\",\n",
       "    'category': 'process/signal'},\n",
       "   {'entity': 'label selectors',\n",
       "    'description': 'way to select pods based on labels',\n",
       "    'category': 'software/framework'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'logical grouping of resources in Kubernetes',\n",
       "    'category': 'software/resource'},\n",
       "   {'entity': 'pod name',\n",
       "    'description': 'unique identifier for a pod',\n",
       "    'category': 'software/identifier'},\n",
       "   {'entity': 'creation_method=manual',\n",
       "    'description': 'label selector to select pods created manually',\n",
       "    'category': 'software/label'},\n",
       "   {'entity': 'rel=canary',\n",
       "    'description': 'label selector to select canary pods',\n",
       "    'category': 'software/label'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"delete pod by name\", \"destination_entity\": \"pod name\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"send SIGTERM signal to process\", \"destination_entity\": \"process\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"kill process through SIGKILL\", \"destination_entity\": \"process\"},\\n  {\"source_entity\": \"SIGTERM\", \"description\": \"terminate all containers in a pod\", \"destination_entity\": \"pod name\"},\\n  {\"source_entity\": \"SIGKILL\", \"description\": \"forcefully kill process\", \"destination_entity\": \"process\"},\\n  {\"source_entity\": \"kubectl delete\", \"description\": \"delete pods using label selector\", \"destination_entity\": \"creation_method=manual\"},\\n  {\"source_entity\": \"label selectors\", \"description\": \"stop both kubia-manual and kubia-manual-v2 pod\", \"destination_entity\": \"kubia-manual-v2\"},\\n  {\"source_entity\": \"kubectl delete\", \"description\": \"delete pods using label selector\", \"destination_entity\": \"rel=canary\"},\\n  {\"source_entity\": \"kubectl delete\", \"description\": \"delete whole namespace and its contents\", \"destination_entity\": \"namespace\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"terminate all containers in a pod\", \"destination_entity\": \"pod name\"}\\n]'},\n",
       " {'page': 113,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '81\\nStopping and removing pods\\ndelete the whole namespace (the pods will be deleted along with the namespace auto-\\nmatically), using the following command:\\n$ kubectl delete ns custom-namespace\\nnamespace \"custom-namespace\" deleted\\n3.8.4\\nDeleting all pods in a namespace, while keeping the namespace\\nYou’ve now cleaned up almost everything. But what about the pod you created with\\nthe kubectl run command in chapter 2? That one is still running:\\n$ kubectl get pods\\nNAME            READY   STATUS    RESTARTS   AGE\\nkubia-zxzij     1/1     Running   0          1d    \\nThis time, instead of deleting the specific pod, tell Kubernetes to delete all pods in the\\ncurrent namespace by using the --all option:\\n$ kubectl delete po --all\\npod \"kubia-zxzij\" deleted\\nNow, double check that no pods were left running:\\n$ kubectl get pods\\nNAME            READY   STATUS        RESTARTS   AGE\\nkubia-09as0     1/1     Running       0          1d    \\nkubia-zxzij     1/1     Terminating   0          1d    \\nUI pod\\napp: ui\\nrel: stable\\nrel=stable\\napp=ui\\nAccount\\nService\\npod\\napp: as\\nrel: stable\\napp=as\\napp: pc\\nrel: stable\\napp=pc\\napp: sc\\nrel: stable\\napp=sc\\napp: os\\nrel: stable\\napp=os\\nProduct\\nCatalog\\npod\\nShopping\\nCart\\npod\\nOrder\\nService\\npod\\nUI pod\\napp: ui\\nrel: beta\\nrel=beta\\napp: pc\\nrel: beta\\napp: os\\nrel: beta\\nProduct\\nCatalog\\npod\\nOrder\\nService\\npod\\nrel=canary\\nAccount\\nService\\npod\\napp: as\\nrel: canary\\napp: pc\\nrel: canary\\napp: os\\nrel: canary\\nProduct\\nCatalog\\npod\\nOrder\\nService\\npod\\nFigure 3.10\\nSelecting and deleting all canary pods through the rel=canary label selector\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'delete',\n",
       "    'description': 'Command to delete resources in Kubernetes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'Logical isolation of resources within a cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'Lightweight, short-lived containers in Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'get',\n",
       "    'description': 'Command to retrieve information about resources in Kubernetes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubectl run',\n",
       "    'description': 'Command to create a new pod in Kubernetes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubia-zxzij',\n",
       "    'description': 'Name of a specific pod',\n",
       "    'category': 'container'},\n",
       "   {'entity': '--all',\n",
       "    'description': 'Option to delete all resources matching a selector',\n",
       "    'category': 'option'},\n",
       "   {'entity': 'ui',\n",
       "    'description': 'Label key for the UI application',\n",
       "    'category': 'label'},\n",
       "   {'entity': 'as',\n",
       "    'description': 'Label key for the Account Service application',\n",
       "    'category': 'label'},\n",
       "   {'entity': 'pc',\n",
       "    'description': 'Label key for the Product Catalog application',\n",
       "    'category': 'label'},\n",
       "   {'entity': 'sc',\n",
       "    'description': 'Label key for the Shopping Cart application',\n",
       "    'category': 'label'},\n",
       "   {'entity': 'os',\n",
       "    'description': 'Label key for the Order Service application',\n",
       "    'category': 'label'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"delete a namespace and all its pods\",\\n    \"destination_entity\": \"namespace\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"get information about running pods\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl run\",\\n    \"description\": \"create a pod with the given name\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl delete\",\\n    \"description\": \"delete all pods in the current namespace\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl get\",\\n    \"description\": \"get information about running pods\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"pod \\'kubia-zxzij\\'\",\\n    \"description\": \"delete a specific pod\",\\n    \"destination_entity\": \"kubia-zxzij\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl delete po\",\\n    \"description\": \"delete all pods in the current namespace\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl run\",\\n    \"description\": \"create a pod with the given name and image\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"app=ui\",\\n    \"description\": \"select UI pods for deletion based on label selector\",\\n    \"destination_entity\": \"UI pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl delete po --all\",\\n    \"description\": \"delete all pods in the current namespace\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"Account Service pod\",\\n    \"description\": \"select Account Service pods for deletion based on label selector\",\\n    \"destination_entity\": \"Account Service pod\"\\n  },\\n  {\\n    \"source_entity\": \"Product Catalog pod\",\\n    \"description\": \"select Product Catalog pods for deletion based on label selector\",\\n    \"destination_entity\": \"Product Catalog pod\"\\n  },\\n  {\\n    \"source_entity\": \"Order Service pod\",\\n    \"description\": \"select Order Service pods for deletion based on label selector\",\\n    \"destination_entity\": \"Order Service pod\"\\n  },\\n  {\\n    \"source_entity\": \"Shopping Cart pod\",\\n    \"description\": \"select Shopping Cart pods for deletion based on label selector\",\\n    \"destination_entity\": \"Shopping Cart pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl delete po --all\",\\n    \"description\": \"delete all canary pods in the current namespace\",\\n    \"destination_entity\": \"canary pods\"\\n  },\\n  {\\n    \"source_entity\": \"Account Service pod\",\\n    \"description\": \"select Account Service pods for deletion based on label selector\",\\n    \"destination_entity\": \"Account Service pod\"\\n  }\\n]\\n```\\n\\nNote that I\\'ve assumed some entities as singular, even though they might be plural in the document page (e.g. \"pods\" instead of \"pod\"). If you need any corrections or further clarification, please let me know!'},\n",
       " {'page': 114,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '82\\nCHAPTER 3\\nPods: running containers in Kubernetes\\nWait, what!?! The kubia-zxzij pod is terminating, but a new pod called kubia-09as0,\\nwhich wasn’t there before, has appeared. No matter how many times you delete all\\npods, a new pod called kubia-something will emerge. \\n You may remember you created your first pod with the kubectl run command. In\\nchapter 2, I mentioned that this doesn’t create a pod directly, but instead creates a\\nReplicationController, which then creates the pod. As soon as you delete a pod cre-\\nated by the ReplicationController, it immediately creates a new one. To delete the\\npod, you also need to delete the ReplicationController. \\n3.8.5\\nDeleting (almost) all resources in a namespace\\nYou can delete the ReplicationController and the pods, as well as all the Services\\nyou’ve created, by deleting all resources in the current namespace with a single\\ncommand:\\n$ kubectl delete all --all\\npod \"kubia-09as0\" deleted\\nreplicationcontroller \"kubia\" deleted\\nservice \"kubernetes\" deleted\\nservice \"kubia-http\" deleted\\nThe first all in the command specifies that you’re deleting resources of all types, and\\nthe --all option specifies that you’re deleting all resource instances instead of speci-\\nfying them by name (you already used this option when you ran the previous delete\\ncommand).\\nNOTE\\nDeleting everything with the all keyword doesn’t delete absolutely\\neverything. Certain resources (like Secrets, which we’ll introduce in chapter 7)\\nare preserved and need to be deleted explicitly.\\nAs it deletes resources, kubectl will print the name of every resource it deletes. In the\\nlist, you should see the kubia ReplicationController and the kubia-http Service you\\ncreated in chapter 2. \\nNOTE\\nThe kubectl delete all --all command also deletes the kubernetes\\nService, but it should be recreated automatically in a few moments.\\n3.9\\nSummary\\nAfter reading this chapter, you should now have a decent knowledge of the central\\nbuilding block in Kubernetes. Every other concept you’ll learn about in the next few\\nchapters is directly related to pods. \\n In this chapter, you’ve learned\\n\\uf0a1How to decide whether certain containers should be grouped together in a pod\\nor not.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'a running container in Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'manages multiple replicas of a pod',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubectl',\n",
       "    'description': 'command-line tool for interacting with Kubernetes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'exposes an application to the network',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'isolates resources within a cluster',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'stores sensitive information',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'kubia-zxzij', 'description': 'a pod name', 'category': 'pod'},\n",
       "   {'entity': 'kubia-09as0', 'description': 'a pod name', 'category': 'pod'},\n",
       "   {'entity': 'kubectl delete all --all',\n",
       "    'description': 'command to delete all resources in a namespace',\n",
       "    'category': 'command'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubectl\", \"description\": \"delete resources\", \"destination_entity\": \"all resources\"},\\n  {\"source_entity\": \"kubectl delete all --all\", \"description\": \"delete specific resources types and instances\", \"destination_entity\": \"resource instances\"},\\n  {\"source_entity\": \"kubectl delete all --all\", \"description\": \"print the name of every resource deleted\", \"destination_entity\": \"resources\"},\\n  {\"source_entity\": \"Kubectl\", \"description\": \"run command to create a ReplicationController\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"Kubectl\", \"description\": \"delete a pod created by ReplicationController\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl delete all --all\", \"description\": \"delete the kubia ReplicationController and Service\", \"destination_entity\": \"ReplicationController and Service\"},\\n  {\"source_entity\": \"Kubectl\", \"description\": \"create a new pod called kubia-09as0\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"immediately create a new pod when one is deleted\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl delete all --all\", \"description\": \"delete the kubernetes Service\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"Secrets\", \"description\": \"are preserved and need to be deleted explicitly\", \"destination_entity\": \"resources\"},\\n  {\"source_entity\": \"Kubectl\", \"description\": \"run command to delete all resources in a namespace\", \"destination_entity\": \"namespace\"},\\n  {\"source_entity\": \"kubectl delete all --all\", \"description\": \"delete the kubia-http Service and kubernetes Service\", \"destination_entity\": \"Service\"}\\n]'},\n",
       " {'page': 115,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '83\\nSummary\\n\\uf0a1Pods can run multiple processes and are similar to physical hosts in the non-\\ncontainer world.\\n\\uf0a1YAML or JSON descriptors can be written and used to create pods and then\\nexamined to see the specification of a pod and its current state.\\n\\uf0a1Labels and label selectors should be used to organize pods and easily perform\\noperations on multiple pods at once.\\n\\uf0a1You can use node labels and selectors to schedule pods only to nodes that have\\ncertain features.\\n\\uf0a1Annotations allow attaching larger blobs of data to pods either by people or\\ntools and libraries.\\n\\uf0a1Namespaces can be used to allow different teams to use the same cluster as\\nthough they were using separate Kubernetes clusters.\\n\\uf0a1How to use the kubectl explain command to quickly look up the information\\non any Kubernetes resource. \\nIn the next chapter, you’ll learn about ReplicationControllers and other resources\\nthat manage pods.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pods',\n",
       "    'description': 'can run multiple processes and are similar to physical hosts in the non-container world.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'YAML',\n",
       "    'description': 'descriptors can be written and used to create pods',\n",
       "    'category': 'software/format'},\n",
       "   {'entity': 'JSON',\n",
       "    'description': 'descriptors can be written and used to create pods',\n",
       "    'category': 'software/format'},\n",
       "   {'entity': 'Labels',\n",
       "    'description': 'should be used to organize pods and easily perform operations on multiple pods at once.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'Label selectors',\n",
       "    'description': 'should be used to organize pods and easily perform operations on multiple pods at once.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'Node labels',\n",
       "    'description': 'and selectors can be used to schedule pods only to nodes that have certain features.',\n",
       "    'category': 'hardware/node'},\n",
       "   {'entity': 'Annotations',\n",
       "    'description': 'allow attaching larger blobs of data to pods either by people or tools and libraries.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'Namespaces',\n",
       "    'description': 'can be used to allow different teams to use the same cluster as though they were using separate Kubernetes clusters.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'kubectl explain command',\n",
       "    'description': 'can be used to quickly look up information on any Kubernetes resource.',\n",
       "    'category': 'software/cli'}],\n",
       "  'relationships': '[{\"source_entity\": \"YAML\", \"description\": \"can be written to create pods and examined for specification and current state\", \"destination_entity\": \"Pods\"},\\n {\"source_entity\": \"Label selectors\", \"description\": \"should be used to organize Pods and perform operations on multiple pods at once\", \"destination_entity\": \"Pods\"},\\n {\"source_entity\": \"Node labels\", \"description\": \"can be used to schedule pods only to nodes with certain features\", \"destination_entity\": \"Pods\"},\\n {\"source_entity\": \"Annotations\", \"description\": \"allow attaching larger blobs of data to Pods\", \"destination_entity\": \"Pods\"},\\n {\"source_entity\": \"Namespaces\", \"description\": \"can be used to allow different teams to use the same cluster as though they were using separate Kubernetes clusters\", \"destination_entity\": \"Kubernetes clusters\"},\\n {\"source_entity\": \"kubectl explain command\", \"description\": \"can be used to quickly look up information on any Kubernetes resource\", \"destination_entity\": \"Kubernetes resources\"},\\n {\"source_entity\": \"Labels\", \"description\": \"should be used to organize Pods and easily perform operations on multiple pods at once\", \"destination_entity\": \"Pods\"}]\\n\\nLet me explain the reasoning behind each relation:\\n\\n1. YAML is written to create Pods, so it\\'s related to Pods.\\n2. Label selectors are used to organize and perform operations on multiple Pods, so they\\'re related to Pods.\\n3. Node labels are used to schedule Pods only to nodes with certain features, so they\\'re related to Pods.\\n4. Annotations allow attaching larger blobs of data to Pods, so they\\'re related to Pods.\\n5. Namespaces can be used to allow different teams to use the same cluster as though they were using separate Kubernetes clusters, which implies a relationship between Namespaces and Kubernetes clusters.\\n6. The kubectl explain command is used to quickly look up information on any Kubernetes resource, which implies a relationship between the command and Kubernetes resources.\\n7. Labels should be used to organize Pods and easily perform operations on multiple pods at once, so they\\'re related to Pods.\\n\\nNote that some of these relations may seem obvious or trivial, but I\\'ve tried to extract all possible connections based on the text.'},\n",
       " {'page': 116,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '84\\nReplication and other\\ncontrollers: deploying\\nmanaged pods\\nAs you’ve learned so far, pods represent the basic deployable unit in Kubernetes.\\nYou know how to create, supervise, and manage them manually. But in real-world\\nuse cases, you want your deployments to stay up and running automatically and\\nremain healthy without any manual intervention. To do this, you almost never cre-\\nate pods directly. Instead, you create other types of resources, such as Replication-\\nControllers or Deployments, which then create and manage the actual pods.\\n When you create unmanaged pods (such as the ones you created in the previ-\\nous chapter), a cluster node is selected to run the pod and then its containers are\\nrun on that node. In this chapter, you’ll learn that Kubernetes then monitors\\nThis chapter covers\\n\\uf0a1Keeping pods healthy\\n\\uf0a1Running multiple instances of the same pod\\n\\uf0a1Automatically rescheduling pods after a node fails\\n\\uf0a1Scaling pods horizontally\\n\\uf0a1Running system-level pods on each cluster node\\n\\uf0a1Running batch jobs\\n\\uf0a1Scheduling jobs to run periodically or once in \\nthe future\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'A container orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'The basic deployable unit in Kubernetes that represents an application or service.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Replication-Controllers',\n",
       "    'description': 'A Kubernetes resource that ensures a specified number of pod replicas are running at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployments',\n",
       "    'description': 'A Kubernetes resource that manages rolling updates and rollbacks for applications or services.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Cluster nodes',\n",
       "    'description': 'The physical or virtual machines that make up a Kubernetes cluster.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Containers',\n",
       "    'description': 'Lightweight and standalone executable packages of code.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Replication',\n",
       "    'description': 'The process of creating multiple instances of the same pod to ensure high availability and scalability.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Scaling',\n",
       "    'description': 'The process of adjusting the number of pods or replicas to match changes in workload or demand.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Scheduling',\n",
       "    'description': 'The process of assigning tasks or jobs to run on specific nodes or resources within a cluster.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Jobs',\n",
       "    'description': 'Temporary tasks or processes that can be scheduled to run periodically or once in the future.',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Pods\", \"description\": \"are created directly by users\", \"destination_entity\": \"Users\"},\\n  {\"source_entity\": \"Replication-Controllers\", \"description\": \"create and manage actual pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Deployments\", \"description\": \"create and manage actual pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"monitors unmanaged pods on cluster nodes\", \"destination_entity\": \"Cluster nodes\"},\\n  {\"source_entity\": \"Users\", \"description\": \"want deployments to stay up and running automatically\", \"destination_entity\": \"Deployments\"},\\n  {\"source_entity\": \"Replication-Controllers\", \"description\": \"deploy managed pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"automatically reschedules pods after a node fails\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Scaling\", \"description\": \"is done horizontally on pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Scheduling\", \"description\": \"jobs are run periodically or once in the future\", \"destination_entity\": \"Jobs\"},\\n  {\"source_entity\": \"Deployments\", \"description\": \"are used to keep pods healthy\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Cluster nodes\", \"description\": \"run system-level pods on each node\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Jobs\", \"description\": \"are run periodically or once in the future\", \"destination_entity\": \"Scheduling\"}\\n]\\n```'},\n",
       " {'page': 117,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '85\\nKeeping pods healthy\\nthose containers and automatically restarts them if they fail. But if the whole node\\nfails, the pods on the node are lost and will not be replaced with new ones, unless\\nthose pods are managed by the previously mentioned ReplicationControllers or simi-\\nlar. In this chapter, you’ll learn how Kubernetes checks if a container is still alive and\\nrestarts it if it isn’t. You’ll also learn how to run managed pods—both those that run\\nindefinitely and those that perform a single task and then stop. \\n4.1\\nKeeping pods healthy\\nOne of the main benefits of using Kubernetes is the ability to give it a list of contain-\\ners and let it keep those containers running somewhere in the cluster. You do this by\\ncreating a Pod resource and letting Kubernetes pick a worker node for it and run\\nthe pod’s containers on that node. But what if one of those containers dies? What if\\nall containers of a pod die? \\n As soon as a pod is scheduled to a node, the Kubelet on that node will run its con-\\ntainers and, from then on, keep them running as long as the pod exists. If the con-\\ntainer’s main process crashes, the Kubelet will restart the container. If your\\napplication has a bug that causes it to crash every once in a while, Kubernetes will\\nrestart it automatically, so even without doing anything special in the app itself, run-\\nning the app in Kubernetes automatically gives it the ability to heal itself. \\n But sometimes apps stop working without their process crashing. For example, a\\nJava app with a memory leak will start throwing OutOfMemoryErrors, but the JVM\\nprocess will keep running. It would be great to have a way for an app to signal to\\nKubernetes that it’s no longer functioning properly and have Kubernetes restart it. \\n We’ve said that a container that crashes is restarted automatically, so maybe you’re\\nthinking you could catch these types of errors in the app and exit the process when\\nthey occur. You can certainly do that, but it still doesn’t solve all your problems. \\n For example, what about those situations when your app stops responding because\\nit falls into an infinite loop or a deadlock? To make sure applications are restarted in\\nsuch cases, you must check an application’s health from the outside and not depend\\non the app doing it internally. \\n4.1.1\\nIntroducing liveness probes\\nKubernetes can check if a container is still alive through liveness probes. You can specify\\na liveness probe for each container in the pod’s specification. Kubernetes will periodi-\\ncally execute the probe and restart the container if the probe fails. \\nNOTE\\nKubernetes also supports readiness probes, which we’ll learn about in the\\nnext chapter. Be sure not to confuse the two. They’re used for two different\\nthings.\\nKubernetes can probe a container using one of the three mechanisms:\\n\\uf0a1An HTTP GET probe performs an HTTP GET request on the container’s IP\\naddress, a port and path you specify. If the probe receives a response, and the\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pods',\n",
       "    'description': 'A pod is a logical host that can contain one or more containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'The Kubelet is an agent running on each node in the cluster, responsible for running and restarting containers.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ReplicationControllers',\n",
       "    'description': 'A replication controller ensures that a specified number of replicas (identical copies) of a pod are running at any given time.',\n",
       "    'category': 'controller'},\n",
       "   {'entity': 'Containers',\n",
       "    'description': 'A container is the basic execution unit in Docker.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Worker node',\n",
       "    'description': 'A worker node is a machine in a Kubernetes cluster where pods are scheduled to run.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'OOMKiller',\n",
       "    'description': 'The OOMKiller (Out Of Memory Killer) is a process that kills processes consuming excessive memory.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'OutOfMemoryErrors',\n",
       "    'description': 'A Java exception thrown when the JVM runs out of memory.',\n",
       "    'category': 'error'},\n",
       "   {'entity': 'JVM',\n",
       "    'description': 'The Java Virtual Machine.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Liveness probes',\n",
       "    'description': 'Kubernetes can check if a container is still alive through liveness probes.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Readiness probes',\n",
       "    'description': 'Kubernetes also supports readiness probes, which we’ll learn about in the next chapter.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'HTTP GET probe',\n",
       "    'description': 'A mechanism for Kubernetes to probe a container using an HTTP GET request.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"automatically restarts containers if they fail\",\\n    \"destination_entity\": \"containers\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationControllers\",\\n    \"description\": \"replaces pods on a failed node\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"checks if containers are still alive through liveness probes\",\\n    \"destination_entity\": \"containers\"\\n  },\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"runs and restarts containers in a pod\",\\n    \"destination_entity\": \"pod\\'s containers\"\\n  },\\n  {\\n    \"source_entity\": \"OOMKiller\",\\n    \"description\": \"kills a container that fails\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"application\",\\n    \"description\": \"can signal to Kubernetes that it\\'s no longer functioning properly\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"health check\",\\n    \"description\": \"can be used to restart an application that stops responding\",\\n    \"destination_entity\": \"application\"\\n  },\\n  {\\n    \"source_entity\": \"liveness probes\",\\n    \"description\": \"can be used to check if a container is still alive\",\\n    \"destination_entity\": \"containers\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"supports readiness probes, which are used for different purposes than liveness probes\",\\n    \"destination_entity\": \"readiness probes\"\\n  },\\n  {\\n    \"source_entity\": \"HTTP GET probe\",\\n    \"description\": \"can be used to check if a container is still alive by performing an HTTP GET request\",\\n    \"destination_entity\": \"containers\"\\n  },\\n  {\\n    \"source_entity\": \"worker node\",\\n    \"description\": \"can run pods, and Kubernetes will pick a worker node for a pod\",\\n    \"destination_entity\": \"pods\"\\n  }\\n]\\n```\\n\\nNote: I\\'ve added some entities that were not in the original list but appeared in the text, such as \"OOMKiller\" and \"readiness probes\". If you want to exclude them, just let me know.'},\n",
       " {'page': 118,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '86\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\nresponse code doesn’t represent an error (in other words, if the HTTP response\\ncode is 2xx or 3xx), the probe is considered successful. If the server returns an\\nerror response code or if it doesn’t respond at all, the probe is considered a fail-\\nure and the container will be restarted as a result.\\n\\uf0a1A TCP Socket probe tries to open a TCP connection to the specified port of the\\ncontainer. If the connection is established successfully, the probe is successful.\\nOtherwise, the container is restarted.\\n\\uf0a1An Exec probe executes an arbitrary command inside the container and checks\\nthe command’s exit status code. If the status code is 0, the probe is successful.\\nAll other codes are considered failures. \\n4.1.2\\nCreating an HTTP-based liveness probe\\nLet’s see how to add a liveness probe to your Node.js app. Because it’s a web app, it\\nmakes sense to add a liveness probe that will check whether its web server is serving\\nrequests. But because this particular Node.js app is too simple to ever fail, you’ll need\\nto make the app fail artificially. \\n To properly demo liveness probes, you’ll modify the app slightly and make it\\nreturn a 500 Internal Server Error HTTP status code for each request after the fifth\\none—your app will handle the first five client requests properly and then return an\\nerror on every subsequent request. Thanks to the liveness probe, it should be restarted\\nwhen that happens, allowing it to properly handle client requests again.\\n You can find the code of the new app in the book’s code archive (in the folder\\nChapter04/kubia-unhealthy). I’ve pushed the container image to Docker Hub, so you\\ndon’t need to build it yourself. \\n You’ll create a new pod that includes an HTTP GET liveness probe. The following\\nlisting shows the YAML for the pod.\\napiVersion: v1\\nkind: pod\\nmetadata:\\n  name: kubia-liveness\\nspec:\\n  containers:\\n  - image: luksa/kubia-unhealthy   \\n    name: kubia\\n    livenessProbe:                 \\n      httpGet:                     \\n        path: /                     \\n        port: 8080       \\nListing 4.1\\nAdding a liveness probe to a pod: kubia-liveness-probe.yaml\\nThis is the image \\ncontaining the \\n(somewhat) \\nbroken app.\\nA liveness probe that will \\nperform an HTTP GET\\nThe path to \\nrequest in the \\nHTTP request\\nThe network port\\nthe probe should\\nconnect to\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'TCP Socket',\n",
       "    'description': 'a type of probe that tries to open a TCP connection to the specified port of the container',\n",
       "    'category': 'probe'},\n",
       "   {'entity': 'Exec',\n",
       "    'description': \"a type of probe that executes an arbitrary command inside the container and checks the command's exit status code\",\n",
       "    'category': 'probe'},\n",
       "   {'entity': 'HTTP-based liveness probe',\n",
       "    'description': 'a type of liveness probe that performs an HTTP GET request to check whether a web server is serving requests',\n",
       "    'category': 'liveness probe'},\n",
       "   {'entity': 'Node.js app',\n",
       "    'description': 'a web application written in Node.js',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Docker Hub',\n",
       "    'description': 'a cloud-based registry for container images',\n",
       "    'category': 'registry'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'a key in the YAML file that specifies the version of the API being used',\n",
       "    'category': 'key'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'a key in the YAML file that specifies the type of object being defined (e.g. pod, service)',\n",
       "    'category': 'key'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'a section in the YAML file that contains metadata about the object being defined',\n",
       "    'category': 'section'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'a key in the metadata section that specifies the name of the object being defined',\n",
       "    'category': 'key'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'a section in the YAML file that contains the specifications for the object being defined',\n",
       "    'category': 'section'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'a list in the spec section that contains information about the containers being used by the pod',\n",
       "    'category': 'list'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'a key in the containers list that specifies the container image being used',\n",
       "    'category': 'key'},\n",
       "   {'entity': 'luksa/kubia-unhealthy',\n",
       "    'description': 'the name of the container image being used in this example',\n",
       "    'category': 'container image'},\n",
       "   {'entity': 'livenessProbe',\n",
       "    'description': 'a section in the spec section that contains information about the liveness probe being used by the pod',\n",
       "    'category': 'section'},\n",
       "   {'entity': 'httpGet',\n",
       "    'description': 'a key in the livenessProbe section that specifies the type of probe being used (in this case, an HTTP GET request)',\n",
       "    'category': 'key'},\n",
       "   {'entity': '/',\n",
       "    'description': 'the path to be used in the HTTP GET request',\n",
       "    'category': 'path'},\n",
       "   {'entity': '8080',\n",
       "    'description': 'the network port to be used in the HTTP GET request',\n",
       "    'category': 'port'}],\n",
       "  'relationships': '[{\"source_entity\": \"API Version\", \"description\": \"Defines the API version used to describe the object\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"Kind\", \"description\": \"Specifies that this object is a pod\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"Liveness Probe\", \"description\": \"Checks whether the web server of the Node.js app is serving requests\", \"destination_entity\": \"Node.js App\"},\\n {\"source_entity\": \"HTTP Get\", \"description\": \"Performs an HTTP GET request to the specified path and port\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"Path\", \"description\": \"Specifies the path to be requested in the HTTP request\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"Port\", \"description\": \"Specifies the network port that the probe should connect to\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"TCP Socket\", \"description\": \"Tries to open a TCP connection to the specified port of the container\", \"destination_entity\": \"Container\"},\\n {\"source_entity\": \"Exec Probe\", \"description\": \"Executes an arbitrary command inside the container and checks the command\\'s exit status code\", \"destination_entity\": \"Container\"},\\n {\"source_entity\": \"Docker Hub\", \"description\": \"A cloud-based registry service that allows you to push your images to a central location and pull them from anywhere\", \"destination_entity\": \"Image\"},\\n {\"source_entity\": \"Node.js App\", \"description\": \"Makes sense to add a liveness probe that will check whether its web server is serving requests\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"Liveness Probe\", \"description\": \"Should be restarted when the app fails, allowing it to properly handle client requests again\", \"destination_entity\": \"Node.js App\"}]'},\n",
       " {'page': 119,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '87\\nKeeping pods healthy\\nThe pod descriptor defines an httpGet liveness probe, which tells Kubernetes to peri-\\nodically perform HTTP GET requests on path / on port 8080 to determine if the con-\\ntainer is still healthy. These requests start as soon as the container is run.\\n After five such requests (or actual client requests), your app starts returning\\nHTTP status code 500, which Kubernetes will treat as a probe failure, and will thus\\nrestart the container. \\n4.1.3\\nSeeing a liveness probe in action\\nTo see what the liveness probe does, try creating the pod now. After about a minute and\\na half, the container will be restarted. You can see that by running kubectl get:\\n$ kubectl get po kubia-liveness\\nNAME             READY     STATUS    RESTARTS   AGE\\nkubia-liveness   1/1       Running   1          2m\\nThe RESTARTS column shows that the pod’s container has been restarted once (if you\\nwait another minute and a half, it gets restarted again, and then the cycle continues\\nindefinitely).\\nYou can see why the container had to be restarted by looking at what kubectl describe\\nprints out, as shown in the following listing.\\n$ kubectl describe po kubia-liveness\\nName:           kubia-liveness\\n...\\nContainers:\\n  kubia:\\n    Container ID:       docker://480986f8\\n    Image:              luksa/kubia-unhealthy\\n    Image ID:           docker://sha256:2b208508\\n    Port:\\n    State:              Running                            \\n      Started:          Sun, 14 May 2017 11:41:40 +0200    \\nObtaining the application log of a crashed container\\nIn the previous chapter, you learned how to print the application’s log with kubectl\\nlogs. If your container is restarted, the kubectl logs command will show the log of\\nthe current container. \\nWhen you want to figure out why the previous container terminated, you’ll want to\\nsee those logs instead of the current container’s logs. This can be done by using\\nthe --previous option:\\n$ kubectl logs mypod --previous\\nListing 4.2\\nA pod’s description after its container is restarted\\nThe container is \\ncurrently running.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A logical host in Kubernetes that runs one or more containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'HTTP GET request',\n",
       "    'description': 'An HTTP request method to retrieve data from a server.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Liveness probe',\n",
       "    'description': 'A mechanism to periodically check if a container is running and responding correctly.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pod descriptor',\n",
       "    'description': 'A configuration file that defines the properties of a pod, such as its name, namespace, and containers.',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'Container ID',\n",
       "    'description': 'A unique identifier for a running container.',\n",
       "    'category': 'identifier'},\n",
       "   {'entity': 'Image',\n",
       "    'description': \"A snapshot of a container's filesystem at a particular point in time.\",\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Port',\n",
       "    'description': 'A communication endpoint that allows incoming connections from other containers or processes.',\n",
       "    'category': 'endpoint'},\n",
       "   {'entity': 'Kubectl',\n",
       "    'description': 'The command-line tool for interacting with Kubernetes clusters.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Describe',\n",
       "    'description': 'A Kubectl command to print detailed information about a pod, including its configuration and events.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Logs',\n",
       "    'description': 'A record of messages produced by an application or process.',\n",
       "    'category': 'output'},\n",
       "   {'entity': 'Previous option',\n",
       "    'description': 'An option for the Kubectl logs command to display the logs from the previous container instance.',\n",
       "    'category': 'option'}],\n",
       "  'relationships': '[{\"source_entity\": \"Kubernetes\", \"description\": \"periodically perform HTTP GET requests to determine if a container is still healthy\", \"destination_entity\": \"Container ID\"},\\n\\n {\"source_entity\": \"Liveness probe\", \"description\": \"tells Kubernetes to periodically perform HTTP GET requests on path / on port 8080\", \"destination_entity\": \"Pod descriptor\"},\\n\\n {\"source_entity\": \"HTTP GET request\", \"description\": \"starts as soon as the container is run and returns HTTP status code 500 if the container is not healthy\", \"destination_entity\": \"Kubernetes\"},\\n\\n {\"source_entity\": \"Kubectl\", \"description\": \"prints out information about a pod, including its containers and images\", \"destination_entity\": \"Pod descriptor\"},\\n\\n {\"source_entity\": \"Kubectl describe\", \"description\": \"prints out detailed information about a pod\\'s containers, including their ID and image\", \"destination_entity\": \"Container ID\"},\\n\\n {\"source_entity\": \"Previous option\", \"description\": \"allows kubectl logs to show the log of the previous container instead of the current one\", \"destination_entity\": \"Kubectl\"},\\n\\n {\"source_entity\": \"Pod descriptor\", \"description\": \"defines an httpGet liveness probe that tells Kubernetes to periodically perform HTTP GET requests on path / on port 8080\", \"destination_entity\": \"Liveness probe\"},\\n\\n {\"source_entity\": \"Kubernetes\", \"description\": \"restarts a container if it fails to respond with HTTP status code 200\", \"destination_entity\": \"Container ID\"},\\n\\n {\"source_entity\": \"Logs\", \"description\": \"can be obtained using kubectl logs, either for the current or previous container\", \"destination_entity\": \"Kubectl\"},\\n\\n {\"source_entity\": \"Image\", \"description\": \"is used to create a container and can be retrieved from the Container ID\", \"destination_entity\": \"Container ID\"},\\n\\n {\"source_entity\": \"Port\", \"description\": \"is the port number on which the HTTP GET request is sent\", \"destination_entity\": \"HTTP GET request\"}]'},\n",
       " {'page': 120,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '88\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\n    Last State:         Terminated                         \\n      Reason:           Error                              \\n      Exit Code:        137                                \\n      Started:          Mon, 01 Jan 0001 00:00:00 +0000    \\n      Finished:         Sun, 14 May 2017 11:41:38 +0200    \\n    Ready:              True\\n    Restart Count:      1                                 \\n    Liveness:           http-get http://:8080/ delay=0s timeout=1s\\n                        period=10s #success=1 #failure=3\\n    ...\\nEvents:\\n... Killing container with id docker://95246981:pod \"kubia-liveness ...\"\\n    container \"kubia\" is unhealthy, it will be killed and re-created.\\nYou can see that the container is currently running, but it previously terminated\\nbecause of an error. The exit code was 137, which has a special meaning—it denotes\\nthat the process was terminated by an external signal. The number 137 is a sum of two\\nnumbers: 128+x, where x is the signal number sent to the process that caused it to ter-\\nminate. In the example, x equals 9, which is the number of the SIGKILL signal, mean-\\ning the process was killed forcibly.\\n The events listed at the bottom show why the container was killed—Kubernetes\\ndetected the container was unhealthy, so it killed and re-created it. \\nNOTE\\nWhen a container is killed, a completely new container is created—it’s\\nnot the same container being restarted again.\\n4.1.4\\nConfiguring additional properties of the liveness probe\\nYou may have noticed that kubectl describe also displays additional information\\nabout the liveness probe:\\nLiveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 \\n          ➥ #failure=3\\nBeside the liveness probe options you specified explicitly, you can also see additional\\nproperties, such as delay, timeout, period, and so on. The delay=0s part shows that\\nthe probing begins immediately after the container is started. The timeout is set to\\nonly 1 second, so the container must return a response in 1 second or the probe is\\ncounted as failed. The container is probed every 10 seconds (period=10s) and the\\ncontainer is restarted after the probe fails three consecutive times (#failure=3). \\n These additional parameters can be customized when defining the probe. For\\nexample, to set the initial delay, add the initialDelaySeconds property to the live-\\nness probe as shown in the following listing.\\n   livenessProbe:          \\n     httpGet:              \\n       path: /             \\nListing 4.3\\nA liveness probe with an initial delay: kubia-liveness-probe-initial-delay.yaml\\nThe previous \\ncontainer terminated \\nwith an error and \\nexited with code 137.\\nThe container \\nhas been \\nrestarted once.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Basic execution unit in Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'Lightweight and stand-alone process',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubectl',\n",
       "    'description': 'Command-line tool for managing Kubernetes resources',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Liveness Probe',\n",
       "    'description': 'Mechanism to check if a container is running correctly',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'HTTP Get',\n",
       "    'description': 'Request method used by liveness probe to check container health',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'SIGKILL',\n",
       "    'description': 'Signal sent to process to terminate it forcibly',\n",
       "    'category': 'signal'},\n",
       "   {'entity': 'Exit Code 137',\n",
       "    'description': 'Special exit code indicating a process was terminated by an external signal',\n",
       "    'category': 'error'},\n",
       "   {'entity': 'Kubia-Liveness-Prob',\n",
       "    'description': 'Container instance running the liveness probe',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Restart Count',\n",
       "    'description': 'Number of times a container has been restarted',\n",
       "    'category': 'metric'},\n",
       "   {'entity': 'Events',\n",
       "    'description': 'List of historical events related to the container',\n",
       "    'category': 'log'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"SIGKILL\",\\n    \"description\": \"sent a signal to terminate the process\",\\n    \"destination_entity\": \"Process\"\\n  },\\n  {\\n    \"source_entity\": \"Kubectl\",\\n    \"description\": \"detected an unhealthy container and killed it\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"Exit Code 137\",\\n    \"description\": \"denotes a process terminated by an external signal\",\\n    \"destination_entity\": \"Process\"\\n  },\\n  {\\n    \"source_entity\": \"Container\",\\n    \"description\": \"terminated with error and exited with code 137\",\\n    \"destination_entity\": \"Exit Code 137\"\\n  },\\n  {\\n    \"source_entity\": \"Kubia-Liveness-Prob\",\\n    \"description\": \"defined a liveness probe to check container health\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"Events\",\\n    \"description\": \"listed why the container was killed and re-created\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"Liveness Probe\",\\n    \"description\": \"probed the container every 10 seconds to check its health\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"killed and re-created a container that was unhealthy\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"detected an unhealthy container and killed it\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"Restart Count\",\\n    \"description\": \"indicates the number of times a container has been restarted\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"HTTP Get\",\\n    \"description\": \"sent an HTTP request to check the container\\'s health\",\\n    \"destination_entity\": \"Container\"\\n  }\\n]'},\n",
       " {'page': 121,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '89\\nKeeping pods healthy\\n       port: 8080          \\n     initialDelaySeconds: 15   \\nIf you don’t set the initial delay, the prober will start probing the container as soon as\\nit starts, which usually leads to the probe failing, because the app isn’t ready to start\\nreceiving requests. If the number of failures exceeds the failure threshold, the con-\\ntainer is restarted before it’s even able to start responding to requests properly. \\nTIP\\nAlways remember to set an initial delay to account for your app’s startup\\ntime.\\nI’ve seen this on many occasions and users were confused why their container was\\nbeing restarted. But if they’d used kubectl describe, they’d have seen that the con-\\ntainer terminated with exit code 137 or 143, telling them that the pod was terminated\\nexternally. Additionally, the listing of the pod’s events would show that the container\\nwas killed because of a failed liveness probe. If you see this happening at pod startup,\\nit’s because you failed to set initialDelaySeconds appropriately.\\nNOTE\\nExit code 137 signals that the process was killed by an external signal\\n(exit code is 128 + 9 (SIGKILL). Likewise, exit code 143 corresponds to 128 +\\n15 (SIGTERM).\\n4.1.5\\nCreating effective liveness probes\\nFor pods running in production, you should always define a liveness probe. Without\\none, Kubernetes has no way of knowing whether your app is still alive or not. As long\\nas the process is still running, Kubernetes will consider the container to be healthy. \\nWHAT A LIVENESS PROBE SHOULD CHECK\\nYour simplistic liveness probe simply checks if the server is responding. While this may\\nseem overly simple, even a liveness probe like this does wonders, because it causes the\\ncontainer to be restarted if the web server running within the container stops\\nresponding to HTTP requests. Compared to having no liveness probe, this is a major\\nimprovement, and may be sufficient in most cases.\\n But for a better liveness check, you’d configure the probe to perform requests on a\\nspecific URL path (/health, for example) and have the app perform an internal sta-\\ntus check of all the vital components running inside the app to ensure none of them\\nhas died or is unresponsive. \\nTIP\\nMake sure the /health HTTP endpoint doesn’t require authentication;\\notherwise the probe will always fail, causing your container to be restarted\\nindefinitely.\\nBe sure to check only the internals of the app and nothing influenced by an external\\nfactor. For example, a frontend web server’s liveness probe shouldn’t return a failure\\nwhen the server can’t connect to the backend database. If the underlying cause is in\\nthe database itself, restarting the web server container will not fix the problem.\\nKubernetes will wait 15 seconds \\nbefore executing the first probe.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Basic execution unit in Kubernetes',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Liveness Probe',\n",
       "    'description': 'Mechanism to check if a container is running and responding',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Probe',\n",
       "    'description': 'Reusable piece of code that runs periodically to perform an action',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'Lightweight and standalone execution environment',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Kubectl',\n",
       "    'description': 'Command-line interface for Kubernetes',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Describe',\n",
       "    'description': 'Command to display detailed information about a pod or container',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Exit Code 137',\n",
       "    'description': 'Signal that the process was killed by an external signal (SIGKILL)',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Exit Code 143',\n",
       "    'description': 'Signal that the process was killed due to a failed liveness probe (SIGTERM)',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubectl\", \"description\": \"checks the status of a pod and displays its events, including the reason for termination if applicable.\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Liveness Probe\", \"description\": \"causes the container to be restarted if it stops responding to HTTP requests.\", \"destination_entity\": \"Container\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"watches over the pods and restarts them if they fail a liveness probe.\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Probe\", \"description\": \"is used to check if a container is running properly or not, and restart it if necessary.\", \"destination_entity\": \"Container\"},\\n  {\"source_entity\": \"Exit Code 143\", \"description\": \"signals that the process was killed by an external signal (SIGTERM).\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Exit Code 137\", \"description\": \"signals that the process was killed by an external signal (SIGKILL).\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Container\", \"description\": \"is terminated if it fails a liveness probe, causing Kubernetes to restart it.\", \"destination_entity\": \"Kubernetes\"},\\n  {\"source_entity\": \"Describe\", \"description\": \"is used by kubectl to display detailed information about a pod, including its events.\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"waits 15 seconds before executing the first probe on a container.\", \"destination_entity\": \"Container\"}\\n]\\n```'},\n",
       " {'page': 122,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '90\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\nBecause the liveness probe will fail again, you’ll end up with the container restarting\\nrepeatedly until the database becomes accessible again. \\nKEEPING PROBES LIGHT\\nLiveness probes shouldn’t use too many computational resources and shouldn’t take\\ntoo long to complete. By default, the probes are executed relatively often and are\\nonly allowed one second to complete. Having a probe that does heavy lifting can slow\\ndown your container considerably. Later in the book, you’ll also learn about how to\\nlimit CPU time available to a container. The probe’s CPU time is counted in the con-\\ntainer’s CPU time quota, so having a heavyweight liveness probe will reduce the CPU\\ntime available to the main application processes.\\nTIP\\nIf you’re running a Java app in your container, be sure to use an HTTP\\nGET liveness probe instead of an Exec probe, where you spin up a whole new\\nJVM to get the liveness information. The same goes for any JVM-based or sim-\\nilar applications, whose start-up procedure requires considerable computa-\\ntional resources.\\nDON’T BOTHER IMPLEMENTING RETRY LOOPS IN YOUR PROBES\\nYou’ve already seen that the failure threshold for the probe is configurable and usu-\\nally the probe must fail multiple times before the container is killed. But even if you\\nset the failure threshold to 1, Kubernetes will retry the probe several times before con-\\nsidering it a single failed attempt. Therefore, implementing your own retry loop into\\nthe probe is wasted effort.\\nLIVENESS PROBE WRAP-UP\\nYou now understand that Kubernetes keeps your containers running by restarting\\nthem if they crash or if their liveness probes fail. This job is performed by the Kubelet\\non the node hosting the pod—the Kubernetes Control Plane components running on\\nthe master(s) have no part in this process. \\n But if the node itself crashes, it’s the Control Plane that must create replacements for\\nall the pods that went down with the node. It doesn’t do that for pods that you create\\ndirectly. Those pods aren’t managed by anything except by the Kubelet, but because the\\nKubelet runs on the node itself, it can’t do anything if the node fails. \\n To make sure your app is restarted on another node, you need to have the pod\\nmanaged by a ReplicationController or similar mechanism, which we’ll discuss in the\\nrest of this chapter. \\n4.2\\nIntroducing ReplicationControllers\\nA ReplicationController is a Kubernetes resource that ensures its pods are always\\nkept running. If the pod disappears for any reason, such as in the event of a node\\ndisappearing from the cluster or because the pod was evicted from the node, the\\nReplicationController notices the missing pod and creates a replacement pod. \\n Figure 4.1 shows what happens when a node goes down and takes two pods with it.\\nPod A was created directly and is therefore an unmanaged pod, while pod B is managed\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Liveness Probe',\n",
       "    'description': 'A probe that checks if a container is running or not',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'A lightweight and stand-alone executable binary package',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'The Kubernetes component that runs on each node, responsible for managing containers',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Control Plane',\n",
       "    'description': 'The master components of the Kubernetes cluster, responsible for overall management',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes resource that ensures pods are always kept running',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A collection of one or more containers',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'JVM',\n",
       "    'description': 'The Java Virtual Machine, a runtime environment for Java applications',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Java',\n",
       "    'description': 'An object-oriented programming language',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'HTTP GET liveness probe',\n",
       "    'description': 'A type of liveness probe that checks HTTP endpoints',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Exec probe',\n",
       "    'description': 'A type of liveness probe that executes a command to check container status',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'CPU time quota',\n",
       "    'description': 'The maximum amount of CPU time allowed for a container',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"ensures containers running by restarting them if they crash or liveness probes fail\", \"destination_entity\": \"Container\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"performs job of keeping containers running on the node hosting the pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Kubernetes Control Plane\", \"description\": \"creates replacements for all pods that went down with the node\", \"destination_entity\": \"Node\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"ensures its pods are always kept running\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Liveness Probe\", \"description\": \"will fail repeatedly until database becomes accessible again\", \"destination_entity\": \"Database\"},\\n  {\"source_entity\": \"HTTP GET liveness probe\", \"description\": \"should be used instead of Exec probe for Java app in container\", \"destination_entity\": \"Java App\"},\\n  {\"source_entity\": \"Exec probe\", \"description\": \"spins up a whole new JVM to get the liveness information\", \"destination_entity\": \"JVM\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"will retry probes several times before considering it a single failed attempt\", \"destination_entity\": \"Probe\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"notifies missing pod and creates replacement pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Node\", \"description\": \"can\\'t do anything if the node fails and ReplicationController is not used\", \"destination_entity\": \"Kubelet\"},\\n  {\"source_entity\": \"Liveness Probe\", \"description\": \"shouldn\\'t use too many computational resources and shouldn\\'t take too long to complete\", \"destination_entity\": \"Container\"}\\n]\\n```\\n\\nNote that I have extracted the relations as per the rules provided, but some of the relations might not be directly mentioned in the text. I have used my understanding of the context and the entities provided to infer the relationships between them.'},\n",
       " {'page': 123,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '91\\nIntroducing ReplicationControllers\\nby a ReplicationController. After the node fails, the ReplicationController creates a\\nnew pod (pod B2) to replace the missing pod B, whereas pod A is lost completely—\\nnothing will ever recreate it.\\n The ReplicationController in the figure manages only a single pod, but Replication-\\nControllers, in general, are meant to create and manage multiple copies (replicas) of a\\npod. That’s where ReplicationControllers got their name from. \\n4.2.1\\nThe operation of a ReplicationController\\nA ReplicationController constantly monitors the list of running pods and makes sure\\nthe actual number of pods of a “type” always matches the desired number. If too few\\nsuch pods are running, it creates new replicas from a pod template. If too many such\\npods are running, it removes the excess replicas. \\n You might be wondering how there can be more than the desired number of repli-\\ncas. This can happen for a few reasons: \\n\\uf0a1Someone creates a pod of the same type manually.\\n\\uf0a1Someone changes an existing pod’s “type.”\\n\\uf0a1Someone decreases the desired number of pods, and so on.\\nNode 1\\nNode 1 fails\\nPod A\\nPod B\\nNode 2\\nVarious\\nother pods\\nCreates and\\nmanages\\nNode 1\\nPod A\\nPod B\\nNode 2\\nVarious\\nother pods\\nReplicationController\\nReplicationController\\nPod A goes down with Node 1 and is\\nnot recreated, because there is no\\nReplicationController overseeing it.\\nRC notices pod B is\\nmissing and creates\\na new pod instance.\\nPod B2\\nFigure 4.1\\nWhen a node fails, only pods backed by a ReplicationController are recreated.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes object that manages the creation and maintenance of multiple copies (replicas) of a pod.',\n",
       "    'category': 'Kubernetes'}],\n",
       "  'relationships': '[\\n    {\\n        \"source_entity\": \"ReplicationController\",\\n        \"description\": \"Creates a new pod to replace the missing pod when a node fails.\",\\n        \"destination_entity\": \"Pod B\"\\n    },\\n    {\\n        \"source_entity\": \"ReplicationController\",\\n        \"description\": \"Manages multiple copies (replicas) of a pod.\",\\n        \"destination_entity\": \"pod A and Pod B\"\\n    },\\n    {\\n        \"source_entity\": \"ReplicationController\",\\n        \"description\": \"Constantly monitors the list of running pods.\",\\n        \"destination_entity\": \"list of running pods\"\\n    },\\n    {\\n        \"source_entity\": \"ReplicationController\",\\n        \"description\": \"Creates new replicas from a pod template if too few such pods are running.\",\\n        \"destination_entity\": \"pod template\"\\n    },\\n    {\\n        \"source_entity\": \"ReplicationController\",\\n        \"description\": \"Removes excess replicas if too many such pods are running.\",\\n        \"destination_entity\": \"excess replicas\"\\n    }\\n]\\n```'},\n",
       " {'page': 124,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '92\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\nI’ve used the term pod “type” a few times. But no such thing exists. Replication-\\nControllers don’t operate on pod types, but on sets of pods that match a certain label\\nselector (you learned about them in the previous chapter). \\nINTRODUCING THE CONTROLLER’S RECONCILIATION LOOP\\nA ReplicationController’s job is to make sure that an exact number of pods always\\nmatches its label selector. If it doesn’t, the ReplicationController takes the appropriate\\naction to reconcile the actual with the desired number. The operation of a Replication-\\nController is shown in figure 4.2.\\nUNDERSTANDING THE THREE PARTS OF A REPLICATIONCONTROLLER\\nA ReplicationController has three essential parts (also shown in figure 4.3):\\n\\uf0a1A label selector, which determines what pods are in the ReplicationController’s scope\\n\\uf0a1A replica count, which specifies the desired number of pods that should be running\\n\\uf0a1A pod template, which is used when creating new pod replicas\\nStart\\nCompare\\nmatched vs.\\ndesired pod\\ncount\\nFind pods\\nmatching the\\nlabel selector\\nCreate additional\\npod(s) from\\ncurrent template\\nDelete the\\nexcess pod(s)\\nToo many\\nJust enough\\nToo few\\nFigure 4.2\\nA ReplicationController’s reconciliation loop\\napp: kubia\\nPod\\nPod template\\nReplicationController: kubia\\nPod selector:\\napp=kubia\\nReplicas: 3\\nFigure 4.3\\nThe three key parts of a \\nReplicationController (pod selector, \\nreplica count, and pod template)\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  ReplicationController: kubia\\nPod selector: Pod template\\napp=kubia\\napp: kubia\\nPod\\nReplicas: 3  \\\n",
       "   0                                               None                                                  \n",
       "   1                                               None                                                  \n",
       "   \n",
       "      Col1  Col2 Col3  \n",
       "   0    ub    ia       \n",
       "   1  None  None       ],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A container that can have one or more containers running in it.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A controller that ensures a specified number of pods are running with a matching label selector.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Label Selector',\n",
       "    'description': 'A way to select pods based on specific labels and values.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Replica Count',\n",
       "    'description': 'The desired number of pods that should be running with a matching label selector.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Pod Template',\n",
       "    'description': 'A template used to create new pod replicas.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"matches desired pod count by creating/deleting pods\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"uses label selector to identify matching pods\", \"destination_entity\": \"Label Selector\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"ensures desired number of replicas by adding/removing pods\", \"destination_entity\": \"Replica Count\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"uses pod template to create new pod replicas\", \"destination_entity\": \"Pod Template\"},\\n  {\"source_entity\": \"Label Selector\", \"description\": \"determines what pods are in ReplicationController\\'s scope\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"manages and schedules pod creation/deletion by ReplicationController\", \"destination_entity\": \"ReplicationController\"}\\n]\\n```\\n\\nHere\\'s a brief explanation of each relation:\\n\\n1. **ReplicationController** matches the desired pod count by creating/deleting pods, which involves the **Pod** entity.\\n2. The **ReplicationController** uses the **Label Selector** to identify matching pods.\\n3. To ensure the desired number of replicas, the **ReplicationController** ensures that the actual number of pods (identified using the **Label Selector**) matches the desired number specified by the **Replica Count**.\\n4. The **ReplicationController** utilizes the **Pod Template** to create new pod replicas.\\n5. The **Label Selector** determines which pods are within the scope of the **ReplicationController**.\\n6. Finally, the **Kubernetes** system manages and schedules the creation/deletion of pods by the **ReplicationController**.'},\n",
       " {'page': 125,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '93\\nIntroducing ReplicationControllers\\nA ReplicationController’s replica count, the label selector, and even the pod tem-\\nplate can all be modified at any time, but only changes to the replica count affect\\nexisting pods. \\nUNDERSTANDING THE EFFECT OF CHANGING THE CONTROLLER’S LABEL SELECTOR OR POD TEMPLATE\\nChanges to the label selector and the pod template have no effect on existing pods.\\nChanging the label selector makes the existing pods fall out of the scope of the\\nReplicationController, so the controller stops caring about them. ReplicationCon-\\ntrollers also don’t care about the actual “contents” of its pods (the container images,\\nenvironment variables, and other things) after they create the pod. The template\\ntherefore only affects new pods created by this ReplicationController. You can think\\nof it as a cookie cutter for cutting out new pods.\\nUNDERSTANDING THE BENEFITS OF USING A REPLICATIONCONTROLLER\\nLike many things in Kubernetes, a ReplicationController, although an incredibly sim-\\nple concept, provides or enables the following powerful features:\\n\\uf0a1It makes sure a pod (or multiple pod replicas) is always running by starting a\\nnew pod when an existing one goes missing.\\n\\uf0a1When a cluster node fails, it creates replacement replicas for all the pods that\\nwere running on the failed node (those that were under the Replication-\\nController’s control).\\n\\uf0a1It enables easy horizontal scaling of pods—both manual and automatic (see\\nhorizontal pod auto-scaling in chapter 15).\\nNOTE\\nA pod instance is never relocated to another node. Instead, the\\nReplicationController creates a completely new pod instance that has no rela-\\ntion to the instance it’s replacing. \\n4.2.2\\nCreating a ReplicationController\\nLet’s look at how to create a ReplicationController and then see how it keeps your\\npods running. Like pods and other Kubernetes resources, you create a Replication-\\nController by posting a JSON or YAML descriptor to the Kubernetes API server.\\n You’re going to create a YAML file called kubia-rc.yaml for your Replication-\\nController, as shown in the following listing.\\napiVersion: v1\\nkind: ReplicationController     \\nmetadata:\\n  name: kubia                      \\nspec:\\n  replicas: 3                     \\n  selector:              \\n    app: kubia           \\nListing 4.4\\nA YAML definition of a ReplicationController: kubia-rc.yaml\\nThis manifest defines a \\nReplicationController (RC)\\nThe name of this \\nReplicationController\\nThe desired number \\nof pod instances\\nThe pod selector determining \\nwhat pods the RC is operating on\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes resource that ensures a specified number of replicas (pods) are running at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes, comprising one or more containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Label Selector',\n",
       "    'description': 'A way to select pods based on labels attached to them, used by ReplicationController to determine which pods it should manage.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod Template',\n",
       "    'description': 'A template that defines the structure of a pod, including the container images and environment variables.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Replica Count',\n",
       "    'description': 'The desired number of replicas (pods) that a ReplicationController should maintain at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': 'The central component of Kubernetes that accepts requests to create, update, or delete resources such as pods and replication controllers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'JSON Descriptor',\n",
       "    'description': 'A file format used to describe the configuration of a ReplicationController, similar to YAML.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'YAML Definition',\n",
       "    'description': 'A human-readable text format used to define the configuration of a ReplicationController, in this case for the kubia-rc.yaml file.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes API',\n",
       "    'description': 'The interface through which users interact with Kubernetes resources such as pods and replication controllers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod Instances',\n",
       "    'description': 'The actual running instances of a pod, managed by a ReplicationController.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"posts a JSON or YAML descriptor to create a ReplicationController\",\\n    \"destination_entity\": \"ReplicationController\"\\n  },\\n  {\\n    \"source_entity\": \"Label Selector\",\\n    \"description\": \"makes the existing pods fall out of the scope of the ReplicationController\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Replica Count\",\\n    \"description\": \"affects existing pods\",\\n    \"destination_entity\": \"Existing Pods\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"keeps pods running by starting a new pod when an existing one goes missing\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"creates replacement replicas for all the pods that were running on the failed node\",\\n    \"destination_entity\": \"Failed Node\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"enables easy horizontal scaling of pods\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"replaces a completely new pod instance that has no relation to the instance it\\'s replacing\",\\n    \"destination_entity\": \"Pod Instances\"\\n  },\\n  {\\n    \"source_entity\": \"JSON Descriptor\",\\n    \"description\": \"defines a ReplicationController\",\\n    \"destination_entity\": \"ReplicationController\"\\n  },\\n  {\\n    \"source_entity\": \"YAML Definition\",\\n    \"description\": \"defines a ReplicationController\",\\n    \"destination_entity\": \"ReplicationController\"\\n  },\\n  {\\n    \"source_entity\": \"Pod Template\",\\n    \"description\": \"only affects new pods created by this ReplicationController\",\\n    \"destination_entity\": \"New Pods\"\\n  }\\n]\\n```\\n\\nNote that some of the relations may seem obvious or trivial, but they are all extracted based on the context and entities provided in the document page.'},\n",
       " {'page': 126,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '94\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\n  template:                        \\n    metadata:                      \\n      labels:                      \\n        app: kubia                 \\n    spec:                          \\n      containers:                  \\n      - name: kubia                \\n        image: luksa/kubia         \\n        ports:                     \\n        - containerPort: 8080      \\nWhen you post the file to the API server, Kubernetes creates a new Replication-\\nController named kubia, which makes sure three pod instances always match the\\nlabel selector app=kubia. When there aren’t enough pods, new pods will be created\\nfrom the provided pod template. The contents of the template are almost identical to\\nthe pod definition you created in the previous chapter. \\n The pod labels in the template must obviously match the label selector of the\\nReplicationController; otherwise the controller would create new pods indefinitely,\\nbecause spinning up a new pod wouldn’t bring the actual replica count any closer to\\nthe desired number of replicas. To prevent such scenarios, the API server verifies the\\nReplicationController definition and will not accept it if it’s misconfigured.\\n Not specifying the selector at all is also an option. In that case, it will be configured\\nautomatically from the labels in the pod template. \\nTIP\\nDon’t specify a pod selector when defining a ReplicationController. Let\\nKubernetes extract it from the pod template. This will keep your YAML\\nshorter and simpler.\\nTo create the ReplicationController, use the kubectl create command, which you\\nalready know:\\n$ kubectl create -f kubia-rc.yaml\\nreplicationcontroller \"kubia\" created\\nAs soon as the ReplicationController is created, it goes to work. Let’s see what\\nit does.\\n4.2.3\\nSeeing the ReplicationController in action\\nBecause no pods exist with the app=kubia label, the ReplicationController should\\nspin up three new pods from the pod template. List the pods to see if the Replication-\\nController has done what it’s supposed to:\\n$ kubectl get pods\\nNAME          READY     STATUS              RESTARTS   AGE\\nkubia-53thy   0/1       ContainerCreating   0          2s\\nkubia-k0xz6   0/1       ContainerCreating   0          2s\\nkubia-q3vkg   0/1       ContainerCreating   0          2s\\nThe pod template \\nfor creating new \\npods\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'Kubernetes component for managing pod replicas',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod template',\n",
       "    'description': 'Template for creating new pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubia',\n",
       "    'description': 'Application name and label selector',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicationController definition',\n",
       "    'description': 'Configuration file for ReplicationController',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Kubernetes command-line tool',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'create command',\n",
       "    'description': 'kubectl command for creating a resource',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pod selector',\n",
       "    'description': 'Selector used to match pods with the ReplicationController',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'Key-value pairs attached to a resource for identification',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'containerPort',\n",
       "    'description': 'Port exposed by a container',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"creates a new Replication-Controller named kubia\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"makes sure three pod instances always match the label selector app=kubia\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"API server\", \"description\": \"verifies the ReplicationController definition\", \"destination_entity\": \"ReplicationController definition\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"will not accept a misconfigured ReplicationController definition\", \"destination_entity\": \"ReplicationController definition\"},\\n  {\"source_entity\": \"API server\", \"description\": \"configures the pod selector automatically from the labels in the pod template\", \"destination_entity\": \"pod selector\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"spins up three new pods from the pod template\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"goes to work and creates new pods\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"kubectl create command\", \"description\": \"creates a ReplicationController\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"lists the pods using kubectl get command\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"displays the pod status and age\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"defines a label selector to match existing pods\", \"destination_entity\": \"labels\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"verifies that the labels in the pod template match the label selector of the ReplicationController\", \"destination_entity\": \"labels and ReplicationController\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"will create new pods indefinitely if the pod selector is not specified\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"provides an option to specify a pod selector\", \"destination_entity\": \"pod selector\"},\\n  {\"source_entity\": \"TIP\", \"description\": \"suggests not specifying a pod selector when defining a ReplicationController\", \"destination_entity\": \"ReplicationController\"}\\n]\\n\\nNote: I\\'ve only extracted relations that involve the entities in the list you provided.'},\n",
       " {'page': 127,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '95\\nIntroducing ReplicationControllers\\nIndeed, it has! You wanted three pods, and it created three pods. It’s now managing\\nthose three pods. Next you’ll mess with them a little to see how the Replication-\\nController responds. \\nSEEING THE REPLICATIONCONTROLLER RESPOND TO A DELETED POD\\nFirst, you’ll delete one of the pods manually to see how the ReplicationController spins\\nup a new one immediately, bringing the number of matching pods back to three:\\n$ kubectl delete pod kubia-53thy\\npod \"kubia-53thy\" deleted\\nListing the pods again shows four of them, because the one you deleted is terminat-\\ning, and a new pod has already been created:\\n$ kubectl get pods\\nNAME          READY     STATUS              RESTARTS   AGE\\nkubia-53thy   1/1       Terminating         0          3m\\nkubia-oini2   0/1       ContainerCreating   0          2s\\nkubia-k0xz6   1/1       Running             0          3m\\nkubia-q3vkg   1/1       Running             0          3m\\nThe ReplicationController has done its job again. It’s a nice little helper, isn’t it?\\nGETTING INFORMATION ABOUT A REPLICATIONCONTROLLER\\nNow, let’s see what information the kubectl get command shows for Replication-\\nControllers:\\n$ kubectl get rc\\nNAME      DESIRED   CURRENT   READY     AGE\\nkubia     3         3         2         3m\\nNOTE\\nWe’re using rc as a shorthand for replicationcontroller.\\nYou see three columns showing the desired number of pods, the actual number of\\npods, and how many of them are ready (you’ll learn what that means in the next chap-\\nter, when we talk about readiness probes).\\n You can see additional information about your ReplicationController with the\\nkubectl describe command, as shown in the following listing.\\n$ kubectl describe rc kubia\\nName:           kubia\\nNamespace:      default\\nSelector:       app=kubia\\nLabels:         app=kubia\\nAnnotations:    <none>\\nReplicas:       3 current / 3 desired               \\nPods Status:    4 Running / 0 Waiting / 0 Succeeded / 0 Failed  \\nPod Template:\\n  Labels:       app=kubia\\n  Containers:   ...\\nListing 4.5\\nDisplaying details of a ReplicationController with kubectl describe\\nThe actual vs. the \\ndesired number of \\npod instances\\nNumber of \\npod instances \\nper pod \\nstatus\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes object that ensures a specified number of replicas (pods) are running at any given time.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes, equivalent to a container in Docker.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for interacting with the Kubernetes API server.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'rc',\n",
       "    'description': 'A shorthand for replicationcontroller.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'deletion',\n",
       "    'description': 'The process of removing a pod from the system.',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'termination',\n",
       "    'description': 'The process of shutting down a pod.',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'container creation',\n",
       "    'description': 'The process of creating a new container in a pod.',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'readiness probes',\n",
       "    'description': 'A mechanism for determining whether a pod is ready to accept traffic.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'pod template',\n",
       "    'description': 'A configuration file that defines the settings for a pod, including labels and containers.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'Key-value pairs used to identify and select pods in Kubernetes.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'annotations',\n",
       "    'description': 'Key-value pairs used to add metadata to pods in Kubernetes.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'selector',\n",
       "    'description': 'A mechanism for selecting specific pods based on labels or other criteria.',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ReplicationController\", \\n   \"description\": \"spins up a new pod immediately after deletion\", \\n   \"destination_entity\": \"pod\"},\\n  \\n  {\"source_entity\": \"kubectl delete command\", \\n   \"description\": \"deletes one of the pods manually\", \\n   \"destination_entity\": \"pod\"},\\n  \\n  {\"source_entity\": \"ReplicationController\", \\n   \"description\": \"terminates the deleted pod\", \\n   \"destination_entity\": \"deleted pod\"},\\n  \\n  {\"source_entity\": \"kubectl get command\", \\n   \"description\": \"lists all running pods\", \\n   \"destination_entity\": \"pod\"},\\n  \\n  {\"source_entity\": \"rc (ReplicationController) shorthand\", \\n   \"description\": \"is used as a shortcut for ReplicationController\", \\n   \"destination_entity\": \"ReplicationController\"},\\n  \\n  {\"source_entity\": \"kubectl describe command\", \\n   \"description\": \"displays additional information about the ReplicationController\", \\n   \"destination_entity\": \"ReplicationController\"},\\n  \\n  {\"source_entity\": \"Selector\", \\n   \"description\": \"is used to select pods based on certain criteria\", \\n   \"destination_entity\": \"pod\"},\\n  \\n  {\"source_entity\": \"kubectl describe command\", \\n   \"description\": \"displays details about the pod template\", \\n   \"destination_entity\": \"Pod Template\"},\\n  \\n  {\"source_entity\": \"ReplicationController\", \\n   \"description\": \"monitors and manages multiple pods\", \\n   \"destination_entity\": \"multiple pods\"},\\n  \\n  {\"source_entity\": \"kubectl get command\", \\n   \"description\": \"displays information about the desired vs actual number of pods\", \\n   \"destination_entity\": \"pod\"}\\n]\\n```\\n\\nNote: I\\'ve only included relations that are explicitly mentioned in the text, and excluded any potential relationships that might be inferred but not directly stated. Let me know if you\\'d like me to clarify anything!'},\n",
       " {'page': 128,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '96\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\n  Volumes:      <none>\\nEvents:                                                   \\nFrom                    Type      Reason           Message\\n----                    -------  ------            -------\\nreplication-controller  Normal   SuccessfulCreate  Created pod: kubia-53thy\\nreplication-controller  Normal   SuccessfulCreate  Created pod: kubia-k0xz6\\nreplication-controller  Normal   SuccessfulCreate  Created pod: kubia-q3vkg\\nreplication-controller  Normal   SuccessfulCreate  Created pod: kubia-oini2\\nThe current number of replicas matches the desired number, because the controller\\nhas already created a new pod. It shows four running pods because a pod that’s termi-\\nnating is still considered running, although it isn’t counted in the current replica count. \\n The list of events at the bottom shows the actions taken by the Replication-\\nController—it has created four pods so far.\\nUNDERSTANDING EXACTLY WHAT CAUSED THE CONTROLLER TO CREATE A NEW POD\\nThe controller is responding to the deletion of a pod by creating a new replacement\\npod (see figure 4.4). Well, technically, it isn’t responding to the deletion itself, but the\\nresulting state—the inadequate number of pods.\\n While a ReplicationController is immediately notified about a pod being deleted\\n(the API server allows clients to watch for changes to resources and resource lists), that’s\\nnot what causes it to create a replacement pod. The notification triggers the controller\\nto check the actual number of pods and take appropriate action.\\nThe events \\nrelated to this \\nReplicationController\\nBefore deletion\\nAfter deletion\\nReplicationController: kubia\\nReplicas: 3\\nSelector: app=kubia\\napp: kubia\\nPod:\\nkubia-q3vkg\\napp: kubia\\nPod:\\nkubia-oini2\\n[ContainerCreating]\\n[Terminating]\\napp: kubia\\nPod:\\nkubia-k0xz6\\napp: kubia\\nPod:\\nkubia-53thy\\nReplicationController: kubia\\nReplicas: 3\\nSelector: app=kubia\\napp: kubia\\nPod:\\nkubia-q3vkg\\napp: kubia\\nPod:\\nkubia-k0xz6\\napp: kubia\\nPod:\\nkubia-53thy\\nDelete kubia-53thy\\nFigure 4.4\\nIf a pod disappears, the ReplicationController sees too few pods and creates a new replacement pod.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes controller that ensures a specified number of replicas (pods) are running.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A unit of deployment in Kubernetes, which can contain one or more containers.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A lightweight and standalone executable package that contains an application and its dependencies.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'replica',\n",
       "    'description': 'A copy of a pod in Kubernetes, which ensures high availability and scalability.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The main component of the Kubernetes control plane that exposes an API for clients to interact with it.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'watching',\n",
       "    'description': 'A mechanism in Kubernetes that allows clients to monitor changes to resources and resource lists.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'selector',\n",
       "    'description': 'A label-based selection mechanism used by ReplicationControllers to identify which pods belong to a particular group.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'label',\n",
       "    'description': 'A key-value pair that can be attached to Kubernetes resources, such as pods and services.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'events',\n",
       "    'description': 'A list of actions taken by the ReplicationController, displayed in a log format.',\n",
       "    'category': 'Software/Application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"creates a new pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"responds to deletion of a pod by creating a replacement pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"API server\", \"description\": \"notifies the ReplicationController about a pod being deleted\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"checks the actual number of pods and takes appropriate action\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"watching clients\", \"description\": \"get notified by the API server about changes to resources and resource lists\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"sees too few pods and creates a new replacement pod\", \"destination_entity\": \"pod\"}\\n]\\n```\\n\\nHere\\'s a brief explanation of each relation:\\n\\n1. **ReplicationController** creates a new **pod**: The ReplicationController is responsible for creating and managing multiple replicas (pods) of an application. When it detects that a pod has been deleted, it creates a replacement pod to maintain the desired number of replicas.\\n2. **ReplicationController** responds to deletion of a **pod**: The ReplicationController receives notification from the API server about a pod being deleted and takes action by creating a new replacement pod.\\n3. **API server** notifies **ReplicationController** about deletion: The API server allows clients (watching clients) to get notified about changes to resources and resource lists, including deletion of a pod.\\n4. **ReplicationController** checks the actual number of **pods**: When the ReplicationController receives notification from the API server about a pod being deleted, it checks the actual number of pods and takes action if necessary.\\n5. **Watching clients** get notified by **API server**: Watching clients can subscribe to receive notifications from the API server about changes to resources and resource lists.\\n6. **ReplicationController** sees too few **pods**: When a pod is deleted, the ReplicationController sees that there are too few pods and takes action by creating a new replacement pod.'},\n",
       " {'page': 129,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"97\\nIntroducing ReplicationControllers\\nRESPONDING TO A NODE FAILURE\\nSeeing the ReplicationController respond to the manual deletion of a pod isn’t too\\ninteresting, so let’s look at a better example. If you’re using Google Kubernetes Engine\\nto run these examples, you have a three-node Kubernetes cluster. You’re going to dis-\\nconnect one of the nodes from the network to simulate a node failure.\\nNOTE\\nIf you’re using Minikube, you can’t do this exercise, because you only\\nhave one node that acts both as a master and a worker node.\\nIf a node fails in the non-Kubernetes world, the ops team would need to migrate the\\napplications running on that node to other machines manually. Kubernetes, on the\\nother hand, does that automatically. Soon after the ReplicationController detects that\\nits pods are down, it will spin up new pods to replace them. \\n Let’s see this in action. You need to ssh into one of the nodes with the gcloud\\ncompute ssh command and then shut down its network interface with sudo ifconfig\\neth0 down, as shown in the following listing.\\nNOTE\\nChoose a node that runs at least one of your pods by listing pods with\\nthe -o wide option.\\n$ gcloud compute ssh gke-kubia-default-pool-b46381f1-zwko\\nEnter passphrase for key '/home/luksa/.ssh/google_compute_engine':\\nWelcome to Kubernetes v1.6.4!\\n...\\nluksa@gke-kubia-default-pool-b46381f1-zwko ~ $ sudo ifconfig eth0 down\\nWhen you shut down the network interface, the ssh session will stop responding, so\\nyou need to open up another terminal or hard-exit from the ssh session. In the new\\nterminal you can list the nodes to see if Kubernetes has detected that the node is\\ndown. This takes a minute or so. Then, the node’s status is shown as NotReady:\\n$ kubectl get node\\nNAME                                   STATUS     AGE\\ngke-kubia-default-pool-b46381f1-opc5   Ready      5h\\ngke-kubia-default-pool-b46381f1-s8gj   Ready      5h\\ngke-kubia-default-pool-b46381f1-zwko   NotReady   5h    \\nIf you list the pods now, you’ll still see the same three pods as before, because Kuber-\\nnetes waits a while before rescheduling pods (in case the node is unreachable because\\nof a temporary network glitch or because the Kubelet is restarting). If the node stays\\nunreachable for several minutes, the status of the pods that were scheduled to that\\nnode changes to Unknown. At that point, the ReplicationController will immediately\\nspin up a new pod. You can see this by listing the pods again:\\nListing 4.6\\nSimulating a node failure by shutting down its network interface\\nNode isn’t ready, \\nbecause it’s \\ndisconnected from \\nthe network\\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes component that ensures a specified number of replicas (pods) are running at any given time.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'node failure',\n",
       "    'description': 'A scenario where a node in the Kubernetes cluster becomes unreachable due to network disconnection or other reasons.',\n",
       "    'category': 'event'},\n",
       "   {'entity': 'Google Kubernetes Engine',\n",
       "    'description': 'A managed environment for deploying containerized applications, powered by Kubernetes.',\n",
       "    'category': 'cloud platform'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': \"A tool for running a single-node Kubernetes cluster locally on a developer's machine.\",\n",
       "    'category': 'development tool'},\n",
       "   {'entity': 'gcloud compute ssh command',\n",
       "    'description': 'A command used to establish an SSH connection to a Google Cloud node.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ifconfig eth0 down command',\n",
       "    'description': 'A command used to shut down the network interface on a Google Cloud node.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ReplicationController detection',\n",
       "    'description': 'The process by which a ReplicationController detects when its pods are unreachable due to a node failure.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pod rescheduling',\n",
       "    'description': 'The process of rescheduling pods to other nodes in the Kubernetes cluster after a node failure.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'NotReady status',\n",
       "    'description': 'A status indicating that a node is currently unreachable due to network disconnection or other reasons.',\n",
       "    'category': 'status'},\n",
       "   {'entity': 'Unknown status',\n",
       "    'description': \"A status indicating that a pod's status cannot be determined due to its scheduled node being unreachable.\",\n",
       "    'category': 'status'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'The Kubernetes agent running on each node, responsible for communicating with the API server and managing pods.',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'ReplicationController spin-up',\n",
       "    'description': 'The process by which a ReplicationController creates new pods to replace ones that were previously unreachable due to a node failure.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Minikube\", \"description\": \"cannot simulate node failure because it only has one node\", \"destination_entity\": \"node failure\"},\\n  {\"source_entity\": \"Google Kubernetes Engine\", \"description\": \"can simulate node failure by disconnecting one of its nodes from the network\", \"destination_entity\": \"node failure\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"detects that its pods are down and spins up new pods to replace them\", \"destination_entity\": \"pod rescheduling\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"restarting may cause temporary network glitch or node unreachability\", \"destination_entity\": \"node failure\"},\\n  {\"source_entity\": \"ifconfig eth0 down command\", \"description\": \"shuts down the network interface of a node\", \"destination_entity\": \"node failure\"},\\n  {\"source_entity\": \"gcloud compute ssh command\", \"description\": \"connects to one of the nodes and shuts down its network interface\", \"destination_entity\": \"node failure\"},\\n  {\"source_entity\": \"ReplicationController detection\", \"description\": \"detects node unreachability after several minutes and changes pod status to Unknown\", \"destination_entity\": \"Unknown status\"},\\n  {\"source_entity\": \"ReplicationController spin-up\", \"description\": \"spins up a new pod immediately when it detects node failure and pods have Unknown status\", \"destination_entity\": \"pod rescheduling\"},\\n  {\"source_entity\": \"NotReady status\", \"description\": \"node\\'s status is shown as NotReady after several minutes of unreachability\", \"destination_entity\": \"node failure\"},\\n  {\"source_entity\": \"ifconfig eth0 down command\", \"description\": \"stops ssh session on the node that has its network interface shut down\", \"destination_entity\": \"ssh session\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"spins up a new pod immediately when it detects node failure and pods have Unknown status\", \"destination_entity\": \"pod rescheduling\"}\\n]'},\n",
       " {'page': 130,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '98\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\n$ kubectl get pods\\nNAME          READY   STATUS    RESTARTS   AGE\\nkubia-oini2   1/1     Running   0          10m\\nkubia-k0xz6   1/1     Running   0          10m\\nkubia-q3vkg   1/1     Unknown   0          10m    \\nkubia-dmdck   1/1     Running   0          5s    \\nLooking at the age of the pods, you see that the kubia-dmdck pod is new. You again\\nhave three pod instances running, which means the ReplicationController has again\\ndone its job of bringing the actual state of the system to the desired state. \\n The same thing happens if a node fails (either breaks down or becomes unreach-\\nable). No immediate human intervention is necessary. The system heals itself\\nautomatically. \\n To bring the node back, you need to reset it with the following command:\\n$ gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko\\nWhen the node boots up again, its status should return to Ready, and the pod whose\\nstatus was Unknown will be deleted.\\n4.2.4\\nMoving pods in and out of the scope of a ReplicationController\\nPods created by a ReplicationController aren’t tied to the ReplicationController in\\nany way. At any moment, a ReplicationController manages pods that match its label\\nselector. By changing a pod’s labels, it can be removed from or added to the scope\\nof a ReplicationController. It can even be moved from one ReplicationController to\\nanother.\\nTIP\\nAlthough a pod isn’t tied to a ReplicationController, the pod does refer-\\nence it in the metadata.ownerReferences field, which you can use to easily\\nfind which ReplicationController a pod belongs to.\\nIf you change a pod’s labels so they no longer match a ReplicationController’s label\\nselector, the pod becomes like any other manually created pod. It’s no longer man-\\naged by anything. If the node running the pod fails, the pod is obviously not resched-\\nuled. But keep in mind that when you changed the pod’s labels, the replication\\ncontroller noticed one pod was missing and spun up a new pod to replace it.\\n Let’s try this with your pods. Because your ReplicationController manages pods\\nthat have the app=kubia label, you need to either remove this label or change its value\\nto move the pod out of the ReplicationController’s scope. Adding another label will\\nhave no effect, because the ReplicationController doesn’t care if the pod has any addi-\\ntional labels. It only cares whether the pod has all the labels referenced in the label\\nselector. \\nThis pod’s status is \\nunknown, because its \\nnode is unreachable.\\nThis pod was created \\nfive seconds ago.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes controller that ensures a specified number of replicas (pod instances) are running at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': '$ kubectl get pods',\n",
       "    'description': 'Kubernetes command to list all pods in the current namespace.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'NAME',\n",
       "    'description': 'Column header in the output of $kubectl get pods',\n",
       "    'category': 'attribute'},\n",
       "   {'entity': 'READY',\n",
       "    'description': \"Column header in the output of $kubectl get pods, indicating pod's readiness status.\",\n",
       "    'category': 'attribute'},\n",
       "   {'entity': 'STATUS',\n",
       "    'description': \"Column header in the output of $kubectl get pods, indicating pod's current status.\",\n",
       "    'category': 'attribute'},\n",
       "   {'entity': 'RESTARTS',\n",
       "    'description': 'Column header in the output of $kubectl get pods, indicating number of times a pod has been restarted.',\n",
       "    'category': 'attribute'},\n",
       "   {'entity': 'AGE',\n",
       "    'description': 'Column header in the output of $kubectl get pods, indicating how long a pod has been running.',\n",
       "    'category': 'attribute'},\n",
       "   {'entity': '$ gcloud compute instances reset',\n",
       "    'description': 'GCloud command to reset an instance (node) in a Google Kubernetes Engine cluster.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'gke-kubia-default-pool-b46381f1-zwko',\n",
       "    'description': 'Name of the node being reset by $ gcloud compute instances reset command.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'ReplicationController label selector',\n",
       "    'description': 'A set of labels used to select which pods are managed by a ReplicationController.',\n",
       "    'category': 'attribute'},\n",
       "   {'entity': 'metadata.ownerReferences field',\n",
       "    'description': 'Kubernetes field that references the controller responsible for managing a pod.',\n",
       "    'category': 'attribute'}],\n",
       "  'relationships': '[\\n    {\\n        \"source_entity\": \"$ kubectl get pods\",\\n        \"description\": \"lists all running pods and their status\",\\n        \"destination_entity\": \"pods\"\\n    },\\n    {\\n        \"source_entity\": \"metadata.ownerReferences field\",\\n        \"description\": \"used to find which ReplicationController a pod belongs to\",\\n        \"destination_entity\": \"ReplicationController\"\\n    },\\n    {\\n        \"source_entity\": \"$ kubectl get pods\",\\n        \"description\": \"lists all running pods and their status\",\\n        \"destination_entity\": \"pods\"\\n    },\\n    {\\n        \"source_entity\": \"NAME\",\\n        \"description\": \"displays the name of a pod\",\\n        \"destination_entity\": \"pod\"\\n    },\\n    {\\n        \"source_entity\": \"$ kubectl get pods\",\\n        \"description\": \"lists all running pods and their status\",\\n        \"destination_entity\": \"pods\"\\n    },\\n    {\\n        \"source_entity\": \"ReplicationController label selector\",\\n        \"description\": \"used to select which pods a ReplicationController manages\",\\n        \"destination_entity\": \"ReplicationController\"\\n    },\\n    {\\n        \"source_entity\": \"$ gcloud compute instances reset\",\\n        \"description\": \"resets a node with the specified name\",\\n        \"destination_entity\": \"gke-kubia-default-pool-b46381f1-zwko\"\\n    },\\n    {\\n        \"source_entity\": \"$ kubectl get pods\",\\n        \"description\": \"lists all running pods and their status\",\\n        \"destination_entity\": \"pods\"\\n    },\\n    {\\n        \"source_entity\": \"$ gcloud compute instances reset\",\\n        \"description\": \"resets a node with the specified name\",\\n        \"destination_entity\": \"node\"\\n    },\\n    {\\n        \"source_entity\": \"STATUS\",\\n        \"description\": \"displays the status of a pod\",\\n        \"destination_entity\": \"pod\"\\n    },\\n    {\\n        \"source_entity\": \"$ kubectl get pods\",\\n        \"description\": \"lists all running pods and their status\",\\n        \"destination_entity\": \"pods\"\\n    },\\n    {\\n        \"source_entity\": \"AGE\",\\n        \"description\": \"displays the age of a pod\",\\n        \"destination_entity\": \"pod\"\\n    },\\n    {\\n        \"source_entity\": \"$ kubectl get pods\",\\n        \"description\": \"lists all running pods and their status\",\\n        \"destination_entity\": \"pods\"\\n    },\\n    {\\n        \"source_entity\": \"kubia-dmdck\",\\n        \"description\": \"is a pod that was created recently\",\\n        \"destination_entity\": \"pod\"\\n    },\\n    {\\n        \"source_entity\": \"$ gcloud compute instances reset\",\\n        \"description\": \"resets a node with the specified name\",\\n        \"destination_entity\": \"node\"\\n    },\\n    {\\n        \"source_entity\": \"ReplicationController\",\\n        \"description\": \"manages pods that match its label selector\",\\n        \"destination_entity\": \"pods\"\\n    },\\n    {\\n        \"source_entity\": \"$ kubectl get pods\",\\n        \"description\": \"lists all running pods and their status\",\\n        \"destination_entity\": \"pods\"\\n    },\\n    {\\n        \"source_entity\": \"$ gcloud compute instances reset\",\\n        \"description\": \"resets a node with the specified name\",\\n        \"destination_entity\": \"node\"\\n    },\\n    {\\n        \"source_entity\": \"READY\",\\n        \"description\": \"displays whether a pod is ready or not\",\\n        \"destination_entity\": \"pod\"\\n    },\\n    {\\n        \"source_entity\": \"$ kubectl get pods\",\\n        \"description\": \"lists all running pods and their status\",\\n        \"destination_entity\": \"pods\"\\n    }\\n]\\n```'},\n",
       " {'page': 131,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '99\\nIntroducing ReplicationControllers\\nADDING LABELS TO PODS MANAGED BY A REPLICATIONCONTROLLER\\nLet’s confirm that a ReplicationController doesn’t care if you add additional labels to\\nits managed pods:\\n$ kubectl label pod kubia-dmdck type=special\\npod \"kubia-dmdck\" labeled\\n$ kubectl get pods --show-labels\\nNAME          READY   STATUS    RESTARTS   AGE   LABELS\\nkubia-oini2   1/1     Running   0          11m   app=kubia\\nkubia-k0xz6   1/1     Running   0          11m   app=kubia\\nkubia-dmdck   1/1     Running   0          1m    app=kubia,type=special\\nYou’ve added the type=special label to one of the pods. Listing all pods again shows\\nthe same three pods as before, because no change occurred as far as the Replication-\\nController is concerned.\\nCHANGING THE LABELS OF A MANAGED POD\\nNow, you’ll change the app=kubia label to something else. This will make the pod no\\nlonger match the ReplicationController’s label selector, leaving it to only match two\\npods. The ReplicationController should therefore start a new pod to bring the num-\\nber back to three:\\n$ kubectl label pod kubia-dmdck app=foo --overwrite\\npod \"kubia-dmdck\" labeled\\nThe --overwrite argument is necessary; otherwise kubectl will only print out a warn-\\ning and won’t change the label, to prevent you from inadvertently changing an exist-\\ning label’s value when your intent is to add a new one. \\n Listing all the pods again should now show four pods: \\n$ kubectl get pods -L app\\nNAME         READY  STATUS             RESTARTS  AGE  APP\\nkubia-2qneh  0/1    ContainerCreating  0         2s   kubia   \\nkubia-oini2  1/1    Running            0         20m  kubia\\nkubia-k0xz6  1/1    Running            0         20m  kubia\\nkubia-dmdck  1/1    Running            0         10m  foo    \\nNOTE\\nYou’re using the -L app option to display the app label in a column.\\nThere, you now have four pods altogether: one that isn’t managed by your Replication-\\nController and three that are. Among them is the newly created pod.\\n Figure 4.5 illustrates what happened when you changed the pod’s labels so they no\\nlonger matched the ReplicationController’s pod selector. You can see your three pods\\nand your ReplicationController. After you change the pod’s label from app=kubia to\\napp=foo, the ReplicationController no longer cares about the pod. Because the con-\\ntroller’s replica count is set to 3 and only two pods match the label selector, the\\nNewly created pod that replaces\\nthe pod you removed from the\\nscope of the ReplicationController\\nPod no longer \\nmanaged by the \\nReplicationController\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'a Kubernetes resource that ensures a specified number of replicas (identical Pod instances) are running at any given time.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the command-line tool for interacting with a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'a lightweight and portable container that can run multiple containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'label',\n",
       "    'description': 'a key-value pair added to a Pod or other Kubernetes object to provide additional metadata.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'selector',\n",
       "    'description': 'a label query used to filter which Pods are managed by a ReplicationController.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'replica count',\n",
       "    'description': 'the number of replicas (Pod instances) that a ReplicationController ensures are running at any given time.',\n",
       "    'category': 'process'},\n",
       "   {'entity': '-L app option',\n",
       "    'description': 'an option for displaying the app label in a column when listing Pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'app=kubia',\n",
       "    'description': \"a label key-value pair indicating that the Pod is an instance of the 'kubia' application.\",\n",
       "    'category': 'database'},\n",
       "   {'entity': 'type=special',\n",
       "    'description': 'a label key-value pair indicating that the Pod has a special type.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'foo',\n",
       "    'description': \"a new value for the app label, replacing 'kubia' in one of the Pods.\",\n",
       "    'category': 'database'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubectl\", \"description\": \"add additional labels to its managed pods\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"not care about the added labels\", \"destination_entity\": \"Pods Managed by ReplicationController\"},\\n  {\"source_entity\": \"User\", \"description\": \"change the app=kubia label to something else\", \"destination_entity\": \"Pod kubia-dmdck\"},\\n  {\"source_entity\": \"Kubectl\", \"description\": \"overwrite the existing label with a new value\", \"destination_entity\": \"Pod kubia-dmdck\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"start a new pod to bring the number back to three\", \"destination_entity\": \"Newly created Pod\"},\\n  {\"source_entity\": \"Kubectl\", \"description\": \"display the app label in a column\", \"destination_entity\": \"Pods Managed by ReplicationController\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"no longer care about the pod with changed labels\", \"destination_entity\": \"Pod kubia-dmdck\"},\\n  {\"source_entity\": \"-L app option\", \"description\": \"display only pods with specific label\", \"destination_entity\": \"Pods Managed by ReplicationController\"},\\n  {\"source_entity\": \"Replica count\", \"description\": \"set to three and only two pods match the label selector\", \"destination_entity\": \"ReplicationController\"}\\n]'},\n",
       " {'page': 132,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '100\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\nReplicationController spins up pod kubia-2qneh to bring the number back up to\\nthree. Pod kubia-dmdck is now completely independent and will keep running until\\nyou delete it manually (you can do that now, because you don’t need it anymore).\\nREMOVING PODS FROM CONTROLLERS IN PRACTICE\\nRemoving a pod from the scope of the ReplicationController comes in handy when\\nyou want to perform actions on a specific pod. For example, you might have a bug\\nthat causes your pod to start behaving badly after a specific amount of time or a spe-\\ncific event. If you know a pod is malfunctioning, you can take it out of the Replication-\\nController’s scope, let the controller replace it with a new one, and then debug or\\nplay with the pod in any way you want. Once you’re done, you delete the pod. \\nCHANGING THE REPLICATIONCONTROLLER’S LABEL SELECTOR\\nAs an exercise to see if you fully understand ReplicationControllers, what do you\\nthink would happen if instead of changing the labels of a pod, you modified the\\nReplicationController’s label selector? \\n If your answer is that it would make all the pods fall out of the scope of the\\nReplicationController, which would result in it creating three new pods, you’re abso-\\nlutely right. And it shows that you understand how ReplicationControllers work. \\n Kubernetes does allow you to change a ReplicationController’s label selector, but\\nthat’s not the case for the other resources that are covered in the second half of this\\nInitial state\\nAfter re-labelling\\nRe-label kubia-dmdck\\napp: kubia\\nPod:\\nkubia-oini2\\napp: kubia\\nPod:\\nkubia-2qneh\\n[ContainerCreating]\\nPod:\\nkubia-dmdck\\napp: kubia\\nPod:\\nkubia-k0xz6\\napp: kubia\\ntype: special\\ntype: special\\napp: foo\\napp: kubia\\nPod:\\nkubia-dmdck\\napp: kubia\\nPod:\\nkubia-k0xz6\\nReplicationController: kubia\\nReplicas: 3\\nSelector: app=kubia\\nReplicationController: kubia\\nReplicas: 3\\nSelector: app=kubia\\nPod:\\nkubia-oini2\\nFigure 4.5\\nRemoving a pod from the scope of a ReplicationController by changing its labels \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes controller that ensures a specified number of replicas (pods) are running at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'An instance of a container running on a node within a cluster.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ContainerCreating',\n",
       "    'description': 'A state indicating that a container is being created.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Selector',\n",
       "    'description': 'A label selector used to identify pods for a ReplicationController.',\n",
       "    'category': 'label'},\n",
       "   {'entity': 'Replicas',\n",
       "    'description': 'The number of replicas (pods) desired by a ReplicationController.',\n",
       "    'category': 'parameter'},\n",
       "   {'entity': 'kubia-2qneh',\n",
       "    'description': 'A pod name created by the ReplicationController to restore the replica count.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubia-dmdck',\n",
       "    'description': 'A pod name that is being removed from the scope of a ReplicationController.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Instances of containers running on nodes within a cluster.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Labels',\n",
       "    'description': 'Metadata attached to objects in Kubernetes, such as pods and replication controllers.',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'app=kubia',\n",
       "    'description': 'A label selector used to identify pods for a ReplicationController.',\n",
       "    'category': 'label'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"spins up a new pod\", \"destination_entity\": \"kubia-2qneh\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"is completely independent and will keep running until deleted manually\", \"destination_entity\": \"kubia-dmdck\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"replaces a malfunctioning pod with a new one\", \"destination_entity\": \"kubia-2qneh\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"can be removed from the scope of the ReplicationController\", \"destination_entity\": \"kubia-dmdck\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"will create three new pods if its label selector is modified\", \"destination_entity\": \"three new pods\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"can be removed from the scope of a ReplicationController by changing its labels\", \"destination_entity\": \"kubia-dmdck\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"can be modified to change its label selector\", \"destination_entity\": \"Kubernetes\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"spins up three pods initially\", \"destination_entity\": \"three pods\"}\\n]\\n```'},\n",
       " {'page': 133,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '101\\nIntroducing ReplicationControllers\\nchapter and which are also used for managing pods. You’ll never change a controller’s\\nlabel selector, but you’ll regularly change its pod template. Let’s take a look at that.\\n4.2.5\\nChanging the pod template\\nA ReplicationController’s pod template can be modified at any time. Changing the pod\\ntemplate is like replacing a cookie cutter with another one. It will only affect the cookies\\nyou cut out afterward and will have no effect on the ones you’ve already cut (see figure\\n4.6). To modify the old pods, you’d need to delete them and let the Replication-\\nController replace them with new ones based on the new template.\\nAs an exercise, you can try editing the ReplicationController and adding a label to the\\npod template. You can edit the ReplicationController with the following command:\\n$ kubectl edit rc kubia\\nThis will open the ReplicationController’s YAML definition in your default text editor.\\nFind the pod template section and add an additional label to the metadata. After you\\nsave your changes and exit the editor, kubectl will update the ReplicationController\\nand print the following message:\\nreplicationcontroller \"kubia\" edited\\nYou can now list pods and their labels again and confirm that they haven’t changed.\\nBut if you delete the pods and wait for their replacements to be created, you’ll see the\\nnew label.\\n Editing a ReplicationController like this to change the container image in the pod\\ntemplate, deleting the existing pods, and letting them be replaced with new ones from\\nthe new template could be used for upgrading pods, but you’ll learn a better way of\\ndoing that in chapter 9. \\nReplication\\nController\\nReplicas: 3\\nTemplate:\\nA\\nB\\nC\\nReplication\\nController\\nReplicas: 3\\nTemplate:\\nA\\nReplication\\nController\\nReplicas: 3\\nTemplate:\\nA\\nReplication\\nController\\nReplicas: 3\\nTemplate:\\nD\\nA\\nB\\nC\\nA\\nB\\nC\\nA\\nB\\nChange\\ntemplate\\nDelete\\na pod\\nRC creates\\nnew pod\\nFigure 4.6\\nChanging a ReplicationController’s pod template only affects pods created afterward and has no \\neffect on existing pods.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A controller for managing pods, used to ensure a specified number of replicas are running at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'The basic execution unit in Kubernetes, equivalent to a Docker container.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'label selector',\n",
       "    'description': 'A way to select and manage pods based on labels attached to them.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod template',\n",
       "    'description': 'The configuration for a pod, defining the container(s) that will be run within it.',\n",
       "    'category': 'software'},\n",
       "   {'entity': '$ kubectl edit rc kubia',\n",
       "    'description': 'A command to edit a ReplicationController using the Kubernetes CLI.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The Kubernetes command-line interface, used for managing and interacting with a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'yaml definition',\n",
       "    'description': 'A human-readable format for representing data, often used to define configurations or resources in Kubernetes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Information about a pod, such as labels and annotations.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'container image',\n",
       "    'description': 'The specific version of an application or service that is being run within a container.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"You\", \"description\": \"will regularly change\", \"destination_entity\": \"label selector\"},\\n  {\"source_entity\": \"You\", \"description\": \"will modify at any time\", \"destination_entity\": \"ReplicationController\\'s pod template\"},\\n  {\"source_entity\": \"You\", \"description\": \"can try editing the ReplicationController and adding a label to the pod template\", \"destination_entity\": \"pod template\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"will update the ReplicationController and print the message\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"You\", \"description\": \"can delete the pods and let them be replaced with new ones from the new template\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"will create a new pod based on the new template\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"You\", \"description\": \"can add an additional label to the metadata in the YAML definition\", \"destination_entity\": \"metadata\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"will edit the ReplicationController\\'s YAML definition\", \"destination_entity\": \"YAML definition\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"has a pod template that can be modified\", \"destination_entity\": \"pod template\"},\\n  {\"source_entity\": \"You\", \"description\": \"can change the container image in the pod template\", \"destination_entity\": \"container image\"}\\n]\\n```'},\n",
       " {'page': 134,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '102\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\n4.2.6\\nHorizontally scaling pods\\nYou’ve seen how ReplicationControllers make sure a specific number of pod instances\\nis always running. Because it’s incredibly simple to change the desired number of rep-\\nlicas, this also means scaling pods horizontally is trivial. \\n Scaling the number of pods up or down is as easy as changing the value of the rep-\\nlicas field in the ReplicationController resource. After the change, the Replication-\\nController will either see too many pods exist (when scaling down) and delete part of\\nthem, or see too few of them (when scaling up) and create additional pods. \\nSCALING UP A REPLICATIONCONTROLLER\\nYour ReplicationController has been keeping three instances of your pod running.\\nYou’re going to scale that number up to 10 now. As you may remember, you’ve\\nalready scaled a ReplicationController in chapter 2. You could use the same com-\\nmand as before:\\n$ kubectl scale rc kubia --replicas=10\\nBut you’ll do it differently this time. \\nSCALING A REPLICATIONCONTROLLER BY EDITING ITS DEFINITION\\nInstead of using the kubectl scale command, you’re going to scale it in a declarative\\nway by editing the ReplicationController’s definition:\\n$ kubectl edit rc kubia\\nWhen the text editor opens, find the spec.replicas field and change its value to 10,\\nas shown in the following listing.\\n# Please edit the object below. Lines beginning with a \\'#\\' will be ignored,\\n# and an empty file will abort the edit. If an error occurs while saving \\n# this file will be reopened with the relevant failures.\\napiVersion: v1\\nkind: ReplicationController\\nConfiguring kubectl edit to use a different text editor\\nYou can tell kubectl to use a text editor of your choice by setting the KUBE_EDITOR\\nenvironment variable. For example, if you’d like to use nano for editing Kubernetes\\nresources, execute the following command (or put it into your ~/.bashrc or an\\nequivalent file):\\nexport KUBE_EDITOR=\"/usr/bin/nano\"\\nIf the KUBE_EDITOR environment variable isn’t set, kubectl edit falls back to using\\nthe default editor, usually configured through the EDITOR environment variable.\\nListing 4.7\\nEditing the RC in a text editor by running kubectl edit\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes resource that ensures a specific number of pod instances is always running.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for interacting with Kubernetes resources.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'rc',\n",
       "    'description': 'Short form for ReplicationController in kubectl commands.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'kubia',\n",
       "    'description': 'Name of the pod instance being managed by a ReplicationController.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'A field in the Kubernetes API specification for a ReplicationController resource.',\n",
       "    'category': 'hardware,software'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'A field in the Kubernetes API specification that indicates this is a ReplicationController resource.',\n",
       "    'category': 'hardware,software'},\n",
       "   {'entity': 'spec.replicas',\n",
       "    'description': 'A field in the Kubernetes API specification for a ReplicationController that specifies the desired number of replicas.',\n",
       "    'category': 'hardware,software,application'},\n",
       "   {'entity': 'scale command',\n",
       "    'description': 'A kubectl command used to change the number of replicas in a ReplicationController.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'kubectl edit',\n",
       "    'description': 'A command for editing a Kubernetes resource directly in a text editor.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'text editor',\n",
       "    'description': 'An application used for editing text, such as nano or vim.',\n",
       "    'category': 'hardware,software'},\n",
       "   {'entity': 'KUBE_EDITOR',\n",
       "    'description': 'Environment variable that specifies the preferred text editor to use with kubectl edit.',\n",
       "    'category': 'hardware,software'},\n",
       "   {'entity': 'EDITOR',\n",
       "    'description': 'Environment variable that specifies the default text editor to use.',\n",
       "    'category': 'hardware,software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"You\", \"description\": \"have seen how ReplicationControllers make sure a specific number of pod instances is always running.\", \"destination_entity\": \"ReplicationControllers\"},\\n  {\"source_entity\": \"This\", \"description\": \"also means scaling pods horizontally is trivial.\", \"destination_entity\": \"scaling pods horizontally\"},\\n  {\"source_entity\": \"You\", \"description\": \"are going to scale that number up to 10 now.\", \"destination_entity\": \"the number of pod instances\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"will see too many pods exist (when scaling down) and delete part of them, or see too few of them (when scaling up) and create additional pods.\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"You\", \"description\": \"are going to scale that number up to 10 now.\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"kubectl scale command\", \"description\": \"is as easy as changing the value of the replicas field in the ReplicationController resource.\", \"destination_entity\": \"ReplicationController resource\"},\\n  {\"source_entity\": \"You\", \"description\": \"could use the same command as before: $ kubectl scale rc kubia --replicas=10.\", \"destination_entity\": \"the same command\"},\\n  {\"source_entity\": \"You\", \"description\": \"are going to scale it in a declarative way by editing the ReplicationController\\'s definition.\", \"destination_entity\": \"ReplicationController\\'s definition\"},\\n  {\"source_entity\": \"kubectl edit\", \"description\": \"will see too many pods exist (when scaling down) and delete part of them, or see too few of them (when scaling up) and create additional pods.\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"The ReplicationController\", \"description\": \"will either see too many pods exist (when scaling down) and delete part of them, or see too few of them (when scaling up) and create additional pods.\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"You\", \"description\": \"are going to scale the number of ReplicationController instances to 10 now.\", \"destination_entity\": \"ReplicationController instances\"},\\n  {\"source_entity\": \"kubectl edit\", \"description\": \"will either see too many pods exist (when scaling down) and delete part of them, or see too few of them (when scaling up) and create additional pods.\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"The ReplicationController\", \"description\": \"will be scaled to 10 instances now.\", \"destination_entity\": \"ReplicationController instances\"},\\n  {\"source_entity\": \"You\", \"description\": \"are going to scale the ReplicationController to 10 instances now.\", \"destination_entity\": \"ReplicationController instances\"},\\n  {\"source_entity\": \"The KUBE_EDITOR environment variable\", \"description\": \"is used to specify a different text editor for kubectl edit.\", \"destination_entity\": \"kubectl edit\"},\\n  {\"source_entity\": \"You\", \"description\": \"can tell kubectl to use a text editor of your choice by setting the KUBE_EDITOR environment variable.\", \"destination_entity\": \"KUBE_EDITOR environment variable\"},\\n  {\"source_entity\": \"The EDITOR environment variable\", \"description\": \"is used as a fallback for kubectl edit if the KUBE_EDITOR environment variable is not set.\", \"destination_entity\": \"kubectl edit\"},\\n  {\"source_entity\": \"You\", \"description\": \"can tell kubectl to use a text editor of your choice by setting the EDITOR environment variable.\", \"destination_entity\": \"EDITOR environment variable\"},\\n  {\"source_entity\": \"The KUBE_EDITOR environment variable\", \"description\": \"is used as a fallback for kubectl edit if the EDITOR environment variable is not set.\", \"destination_entity\": \"kubectl edit\"}\\n]\\n```'},\n",
       " {'page': 135,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '103\\nIntroducing ReplicationControllers\\nmetadata:\\n  ...\\nspec:\\n  replicas: 3        \\n  selector:\\n    app: kubia\\n  ...\\nWhen you save the file and close the editor, the ReplicationController is updated and\\nit immediately scales the number of pods to 10:\\n$ kubectl get rc\\nNAME      DESIRED   CURRENT   READY     AGE\\nkubia     10        10        4         21m\\nThere you go. If the kubectl scale command makes it look as though you’re telling\\nKubernetes exactly what to do, it’s now much clearer that you’re making a declarative\\nchange to the desired state of the ReplicationController and not telling Kubernetes to\\ndo something.\\nSCALING DOWN WITH THE KUBECTL SCALE COMMAND\\nNow scale back down to 3. You can use the kubectl scale command:\\n$ kubectl scale rc kubia --replicas=3\\nAll this command does is modify the spec.replicas field of the ReplicationController’s\\ndefinition—like when you changed it through kubectl edit. \\nUNDERSTANDING THE DECLARATIVE APPROACH TO SCALING\\nHorizontally scaling pods in Kubernetes is a matter of stating your desire: “I want to\\nhave x number of instances running.” You’re not telling Kubernetes what or how to do\\nit. You’re just specifying the desired state. \\n This declarative approach makes interacting with a Kubernetes cluster easy. Imag-\\nine if you had to manually determine the current number of running instances and\\nthen explicitly tell Kubernetes how many additional instances to run. That’s more\\nwork and is much more error-prone. Changing a simple number is much easier, and\\nin chapter 15, you’ll learn that even that can be done by Kubernetes itself if you\\nenable horizontal pod auto-scaling. \\n4.2.7\\nDeleting a ReplicationController\\nWhen you delete a ReplicationController through kubectl delete, the pods are also\\ndeleted. But because pods created by a ReplicationController aren’t an integral part\\nof the ReplicationController, and are only managed by it, you can delete only the\\nReplicationController and leave the pods running, as shown in figure 4.7.\\n This may be useful when you initially have a set of pods managed by a Replication-\\nController, and then decide to replace the ReplicationController with a ReplicaSet,\\nfor example (you’ll learn about them next.). You can do this without affecting the\\nChange the number 3 \\nto number 10 in \\nthis line.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationControllers',\n",
       "    'description': 'A Kubernetes resource that manages replicas of a pod.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool for interacting with a Kubernetes cluster.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'scale',\n",
       "    'description': 'A command to modify the desired state of a ReplicationController.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Replicas',\n",
       "    'description': 'The number of desired pods in a ReplicationController.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'selector',\n",
       "    'description': 'A label used to select pods for a ReplicationController.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'The basic execution unit in Kubernetes.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A more advanced resource than ReplicationControllers, also manages replicas of a pod.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'horizontally scaling pods',\n",
       "    'description': 'The process of increasing or decreasing the number of running instances of a pod.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ReplicationControllers\", \"description\": \"manages multiple instances of pods\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"scales the number of pods managed by a ReplicationController\", \"destination_entity\": \"ReplicationControllers\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"modifies the spec.replicas field of a ReplicationController\\'s definition\", \"destination_entity\": \"ReplicationControllers\"},\\n  {\"source_entity\": \"selector\", \"description\": \"specifies the label to match for pod selection\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"Replicas\", \"description\": \"defines the number of desired replicas\", \"destination_entity\": \"ReplicationControllers\"},\\n  {\"source_entity\": \"kubectl scale command\", \"description\": \"makes a declarative change to the desired state of a ReplicationController\", \"destination_entity\": \"ReplicationControllers\"},\\n  {\"source_entity\": \"kubectl delete command\", \"description\": \"deletes a ReplicationController and its managed pods\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"ReplicaSet\", \"description\": \"replaces an existing ReplicationController with a new one\", \"destination_entity\": \"ReplicationControllers\"},\\n  {\"source_entity\": \"kubectl scale command\", \"description\": \"scales down the number of replicas managed by a ReplicationController\", \"destination_entity\": \"ReplicationControllers\"},\\n  {\"source_entity\": \"horizontally scaling pods\", \"description\": \"stating the desired number of instances running\", \"destination_entity\": \"pods\"}\\n]\\n```'},\n",
       " {'page': 136,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '104\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\npods and keep them running without interruption while you replace the Replication-\\nController that manages them. \\n When deleting a ReplicationController with kubectl delete, you can keep its\\npods running by passing the --cascade=false option to the command. Try that now:\\n$ kubectl delete rc kubia --cascade=false\\nreplicationcontroller \"kubia\" deleted\\nYou’ve deleted the ReplicationController so the pods are on their own. They are no\\nlonger managed. But you can always create a new ReplicationController with the\\nproper label selector and make them managed again.\\n4.3\\nUsing ReplicaSets instead of ReplicationControllers\\nInitially, ReplicationControllers were the only Kubernetes component for replicating\\npods and rescheduling them when nodes failed. Later, a similar resource called a\\nReplicaSet was introduced. It’s a new generation of ReplicationController and\\nreplaces it completely (ReplicationControllers will eventually be deprecated). \\n You could have started this chapter by creating a ReplicaSet instead of a Replication-\\nController, but I felt it would be a good idea to start with what was initially available in\\nKubernetes. Plus, you’ll still see ReplicationControllers used in the wild, so it’s good\\nfor you to know about them. That said, you should always create ReplicaSets instead\\nof ReplicationControllers from now on. They’re almost identical, so you shouldn’t\\nhave any trouble using them instead. \\nBefore the RC deletion\\nAfter the RC deletion\\nDelete RC\\nPod:\\nkubia-q3vkg\\nPod:\\nkubia-53thy\\nPod:\\nkubia-k0xz6\\nPod:\\nkubia-q3vkg\\nPod:\\nkubia-53thy\\nPod:\\nkubia-k0xz6\\nReplicationController: kubia\\nReplicas: 3\\nSelector: app=kubia\\napp: kubia\\napp: kubia\\napp: kubia\\napp: kubia\\napp: kubia\\napp: kubia\\nFigure 4.7\\nDeleting a replication controller with --cascade=false leaves pods unmanaged.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'a Kubernetes component for replicating pods and rescheduling them when nodes failed',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'a new generation of ReplicationController that replaces it completely',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'the basic execution unit in a containerized environment',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubia',\n",
       "    'description': 'the name of the replication controller and replica set',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the command-line tool for interacting with Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'delete',\n",
       "    'description': 'a command to delete a resource',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'cascade=false',\n",
       "    'description': 'an option to keep pods running when deleting a ReplicationController',\n",
       "    'category': 'option'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"manages\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"delete\",\\n    \"destination_entity\": \"ReplicationController\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"leave pods unmanaged\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"keep pods running\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSet\",\\n    \"description\": \"replace\",\\n    \"destination_entity\": \"ReplicationController\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"reschedule pods\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"create\",\\n    \"destination_entity\": \"ReplicaSet\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"be deprecated\",\\n    \"destination_entity\": \"ReplicaSet\"\\n  }\\n]\\n\\nNote: The entities \\'delete\\', \\'cascade=false\\' are not included in the relations as they are not entities but rather actions or options.'},\n",
       " {'page': 137,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '105\\nUsing ReplicaSets instead of ReplicationControllers\\n You usually won’t create them directly, but instead have them created automati-\\ncally when you create the higher-level Deployment resource, which you’ll learn about\\nin chapter 9. In any case, you should understand ReplicaSets, so let’s see how they dif-\\nfer from ReplicationControllers.\\n4.3.1\\nComparing a ReplicaSet to a ReplicationController\\nA ReplicaSet behaves exactly like a ReplicationController, but it has more expressive\\npod selectors. Whereas a ReplicationController’s label selector only allows matching\\npods that include a certain label, a ReplicaSet’s selector also allows matching pods\\nthat lack a certain label or pods that include a certain label key, regardless of\\nits value.\\n Also, for example, a single ReplicationController can’t match pods with the label\\nenv=production and those with the label env=devel at the same time. It can only match\\neither pods with the env=production label or pods with the env=devel label. But a sin-\\ngle ReplicaSet can match both sets of pods and treat them as a single group. \\n Similarly, a ReplicationController can’t match pods based merely on the presence\\nof a label key, regardless of its value, whereas a ReplicaSet can. For example, a Replica-\\nSet can match all pods that include a label with the key env, whatever its actual value is\\n(you can think of it as env=*).\\n4.3.2\\nDefining a ReplicaSet\\nYou’re going to create a ReplicaSet now to see how the orphaned pods that were cre-\\nated by your ReplicationController and then abandoned earlier can now be adopted\\nby a ReplicaSet. First, you’ll rewrite your ReplicationController into a ReplicaSet by\\ncreating a new file called kubia-replicaset.yaml with the contents in the following\\nlisting.\\napiVersion: apps/v1beta2      \\nkind: ReplicaSet                    \\nmetadata:\\n  name: kubia\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:                 \\n      app: kubia                 \\n  template:                        \\n    metadata:                      \\n      labels:                      \\n        app: kubia                 \\n    spec:                          \\n      containers:                  \\n      - name: kubia                \\n        image: luksa/kubia         \\nListing 4.8\\nA YAML definition of a ReplicaSet: kubia-replicaset.yaml\\nReplicaSets aren’t part of the v1 \\nAPI, but belong to the apps API \\ngroup and version v1beta2.\\nYou’re using the simpler matchLabels \\nselector here, which is much like a \\nReplicationController’s selector.\\nThe template is \\nthe same as in the \\nReplicationController.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicaSets',\n",
       "    'description': 'A resource that ensures a specified number of replica pods are running at any given time.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicationControllers',\n",
       "    'description': 'An older version of ReplicaSets, which has been deprecated in favor of ReplicaSets.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'A field that specifies the API version being used by a Kubernetes resource.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'A field that specifies the type of Kubernetes resource being defined.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'A section within a Kubernetes resource definition that provides metadata about the resource.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'A field that specifies the name of a Kubernetes resource.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'A section within a Kubernetes resource definition that provides specifications for the resource.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'replicas',\n",
       "    'description': 'A field that specifies the number of replica pods desired for a ReplicaSet.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'selector',\n",
       "    'description': 'A section within a Kubernetes resource definition that provides a selector for matching pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'matchLabels',\n",
       "    'description': 'A field that specifies the labels to match when selecting pods for a ReplicaSet or ReplicationController.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'template',\n",
       "    'description': 'A section within a Kubernetes resource definition that provides a template for creating new pods.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'A section within a Kubernetes resource definition that provides metadata about the pod being created.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'A field that specifies the labels to be applied to a pod when it is created.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'A section within a Kubernetes resource definition that provides specifications for the pod being created.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'A field that specifies the containers to be run within a pod when it is created.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'A field that specifies the name of a container being run within a pod.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'A field that specifies the image to be used for a container being run within a pod.',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ReplicaSets\", \"description\": \"behaves exactly like a ReplicationController\", \"destination_entity\": \"ReplicationControllers\"},\\n  {\"source_entity\": \"ReplicaSets\", \"description\": \"has more expressive pod selectors\", \"destination_entity\": \"pod selectors\"},\\n  {\"source_entity\": \"ReplicaSets\", \"description\": \"allows matching pods that include a certain label key, regardless of its value\", \"destination_entity\": \"labels\"},\\n  {\"source_entity\": \"ReplicationControllers\", \"description\": \"can\\'t match both sets of pods with the env=production and env=devel labels at the same time\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"ReplicaSets\", \"description\": \"can match both sets of pods with the env=production and env=devel labels as a single group\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"ReplicationControllers\", \"description\": \"can\\'t match pods based merely on the presence of a label key, regardless of its value\", \"destination_entity\": \"label keys\"},\\n  {\"source_entity\": \"ReplicaSets\", \"description\": \"can match all pods that include a label with the key env, whatever its actual value is\", \"destination_entity\": \"label keys\"},\\n  {\"source_entity\": \"kubia-replicaset.yaml\", \"description\": \"defines a ReplicaSet\", \"destination_entity\": \"ReplicaSets\"},\\n  {\"source_entity\": \"kubia-replicaset.yaml\", \"description\": \"specifies the replicas to be created\", \"destination_entity\": \"replicas\"},\\n  {\"source_entity\": \"kubia-replicaset.yaml\", \"description\": \"defines the selector for the ReplicaSet\", \"destination_entity\": \"selector\"},\\n  {\"source_entity\": \"kubia-replicaset.yaml\", \"description\": \"specifies the template for the pods to be created\", \"destination_entity\": \"template\"},\\n  {\"source_entity\": \"ReplicaSets\", \"description\": \"can adopt orphaned pods created by a ReplicationController\", \"destination_entity\": \"orphaned pods\"},\\n  {\"source_entity\": \"kubia-replicaset.yaml\", \"description\": \"specifies the name of the ReplicaSet\", \"destination_entity\": \"name\"},\\n  {\"source_entity\": \"kubia-replicaset.yaml\", \"description\": \"specifies the image to be used for the containers\", \"destination_entity\": \"image\"}\\n]\\n```'},\n",
       " {'page': 138,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '106\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\nThe first thing to note is that ReplicaSets aren’t part of the v1 API, so you need to\\nensure you specify the proper apiVersion when creating the resource. You’re creating a\\nresource of type ReplicaSet which has much the same contents as the Replication-\\nController you created earlier. \\n The only difference is in the selector. Instead of listing labels the pods need to\\nhave directly under the selector property, you’re specifying them under selector\\n.matchLabels. This is the simpler (and less expressive) way of defining label selectors\\nin a ReplicaSet. Later, you’ll look at the more expressive option, as well.\\nBecause you still have three pods matching the app=kubia selector running from ear-\\nlier, creating this ReplicaSet will not cause any new pods to be created. The ReplicaSet\\nwill take those existing three pods under its wing. \\n4.3.3\\nCreating and examining a ReplicaSet\\nCreate the ReplicaSet from the YAML file with the kubectl create command. After\\nthat, you can examine the ReplicaSet with kubectl get and kubectl describe:\\n$ kubectl get rs\\nNAME      DESIRED   CURRENT   READY     AGE\\nkubia     3         3         3         3s\\nTIP\\nUse rs shorthand, which stands for replicaset.\\n$ kubectl describe rs\\nName:           kubia\\nNamespace:      default\\nSelector:       app=kubia\\nLabels:         app=kubia\\nAnnotations:    <none>\\nReplicas:       3 current / 3 desired\\nPods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed\\nPod Template:\\n  Labels:       app=kubia\\nAbout the API version attribute\\nThis is your first opportunity to see that the apiVersion property specifies two things:\\n\\uf0a1The API group (which is apps in this case)\\n\\uf0a1The actual API version (v1beta2)\\nYou’ll see throughout the book that certain Kubernetes resources are in what’s called\\nthe core API group, which doesn’t need to be specified in the apiVersion field (you\\njust specify the version—for example, you’ve been using apiVersion: v1 when\\ndefining Pod resources). Other resources, which were introduced in later Kubernetes\\nversions, are categorized into several API groups. Look at the inside of the book’s\\ncovers to see all resources and their respective API groups.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicaSet',\n",
       "    'description': 'A resource that ensures a specified number of replica pods are running at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Replication-Controller',\n",
       "    'description': 'An older way of managing replicas, being replaced by ReplicaSets.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'A property that specifies the API group and version.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'selector.matchLabels',\n",
       "    'description': 'A simpler way of defining label selectors in a ReplicaSet.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for managing Kubernetes resources.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'rs',\n",
       "    'description': 'The shorthand for replicaset.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pods Status',\n",
       "    'description': 'The status of the pods managed by a ReplicaSet, including running, waiting, succeeded, and failed states.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod Template',\n",
       "    'description': 'A template that defines the configuration for each pod created by a ReplicaSet.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'app=kubia',\n",
       "    'description': 'A label selector used to identify pods managed by a ReplicaSet.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiGroup',\n",
       "    'description': 'The group of APIs, which in this case is the core API group for Kubernetes resources.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API version (v1beta2)',\n",
       "    'description': 'The actual version of the API used to manage ReplicaSets.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"ReplicaSet\", \"description\": \"is created using the kubectl create command\", \"destination_entity\": \"kubectl\"}, {\"source_entity\": \"ReplicaSet\", \"description\": \"takes existing pods under its wing\", \"destination_entity\": \"pods\"}, {\"source_entity\": \"selector.matchLabels\", \"description\": \"defines label selectors in a ReplicaSet\", \"destination_entity\": \"labels\"}, {\"source_entity\": \"rs\", \"description\": \"stands for replicaset\", \"destination_entity\": \"ReplicaSet\"}, {\"source_entity\": \"apiVersion\", \"description\": \"specifies API group and actual API version\", \"destination_entity\": \"API\"}, {\"source_entity\": \"selector.matchLabels\", \"description\": \"is used to define label selectors in a ReplicaSet\", \"destination_entity\": \"labels\"}, {\"source_entity\": \"kubectl get rs\", \"description\": \"examines the ReplicaSet\", \"destination_entity\": \"ReplicaSet\"}, {\"source_entity\": \"kubectl describe rs\", \"description\": \"gets detailed information about the ReplicaSet\", \"destination_entity\": \"ReplicaSet\"}, {\"source_entity\": \"Pod Template\", \"description\": \"defines the template for creating pods\", \"destination_entity\": \"pods\"}, {\"source_entity\": \"apiVersion\", \"description\": \"specifies the API version used by a resource\", \"destination_entity\": \"resource\"}, {\"source_entity\": \"Replication-Controller\", \"description\": \"is a type of controller that manages pods\", \"destination_entity\": \"pods\"}, {\"source_entity\": \"app=kubia\", \"description\": \"defines the label selector for identifying pods\", \"destination_entity\": \"labels\"}, {\"source_entity\": \"kubectl create\", \"description\": \"creates a new resource, such as a ReplicaSet\", \"destination_entity\": \"resource\"}]'},\n",
       " {'page': 139,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '107\\nUsing ReplicaSets instead of ReplicationControllers\\n  Containers:   ...\\n  Volumes:      <none>\\nEvents:         <none>\\nAs you can see, the ReplicaSet isn’t any different from a ReplicationController. It’s\\nshowing it has three replicas matching the selector. If you list all the pods, you’ll see\\nthey’re still the same three pods you had before. The ReplicaSet didn’t create any new\\nones. \\n4.3.4\\nUsing the ReplicaSet’s more expressive label selectors\\nThe main improvements of ReplicaSets over ReplicationControllers are their more\\nexpressive label selectors. You intentionally used the simpler matchLabels selector in\\nthe first ReplicaSet example to see that ReplicaSets are no different from Replication-\\nControllers. Now, you’ll rewrite the selector to use the more powerful matchExpressions\\nproperty, as shown in the following listing.\\n selector:\\n   matchExpressions:                 \\n     - key: app           \\n       operator: In                  \\n       values:                       \\n         - kubia                     \\nNOTE\\nOnly the selector is shown. You’ll find the whole ReplicaSet definition\\nin the book’s code archive.\\nYou can add additional expressions to the selector. As in the example, each expression\\nmust contain a key, an operator, and possibly (depending on the operator) a list of\\nvalues. You’ll see four valid operators:\\n\\uf0a1\\nIn—Label’s value must match one of the specified values.\\n\\uf0a1\\nNotIn—Label’s value must not match any of the specified values.\\n\\uf0a1\\nExists—Pod must include a label with the specified key (the value isn’t import-\\nant). When using this operator, you shouldn’t specify the values field.\\n\\uf0a1\\nDoesNotExist—Pod must not include a label with the specified key. The values\\nproperty must not be specified.\\nIf you specify multiple expressions, all those expressions must evaluate to true for the\\nselector to match a pod. If you specify both matchLabels and matchExpressions, all\\nthe labels must match and all the expressions must evaluate to true for the pod to\\nmatch the selector.\\nListing 4.9\\nA matchExpressions selector: kubia-replicaset-matchexpressions.yaml\\nThis selector requires the pod to \\ncontain a label with the “app” key.\\nThe label’s value \\nmust be “kubia”.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicaSet',\n",
       "    'description': 'A Kubernetes resource that ensures a specified number of replicas (identical pods) are running at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A legacy Kubernetes resource that also ensures a specified number of replicas are running, but with less expressive selectors.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'matchLabels selector',\n",
       "    'description': 'A simpler label selector used in ReplicaSets to match pods based on key-value pairs.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'matchExpressions selector',\n",
       "    'description': 'A more expressive label selector used in ReplicaSets to match pods based on a combination of conditions.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'operator',\n",
       "    'description': 'A keyword used in the matchExpressions selector to specify the condition for matching labels.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'key',\n",
       "    'description': 'The key of a label that must be matched by the selector.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'values',\n",
       "    'description': \"A list of values that the label's value must match for the selector to match the pod.\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'selector',\n",
       "    'description': 'A configuration that specifies the conditions under which a ReplicaSet or ReplicationController will select and manage pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'label',\n",
       "    'description': 'A key-value pair attached to a pod or other Kubernetes resource for identification, selection, or annotation purposes.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"ReplicaSet\", \"description\": \"is no different from\", \"destination_entity\": \"ReplicationController\"}, \\n {\"source_entity\": \"ReplicaSet\", \"description\": \"didn’t create any new ones\", \"destination_entity\": \"pods\"}, \\n {\"source_entity\": \"selector\", \"description\": \"showing it has three replicas matching the selector\", \"destination_entity\": \"ReplicaSet\"}, \\n {\"source_entity\": \"matchExpressions selector\", \"description\": \"requires the pod to contain a label with the \\\\\"app\\\\\" key\", \"destination_entity\": \"pod\"}, \\n {\"source_entity\": \"values\", \"description\": \"must match one of the specified values\", \"destination_entity\": \"label\"}, \\n {\"source_entity\": \"key\", \"description\": \"the value must be \\\\\"kubia\\\\\"\", \"destination_entity\": \"label\"}, \\n {\"source_entity\": \"matchLabels selector\", \"description\": \"evaluate to true for the pod to match the selector\", \"destination_entity\": \"selector\"}, \\n {\"source_entity\": \"ReplicationController\", \"description\": \"is no different from ReplicaSet\", \"destination_entity\": \"ReplicaSet\"}, \\n {\"source_entity\": \"ReplicaSet\", \"description\": \"has three replicas matching the selector\", \"destination_entity\": \"pods\"}]'},\n",
       " {'page': 140,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '108\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\n4.3.5\\nWrapping up ReplicaSets\\nThis was a quick introduction to ReplicaSets as an alternative to ReplicationControllers.\\nRemember, always use them instead of ReplicationControllers, but you may still find\\nReplicationControllers in other people’s deployments.\\n Now, delete the ReplicaSet to clean up your cluster a little. You can delete the\\nReplicaSet the same way you’d delete a ReplicationController:\\n$ kubectl delete rs kubia\\nreplicaset \"kubia\" deleted\\nDeleting the ReplicaSet should delete all the pods. List the pods to confirm that’s\\nthe case. \\n4.4\\nRunning exactly one pod on each node with \\nDaemonSets\\nBoth ReplicationControllers and ReplicaSets are used for running a specific number\\nof pods deployed anywhere in the Kubernetes cluster. But certain cases exist when you\\nwant a pod to run on each and every node in the cluster (and each node needs to run\\nexactly one instance of the pod, as shown in figure 4.8).\\n Those cases include infrastructure-related pods that perform system-level opera-\\ntions. For example, you’ll want to run a log collector and a resource monitor on every\\nnode. Another good example is Kubernetes’ own kube-proxy process, which needs to\\nrun on all nodes to make services work.\\nNode 1\\nPod\\nPod\\nPod\\nReplicaSet\\nReplicas: 5\\nNode 2\\nPod\\nPod\\nNode 3\\nPod\\nDaemonSet\\nExactly one replica\\non each node\\nNode 4\\nPod\\nPod\\nPod\\nFigure 4.8\\nDaemonSets run only a single pod replica on each node, whereas ReplicaSets \\nscatter them around the whole cluster randomly. \\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [   Col0    Node 2\n",
       "   0  None  Pod\\nPod\n",
       "   1            None\n",
       "   2                \n",
       "   3  None          ],\n",
       "  'entities': [{'entity': 'ReplicaSet',\n",
       "    'description': 'A set of replicas in Kubernetes that ensures a specified number of pods are running at any given time.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'An older way to run multiple replicas of a pod in a cluster, but not recommended for new deployments.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool used to interact with Kubernetes clusters.',\n",
       "    'category': 'software/command'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A basic execution unit in Kubernetes that can contain one or more containers.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'DaemonSet',\n",
       "    'description': 'A type of deployment in Kubernetes that runs a pod on each node in the cluster, with exactly one replica per node.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'Cluster',\n",
       "    'description': 'A group of machines (nodes) that run containerized applications and are managed by Kubernetes.',\n",
       "    'category': 'hardware/network'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ReplicaSet\", \"description\": \"used instead of ReplicationControllers\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"You\", \"description\": \"delete the ReplicaSet to clean up your cluster\", \"destination_entity\": \"ReplicaSet\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"delete the ReplicaSet\", \"destination_entity\": \"ReplicaSet\"},\\n  {\"source_entity\": \"ReplicaSet\", \"description\": \"should delete all the pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"DaemonSets\", \"description\": \"run a pod to run on each and every node in the cluster\", \"destination_entity\": \"Cluster\"},\\n  {\"source_entity\": \"You\", \"description\": \"want to run a log collector and a resource monitor\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Kubernetes\\'\", \"description\": \"own kube-proxy process\", \"destination_entity\": \"Node\"},\\n  {\"source_entity\": \"DaemonSet\", \"description\": \"run only a single pod replica on each node\", \"destination_entity\": \"Node\"},\\n  {\"source_entity\": \"ReplicaSets\", \"description\": \"scatter them around the whole cluster randomly\", \"destination_entity\": \"Cluster\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"delete a ReplicationController\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"You\", \"description\": \"clean up your cluster\", \"destination_entity\": \"Cluster\"}\\n]\\n```\\n\\nNote: I\\'ve used the entities provided in the input list, and extracted relations based on the context of the document page.'},\n",
       " {'page': 141,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '109\\nRunning exactly one pod on each node with DaemonSets\\nOutside of Kubernetes, such processes would usually be started through system init\\nscripts or the systemd daemon during node boot up. On Kubernetes nodes, you can\\nstill use systemd to run your system processes, but then you can’t take advantage of all\\nthe features Kubernetes provides. \\n4.4.1\\nUsing a DaemonSet to run a pod on every node\\nTo run a pod on all cluster nodes, you create a DaemonSet object, which is much\\nlike a ReplicationController or a ReplicaSet, except that pods created by a Daemon-\\nSet already have a target node specified and skip the Kubernetes Scheduler. They\\naren’t scattered around the cluster randomly. \\n A DaemonSet makes sure it creates as many pods as there are nodes and deploys\\neach one on its own node, as shown in figure 4.8.\\n Whereas a ReplicaSet (or ReplicationController) makes sure that a desired num-\\nber of pod replicas exist in the cluster, a DaemonSet doesn’t have any notion of a\\ndesired replica count. It doesn’t need it because its job is to ensure that a pod match-\\ning its pod selector is running on each node. \\n If a node goes down, the DaemonSet doesn’t cause the pod to be created else-\\nwhere. But when a new node is added to the cluster, the DaemonSet immediately\\ndeploys a new pod instance to it. It also does the same if someone inadvertently\\ndeletes one of the pods, leaving the node without the DaemonSet’s pod. Like a Replica-\\nSet, a DaemonSet creates the pod from the pod template configured in it.\\n4.4.2\\nUsing a DaemonSet to run pods only on certain nodes\\nA DaemonSet deploys pods to all nodes in the cluster, unless you specify that the pods\\nshould only run on a subset of all the nodes. This is done by specifying the node-\\nSelector property in the pod template, which is part of the DaemonSet definition\\n(similar to the pod template in a ReplicaSet or ReplicationController). \\n You’ve already used node selectors to deploy a pod onto specific nodes in chapter 3.\\nA node selector in a DaemonSet is similar—it defines the nodes the DaemonSet must\\ndeploy its pods to. \\nNOTE\\nLater in the book, you’ll learn that nodes can be made unschedulable,\\npreventing pods from being deployed to them. A DaemonSet will deploy pods\\neven to such nodes, because the unschedulable attribute is only used by the\\nScheduler, whereas pods managed by a DaemonSet bypass the Scheduler\\ncompletely. This is usually desirable, because DaemonSets are meant to run\\nsystem services, which usually need to run even on unschedulable nodes.\\nEXPLAINING DAEMONSETS WITH AN EXAMPLE\\nLet’s imagine having a daemon called ssd-monitor that needs to run on all nodes\\nthat contain a solid-state drive (SSD). You’ll create a DaemonSet that runs this dae-\\nmon on all nodes that are marked as having an SSD. The cluster administrators have\\nadded the disk=ssd label to all such nodes, so you’ll create the DaemonSet with a\\nnode selector that only selects nodes with that label, as shown in figure 4.9.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'DaemonSet',\n",
       "    'description': 'A Kubernetes object that runs a pod on every node in the cluster, or a subset of nodes based on a node selector.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A Kubernetes object that ensures a desired number of pod replicas exist in the cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A legacy Kubernetes object similar to ReplicaSet, used for ensuring a desired number of pod replicas exist in the cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A container that runs an application or service, created by a DaemonSet or other Kubernetes objects.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'A physical or virtual machine in the cluster where pods are run.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'The Kubernetes component that assigns pods to nodes based on resource availability and other factors.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'node selector',\n",
       "    'description': 'A property in the pod template that specifies which nodes a DaemonSet should deploy its pods to, based on labels or other criteria.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'Key-value pairs that describe characteristics of a node, such as disk type (SSD) or other attributes.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'systemd',\n",
       "    'description': 'A system manager for Linux operating systems that runs services and applications on boot-up or when needed.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"runs system init scripts or systemd daemon during node boot up\", \"destination_entity\": \"systemd\"},\\n  \\n  {\"source_entity\": \"DaemonSet\", \"description\": \"creates as many pods as there are nodes and deploys each one on its own node\", \"destination_entity\": \"node\"},\\n  \\n  {\"source_entity\": \"DaemonSet\", \"description\": \"ensures that a pod matching its pod selector is running on each node\", \"destination_entity\": \"node\"},\\n  \\n  {\"source_entity\": \"DaemonSet\", \"description\": \"deploys pods to all nodes in the cluster, unless specified otherwise\", \"destination_entity\": \"cluster\"},\\n  \\n  {\"source_entity\": \"DaemonSet\", \"description\": \"uses node selector property to deploy pods on specific nodes\", \"destination_entity\": \"node selector\"},\\n  \\n  {\"source_entity\": \"DaemonSet\", \"description\": \"bypasses Scheduler and deploys pods even on unschedulable nodes\", \"destination_entity\": \"Scheduler\"},\\n  \\n  {\"source_entity\": \"DaemonSet\", \"description\": \"runs system services that need to run even on unschedulable nodes\", \"destination_entity\": \"system services\"},\\n  \\n  {\"source_entity\": \"ssd-monitor daemon\", \"description\": \"needs to run on all nodes with a solid-state drive (SSD)\", \"destination_entity\": \"node\"},\\n  \\n  {\"source_entity\": \"DaemonSet\", \"description\": \"runs ssd-monitor daemon on all nodes that contain an SSD\", \"destination_entity\": \"ssd-monitor daemon\"},\\n  \\n  {\"source_entity\": \"ReplicationController/ReplicaSet\", \"description\": \"ensures a desired number of pod replicas exist in the cluster\", \"destination_entity\": \"cluster\"},\\n  \\n  {\"source_entity\": \"DaemonSet\", \"description\": \"does not have any notion of a desired replica count\", \"destination_entity\": \"desired replica count\"},\\n  \\n  {\"source_entity\": \"node selector\", \"description\": \"defines nodes that a DaemonSet must deploy its pods to\", \"destination_entity\": \"DaemonSet\"},\\n  \\n  {\"source_entity\": \"Scheduler\", \"description\": \"uses unschedulable attribute only, whereas DaemonSets bypass the Scheduler completely\", \"destination_entity\": \"DaemonSet\"}\\n]'},\n",
       " {'page': 142,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '110\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\nCREATING A DAEMONSET YAML DEFINITION\\nYou’ll create a DaemonSet that runs a mock ssd-monitor process, which prints\\n“SSD OK” to the standard output every five seconds. I’ve already prepared the mock\\ncontainer image and pushed it to Docker Hub, so you can use it instead of building\\nyour own. Create the YAML for the DaemonSet, as shown in the following listing.\\napiVersion: apps/v1beta2      \\nkind: DaemonSet                     \\nmetadata:\\n  name: ssd-monitor\\nspec:                            \\n  selector:\\n    matchLabels:\\n      app: ssd-monitor\\n  template:\\n    metadata:\\n      labels:\\n        app: ssd-monitor\\n    spec:\\n      nodeSelector:                \\n        disk: ssd                  \\n      containers:\\n      - name: main\\n        image: luksa/ssd-monitor\\nYou’re defining a DaemonSet that will run a pod with a single container based on the\\nluksa/ssd-monitor container image. An instance of this pod will be created for each\\nnode that has the disk=ssd label.\\nListing 4.10\\nA YAML for a DaemonSet: ssd-monitor-daemonset.yaml\\nNode 1\\nPod:\\nssd-monitor\\nNode 2\\nNode 3\\nDaemonSet:\\nsssd-monitor\\nNode selector:\\ndisk=ssd\\nNode 4\\ndisk: ssd\\ndisk: ssd\\ndisk: ssd\\nUnschedulable\\nPod:\\nssd-monitor\\nPod:\\nssd-monitor\\nFigure 4.9\\nUsing a DaemonSet with a node selector to deploy system pods only on certain \\nnodes\\nDaemonSets are in the \\napps API group, \\nversion v1beta2.\\nThe pod template includes a \\nnode selector, which selects \\nnodes with the disk=ssd label.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'DaemonSet',\n",
       "    'description': 'A Kubernetes resource that ensures a specified number of replicas (pods) are running at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Node Selector',\n",
       "    'description': 'A way to select nodes in a cluster based on labels.',\n",
       "    'category': 'hardware/network'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit of a Kubernetes application.',\n",
       "    'category': 'container/process'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'A lightweight and standalone executable package that contains an application and its dependencies.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'Image',\n",
       "    'description': 'A Docker image used to create a container.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'API Group',\n",
       "    'description': 'In Kubernetes, a way to group related APIs together.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'YAML',\n",
       "    'description': 'A human-readable serialization format for data.',\n",
       "    'category': 'software/programming language'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"You\", \"description\": \"define a DaemonSet YAML definition\", \"destination_entity\": \"DaemonSet\"},\\n  {\"source_entity\": \"A DaemonSet\", \"description\": \"runs a mock ssd-monitor process, printing \\'SSD OK\\' every five seconds\", \"destination_entity\": \"Process\"},\\n  {\"source_entity\": \"You\", \"description\": \"use a pre-built container image from Docker Hub\", \"destination_entity\": \"Image\"},\\n  {\"source_entity\": \"A DaemonSet\", \"description\": \"specifies a node selector with the disk=ssd label\", \"destination_entity\": \"Node Selector\"},\\n  {\"source_entity\": \"The pod template\", \"description\": \"includes a node selector to select nodes with the disk=ssd label\", \"destination_entity\": \"Node\"},\\n  {\"source_entity\": \"A DaemonSet\", \"description\": \"is in the apps API group, version v1beta2\", \"destination_entity\": \"API Group\"},\\n  {\"source_entity\": \"The pod template\", \"description\": \"specifies a node selector to deploy system pods only on certain nodes\", \"destination_entity\": \"Node\"},\\n  {\"source_entity\": \"A DaemonSet\", \"description\": \"runs a container based on the luksa/ssd-monitor container image\", \"destination_entity\": \"Container\"}\\n]'},\n",
       " {'page': 143,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '111\\nRunning exactly one pod on each node with DaemonSets\\nCREATING THE DAEMONSET\\nYou’ll create the DaemonSet like you always create resources from a YAML file:\\n$ kubectl create -f ssd-monitor-daemonset.yaml\\ndaemonset \"ssd-monitor\" created\\nLet’s see the created DaemonSet:\\n$ kubectl get ds\\nNAME          DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE-SELECTOR  \\nssd-monitor   0        0        0      0           0          disk=ssd\\nThose zeroes look strange. Didn’t the DaemonSet deploy any pods? List the pods:\\n$ kubectl get po\\nNo resources found.\\nWhere are the pods? Do you know what’s going on? Yes, you forgot to label your nodes\\nwith the disk=ssd label. No problem—you can do that now. The DaemonSet should\\ndetect that the nodes’ labels have changed and deploy the pod to all nodes with a\\nmatching label. Let’s see if that’s true. \\nADDING THE REQUIRED LABEL TO YOUR NODE(S)\\nRegardless if you’re using Minikube, GKE, or another multi-node cluster, you’ll need\\nto list the nodes first, because you’ll need to know the node’s name when labeling it:\\n$ kubectl get node\\nNAME       STATUS    AGE       VERSION\\nminikube   Ready     4d        v1.6.0\\nNow, add the disk=ssd label to one of your nodes like this:\\n$ kubectl label node minikube disk=ssd\\nnode \"minikube\" labeled\\nNOTE\\nReplace minikube with the name of one of your nodes if you’re not\\nusing Minikube.\\nThe DaemonSet should have created one pod now. Let’s see:\\n$ kubectl get po\\nNAME                READY     STATUS    RESTARTS   AGE\\nssd-monitor-hgxwq   1/1       Running   0          35s\\nOkay; so far so good. If you have multiple nodes and you add the same label to further\\nnodes, you’ll see the DaemonSet spin up pods for each of them. \\nREMOVING THE REQUIRED LABEL FROM THE NODE\\nNow, imagine you’ve made a mistake and have mislabeled one of the nodes. It has a\\nspinning disk drive, not an SSD. What happens if you change the node’s label?\\n$ kubectl label node minikube disk=hdd --overwrite\\nnode \"minikube\" labeled\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'DaemonSet',\n",
       "    'description': 'A type of Kubernetes resource that automates deployment and scaling of applications.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool for running commands against Kubernetes clusters.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'daemonset.yaml',\n",
       "    'description': 'The YAML file containing the configuration for the DaemonSet.',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A single instance of a running process in Kubernetes.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'node-selectors',\n",
       "    'description': 'A feature in Kubernetes that allows for node selection based on labels.',\n",
       "    'category': 'feature'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'Key-value pairs used to annotate resources in Kubernetes.',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'disk=ssd',\n",
       "    'description': 'A label applied to nodes with solid-state drives (SSDs).',\n",
       "    'category': 'label'},\n",
       "   {'entity': 'minikube',\n",
       "    'description': 'A tool for running a single-node Kubernetes cluster locally.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'GKE',\n",
       "    'description': 'Google Kubernetes Engine, a managed container service for Google Cloud Platform.',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'nodes',\n",
       "    'description': 'The machines in a Kubernetes cluster that run the containers and pods.',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"create DaemonSet from YAML file\",\\n    \"destination_entity\": \"daemonset.yaml\"\\n  },\\n  {\\n    \"source_entity\": \"DaemonSet\",\\n    \"description\": \"deploy pods to nodes with matching label\",\\n    \"destination_entity\": \"labels\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"get list of nodes\",\\n    \"destination_entity\": \"nodes\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"label node with SSD disk label\",\\n    \"destination_entity\": \"minikube\"\\n  },\\n  {\\n    \"source_entity\": \"DaemonSet\",\\n    \"description\": \"create pod on labeled node\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"get list of running pods\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"remove SSD disk label from node\",\\n    \"destination_entity\": \"minikube\"\\n  }\\n]\\n```\\n\\nI have followed the rules to define each relation by means of a JSON having three keys: `source_entity`, `description`, and `destination_entity`. The `source_entity` is the entity that performs some action, the `destination_entity` is the object on which the source entity carries out the action, and the `description` contains a brief summary about what action was being carried by the source entity on the destination entity.'},\n",
       " {'page': 144,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '112\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\nLet’s see if the change has any effect on the pod that was running on that node:\\n$ kubectl get po\\nNAME                READY     STATUS        RESTARTS   AGE\\nssd-monitor-hgxwq   1/1       Terminating   0          4m\\nThe pod is being terminated. But you knew that was going to happen, right? This\\nwraps up your exploration of DaemonSets, so you may want to delete your ssd-monitor\\nDaemonSet. If you still have any other daemon pods running, you’ll see that deleting\\nthe DaemonSet deletes those pods as well. \\n4.5\\nRunning pods that perform a single completable task \\nUp to now, we’ve only talked about pods than need to run continuously. You’ll have\\ncases where you only want to run a task that terminates after completing its work.\\nReplicationControllers, ReplicaSets, and DaemonSets run continuous tasks that are\\nnever considered completed. Processes in such pods are restarted when they exit. But\\nin a completable task, after its process terminates, it should not be restarted again. \\n4.5.1\\nIntroducing the Job resource\\nKubernetes includes support for this through the Job resource, which is similar to the\\nother resources we’ve discussed in this chapter, but it allows you to run a pod whose\\ncontainer isn’t restarted when the process running inside finishes successfully. Once it\\ndoes, the pod is considered complete. \\n In the event of a node failure, the pods on that node that are managed by a Job will\\nbe rescheduled to other nodes the way ReplicaSet pods are. In the event of a failure of\\nthe process itself (when the process returns an error exit code), the Job can be config-\\nured to either restart the container or not.\\n Figure 4.10 shows how a pod created by a Job is rescheduled to a new node if the\\nnode it was initially scheduled to fails. The figure also shows both a managed pod,\\nwhich isn’t rescheduled, and a pod backed by a ReplicaSet, which is.\\n For example, Jobs are useful for ad hoc tasks, where it’s crucial that the task fin-\\nishes properly. You could run the task in an unmanaged pod and wait for it to finish,\\nbut in the event of a node failing or the pod being evicted from the node while it is\\nperforming its task, you’d need to manually recreate it. Doing this manually doesn’t\\nmake sense—especially if the job takes hours to complete. \\n An example of such a job would be if you had data stored somewhere and you\\nneeded to transform and export it somewhere. You’re going to emulate this by run-\\nning a container image built on top of the busybox image, which invokes the sleep\\ncommand for two minutes. I’ve already built the image and pushed it to Docker Hub,\\nbut you can peek into its Dockerfile in the book’s code archive.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A pod is a logical host for one or more containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A ReplicationController ensures that a specified number of replicas (identical pods) are running at any given time.',\n",
       "    'category': 'controller'},\n",
       "   {'entity': 'DaemonSet',\n",
       "    'description': \"A DaemonSet is used to run a container on each machine in a cluster. It's like a service, but it runs on every node.\",\n",
       "    'category': 'controller'},\n",
       "   {'entity': 'Job',\n",
       "    'description': 'A Job is similar to a ReplicaSet, but it ensures that the pod created by the job completes successfully before rescheduling another pod.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'A platform that enables developers to package their applications and its dependencies into a single container.',\n",
       "    'category': 'platform'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': \"A ReplicaSet ensures that a specified number of replicas (identical pods) are running at any given time. It's similar to a ReplicationController, but more flexible.\",\n",
       "    'category': 'controller'},\n",
       "   {'entity': '$ kubectl get po',\n",
       "    'description': 'A command used to retrieve information about pods in a Kubernetes cluster.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'sleep command',\n",
       "    'description': 'A Linux command that pauses the execution of a process for a specified amount of time.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'busybox image',\n",
       "    'description': 'A lightweight Docker image that contains common Unix utilities.',\n",
       "    'category': 'image'},\n",
       "   {'entity': 'Docker Hub',\n",
       "    'description': 'A cloud-based registry where users can store and share container images.',\n",
       "    'category': 'platform'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"DaemonSet\",\\n    \"description\": \"Terminates pods when deleted\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Job\",\\n    \"description\": \"Runs a single completable task that terminates after completing its work\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSet\",\\n    \"description\": \"Runs continuous tasks that are never considered completed\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"$ kubectl get po\",\\n    \"description\": \"Lists running pods\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"Includes support for Jobs and ReplicaSets\",\\n    \"destination_entity\": \"Job\"\\n  },\\n  {\\n    \"source_entity\": \"Docker Hub\",\\n    \"description\": \"Stores images, including the busybox image\",\\n    \"destination_entity\": \"busybox image\"\\n  },\\n  {\\n    \"source_entity\": \"busybox image\",\\n    \"description\": \"Invokes the sleep command for two minutes\",\\n    \"destination_entity\": \"sleep command\"\\n  },\\n  {\\n    \"source_entity\": \"sleep command\",\\n    \"description\": \"Pauses execution for two minutes\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"Can be managed by a Job or ReplicaSet\",\\n    \"destination_entity\": \"Job\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"Runs continuous tasks that are never considered completed\",\\n    \"destination_entity\": \"Pod\"\\n  }\\n]\\n```'},\n",
       " {'page': 145,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '113\\nRunning pods that perform a single completable task\\n4.5.2\\nDefining a Job resource\\nCreate the Job manifest as in the following listing.\\napiVersion: batch/v1        \\nkind: Job                   \\nmetadata:\\n  name: batch-job\\nspec:                                \\n  template: \\n    metadata:\\n      labels:                        \\n        app: batch-job               \\n    spec:\\n      restartPolicy: OnFailure         \\n      containers:\\n      - name: main\\n        image: luksa/batch-job\\nJobs are part of the batch API group and v1 API version. The YAML defines a\\nresource of type Job that will run the luksa/batch-job image, which invokes a pro-\\ncess that runs for exactly 120 seconds and then exits. \\n In a pod’s specification, you can specify what Kubernetes should do when the\\nprocesses running in the container finish. This is done through the restartPolicy\\nListing 4.11\\nA YAML definition of a Job: exporter.yaml\\nNode 1\\nPod A (unmanaged)\\nPod B (managed by a ReplicaSet)\\nPod C (managed by a Job)\\nNode 2\\nNode 1 fails\\nJob C2 ﬁnishes\\nTime\\nPod B2 (managed by a ReplicaSet)\\nPod C2 (managed by a Job)\\nPod A isn’t rescheduled,\\nbecause there is nothing\\nmanaging it.\\nFigure 4.10\\nPods managed by Jobs are rescheduled until they finish successfully.\\nJobs are in the batch \\nAPI group, version v1.\\nYou’re not specifying a pod \\nselector (it will be created \\nbased on the labels in the \\npod template).\\nJobs can’t use the \\ndefault restart policy, \\nwhich is Always.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [   Col0                           Col1  Col2\n",
       "   0  None  Pod B2 (managed by a ReplicaS   et)\n",
       "   1  None      Pod C2 (managed by a Job)      \n",
       "   2  None                                 None\n",
       "   3  None                                 None],\n",
       "  'entities': [{'entity': 'Job',\n",
       "    'description': 'A resource that runs a single completable task',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A group of one or more containers running on the same node',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'An instance of an application or process running in a pod',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'RestartPolicy',\n",
       "    'description': 'The policy for restarting containers when they finish execution',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'API Group',\n",
       "    'description': 'A group of related APIs, such as batch or apps',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'API Version',\n",
       "    'description': 'The version of an API, such as v1 for the batch API group',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A resource that ensures a specified number of replicas (pods) are running at any given time',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Job Manifest',\n",
       "    'description': 'A YAML file defining the properties and configuration of a Job resource',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod Selector',\n",
       "    'description': 'A label or selector used to identify pods for management by a ReplicaSet or Job',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Container Image',\n",
       "    'description': 'An instance of an application or process stored in a container registry, such as luksa/batch-job',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Job\", \"description\": \"runs a single completable task\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Job Manifest\", \"description\": \"defines a resource of type Job\", \"destination_entity\": \"Kubernetes\"},\\n  {\"source_entity\": \"API Version\", \"description\": \"defines the version of API used by Job\", \"destination_entity\": \"Job\"},\\n  {\"source_entity\": \"Container Image\", \"description\": \"invokes a process that runs for exactly 120 seconds\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"RestartPolicy\", \"description\": \"specifies what Kubernetes should do when processes finish\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Pod Selector\", \"description\": \"is not specified, labels in the pod template will be used\", \"destination_entity\": \"Job\"},\\n  {\"source_entity\": \"ReplicaSet\", \"description\": \"manages a Pod until it finishes successfully\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"API Group\", \"description\": \"defines the group of API used by Job\", \"destination_entity\": \"Kubernetes\"},\\n  {\"source_entity\": \"Job Manifest\", \"description\": \"creates a pod based on labels in the pod template\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Container\", \"description\": \"runs a process that exits after 120 seconds\", \"destination_entity\": \"container image\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"is rescheduled until it finishes successfully\", \"destination_entity\": \"Job\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"manages pods based on labels in the pod template\", \"destination_entity\": \"Pod\"}\\n]'},\n",
       " {'page': 146,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '114\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\npod spec property, which defaults to Always. Job pods can’t use the default policy,\\nbecause they’re not meant to run indefinitely. Therefore, you need to explicitly set\\nthe restart policy to either OnFailure or Never. This setting is what prevents the con-\\ntainer from being restarted when it finishes (not the fact that the pod is being man-\\naged by a Job resource).\\n4.5.3\\nSeeing a Job run a pod\\nAfter you create this Job with the kubectl create command, you should see it start up\\na pod immediately:\\n$ kubectl get jobs\\nNAME        DESIRED   SUCCESSFUL   AGE\\nbatch-job   1         0            2s\\n$ kubectl get po\\nNAME              READY     STATUS    RESTARTS   AGE\\nbatch-job-28qf4   1/1       Running   0          4s\\nAfter the two minutes have passed, the pod will no longer show up in the pod list and\\nthe Job will be marked as completed. By default, completed pods aren’t shown when\\nyou list pods, unless you use the --show-all (or -a) switch:\\n$ kubectl get po -a\\nNAME              READY     STATUS      RESTARTS   AGE\\nbatch-job-28qf4   0/1       Completed   0          2m\\nThe reason the pod isn’t deleted when it completes is to allow you to examine its logs;\\nfor example:\\n$ kubectl logs batch-job-28qf4\\nFri Apr 29 09:58:22 UTC 2016 Batch job starting\\nFri Apr 29 10:00:22 UTC 2016 Finished succesfully\\nThe pod will be deleted when you delete it or the Job that created it. Before you do\\nthat, let’s look at the Job resource again:\\n$ kubectl get job\\nNAME        DESIRED   SUCCESSFUL   AGE\\nbatch-job   1         1            9m\\nThe Job is shown as having completed successfully. But why is that piece of informa-\\ntion shown as a number instead of as yes or true? And what does the DESIRED column\\nindicate? \\n4.5.4\\nRunning multiple pod instances in a Job\\nJobs may be configured to create more than one pod instance and run them in paral-\\nlel or sequentially. This is done by setting the completions and the parallelism prop-\\nerties in the Job spec.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'pod',\n",
       "    'description': 'a managed container that runs a process',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Job',\n",
       "    'description': \"a Kubernetes resource that manages a pod's execution\",\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the Kubernetes command-line tool',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'get jobs',\n",
       "    'description': 'a kubectl command to list job resources',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'get po',\n",
       "    'description': 'a kubectl command to list pod resources',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pod spec property',\n",
       "    'description': 'a configuration property of a pod',\n",
       "    'category': 'property'},\n",
       "   {'entity': 'restart policy',\n",
       "    'description': 'a setting that determines when a container is restarted',\n",
       "    'category': 'setting'},\n",
       "   {'entity': 'OnFailure',\n",
       "    'description': 'a restart policy that restarts a container on failure',\n",
       "    'category': 'policy'},\n",
       "   {'entity': 'Never',\n",
       "    'description': 'a restart policy that never restarts a container',\n",
       "    'category': 'policy'},\n",
       "   {'entity': 'batch-job',\n",
       "    'description': 'an example job resource',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'completions',\n",
       "    'description': 'a Job spec property that determines the number of pod instances to run',\n",
       "    'category': 'property'},\n",
       "   {'entity': 'parallelism',\n",
       "    'description': 'a Job spec property that determines how many pod instances to run in parallel',\n",
       "    'category': 'property'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Job\", \"description\": \"create pod instance\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Job\", \"description\": \"set restart policy to OnFailure or Never\", \"destination_entity\": \"restart policy\"},\\n  {\"source_entity\": \"Job\", \"description\": \"run pod in parallel or sequentially\", \"destination_entity\": \"completions\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"create Job resource\", \"destination_entity\": \"Job\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"list pods\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"get Job information\", \"destination_entity\": \"Job\"},\\n  {\"source_entity\": \"pod spec property\", \"description\": \"set restart policy to Always by default\", \"destination_entity\": \"restart policy\"},\\n  {\"source_entity\": \"OnFailure\", \"description\": \"set restart policy to OnFailure\", \"destination_entity\": \"restart policy\"},\\n  {\"source_entity\": \"Never\", \"description\": \"set restart policy to Never\", \"destination_entity\": \"restart policy\"},\\n  {\"source_entity\": \"batch-job\", \"description\": \"start pod instance\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"get po\", \"description\": \"list pod information\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"get jobs\", \"description\": \"list Job information\", \"destination_entity\": \"Job\"},\\n  {\"source_entity\": \"completions\", \"description\": \"run multiple pod instances in a Job\", \"destination_entity\": \"Job\"}\\n]\\n```\\n\\nNote that I\\'ve followed the rules you specified to extract the relations, and included all possible relationships between the entities listed. Let me know if you have any further questions!'},\n",
       " {'page': 147,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '115\\nRunning pods that perform a single completable task\\nRUNNING JOB PODS SEQUENTIALLY\\nIf you need a Job to run more than once, you set completions to how many times you\\nwant the Job’s pod to run. The following listing shows an example.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: multi-completion-batch-job\\nspec:\\n  completions: 5                  \\n  template:\\n    <template is the same as in listing 4.11>\\nThis Job will run five pods one after the other. It initially creates one pod, and when\\nthe pod’s container finishes, it creates the second pod, and so on, until five pods com-\\nplete successfully. If one of the pods fails, the Job creates a new pod, so the Job may\\ncreate more than five pods overall.\\nRUNNING JOB PODS IN PARALLEL\\nInstead of running single Job pods one after the other, you can also make the Job run\\nmultiple pods in parallel. You specify how many pods are allowed to run in parallel\\nwith the parallelism  Job spec property, as shown in the following listing.\\napiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: multi-completion-batch-job\\nspec:\\n  completions: 5                    \\n  parallelism: 2                    \\n  template:\\n    <same as in listing 4.11>\\nBy setting parallelism to 2, the Job creates two pods and runs them in parallel:\\n$ kubectl get po\\nNAME                               READY   STATUS     RESTARTS   AGE\\nmulti-completion-batch-job-lmmnk   1/1     Running    0          21s\\nmulti-completion-batch-job-qx4nq   1/1     Running    0          21s\\nAs soon as one of them finishes, the Job will run the next pod, until five pods finish\\nsuccessfully.\\nListing 4.12\\nA Job requiring multiple completions: multi-completion-batch-job.yaml\\nListing 4.13\\nRunning Job pods in parallel: multi-completion-parallel-batch-job.yaml\\nSetting completions to \\n5 makes this Job run \\nfive pods sequentially.\\nThis job must ensure \\nfive pods complete \\nsuccessfully.\\nUp to two pods \\ncan run in parallel.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Job',\n",
       "    'description': 'A Kubernetes object that creates one or more pods and ensures a specified number of them successfully terminate.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A Kubernetes object that represents a single container running on a node.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Completions',\n",
       "    'description': \"The number of times a Job's pod is allowed to run.\",\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Template',\n",
       "    'description': 'A specification for the pods created by a Job, including container and command information.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'API Version',\n",
       "    'description': 'The version of the Kubernetes API being used to create or update an object.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Kind',\n",
       "    'description': 'The type of Kubernetes object being created or updated (e.g., Job, Pod, Service).',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Metadata',\n",
       "    'description': 'Information about the Kubernetes object being created or updated, such as its name and labels.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Spec',\n",
       "    'description': 'The specification for a Job, including information like completions, parallelism, and template.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool used to interact with a Kubernetes cluster.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'get',\n",
       "    'description': 'A command used to retrieve information about a Kubernetes object (e.g., pods, services).',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Pod Status',\n",
       "    'description': 'The status of a Pod, including whether it is Running or NotReady.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'parallelism',\n",
       "    'description': 'A Job specification property that determines how many pods are allowed to run in parallel.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Job\", \"description\": \"creates one pod initially\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Job\", \"description\": \"runs a new pod when previous pod\\'s container finishes\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Job\", \"description\": \"may create more than five pods overall if some fail\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Job\", \"description\": \"specifies how many pods are allowed to run in parallel\", \"destination_entity\": \"parallelism\"},\\n  {\"source_entity\": \"Job\", \"description\": \"runs multiple pods in parallel\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Job\", \"description\": \"creates two pods and runs them in parallel\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"gets the status of running pods\", \"destination_entity\": \"Pod Status\"},\\n  {\"source_entity\": \"Completions\", \"description\": \"sets the number of times a Job\\'s pod should run\", \"destination_entity\": \"Job\"},\\n  {\"source_entity\": \"Parallelism\", \"description\": \"specifies how many pods are allowed to run in parallel\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Template\", \"description\": \"defines the configuration for each pod\", \"destination_entity\": \"Pod\"}\\n]\\n```\\n\\nNote that I extracted relations between entities, not sentences or phrases. Each relation has a brief description of what action was being carried out by the source entity on the destination entity.'},\n",
       " {'page': 148,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '116\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\nSCALING A JOB\\nYou can even change a Job’s parallelism property while the Job is running. This is\\nsimilar to scaling a ReplicaSet or ReplicationController, and can be done with the\\nkubectl scale command:\\n$ kubectl scale job multi-completion-batch-job --replicas 3\\njob \"multi-completion-batch-job\" scaled\\nBecause you’ve increased parallelism from 2 to 3, another pod is immediately spun\\nup, so three pods are now running.\\n4.5.5\\nLimiting the time allowed for a Job pod to complete\\nWe need to discuss one final thing about Jobs. How long should the Job wait for a pod\\nto finish? What if the pod gets stuck and can’t finish at all (or it can’t finish fast\\nenough)?\\n A pod’s time can be limited by setting the activeDeadlineSeconds property in the\\npod spec. If the pod runs longer than that, the system will try to terminate it and will\\nmark the Job as failed. \\nNOTE\\nYou can configure how many times a Job can be retried before it is\\nmarked as failed by specifying the spec.backoffLimit field in the Job mani-\\nfest. If you don\\'t explicitly specify it, it defaults to 6.\\n4.6\\nScheduling Jobs to run periodically or once \\nin the future\\nJob resources run their pods immediately when you create the Job resource. But many\\nbatch jobs need to be run at a specific time in the future or repeatedly in the specified\\ninterval. In Linux- and UNIX-like operating systems, these jobs are better known as\\ncron jobs. Kubernetes supports them, too.\\n A cron job in Kubernetes is configured by creating a CronJob resource. The\\nschedule for running the job is specified in the well-known cron format, so if you’re\\nfamiliar with regular cron jobs, you’ll understand Kubernetes’ CronJobs in a matter\\nof seconds.\\n At the configured time, Kubernetes will create a Job resource according to the Job\\ntemplate configured in the CronJob object. When the Job resource is created, one or\\nmore pod replicas will be created and started according to the Job’s pod template, as\\nyou learned in the previous section. There’s nothing more to it.\\n Let’s look at how to create CronJobs. \\n4.6.1\\nCreating a CronJob\\nImagine you need to run the batch job from your previous example every 15 minutes.\\nTo do that, create a CronJob resource with the following specification.\\n \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command-line tool for managing Kubernetes resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Job',\n",
       "    'description': 'resource that manages a batch job, responsible for creating and scaling pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'resource that ensures a specified number of replicas of a pod are running at any given time',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'legacy resource that provides similar functionality to ReplicaSet, but with some differences in behavior',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'lightweight and portable container for running an application or service',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'activeDeadlineSeconds',\n",
       "    'description': 'property that specifies the time limit for a pod to complete its work',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'spec.backoffLimit',\n",
       "    'description': 'field in the Job manifest that configures how many times a Job can be retried before it is marked as failed',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'CronJob',\n",
       "    'description': 'resource that schedules a batch job to run at a specific time or interval',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'cron format',\n",
       "    'description': 'standardized format for specifying schedules in Linux- and UNIX-like operating systems',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CronJob object',\n",
       "    'description': 'resource that defines the schedule and other details of a CronJob',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Job template',\n",
       "    'description': 'template that specifies the pod configuration for a Job resource',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[{\"source_entity\": \"Job\", \"description\": \"can be scaled up or down by increasing or decreasing its parallelism property using the kubectl scale command.\", \"destination_entity\": \"kubectl\"}, \\n{\"source_entity\": \"Job\", \"description\": \"has a property activeDeadlineSeconds which can be set to limit the time allowed for a pod to complete.\", \"destination_entity\": \"activeDeadlineSeconds\"}, \\n{\"source_entity\": \"Job\", \"description\": \"can be retried up to 6 times before being marked as failed if specified in its spec.backoffLimit field.\", \"destination_entity\": \"spec.backoffLimit\"}, \\n{\"source_entity\": \"Job\", \"description\": \"has a Job template which is used when creating a CronJob resource.\", \"destination_entity\": \"Job template\"}, \\n{\"source_entity\": \"ReplicationController\", \"description\": \"similar to scaling, can be done with the kubectl scale command.\", \"destination_entity\": \"kubectl\"}, \\n{\"source_entity\": \"CronJob\", \"description\": \"is configured by creating a CronJob resource and specifying its schedule in cron format.\", \"destination_entity\": \"cron format\"}, \\n{\"source_entity\": \"CronJob\", \"description\": \"has a Job template which is used when creating a CronJob resource.\", \"destination_entity\": \"Job template\"}, \\n{\"source_entity\": \"kubectl\", \"description\": \"can be used to scale Jobs and ReplicationControllers.\", \"destination_entity\": \"Job\"}, \\n{\"source_entity\": \"kubectl\", \"description\": \"can be used to scale ReplicationControllers.\", \"destination_entity\": \"ReplicationController\"}, \\n{\"source_entity\": \"CronJob object\", \"description\": \"is used when creating a CronJob resource and specifying its schedule in cron format.\", \"destination_entity\": \"cron format\"}]'},\n",
       " {'page': 149,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '117\\nScheduling Jobs to run periodically or once in the future\\napiVersion: batch/v1beta1               \\nkind: CronJob\\nmetadata:\\n  name: batch-job-every-fifteen-minutes\\nspec:\\n  schedule: \"0,15,30,45 * * * *\"           \\n  jobTemplate:\\n    spec:\\n      template:                            \\n        metadata:                          \\n          labels:                          \\n            app: periodic-batch-job        \\n        spec:                              \\n          restartPolicy: OnFailure         \\n          containers:                      \\n          - name: main                     \\n            image: luksa/batch-job         \\nAs you can see, it’s not too complicated. You’ve specified a schedule and a template\\nfrom which the Job objects will be created. \\nCONFIGURING THE SCHEDULE\\nIf you’re unfamiliar with the cron schedule format, you’ll find great tutorials and\\nexplanations online, but as a quick introduction, from left to right, the schedule con-\\ntains the following five entries:\\n\\uf0a1Minute\\n\\uf0a1Hour\\n\\uf0a1Day of month\\n\\uf0a1Month\\n\\uf0a1Day of week.\\nIn the example, you want to run the job every 15 minutes, so the schedule needs to be\\n\"0,15,30,45 * * * *\", which means at the 0, 15, 30 and 45 minutes mark of every hour\\n(first asterisk), of every day of the month (second asterisk), of every month (third\\nasterisk) and on every day of the week (fourth asterisk). \\n If, instead, you wanted it to run every 30 minutes, but only on the first day of the\\nmonth, you’d set the schedule to \"0,30 * 1 * *\", and if you want it to run at 3AM every\\nSunday, you’d set it to \"0 3 * * 0\" (the last zero stands for Sunday).\\nCONFIGURING THE JOB TEMPLATE\\nA CronJob creates Job resources from the jobTemplate property configured in the\\nCronJob spec, so refer to section 4.5 for more information on how to configure it.\\n4.6.2\\nUnderstanding how scheduled jobs are run\\nJob resources will be created from the CronJob resource at approximately the sched-\\nuled time. The Job then creates the pods. \\nListing 4.14\\nYAML for a CronJob resource: cronjob.yaml\\nAPI group is batch, \\nversion is v1beta1\\nThis job should run at the \\n0, 15, 30 and 45 minutes of \\nevery hour, every day.\\nThe template for the \\nJob resources that \\nwill be created by \\nthis CronJob\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'CronJob',\n",
       "    'description': 'A resource used to schedule jobs to run periodically or once in the future.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'schedule',\n",
       "    'description': 'The cron schedule format for specifying when a job should be run.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'jobTemplate',\n",
       "    'description': 'A property of the CronJob spec that defines the template from which Job objects will be created.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'A field in the CronJob resource that contains metadata about the job.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'A field in the CronJob resource that contains the specification for the job.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'restartPolicy',\n",
       "    'description': 'The policy for restarting containers when they fail or exit.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'A field in the Job resource that contains a list of container specifications.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'The Docker image used to create containers for the job.',\n",
       "    'category': 'Software/Container'},\n",
       "   {'entity': 'Kubernetes API group',\n",
       "    'description': 'The group name for Kubernetes APIs, which includes batch and core groups.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'batch/v1beta1',\n",
       "    'description': 'The version of the Kubernetes batch API used in this example.',\n",
       "    'category': 'Software/Application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"spec\",\\n    \"description\": \"Defines the specification for a CronJob resource.\",\\n    \"destination_entity\": \"CronJob\"\\n  },\\n  {\\n    \"source_entity\": \"jobTemplate\",\\n    \"description\": \"Defines the template for Job resources created by the CronJob.\",\\n    \"destination_entity\": \"Job\"\\n  },\\n  {\\n    \"source_entity\": \"batch/v1beta1\",\\n    \"description\": \"Specifies the API group and version for a CronJob resource.\",\\n    \"destination_entity\": \"CronJob\"\\n  },\\n  {\\n    \"source_entity\": \"containers\",\\n    \"description\": \"Defines the containers for a Job resource created by the CronJob.\",\\n    \"destination_entity\": \"Job\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes API group\",\\n    \"description\": \"Specifies the API group for a CronJob resource.\",\\n    \"destination_entity\": \"CronJob\"\\n  },\\n  {\\n    \"source_entity\": \"metadata\",\\n    \"description\": \"Provides metadata for a CronJob resource.\",\\n    \"destination_entity\": \"CronJob\"\\n  },\\n  {\\n    \"source_entity\": \"image\",\\n    \"description\": \"Specifies the image for a container in a Job resource created by the CronJob.\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"restartPolicy\",\\n    \"description\": \"Defines the restart policy for a Job resource created by the CronJob.\",\\n    \"destination_entity\": \"Job\"\\n  },\\n  {\\n    \"source_entity\": \"CronJob\",\\n    \"description\": \"Creates Job resources at approximately scheduled times.\",\\n    \"destination_entity\": \"Job\"\\n  },\\n  {\\n    \"source_entity\": \"schedule\",\\n    \"description\": \"Specifies the schedule for a CronJob to create Job resources.\",\\n    \"destination_entity\": \"CronJob\"\\n  }\\n]\\n```'},\n",
       " {'page': 150,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '118\\nCHAPTER 4\\nReplication and other controllers: deploying managed pods\\n It may happen that the Job or pod is created and run relatively late. You may have\\na hard requirement for the job to not be started too far over the scheduled time. In\\nthat case, you can specify a deadline by specifying the startingDeadlineSeconds field\\nin the CronJob specification as shown in the following listing.\\napiVersion: batch/v1beta1\\nkind: CronJob\\nspec:\\n  schedule: \"0,15,30,45 * * * *\"\\n  startingDeadlineSeconds: 15    \\n  ...\\nIn the example in listing 4.15, one of the times the job is supposed to run is 10:30:00.\\nIf it doesn’t start by 10:30:15 for whatever reason, the job will not run and will be\\nshown as Failed. \\n In normal circumstances, a CronJob always creates only a single Job for each exe-\\ncution configured in the schedule, but it may happen that two Jobs are created at the\\nsame time, or none at all. To combat the first problem, your jobs should be idempo-\\ntent (running them multiple times instead of once shouldn’t lead to unwanted\\nresults). For the second problem, make sure that the next job run performs any work\\nthat should have been done by the previous (missed) run.\\n4.7\\nSummary\\nYou’ve now learned how to keep pods running and have them rescheduled in the\\nevent of node failures. You should now know that\\n\\uf0a1You can specify a liveness probe to have Kubernetes restart your container as\\nsoon as it’s no longer healthy (where the app defines what’s considered\\nhealthy).\\n\\uf0a1Pods shouldn’t be created directly, because they will not be re-created if they’re\\ndeleted by mistake, if the node they’re running on fails, or if they’re evicted\\nfrom the node.\\n\\uf0a1ReplicationControllers always keep the desired number of pod replicas\\nrunning.\\n\\uf0a1Scaling pods horizontally is as easy as changing the desired replica count on a\\nReplicationController.\\n\\uf0a1Pods aren’t owned by the ReplicationControllers and can be moved between\\nthem if necessary.\\n\\uf0a1A ReplicationController creates new pods from a pod template. Changing the\\ntemplate has no effect on existing pods.\\nListing 4.15\\nSpecifying a startingDeadlineSeconds for a CronJob\\nAt the latest, the pod must \\nstart running at 15 seconds \\npast the scheduled time.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'CronJob',\n",
       "    'description': 'A Kubernetes object that runs a job on a schedule.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'startingDeadlineSeconds',\n",
       "    'description': 'The maximum number of seconds after the scheduled start time that a CronJob will run, defaulting to 0 (i.e., no deadline).',\n",
       "    'category': 'parameter'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes object that ensures a specified number of replicas of a pod are running at any given time.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'liveness probe',\n",
       "    'description': 'A mechanism to detect and restart containers that are not running as expected.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pod template',\n",
       "    'description': 'The blueprint for creating new pods, used by ReplicationControllers to create new replicas.',\n",
       "    'category': 'template'},\n",
       "   {'entity': 'node failures',\n",
       "    'description': 'An event where a node in the Kubernetes cluster becomes unavailable due to hardware or software issues.',\n",
       "    'category': 'event'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes object that ensures a specified number of replicas of a pod are running at any given time.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Job',\n",
       "    'description': 'A Kubernetes object that represents a one-off task or batch process.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'The smallest deployable unit in Kubernetes, representing a container and its associated resources.',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"CronJob\", \"description\": \"specifies a deadline for job to start running\", \"destination_entity\": \"startingDeadlineSeconds\"},\\n  {\"source_entity\": \"CronJob\", \"description\": \"creates a single Job for each execution configured in schedule\", \"destination_entity\": \"Job\"},\\n  {\"source_entity\": \"CronJob\", \"description\": \"may create two Jobs at the same time if not idempotent\", \"destination_entity\": \"Job\"},\\n  {\"source_entity\": \"CronJob\", \"description\": \"may miss a job run and the next run should perform any missed work\", \"destination_entity\": \"Job\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"keeps the desired number of pod replicas running\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"creates new pods from a pod template\", \"destination_entity\": \"pod template\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"does not own the pods and can be moved between controllers\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Job\", \"description\": \"should be created by CronJob instead of directly\", \"destination_entity\": \"CronJob\"},\\n  {\"source_entity\": \"liveness probe\", \"description\": \"restarts container as soon as it\\'s no longer healthy\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"scales pods horizontally by changing desired replica count\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"CronJob\", \"description\": \"specifies a schedule for job to run\", \"destination_entity\": \"schedule\"},\\n  {\"source_entity\": \"node failures\", \"description\": \"can cause pods to be recreated or evicted\", \"destination_entity\": \"pod\"}\\n]'},\n",
       " {'page': 151,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '119\\nSummary\\n\\uf0a1ReplicationControllers should be replaced with ReplicaSets and Deployments,\\nwhich provide the same functionality, but with additional powerful features.\\n\\uf0a1ReplicationControllers and ReplicaSets schedule pods to random cluster nodes,\\nwhereas DaemonSets make sure every node runs a single instance of a pod\\ndefined in the DaemonSet.\\n\\uf0a1Pods that perform a batch task should be created through a Kubernetes Job\\nresource, not directly or through a ReplicationController or similar object.\\n\\uf0a1Jobs that need to run sometime in the future can be created through CronJob\\nresources. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationControllers',\n",
       "    'description': 'should be replaced with ReplicaSets and Deployments',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSets',\n",
       "    'description': 'provide the same functionality as ReplicationControllers, but with additional powerful features',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployments',\n",
       "    'description': 'provide the same functionality as ReplicationControllers, but with additional powerful features',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'DaemonSets',\n",
       "    'description': 'make sure every node runs a single instance of a pod defined in the DaemonSet',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'perform a batch task should be created through a Kubernetes Job resource',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubernetes Job',\n",
       "    'description': 'resource for creating pods that perform a batch task',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CronJob resources',\n",
       "    'description': 'for creating Jobs that need to run sometime in the future',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ReplicationControllers\", \"description\": \"should be replaced by more powerful features\", \"destination_entity\": \"ReplicaSets and Deployments\"},\\n  {\"source_entity\": \"ReplicationControllers and ReplicaSets\", \"description\": \"schedule pods to random cluster nodes\", \"destination_entity\": \"cluster nodes\"},\\n  {\"source_entity\": \"DaemonSets\", \"description\": \"make sure every node runs a single instance of a pod\", \"destination_entity\": \"nodes\"},\\n  {\"source_entity\": \"ReplicationControllers\", \"description\": \"and similar objects are not suitable for creating pods that perform batch tasks\", \"destination_entity\": \"Kubernetes Job resource\"},\\n  {\"source_entity\": \"Jobs\", \"description\": \"can be created through CronJob resources to run sometime in the future\", \"destination_entity\": \"CronJob resources\"},\\n  {\"source_entity\": \"DaemonSets\", \"description\": \"are not used for scheduling pods\", \"destination_entity\": \"pods and ReplicaSets\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"perform batch tasks should be created through Kubernetes Job resource\", \"destination_entity\": \"Kubernetes Job resource\"}\\n]\\n```'},\n",
       " {'page': 152,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '120\\nServices: enabling\\nclients to discover\\nand talk to pods\\nYou’ve learned about pods and how to deploy them through ReplicaSets and similar\\nresources to ensure they keep running. Although certain pods can do their work\\nindependently of an external stimulus, many applications these days are meant to\\nrespond to external requests. For example, in the case of microservices, pods will\\nusually respond to HTTP requests coming either from other pods inside the cluster\\nor from clients outside the cluster. \\n Pods need a way of finding other pods if they want to consume the services they\\nprovide. Unlike in the non-Kubernetes world, where a sysadmin would configure\\nThis chapter covers\\n\\uf0a1Creating Service resources to expose a group of \\npods at a single address\\n\\uf0a1Discovering services in the cluster\\n\\uf0a1Exposing services to external clients\\n\\uf0a1Connecting to external services from inside the \\ncluster\\n\\uf0a1Controlling whether a pod is ready to be part of \\nthe service or not\\n\\uf0a1Troubleshooting services\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Services',\n",
       "    'description': 'enabling clients to discover and talk to pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'independent units of deployment',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ReplicaSets',\n",
       "    'description': 'resources to ensure pods keep running',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'HTTP requests',\n",
       "    'description': 'external requests to microservices',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'clients',\n",
       "    'description': 'outside applications requesting services from pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'sysadmin',\n",
       "    'description': 'system administrator configuring external connections',\n",
       "    'category': 'human'},\n",
       "   {'entity': 'Service resources',\n",
       "    'description': 'exposing a group of pods at a single address',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'cluster',\n",
       "    'description': 'group of pods and services',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'external clients',\n",
       "    'description': 'clients outside the cluster requesting services from pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'external services',\n",
       "    'description': 'services provided by external systems that need to be connected to from inside the cluster',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'pod readiness',\n",
       "    'description': 'control of whether a pod is ready to be part of the service or not',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Services\", \"description\": \"enabling clients to discover and talk to pods\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"clients\", \"description\": \"making HTTP requests\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"pods\", \"description\": \"responding to external requests\", \"destination_entity\": \"HTTP requests\"},\\n  {\"source_entity\": \"Services\", \"description\": \"exposing a group of pods at a single address\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"clients\", \"description\": \"discovering services in the cluster\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"Services\", \"description\": \"being consumed by clients\", \"destination_entity\": \"clients\"},\\n  {\"source_entity\": \"sysadmin\", \"description\": \"configuring external services for pods\", \"destination_entity\": \"external services\"},\\n  {\"source_entity\": \"pods\", \"description\": \"connecting to external services from inside the cluster\", \"destination_entity\": \"external services\"},\\n  {\"source_entity\": \"Services\", \"description\": \"being controlled by pod readiness\", \"destination_entity\": \"pod readiness\"},\\n  {\"source_entity\": \"Service resources\", \"description\": \"creating and managing Services\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"ReplicaSets\", \"description\": \"deploying and ensuring running of pods\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"clients\", \"description\": \"making requests to external services\", \"destination_entity\": \"external services\"},\\n  {\"source_entity\": \"pods\", \"description\": \"being exposed to external clients\", \"destination_entity\": \"external clients\"}\\n]\\n```'},\n",
       " {'page': 153,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '121\\nIntroducing services\\neach client app by specifying the exact IP address or hostname of the server providing\\nthe service in the client’s configuration files, doing the same in Kubernetes wouldn’t\\nwork, because\\n\\uf0a1Pods are ephemeral—They may come and go at any time, whether it’s because a\\npod is removed from a node to make room for other pods, because someone\\nscaled down the number of pods, or because a cluster node has failed.\\n\\uf0a1Kubernetes assigns an IP address to a pod after the pod has been scheduled to a node\\nand before it’s started—Clients thus can’t know the IP address of the server pod\\nup front.\\n\\uf0a1Horizontal scaling means multiple pods may provide the same service—Each of those\\npods has its own IP address. Clients shouldn’t care how many pods are backing\\nthe service and what their IPs are. They shouldn’t have to keep a list of all the\\nindividual IPs of pods. Instead, all those pods should be accessible through a\\nsingle IP address.\\nTo solve these problems, Kubernetes also provides another resource type—Services—\\nthat we’ll discuss in this chapter.\\n5.1\\nIntroducing services\\nA Kubernetes Service is a resource you create to make a single, constant point of\\nentry to a group of pods providing the same service. Each service has an IP address\\nand port that never change while the service exists. Clients can open connections to\\nthat IP and port, and those connections are then routed to one of the pods backing\\nthat service. This way, clients of a service don’t need to know the location of individ-\\nual pods providing the service, allowing those pods to be moved around the cluster\\nat any time. \\nEXPLAINING SERVICES WITH AN EXAMPLE\\nLet’s revisit the example where you have a frontend web server and a backend data-\\nbase server. There may be multiple pods that all act as the frontend, but there may\\nonly be a single backend database pod. You need to solve two problems to make the\\nsystem function:\\n\\uf0a1External clients need to connect to the frontend pods without caring if there’s\\nonly a single web server or hundreds.\\n\\uf0a1The frontend pods need to connect to the backend database. Because the data-\\nbase runs inside a pod, it may be moved around the cluster over time, causing\\nits IP address to change. You don’t want to reconfigure the frontend pods every\\ntime the backend database is moved.\\nBy creating a service for the frontend pods and configuring it to be accessible from\\noutside the cluster, you expose a single, constant IP address through which external\\nclients can connect to the pods. Similarly, by also creating a service for the backend\\npod, you create a stable address for the backend pod. The service address doesn’t\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pods',\n",
       "    'description': 'Ephemeral resources that may come and go at any time',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'IP Address',\n",
       "    'description': 'Assigned to a pod after scheduling, used for client-server communication',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Kubernetes Service',\n",
       "    'description': 'A resource providing a single, constant point of entry to a group of pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pods (multiple)',\n",
       "    'description': 'Multiple pods providing the same service, each with its own IP address',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Frontend Web Server',\n",
       "    'description': 'A server providing a web interface for clients',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Backend Database Server',\n",
       "    'description': 'A server providing data storage and retrieval services',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Service (frontend)',\n",
       "    'description': 'A Kubernetes resource exposing a single, constant IP address for frontend pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service (backend)',\n",
       "    'description': 'A Kubernetes resource providing a stable address for the backend pod',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\":\"Client App\",\"description\":\"connect to\",\"destination_entity\":\"Server Pod\"},{\"source_entity\":\"Kubernetes\",\"description\":\"assign an IP address to\",\"destination_entity\":\"Pod\"},{\"source_entity\":\"Pod\",\"description\":\"come and go at any time\",\"destination_entity\":\"Node\"},{\"source_entity\":\"Cluster Node\",\"description\":\"failed\",\"destination_entity\":\"Pod\"},{\"source_entity\":\"Client App\",\"description\":\"keep a list of all individual IPs of pods\",\"destination_entity\":null},{\"source_entity\":\"Service (frontend)\",\"description\":\"make a single, constant point of entry to a group of pods\",\"destination_entity\":\"Frontend Web Server Pods\"},{\"source_entity\":\"Service (frontend)\",\"description\":\"expose a single, constant IP address through which external clients can connect\",\"destination_entity\":null},{\"source_entity\":\"Client App\",\"description\":\"connect to\",\"destination_entity\":\"Service (frontend)\"},{\"source_entity\":\"Frontend Web Server Pod\",\"description\":\"connect to\",\"destination_entity\":\"Backend Database Server\"},{\"source_entity\":\"Service (backend)\",\"description\":\"create a stable address for the backend pod\",\"destination_entity\":\"Backend Database Server\"},{\"source_entity\":\"Service (backend)\",\"description\":\"expose a single, constant IP address through which external clients can connect\",\"destination_entity\":null},{\"source_entity\":\"Client App\",\"description\":\"connect to\",\"destination_entity\":\"Service (backend)\"},{\"source_entity\":\"Kubernetes Service\",\"description\":\"provide another resource type that makes it possible for clients not to care how many pods are backing the service and what their IPs are\",\"destination_entity\":null},{\"source_entity\":\"Pods (multiple)\",\"description\":\"provide the same service\",\"destination_entity\":null},{\"source_entity\":\"Service\",\"description\":\"make a single, constant point of entry to a group of pods providing the same service\",\"destination_entity\":\"Pods (multiple)\"},{ \"source_entity\":\"Client App\",\"description\":\"not need to know the location of individual pods providing the service\",\"destination_entity\":null}]\\n\\nNote that I\\'ve excluded relations where the source or destination entity is not present in the list, such as when a sentence mentions an external client but doesn\\'t specify which one.'},\n",
       " {'page': 154,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '122\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\nchange even if the pod’s IP address changes. Additionally, by creating the service, you\\nalso enable the frontend pods to easily find the backend service by its name through\\neither environment variables or DNS. All the components of your system (the two ser-\\nvices, the two sets of pods backing those services, and the interdependencies between\\nthem) are shown in figure 5.1.\\nYou now understand the basic idea behind services. Now, let’s dig deeper by first see-\\ning how they can be created.\\n5.1.1\\nCreating services\\nAs you’ve seen, a service can be backed by more than one pod. Connections to the ser-\\nvice are load-balanced across all the backing pods. But how exactly do you define\\nwhich pods are part of the service and which aren’t? \\n You probably remember label selectors and how they’re used in Replication-\\nControllers and other pod controllers to specify which pods belong to the same set.\\nThe same mechanism is used by services in the same way, as you can see in figure 5.2.\\n In the previous chapter, you created a ReplicationController which then ran three\\ninstances of the pod containing the Node.js app. Create the ReplicationController\\nagain and verify three pod instances are up and running. After that, you’ll create a\\nService for those three pods. \\nFrontend pod 1\\nIP: 2.1.1.1\\nExternal client\\nFrontend pod 2\\nIP: 2.1.1.2\\nFrontend pod 3\\nIP: 2.1.1.3\\nBackend pod\\nIP: 2.1.1.4\\nFrontend service\\nIP: 1.1.1.1\\nBackend service\\nIP: 1.1.1.2\\nFrontend components\\nBackend components\\nFigure 5.1\\nBoth internal and external clients usually connect to pods through services.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [                      them) are shown in figure 5.1.  \\\n",
       "   0  Frontend components\\nFrontend service\\nExterna...   \n",
       "   1                                               None   \n",
       "   2                                               None   \n",
       "   \n",
       "                                          n figure 5.1.  Col2  Col3  Col4  \n",
       "   0  Frontend components\\nFrontend service\\nIP: 1.1...  None  None  None  \n",
       "   1                                               None                    \n",
       "   2  Backend components\\nBackend service\\nIP: 1.1.1...  None  None  None  ],\n",
       "  'entities': [{'entity': 'Service',\n",
       "    'description': \"a service that enables clients to discover and talk to pods, even if the pod's IP address changes\",\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'an instance of a container running an application',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'a controller that manages multiple instances of a pod',\n",
       "    'category': 'Software/Framework'},\n",
       "   {'entity': 'Label selector',\n",
       "    'description': 'a mechanism used to select which pods belong to the same set',\n",
       "    'category': 'Software/Framework'},\n",
       "   {'entity': 'Frontend service',\n",
       "    'description': 'a service that provides access to the frontend components',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Backend service',\n",
       "    'description': 'a service that provides access to the backend components',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'DNS',\n",
       "    'description': 'a domain name system used for resolving hostnames to IP addresses',\n",
       "    'category': 'Network/Protocol'},\n",
       "   {'entity': 'Environment variables',\n",
       "    'description': 'variables that can be set and accessed by a running process',\n",
       "    'category': 'Software/Process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Service\", \"description\": \"can be backed by multiple pods\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Service\", \"description\": \"connections are load-balanced across all backing pods\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Label selector\", \"description\": \"used to specify which pods belong to the same set\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"Service\", \"description\": \"uses label selectors to determine which pods are part of the service\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Frontend pod 1\", \"description\": \"can connect to Backend service through Frontend service\", \"destination_entity\": \"Backend service\"},\\n  {\"source_entity\": \"External client\", \"description\": \"can connect to Backend service through Frontend service\", \"destination_entity\": \"Backend service\"},\\n  {\"source_entity\": \"Frontend pod 2\", \"description\": \"can connect to Backend service through Frontend service\", \"destination_entity\": \"Backend service\"},\\n  {\"source_entity\": \"Frontend pod 3\", \"description\": \"can connect to Backend service through Frontend service\", \"destination_entity\": \"Backend service\"},\\n  {\"source_entity\": \"Frontend service\", \"description\": \"provides a stable IP address for clients\", \"destination_entity\": \"Client\"},\\n  {\"source_entity\": \"Backend service\", \"description\": \"provides a stable IP address for clients\", \"destination_entity\": \"Client\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"creates multiple instances of the same pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Service\", \"description\": \"exposes the ReplicationController to clients\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"Label selector\", \"description\": \"used by Service to determine which pods are part of the service\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"DNS\", \"description\": \"used by clients to connect to services\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"Environment variables\", \"description\": \"used by clients to connect to services\", \"destination_entity\": \"Service\"}\\n]\\n```\\nNote that I extracted the following relations:\\n\\n* Service and Pod (multiple connections, load-balancing)\\n* Label selector and ReplicationController\\n* Frontend service and Backend service (connection through frontend service)\\n* Client and Backend service (connection through frontend service)\\n* ReplicationController and Pod (creation of multiple instances)\\n* Service and ReplicationController (exposure to clients)\\n* Label selector and Pod (used by Service to determine which pods are part of the service)\\n* DNS and Service (used by clients to connect to services)\\n* Environment variables and Service (used by clients to connect to services)'},\n",
       " {'page': 155,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '123\\nIntroducing services\\nCREATING A SERVICE THROUGH KUBECTL EXPOSE\\nThe easiest way to create a service is through kubectl expose, which you’ve already\\nused in chapter 2 to expose the ReplicationController you created earlier. The\\nexpose command created a Service resource with the same pod selector as the one\\nused by the ReplicationController, thereby exposing all its pods through a single IP\\naddress and port. \\n Now, instead of using the expose command, you’ll create a service manually by\\nposting a YAML to the Kubernetes API server. \\nCREATING A SERVICE THROUGH A YAML DESCRIPTOR\\nCreate a file called kubia-svc.yaml with the following listing’s contents.\\napiVersion: v1\\nkind: Service             \\nmetadata:\\n  name: kubia              \\nspec:\\n  ports:\\n  - port: 80              \\n    targetPort: 8080       \\n  selector:                 \\n    app: kubia              \\nYou’re defining a service called kubia, which will accept connections on port 80 and\\nroute each connection to port 8080 of one of the pods matching the app=kubia\\nlabel selector. \\n Go ahead and create the service by posting the file using kubectl create.\\nListing 5.1\\nA definition of a service: kubia-svc.yaml\\napp: kubia\\nPod: kubia-q3vkg\\nPod: kubia-k0xz6\\nPod: kubia-53thy\\nClient\\nService: kubia\\nSelector: app=kubia\\napp: kubia\\napp: kubia\\nFigure 5.2\\nLabel selectors \\ndetermine which pods belong \\nto the Service.\\nThe port this service \\nwill be available on\\nThe container port the \\nservice will forward to\\nAll pods with the app=kubia \\nlabel will be part of this service.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'a command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'expose',\n",
       "    'description': 'a command used to create a service through kubectl',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'a resource in Kubernetes that ensures a specified number of replicas are running at any given time',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'a resource in Kubernetes that exposes a set of pods as a network interface',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'YAML',\n",
       "    'description': 'a human-readable serialization format for data, used to define resources like services',\n",
       "    'category': 'format'},\n",
       "   {'entity': 'port',\n",
       "    'description': 'an endpoint that allows communication between applications or processes',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'targetPort',\n",
       "    'description': 'the port on which a service will forward incoming requests to a pod',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'selector',\n",
       "    'description': 'a label selector used to determine which pods belong to a service',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'label',\n",
       "    'description': 'a key-value pair that identifies a pod or other resource in Kubernetes',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'app=kubia',\n",
       "    'description': 'a specific label value used to select pods for a service',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'a field in a YAML descriptor that specifies the API version of the resource being defined',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'a field in a YAML descriptor that specifies the type of resource being defined (e.g. Service, Pod)',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'a section in a YAML descriptor that contains metadata about the resource being defined',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'a section in a YAML descriptor that defines the configuration of the resource being created',\n",
       "    'category': 'metadata'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to expose a service through kubectl expose command\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"kubectl expose\", \"description\": \"created a Service resource with the same pod selector as the ReplicationController\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"metadata\", \"description\": \"contains information about the service, such as its name\", \"destination_entity\": \"service\"},\\n  {\"source_entity\": \"spec\", \"description\": \"defines the specification of the service, including ports and selectors\", \"destination_entity\": \"service\"},\\n  {\"source_entity\": \"apiVersion\", \"description\": \" specifies the API version for the Service resource\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"kind\", \"description\": \"specifies that this is a Service resource\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"selector\", \"description\": \"determines which pods belong to the service based on labels\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"app=kubia\", \"description\": \"label selector used to determine which pods belong to the service\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"kubectl create\", \"description\": \"used to create a new Service resource from a YAML descriptor\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"YAML\", \"description\": \"describes the format of the data being posted to the Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"port\", \"description\": \" specifies the port that the service will be available on\", \"destination_entity\": \"service\"},\\n  {\"source_entity\": \"targetPort\", \"description\": \"specifies the container port that the service will forward to\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"label\", \"description\": \"determines which pods belong to the service based on labels\", \"destination_entity\": \"pods\"}\\n]\\n```\\n\\nNote: I\\'ve used the exact wording from the document page to fill in the description field for each relation.'},\n",
       " {'page': 156,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '124\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\nEXAMINING YOUR NEW SERVICE\\nAfter posting the YAML, you can list all Service resources in your namespace and see\\nthat an internal cluster IP has been assigned to your service:\\n$ kubectl get svc\\nNAME         CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE\\nkubernetes   10.111.240.1     <none>        443/TCP   30d\\nkubia        10.111.249.153   <none>        80/TCP    6m     \\nThe list shows that the IP address assigned to the service is 10.111.249.153. Because\\nthis is the cluster IP, it’s only accessible from inside the cluster. The primary purpose\\nof services is exposing groups of pods to other pods in the cluster, but you’ll usually\\nalso want to expose services externally. You’ll see how to do that later. For now, let’s\\nuse your service from inside the cluster and see what it does.\\nTESTING YOUR SERVICE FROM WITHIN THE CLUSTER\\nYou can send requests to your service from within the cluster in a few ways:\\n\\uf0a1The obvious way is to create a pod that will send the request to the service’s\\ncluster IP and log the response. You can then examine the pod’s log to see\\nwhat the service’s response was.\\n\\uf0a1You can ssh into one of the Kubernetes nodes and use the curl command.\\n\\uf0a1You can execute the curl command inside one of your existing pods through\\nthe kubectl exec command.\\nLet’s go for the last option, so you also learn how to run commands in existing pods. \\nREMOTELY EXECUTING COMMANDS IN RUNNING CONTAINERS\\nThe kubectl exec command allows you to remotely run arbitrary commands inside\\nan existing container of a pod. This comes in handy when you want to examine the\\ncontents, state, and/or environment of a container. List the pods with the kubectl\\nget pods command and choose one as your target for the exec command (in the fol-\\nlowing example, I’ve chosen the kubia-7nog1 pod as the target). You’ll also need to\\nobtain the cluster IP of your service (using kubectl get svc, for example). When run-\\nning the following commands yourself, be sure to replace the pod name and the ser-\\nvice IP with your own: \\n$ kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153\\nYou’ve hit kubia-gzwli\\nIf you’ve used ssh to execute commands on a remote system before, you’ll recognize\\nthat kubectl exec isn’t much different.\\n \\n \\n \\n \\nHere’s your \\nservice.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Service',\n",
       "    'description': 'a resource in Kubernetes that enables clients to discover and talk to pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Cluster IP',\n",
       "    'description': 'an internal IP address assigned to a service, only accessible from inside the cluster',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'a lightweight and portable container that can be run on a Kubernetes node',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl get svc',\n",
       "    'description': 'a command used to list all Service resources in a namespace',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Cluster IP',\n",
       "    'description': 'the IP address assigned to the service, only accessible from inside the cluster',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'exposing groups of pods to other pods in the cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'curl command',\n",
       "    'description': 'a way to send requests to a service from within the cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubectl exec',\n",
       "    'description': 'a command used to remotely run arbitrary commands inside an existing container of a pod',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'the environment in which a process runs, isolated from other processes and resources',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Pod name',\n",
       "    'description': 'the identifier for a running pod, used to target the exec command',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Service IP',\n",
       "    'description': 'the IP address of the service, used in conjunction with the kubectl exec command',\n",
       "    'category': 'network'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl get svc\", \"description\": \"list Service resources and see that an internal cluster IP has been assigned to your service\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"kubia-7nog1 pod\", \"description\": \"send a request to the service\\'s cluster IP and log the response\", \"destination_entity\": \"Service IP\"},\\n  {\"source_entity\": \"curl command\", \"description\": \"send a request to the service\\'s cluster IP\", \"destination_entity\": \"Service IP\"},\\n  {\"source_entity\": \"kubectl exec command\", \"description\": \"run arbitrary commands inside an existing container of a pod\", \"destination_entity\": \"Container\"},\\n  {\"source_entity\": \"kubectl get pods command\", \"description\": \"list pods with kubectl and choose one as your target for the exec command\", \"destination_entity\": \"Pod name\"},\\n  {\"source_entity\": \"kubectl exec command\", \"description\": \"execute commands inside an existing container of a pod\", \"destination_entity\": \"Container\"},\\n  {\"source_entity\": \"kubia-7nog1 pod\", \"description\": \"be used as the target for the kubectl exec command\", \"destination_entity\": \"Pod name\"},\\n  {\"source_entity\": \"Service IP\", \"description\": \"be accessed from inside the cluster\", \"destination_entity\": \"Cluster IP\"},\\n  {\"source_entity\": \"curl command\", \"description\": \"hit a service\", \"destination_entity\": \"Service\"}\\n]\\n```\\n\\nNote that I\\'ve extracted all the relations mentioned in the document page, and each relation has been defined with three keys: `source_entity`, `description`, and `destination_entity`.'},\n",
       " {'page': 157,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '125\\nIntroducing services\\nLet’s go over what transpired when you ran the command. Figure 5.3 shows the\\nsequence of events. You instructed Kubernetes to execute the curl command inside the\\ncontainer of one of your pods. Curl sent an HTTP request to the service IP, which is\\nbacked by three pods. The Kubernetes service proxy intercepted the connection,\\nselected a random pod among the three pods, and forwarded the request to it. Node.js\\nrunning inside that pod then handled the request and returned an HTTP response con-\\ntaining the pod’s name. Curl then printed the response to the standard output, which\\nwas intercepted and printed to its standard output on your local machine by kubectl.\\nWhy the double dash?\\nThe double dash (--) in the command signals the end of command options for\\nkubectl. Everything after the double dash is the command that should be executed\\ninside the pod. Using the double dash isn’t necessary if the command has no\\narguments that start with a dash. But in your case, if you don’t use the double dash\\nthere, the -s option would be interpreted as an option for kubectl exec and would\\nresult in the following strange and highly misleading error:\\n$ kubectl exec kubia-7nog1 curl -s http://10.111.249.153\\nThe connection to the server 10.111.249.153 was refused – did you \\nspecify the right host or port?\\nThis has nothing to do with your service refusing the connection. It’s because\\nkubectl is not able to connect to an API server at 10.111.249.153 (the -s option\\nis used to tell kubectl to connect to a different API server than the default).\\n3. Curl sends HTTP\\nGET request\\n4. Service redirects HTTP\\nconnection to a randomly\\nselected pod\\n2. Curl is executed\\ninside the container\\nrunning node.js\\n6. The output of the\\ncommand is sent\\ncurl\\nback to kubectl and\\nprinted by it\\n5. HTTP response is\\nsent back to curl\\nPod: kubia-7nog1\\nContainer\\nnode.js\\ncurl http://\\n10.111.249.153\\nPod: kubia-gzwli\\nContainer\\nnode.js\\nPod: kubia-5fje3\\nContainer\\nnode.js\\n1. kubectl exec\\nService: kubia\\n10.111.249.153:80\\nFigure 5.3\\nUsing kubectl exec to test out a connection to the service by running curl in one of the pods\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'command-line tool for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'tool for transferring data with URL syntax',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'lightweight and portable container runtime',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'node.js',\n",
       "    'description': 'JavaScript runtime environment',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'service',\n",
       "    'description': \" abstraction for accessing a pod's IP address\",\n",
       "    'category': 'application'},\n",
       "   {'entity': 'HTTP request',\n",
       "    'description': 'client-server communication protocol',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'server that manages access to cluster resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl exec',\n",
       "    'description': 'command for executing a command in a pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod IP address',\n",
       "    'description': \"unique identifier for each pod's network connection\",\n",
       "    'category': 'network'},\n",
       "   {'entity': 'container runtime',\n",
       "    'description': 'platform for running and managing containers',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"executes command to test out a connection to the service\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"curl\",\\n    \"description\": \"sends HTTP GET request\",\\n    \"destination_entity\": \"HTTP request\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl exec\",\\n    \"description\": \"executes curl command inside the container running node.js\",\\n    \"destination_entity\": \"node.js\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"proxies HTTP connection to a randomly selected pod\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"service\",\\n    \"description\": \"redirects HTTP connection to a randomly selected pod\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"intercepts and prints output of curl command\",\\n    \"destination_entity\": \"curl\"\\n  },\\n  {\\n    \"source_entity\": \"node.js\",\\n    \"description\": \"handles HTTP request and returns HTTP response\",\\n    \"destination_entity\": \"HTTP response\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl exec\",\\n    \"description\": \"executes command inside a pod with container runtime\",\\n    \"destination_entity\": \"container runtime\"\\n  },\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"connects to an API server at the given IP address\",\\n    \"destination_entity\": \"pod IP address\"\\n  }\\n]\\n```'},\n",
       " {'page': 158,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '126\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\nIn the previous example, you executed the curl command as a separate process, but\\ninside the pod’s main container. This isn’t much different from the actual main pro-\\ncess in the container talking to the service.\\nCONFIGURING SESSION AFFINITY ON THE SERVICE\\nIf you execute the same command a few more times, you should hit a different pod\\nwith every invocation, because the service proxy normally forwards each connection\\nto a randomly selected backing pod, even if the connections are coming from the\\nsame client. \\n If, on the other hand, you want all requests made by a certain client to be redi-\\nrected to the same pod every time, you can set the service’s sessionAffinity property\\nto ClientIP (instead of None, which is the default), as shown in the following listing.\\napiVersion: v1\\nkind: Service             \\nspec:\\n  sessionAffinity: ClientIP\\n  ...\\nThis makes the service proxy redirect all requests originating from the same client IP\\nto the same pod. As an exercise, you can create an additional service with session affin-\\nity set to ClientIP and try sending requests to it.\\n Kubernetes supports only two types of service session affinity: None and ClientIP.\\nYou may be surprised it doesn’t have a cookie-based session affinity option, but you\\nneed to understand that Kubernetes services don’t operate at the HTTP level. Services\\ndeal with TCP and UDP packets and don’t care about the payload they carry. Because\\ncookies are a construct of the HTTP protocol, services don’t know about them, which\\nexplains why session affinity cannot be based on cookies. \\nEXPOSING MULTIPLE PORTS IN THE SAME SERVICE\\nYour service exposes only a single port, but services can also support multiple ports. For\\nexample, if your pods listened on two ports—let’s say 8080 for HTTP and 8443 for\\nHTTPS—you could use a single service to forward both port 80 and 443 to the pod’s\\nports 8080 and 8443. You don’t need to create two different services in such cases. Using\\na single, multi-port service exposes all the service’s ports through a single cluster IP.\\nNOTE\\nWhen creating a service with multiple ports, you must specify a name\\nfor each port.\\nThe spec for a multi-port service is shown in the following listing.\\napiVersion: v1\\nkind: Service             \\nmetadata:\\n  name: kubia              \\nListing 5.2\\nA example of a service with ClientIP session affinity configured\\nListing 5.3\\nSpecifying multiple ports in a service definition\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'curl command',\n",
       "    'description': \"a separate process executed inside the pod's main container\",\n",
       "    'category': 'command'},\n",
       "   {'entity': 'service proxy',\n",
       "    'description': 'forwards each connection to a randomly selected backing pod',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'sessionAffinity property',\n",
       "    'description': 'configures service to redirect all requests from same client to same pod',\n",
       "    'category': 'property'},\n",
       "   {'entity': 'ClientIP',\n",
       "    'description': 'value of sessionAffinity property that redirects requests from same client to same pod',\n",
       "    'category': 'value'},\n",
       "   {'entity': 'None',\n",
       "    'description': \"default value of sessionAffinity property that doesn't redirect requests\",\n",
       "    'category': 'value'},\n",
       "   {'entity': 'Kubernetes service',\n",
       "    'description': \"deals with TCP and UDP packets, doesn't care about payload or HTTP protocol\",\n",
       "    'category': 'component'},\n",
       "   {'entity': 'TCP packets',\n",
       "    'description': 'type of packet handled by Kubernetes services',\n",
       "    'category': 'packet type'},\n",
       "   {'entity': 'UDP packets',\n",
       "    'description': 'type of packet handled by Kubernetes services',\n",
       "    'category': 'packet type'},\n",
       "   {'entity': 'HTTP protocol',\n",
       "    'description': 'protocol that uses cookies, not supported by Kubernetes services',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'cookies',\n",
       "    'description': 'construct of HTTP protocol used for session affinity',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'service',\n",
       "    'description': 'exposes a single port or multiple ports to clients',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'cluster IP',\n",
       "    'description': \"single IP address that exposes all service's ports\",\n",
       "    'category': 'IP address'},\n",
       "   {'entity': 'port',\n",
       "    'description': 'single endpoint on which service listens for connections',\n",
       "    'category': 'endpoint'}],\n",
       "  'relationships': '[{\"source_entity\": \"curl command\", \"description\": \"executes as a separate process inside the pod\\'s main container\", \"destination_entity\": \"pod\"}]\\n\\n'},\n",
       " {'page': 159,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '127\\nIntroducing services\\nspec:\\n  ports:\\n  - name: http              \\n    port: 80                \\n    targetPort: 8080        \\n  - name: https             \\n    port: 443               \\n    targetPort: 8443        \\n  selector:                 \\n    app: kubia              \\nNOTE\\nThe label selector applies to the service as a whole—it can’t be config-\\nured for each port individually. If you want different ports to map to different\\nsubsets of pods, you need to create two services.\\nBecause your kubia pods don’t listen on multiple ports, creating a multi-port service\\nand a multi-port pod is left as an exercise to you.\\nUSING NAMED PORTS\\nIn all these examples, you’ve referred to the target port by its number, but you can also\\ngive a name to each pod’s port and refer to it by name in the service spec. This makes\\nthe service spec slightly clearer, especially if the port numbers aren’t well-known.\\n For example, suppose your pod defines names for its ports as shown in the follow-\\ning listing.\\nkind: Pod\\nspec:\\n  containers:\\n  - name: kubia\\n    ports:\\n    - name: http               \\n      containerPort: 8080      \\n    - name: https              \\n      containerPort: 8443      \\nYou can then refer to those ports by name in the service spec, as shown in the follow-\\ning listing.\\napiVersion: v1\\nkind: Service             \\nspec:\\n  ports:\\n  - name: http              \\n    port: 80                \\n    targetPort: http        \\n  - name: https             \\n    port: 443               \\n    targetPort: https       \\nListing 5.4\\nSpecifying port names in a pod definition\\nListing 5.5\\nReferring to named ports in a service\\nPort 80 is mapped to \\nthe pods’ port 8080.\\nPort 443 is mapped to \\npods’ port 8443.\\nThe label selector always \\napplies to the whole service.\\nContainer’s port \\n8080 is called http\\nPort 8443 is called https.\\nPort 80 is mapped to the \\ncontainer’s port called http.\\nPort 443 is mapped to the container’s \\nport, whose name is https.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'service',\n",
       "    'description': 'A service that exposes a single port.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ports',\n",
       "    'description': 'A list of ports exposed by a service.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'http',\n",
       "    'description': \"Port number 80, mapped to the pod's port 8080.\",\n",
       "    'category': 'port'},\n",
       "   {'entity': 'https',\n",
       "    'description': \"Port number 443, mapped to the pod's port 8443.\",\n",
       "    'category': 'port'},\n",
       "   {'entity': 'selector',\n",
       "    'description': 'A label selector that applies to the service as a whole.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'label',\n",
       "    'description': 'A key-value pair used in the label selector.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kubia',\n",
       "    'description': 'The name of the pod and service.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'A version number for the API used to create a resource.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'The type of resource being created (e.g. Pod, Service).',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'containerPort',\n",
       "    'description': 'A port exposed by a container.',\n",
       "    'category': 'port'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ports\",\\n    \"description\": \"Defines ports for a service\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"containerPort\",\\n    \"description\": \"Maps container port to target port\",\\n    \"destination_entity\": \"targetPort\"\\n  },\\n  {\\n    \"source_entity\": \"selector\",\\n    \"description\": \"Applies label selector to the whole service\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"port\",\\n    \"description\": \"Maps port number to container\\'s port name\",\\n    \"destination_entity\": \"containerPort\"\\n  },\\n  {\\n    \"source_entity\": \"http\",\\n    \"description\": \"Defines HTTP port for a service\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"https\",\\n    \"description\": \"Defines HTTPS port for a service\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"kubia\",\\n    \"description\": \" Defines ports for kubia pod\",\\n    \"destination_entity\": \"ports\"\\n  },\\n  {\\n    \"source_entity\": \"label\",\\n    \"description\": \"Applies label selector to the whole service\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"apiVersion\",\\n    \"description\": \"Specifies API version for a service\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"kind\",\\n    \"description\": \"Specifies kind of an object (e.g., Pod, Service)\",\\n    \"destination_entity\": \"Pod\"\\n  }\\n]\\n```'},\n",
       " {'page': 160,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '128\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\nBut why should you even bother with naming ports? The biggest benefit of doing so is\\nthat it enables you to change port numbers later without having to change the service\\nspec. Your pod currently uses port 8080 for http, but what if you later decide you’d\\nlike to move that to port 80? \\n If you’re using named ports, all you need to do is change the port number in the\\npod spec (while keeping the port’s name unchanged). As you spin up pods with the\\nnew ports, client connections will be forwarded to the appropriate port numbers,\\ndepending on the pod receiving the connection (port 8080 on old pods and port 80\\non the new ones).\\n5.1.2\\nDiscovering services\\nBy creating a service, you now have a single and stable IP address and port that you\\ncan hit to access your pods. This address will remain unchanged throughout the\\nwhole lifetime of the service. Pods behind this service may come and go, their IPs may\\nchange, their number can go up or down, but they’ll always be accessible through the\\nservice’s single and constant IP address. \\n But how do the client pods know the IP and port of a service? Do you need to cre-\\nate the service first, then manually look up its IP address and pass the IP to the config-\\nuration options of the client pod? Not really. Kubernetes also provides ways for client\\npods to discover a service’s IP and port.\\nDISCOVERING SERVICES THROUGH ENVIRONMENT VARIABLES\\nWhen a pod is started, Kubernetes initializes a set of environment variables pointing\\nto each service that exists at that moment. If you create the service before creating the\\nclient pods, processes in those pods can get the IP address and port of the service by\\ninspecting their environment variables. \\n Let’s see what those environment variables look like by examining the environment\\nof one of your running pods. You’ve already learned that you can use the kubectl exec\\ncommand to run a command in the pod, but because you created the service only\\nafter your pods had been created, the environment variables for the service couldn’t\\nhave been set yet. You’ll need to address that first.\\n Before you can see environment variables for your service, you first need to delete\\nall the pods and let the ReplicationController create new ones. You may remember\\nyou can delete all pods without specifying their names like this:\\n$ kubectl delete po --all\\npod \"kubia-7nog1\" deleted\\npod \"kubia-bf50t\" deleted\\npod \"kubia-gzwli\" deleted\\nNow you can list the new pods (I’m sure you know how to do that) and pick one as\\nyour target for the kubectl exec command. Once you’ve selected your target pod,\\nyou can list environment variables by running the env command inside the container,\\nas shown in the following listing.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Services',\n",
       "    'description': 'Enabling clients to discover and talk to pods',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'ports',\n",
       "    'description': 'Named ports for HTTP communication',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'pod spec',\n",
       "    'description': 'Configuration for pod specification',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'Cloud Platform'},\n",
       "   {'entity': 'client pods',\n",
       "    'description': 'Pods that access services',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'environment variables',\n",
       "    'description': 'Variables set by Kubernetes for service discovery',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'kubectl exec command',\n",
       "    'description': 'Command to run in a pod',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'Component that creates and manages pods',\n",
       "    'category': 'Cloud Platform'},\n",
       "   {'entity': 'pod names',\n",
       "    'description': 'Unique identifiers for pods',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'env command',\n",
       "    'description': 'Command to list environment variables in a pod',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'service spec',\n",
       "    'description': 'Configuration for service specification',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"provides ways for client pods to discover a service\\'s IP and port\",\\n    \"destination_entity\": \"client pods\"\\n  },\\n  {\\n    \"source_entity\": \"Services\",\\n    \"description\": \"enables clients to discover and talk to pods\",\\n    \"destination_entity\": \"client pods\"\\n  },\\n  {\\n    \"source_entity\": \"pod spec\",\\n    \"description\": \"can be changed without affecting the service spec\",\\n    \"destination_entity\": \"service spec\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"creates new pods that can access environment variables\",\\n    \"destination_entity\": \"environment variables\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl exec command\",\\n    \"description\": \"allows running a command inside the container to list environment variables\",\\n    \"destination_entity\": \"environment variables\"\\n  },\\n  {\\n    \"source_entity\": \"env command\",\\n    \"description\": \"lists environment variables in the container\",\\n    \"destination_entity\": \"environment variables\"\\n  },\\n  {\\n    \"source_entity\": \"client pods\",\\n    \"description\": \"can access the IP and port of a service through environment variables\",\\n    \"destination_entity\": \"Services\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"provides named ports that can be used to change port numbers without affecting the service spec\",\\n    \"destination_entity\": \"ports\"\\n  },\\n  {\\n    \"source_entity\": \"service spec\",\\n    \"description\": \"can have its port number changed without affecting client connections\",\\n    \"destination_entity\": \"ports\"\\n  }\\n]\\n```'},\n",
       " {'page': 161,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '129\\nIntroducing services\\n$ kubectl exec kubia-3inly env\\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\nHOSTNAME=kubia-3inly\\nKUBERNETES_SERVICE_HOST=10.111.240.1\\nKUBERNETES_SERVICE_PORT=443\\n...\\nKUBIA_SERVICE_HOST=10.111.249.153             \\nKUBIA_SERVICE_PORT=80                            \\n...\\nTwo services are defined in your cluster: the kubernetes and the kubia service (you\\nsaw this earlier with the kubectl get svc command); consequently, two sets of service-\\nrelated environment variables are in the list. Among the variables that pertain to the\\nkubia service you created at the beginning of the chapter, you’ll see the KUBIA_SERVICE\\n_HOST and the KUBIA_SERVICE_PORT environment variables, which hold the IP address\\nand port of the kubia service, respectively. \\n Turning back to the frontend-backend example we started this chapter with, when\\nyou have a frontend pod that requires the use of a backend database server pod, you\\ncan expose the backend pod through a service called backend-database and then\\nhave the frontend pod look up its IP address and port through the environment vari-\\nables BACKEND_DATABASE_SERVICE_HOST and BACKEND_DATABASE_SERVICE_PORT.\\nNOTE\\nDashes in the service name are converted to underscores and all let-\\nters are uppercased when the service name is used as the prefix in the envi-\\nronment variable’s name. \\nEnvironment variables are one way of looking up the IP and port of a service, but isn’t\\nthis usually the domain of DNS? Why doesn’t Kubernetes include a DNS server and\\nallow you to look up service IPs through DNS instead? As it turns out, it does!\\nDISCOVERING SERVICES THROUGH DNS\\nRemember in chapter 3 when you listed pods in the kube-system namespace? One of\\nthe pods was called kube-dns. The kube-system namespace also includes a corre-\\nsponding service with the same name.\\n As the name suggests, the pod runs a DNS server, which all other pods running in\\nthe cluster are automatically configured to use (Kubernetes does that by modifying\\neach container’s /etc/resolv.conf file). Any DNS query performed by a process run-\\nning in a pod will be handled by Kubernetes’ own DNS server, which knows all the ser-\\nvices running in your system. \\nNOTE\\nWhether a pod uses the internal DNS server or not is configurable\\nthrough the dnsPolicy property in each pod’s spec.\\nEach service gets a DNS entry in the internal DNS server, and client pods that know\\nthe name of the service can access it through its fully qualified domain name (FQDN)\\ninstead of resorting to environment variables. \\nListing 5.6\\nService-related environment variables in a container\\nHere’s the cluster \\nIP of the service.\\nAnd here’s the port the \\nservice is available on.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'exec', 'description': 'command', 'category': 'software'},\n",
       "   {'entity': 'env', 'description': 'command', 'category': 'software'},\n",
       "   {'entity': 'PATH',\n",
       "    'description': 'environment variable',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'HOSTNAME',\n",
       "    'description': 'environment variable',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'KUBERNETES_SERVICE_HOST',\n",
       "    'description': 'environment variable',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'KUBERNETES_SERVICE_PORT',\n",
       "    'description': 'environment variable',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'KUBIA_SERVICE_HOST',\n",
       "    'description': 'environment variable',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'KUBIA_SERVICE_PORT',\n",
       "    'description': 'environment variable',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'BACKEND_DATABASE_SERVICE_HOST',\n",
       "    'description': 'environment variable',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'BACKEND_DATABASE_SERVICE_PORT',\n",
       "    'description': 'environment variable',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubia service',\n",
       "    'description': 'service',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'backend-database service',\n",
       "    'description': 'service',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kube-system namespace',\n",
       "    'description': 'namespace',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kube-dns pod', 'description': 'pod', 'category': 'application'},\n",
       "   {'entity': 'DNS server', 'description': 'service', 'category': 'software'},\n",
       "   {'entity': 'resolv.conf', 'description': 'file', 'category': 'hardware'},\n",
       "   {'entity': 'FQDN',\n",
       "    'description': 'fully qualified domain name',\n",
       "    'category': 'network'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"PATH\", \"description\": \"environment variable for path\", \"destination_entity\": null},\\n  {\"source_entity\": \"BACKEND_DATABASE_SERVICE_PORT\", \"description\": \"environment variable for backend database service port\", \"destination_entity\": null},\\n  {\"source_entity\": \"FQDN\", \"description\": \"fully qualified domain name of a service\", \"destination_entity\": null},\\n  {\"source_entity\": \"HOSTNAME\", \"description\": \"hostname of a pod or container\", \"destination_entity\": null},\\n  {\"source_entity\": \"BACKEND_DATABASE_SERVICE_HOST\", \"description\": \"environment variable for backend database service host\", \"destination_entity\": null},\\n  {\"source_entity\": \"KUBERNETES_SERVICE_PORT\", \"description\": \"environment variable for Kubernetes service port\", \"destination_entity\": null},\\n  {\"source_entity\": \"exec\", \"description\": \"execute a command in a container\", \"destination_entity\": \"kubectl\"},\\n  {\"source_entity\": \"resolv.conf\", \"description\": \"DNS configuration file\", \"destination_entity\": null},\\n  {\"source_entity\": \"kubectl\", \"description\": \"command to interact with Kubernetes cluster\", \"destination_entity\": null},\\n  {\"source_entity\": \"kubia service\", \"description\": \"service created in a Kubernetes cluster\", \"destination_entity\": \"KUBIA_SERVICE_PORT\"},\\n  {\"source_entity\": \"kube-system namespace\", \"description\": \"namespace where the kube-dns pod is running\", \"destination_entity\": null},\\n  {\"source_entity\": \"kube-dns pod\", \"description\": \"pod running a DNS server in Kubernetes cluster\", \"destination_entity\": null},\\n  {\"source_entity\": \"KUBIA_SERVICE_PORT\", \"description\": \"environment variable for kubia service port\", \"destination_entity\": null},\\n  {\"source_entity\": \"KUBERNETES_SERVICE_HOST\", \"description\": \"environment variable for Kubernetes service host\", \"destination_entity\": null},\\n  {\"source_entity\": \"DNS server\", \"description\": \"server responsible for resolving DNS queries in a Kubernetes cluster\", \"destination_entity\": null},\\n  {\"source_entity\": \"env\", \"description\": \"display environment variables of a container\", \"destination_entity\": null},\\n  {\"source_entity\": \"backend-database service\", \"description\": \"service exposing a backend database server\", \"destination_entity\": null},\\n  {\"source_entity\": \"KUBIA_SERVICE_HOST\", \"description\": \"environment variable for kubia service host\", \"destination_entity\": null}\\n]\\n```\\n\\nNote that I\\'ve added `null` as the destination entity when there is no explicit object or entity being referenced by the source entity.'},\n",
       " {'page': 162,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '130\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\nCONNECTING TO THE SERVICE THROUGH ITS FQDN\\nTo revisit the frontend-backend example, a frontend pod can connect to the backend-\\ndatabase service by opening a connection to the following FQDN:\\nbackend-database.default.svc.cluster.local\\nbackend-database corresponds to the service name, default stands for the name-\\nspace the service is defined in, and svc.cluster.local is a configurable cluster\\ndomain suffix used in all cluster local service names. \\nNOTE\\nThe client must still know the service’s port number. If the service is\\nusing a standard port (for example, 80 for HTTP or 5432 for Postgres), that\\nshouldn’t be a problem. If not, the client can get the port number from the\\nenvironment variable.\\nConnecting to a service can be even simpler than that. You can omit the svc.cluster\\n.local suffix and even the namespace, when the frontend pod is in the same name-\\nspace as the database pod. You can thus refer to the service simply as backend-\\ndatabase. That’s incredibly simple, right?\\n Let’s try this. You’ll try to access the kubia service through its FQDN instead of its\\nIP. Again, you’ll need to do that inside an existing pod. You already know how to use\\nkubectl exec to run a single command in a pod’s container, but this time, instead of\\nrunning the curl command directly, you’ll run the bash shell instead, so you can then\\nrun multiple commands in the container. This is similar to what you did in chapter 2\\nwhen you entered the container you ran with Docker by using the docker exec -it\\nbash command. \\nRUNNING A SHELL IN A POD’S CONTAINER\\nYou can use the kubectl exec command to run bash (or any other shell) inside a\\npod’s container. This way you’re free to explore the container as long as you want,\\nwithout having to perform a kubectl exec for every command you want to run.\\nNOTE\\nThe shell’s binary executable must be available in the container image\\nfor this to work.\\nTo use the shell properly, you need to pass the -it option to kubectl exec:\\n$ kubectl exec -it kubia-3inly bash\\nroot@kubia-3inly:/# \\nYou’re now inside the container. You can use the curl command to access the kubia\\nservice in any of the following ways:\\nroot@kubia-3inly:/# curl http://kubia.default.svc.cluster.local\\nYou’ve hit kubia-5asi2\\nroot@kubia-3inly:/# curl http://kubia.default\\nYou’ve hit kubia-3inly\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'frontend pod',\n",
       "    'description': 'A pod that runs a frontend application',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'backend-database service',\n",
       "    'description': 'A service that provides access to the backend database',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'FQDN (Fully Qualified Domain Name)',\n",
       "    'description': 'A domain name that includes the hostname, namespace, and cluster domain suffix',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'svc.cluster.local',\n",
       "    'description': 'The configurable cluster domain suffix used in all cluster local service names',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'A logical partitioning of resources within a Kubernetes cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'port number',\n",
       "    'description': 'The numerical identifier of a service port',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'environment variable',\n",
       "    'description': 'A variable that can be set to store values for later use in a process',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kubectl exec',\n",
       "    'description': \"A command used to run a single command or shell inside a pod's container\",\n",
       "    'category': 'command'},\n",
       "   {'entity': 'bash shell',\n",
       "    'description': 'A Unix shell that allows running multiple commands in the container',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'curl command',\n",
       "    'description': 'A command used to transfer data over HTTP or HTTPS protocols',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubia service',\n",
       "    'description': 'A service that provides access to the kubia application',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A logical host in a Kubernetes cluster that can run one or more containers',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\": \"frontend pod\", \"description\": \"connects to\", \"destination_entity\": \"backend-database service\"}]\\n'},\n",
       " {'page': 163,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '131\\nConnecting to services living outside the cluster\\nroot@kubia-3inly:/# curl http://kubia\\nYou’ve hit kubia-8awf3\\nYou can hit your service by using the service’s name as the hostname in the requested\\nURL. You can omit the namespace and the svc.cluster.local suffix because of how\\nthe DNS resolver inside each pod’s container is configured. Look at the /etc/resolv.conf\\nfile in the container and you’ll understand:\\nroot@kubia-3inly:/# cat /etc/resolv.conf\\nsearch default.svc.cluster.local svc.cluster.local cluster.local ...\\nUNDERSTANDING WHY YOU CAN’T PING A SERVICE IP\\nOne last thing before we move on. You know how to create services now, so you’ll soon\\ncreate your own. But what if, for whatever reason, you can’t access your service?\\n You’ll probably try to figure out what’s wrong by entering an existing pod and try-\\ning to access the service like you did in the last example. Then, if you still can’t access\\nthe service with a simple curl command, maybe you’ll try to ping the service IP to see\\nif it’s up. Let’s try that now:\\nroot@kubia-3inly:/# ping kubia\\nPING kubia.default.svc.cluster.local (10.111.249.153): 56 data bytes\\n^C--- kubia.default.svc.cluster.local ping statistics ---\\n54 packets transmitted, 0 packets received, 100% packet loss\\nHmm. curl-ing the service works, but pinging it doesn’t. That’s because the service’s\\ncluster IP is a virtual IP, and only has meaning when combined with the service port.\\nWe’ll explain what that means and how services work in chapter 11. I wanted to men-\\ntion that here because it’s the first thing users do when they try to debug a broken\\nservice and it catches most of them off guard.\\n5.2\\nConnecting to services living outside the cluster\\nUp to now, we’ve talked about services backed by one or more pods running inside\\nthe cluster. But cases exist when you’d like to expose external services through the\\nKubernetes services feature. Instead of having the service redirect connections to\\npods in the cluster, you want it to redirect to external IP(s) and port(s). \\n This allows you to take advantage of both service load balancing and service discov-\\nery. Client pods running in the cluster can connect to the external service like they\\nconnect to internal services.\\n5.2.1\\nIntroducing service endpoints\\nBefore going into how to do this, let me first shed more light on services. Services\\ndon’t link to pods directly. Instead, a resource sits in between—the Endpoints\\nresource. You may have already noticed endpoints if you used the kubectl describe\\ncommand on your service, as shown in the following listing.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'curl',\n",
       "    'description': 'A command-line tool for transferring data to and from a web server.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'http://kubia',\n",
       "    'description': 'The URL used to access the kubia service.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'kubia-8awf3',\n",
       "    'description': 'A pod name in the cluster.',\n",
       "    'category': 'container'},\n",
       "   {'entity': '/etc/resolv.conf',\n",
       "    'description': 'A file in a container that configures the DNS resolver.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ping',\n",
       "    'description': 'A command-line tool for testing network connectivity.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubia.default.svc.cluster.local',\n",
       "    'description': 'The hostname used to access the kubia service.',\n",
       "    'category': 'network'},\n",
       "   {'entity': '10.111.249.153',\n",
       "    'description': 'A cluster IP address.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'curl-ing',\n",
       "    'description': 'Using the curl command to transfer data.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kubectl describe',\n",
       "    'description': 'A command used to display detailed information about a resource.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Endpoints resource',\n",
       "    'description': 'A resource that sits between services and pods, managing service endpoints.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'service endpoints',\n",
       "    'description': 'A concept used to manage external IP(s) and port(s) for a service.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"root\", \"description\": \"accessing kubia service\", \"destination_entity\": \"http://kubia\"},\\n  {\"source_entity\": \"curl\", \"description\": \"connecting to kubia service\", \"destination_entity\": \"http://kubia\"},\\n  {\"source_entity\": \"kubectl describe\", \"description\": \"getting information about a resource\", \"destination_entity\": \"Endpoints resource\"},\\n  {\"source_entity\": \"kubectl describe\", \"description\": \"getting information about a service\", \"destination_entity\": \"service endpoints\"},\\n  {\"source_entity\": \"ping\", \"description\": \"trying to access kubia service using IP address\", \"destination_entity\": \"kubia.default.svc.cluster.local\"},\\n  {\"source_entity\": \"ping\", \"description\": \"not being able to access kubia service using IP address\", \"destination_entity\": \"10.111.249.153\"},\\n  {\"source_entity\": \"/etc/resolv.conf\", \"description\": \"showing DNS resolver configuration\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"curl-ing\", \"description\": \"connecting to kubia service using curl command\", \"destination_entity\": \"http://kubia\"},\\n  {\"source_entity\": \"curl\", \"description\": \"connecting to kubia service\", \"destination_entity\": \"http://kubia\"},\\n  {\"source_entity\": \"cluster IP\", \"description\": \"not being able to access kubia service using ping command\", \"destination_entity\": \"10.111.249.153\"},\\n  {\"source_entity\": \"Kubernetes services feature\", \"description\": \"exposing external services through Kubernetes\", \"destination_entity\": \"external services\"},\\n  {\"source_entity\": \"service endpoints\", \"description\": \"listing all the IP addresses and ports associated with a service\", \"destination_entity\": \"Endpoints resource\"}\\n]\\n```'},\n",
       " {'page': 164,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '132\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\n$ kubectl describe svc kubia\\nName:                kubia\\nNamespace:           default\\nLabels:              <none>\\nSelector:            app=kubia         \\nType:                ClusterIP\\nIP:                  10.111.249.153\\nPort:                <unset> 80/TCP\\nEndpoints:           10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   \\nSession Affinity:    None\\nNo events.\\nAn Endpoints resource (yes, plural) is a list of IP addresses and ports exposing a ser-\\nvice. The Endpoints resource is like any other Kubernetes resource, so you can display\\nits basic info with kubectl get:\\n$ kubectl get endpoints kubia\\nNAME    ENDPOINTS                                         AGE\\nkubia   10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   1h\\nAlthough the pod selector is defined in the service spec, it’s not used directly when\\nredirecting incoming connections. Instead, the selector is used to build a list of IPs\\nand ports, which is then stored in the Endpoints resource. When a client connects to a\\nservice, the service proxy selects one of those IP and port pairs and redirects the\\nincoming connection to the server listening at that location.\\n5.2.2\\nManually configuring service endpoints\\nYou may have probably realized this already, but having the service’s endpoints decou-\\npled from the service allows them to be configured and updated manually. \\n If you create a service without a pod selector, Kubernetes won’t even create the\\nEndpoints resource (after all, without a selector, it can’t know which pods to include\\nin the service). It’s up to you to create the Endpoints resource to specify the list of\\nendpoints for the service.\\n To create a service with manually managed endpoints, you need to create both a\\nService and an Endpoints resource. \\nCREATING A SERVICE WITHOUT A SELECTOR\\nYou’ll first create the YAML for the service itself, as shown in the following listing.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: external-service     \\nspec:                       \\n  ports:\\n  - port: 80                  \\nListing 5.7\\nFull details of a service displayed with kubectl describe\\nListing 5.8\\nA service without a pod selector: external-service.yaml\\nThe service’s pod \\nselector is used to \\ncreate the list of \\nendpoints.\\nThe list of pod\\nIPs and ports\\nthat represent\\nthe endpoints of\\nthis service\\nThe name of the service must \\nmatch the name of the Endpoints \\nobject (see next listing).\\nThis service has no \\nselector defined.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'Command to manage Kubernetes resources',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'describe',\n",
       "    'description': 'Command to display detailed information about a resource',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'svc',\n",
       "    'description': \"Short form of command 'get svc'\",\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterIP',\n",
       "    'description': 'Service type that exposes a service on a cluster-wide IP address',\n",
       "    'category': 'service_type'},\n",
       "   {'entity': 'Endpoints',\n",
       "    'description': 'Resource that represents a list of IP addresses and ports exposing a service',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'pod selector',\n",
       "    'description': 'Selector used to identify pods associated with a service',\n",
       "    'category': 'selector'},\n",
       "   {'entity': 'service proxy',\n",
       "    'description': 'Component that selects an endpoint from the Endpoints resource and redirects incoming connections',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'Resource that defines a network service',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Endpoints resource',\n",
       "    'description': 'List of IP addresses and ports exposing a service',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'Field in YAML file specifying the API version of the Kubernetes object',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'Field in YAML file specifying the type of Kubernetes object',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Field in YAML file containing metadata about the object',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'ports',\n",
       "    'description': 'Field in YAML file specifying the ports exposed by the service',\n",
       "    'category': 'field'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"uses to display basic info of Kubernetes resource\",\\n    \"destination_entity\": \"metadata\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"is a Kubernetes resource that enables clients to discover and talk to pods\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"service proxy\",\\n    \"description\": \"selects one of the IP and port pairs stored in Endpoints resource and redirects incoming connection to server\",\\n    \"destination_entity\": \"Endpoints resource\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl describe svc kubia\",\\n    \"description\": \"displays detailed info about service named kubia\",\\n    \"destination_entity\": \"svc kubia\"\\n  },\\n  {\\n    \"source_entity\": \"pod selector\",\\n    \"description\": \"is used to create list of IPs and ports exposing a service\",\\n    \"destination_entity\": \"Endpoints resource\"\\n  },\\n  {\\n    \"source_entity\": \"apiVersion\",\\n    \"description\": \"specifies the version of Kubernetes API being used\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterIP\",\\n    \"description\": \"is a type of service that only allows pods within the same cluster to access it\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl get endpoints kubia\",\\n    \"description\": \"displays basic info about Endpoints resource named kubia\",\\n    \"destination_entity\": \"Endpoints kubia\"\\n  },\\n  {\\n    \"source_entity\": \"service\\'s pod selector\",\\n    \"description\": \"is used to build list of IPs and ports exposing a service\",\\n    \"destination_entity\": \"Endpoints resource\"\\n  },\\n  {\\n    \"source_entity\": \"manual configuration\",\\n    \"description\": \"allows configuring service endpoints manually\",\\n    \"destination_entity\": \"Endpoints resource\"\\n  }\\n]\\n```\\n\\nNote: I have assumed that `svc kubia`, `Endpoints kubia` and `service\\'s pod selector` are entities, although they might not be explicitly listed in the input entities. If you want me to remove them, please let me know!'},\n",
       " {'page': 165,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '133\\nConnecting to services living outside the cluster\\nYou’re defining a service called external-service that will accept incoming connec-\\ntions on port 80. You didn’t define a pod selector for the service.\\nCREATING AN ENDPOINTS RESOURCE FOR A SERVICE WITHOUT A SELECTOR\\nEndpoints are a separate resource and not an attribute of a service. Because you cre-\\nated the service without a selector, the corresponding Endpoints resource hasn’t been\\ncreated automatically, so it’s up to you to create it. The following listing shows its\\nYAML manifest.\\napiVersion: v1\\nkind: Endpoints\\nmetadata:\\n  name: external-service      \\nsubsets:\\n  - addresses:\\n    - ip: 11.11.11.11         \\n    - ip: 22.22.22.22         \\n    ports:\\n    - port: 80      \\nThe Endpoints object needs to have the same name as the service and contain the list\\nof target IP addresses and ports for the service. After both the Service and the End-\\npoints resource are posted to the server, the service is ready to be used like any regular\\nservice with a pod selector. Containers created after the service is created will include\\nthe environment variables for the service, and all connections to its IP:port pair will be\\nload balanced between the service’s endpoints. \\n Figure 5.4 shows three pods connecting to the service with external endpoints.\\nIf you later decide to migrate the external service to pods running inside Kubernetes,\\nyou can add a selector to the service, thereby making its Endpoints managed automat-\\nically. The same is also true in reverse—by removing the selector from a Service,\\nListing 5.9\\nA manually created Endpoints resource: external-service-endpoints.yaml\\nThe name of the Endpoints object \\nmust match the name of the \\nservice (see previous listing).\\nThe IPs of the endpoints that the \\nservice will forward connections to\\nThe target port of the endpoints\\nPod\\nPod\\nPod\\nExternal server 1\\nIP: 11.11.11.11:80\\nExternal server 2\\nIP: 22.22.22.22:80\\nService\\n10.111.249.214:80\\nKubernetes cluster\\nInternet\\nFigure 5.4\\nPods consuming a service with two external endpoints.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Pod Pod Pod\n",
       "   Service\n",
       "   10.111.249.214:80\n",
       "   Kubernetes cluster, External server 1\n",
       "   IP: 11.11.11.11:80\n",
       "   External server 2\n",
       "   IP: 22.22.22.22:80\n",
       "   Internet, Col2]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'service',\n",
       "    'description': 'a service called external-service that will accept incoming connections on port 80',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a pod created after the service is created will include environment variables for the service',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'endpoints',\n",
       "    'description': 'a separate resource and not an attribute of a service, containing list of target IP addresses and ports for the service',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ip address',\n",
       "    'description': \"11.11.11.11 is one of the external server's ip addresses that will forward connections to\",\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'port',\n",
       "    'description': '80 is the target port of the endpoints that the service will forward connections to',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubernetes cluster',\n",
       "    'description': 'a kubernetes cluster running inside Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'internet',\n",
       "    'description': 'the external servers are connected to the internet',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'selector',\n",
       "    'description': 'a selector is used to add a pod selector for the service, making its Endpoints managed automatically',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes cluster\",\\n    \"description\": \"connecting to services living outside the cluster\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"accepting incoming connections on port 80\",\\n    \"destination_entity\": \"port\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"not defining a pod selector\",\\n    \"destination_entity\": \"selector\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"creating an Endpoints resource\",\\n    \"destination_entity\": \"endpoints\"\\n  },\\n  {\\n    \"source_entity\": \"Endpoints object\",\\n    \"description\": \"containing the list of target IP addresses and ports for the service\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"Containers created after the service is created\",\\n    \"description\": \"including environment variables for the service\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"Connections to the IP:port pair\",\\n    \"description\": \"being load balanced between the service\\'s endpoints\",\\n    \"destination_entity\": \"endpoints\"\\n  },\\n  {\\n    \"source_entity\": \"External server\",\\n    \"description\": \"forwarding connections to the IPs of the endpoints\",\\n    \"destination_entity\": \"ip address\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes cluster\",\\n    \"description\": \"providing a service with external endpoints\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"migrating to pods running inside Kubernetes\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"connecting to the service with external endpoints\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"Internet\",\\n    \"description\": \"accessing the Kubernetes cluster\",\\n    \"destination_entity\": \"kubernetes cluster\"\\n  }\\n]'},\n",
       " {'page': 166,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '134\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\nKubernetes stops updating its Endpoints. This means a service IP address can remain\\nconstant while the actual implementation of the service is changed. \\n5.2.3\\nCreating an alias for an external service\\nInstead of exposing an external service by manually configuring the service’s End-\\npoints, a simpler method allows you to refer to an external service by its fully qualified\\ndomain name (FQDN).\\nCREATING AN EXTERNALNAME SERVICE\\nTo create a service that serves as an alias for an external service, you create a Service\\nresource with the type field set to ExternalName. For example, let’s imagine there’s a\\npublic API available at api.somecompany.com. You can define a service that points to\\nit as shown in the following listing.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: external-service\\nspec:\\n  type: ExternalName                       \\n  externalName: someapi.somecompany.com     \\n  ports:\\n  - port: 80\\nAfter the service is created, pods can connect to the external service through the\\nexternal-service.default.svc.cluster.local domain name (or even external-\\nservice) instead of using the service’s actual FQDN. This hides the actual service\\nname and its location from pods consuming the service, allowing you to modify the\\nservice definition and point it to a different service any time later, by only changing\\nthe externalName attribute or by changing the type back to ClusterIP and creating\\nan Endpoints object for the service—either manually or by specifying a label selector\\non the service and having it created automatically.\\n ExternalName services are implemented solely at the DNS level—a simple CNAME\\nDNS record is created for the service. Therefore, clients connecting to the service will\\nconnect to the external service directly, bypassing the service proxy completely. For\\nthis reason, these types of services don’t even get a cluster IP. \\nNOTE\\nA CNAME record points to a fully qualified domain name instead of a\\nnumeric IP address.\\n5.3\\nExposing services to external clients\\nUp to now, we’ve only talked about how services can be consumed by pods from inside\\nthe cluster. But you’ll also want to expose certain services, such as frontend webserv-\\ners, to the outside, so external clients can access them, as depicted in figure 5.5.\\nListing 5.10\\nAn ExternalName-type service: external-service-externalname.yaml\\nService type is set \\nto ExternalName\\nThe fully qualified domain \\nname of the actual service\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': '',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'enabling clients to discover and talk to pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Endpoints', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'API', 'description': '', 'category': 'database'},\n",
       "   {'entity': 'CNAME',\n",
       "    'description': 'DNS record points to a fully qualified domain name instead of a numeric IP address',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'ExternalName',\n",
       "    'description': 'service type that serves as an alias for an external service',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service resource', 'description': '', 'category': 'database'},\n",
       "   {'entity': 'Service proxy', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'ClusterIP', 'description': '', 'category': 'network'},\n",
       "   {'entity': 'DNS level', 'description': '', 'category': 'hardware'},\n",
       "   {'entity': 'Domain name', 'description': '', 'category': 'network'},\n",
       "   {'entity': 'FQDN',\n",
       "    'description': 'fully qualified domain name',\n",
       "    'category': 'network'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"stops updating Endpoints\", \"destination_entity\": \"EndPoints\"},\\n  {\"source_entity\": \"Service resource\", \"description\": \"enables clients to discover and talk to pods\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"DNS level\", \"description\": \"implements ExternalName services solely at the DNS level\", \"destination_entity\": \"ExternalName services\"},\\n  {\"source_entity\": \"FQDN\", \"description\": \"points to a fully qualified domain name instead of a numeric IP address\", \"destination_entity\": \"CNAME record\"},\\n  {\"source_entity\": \"Endpoints\", \"description\": \"can remain constant while the actual implementation of the service is changed\", \"destination_entity\": \"Service IP address\"},\\n  {\"source_entity\": \"ExternalName services\", \"description\": \"are implemented solely at the DNS level—a simple CNAME DNS record is created for the service\", \"destination_entity\": \"DNS level\"},\\n  {\"source_entity\": \"ClusterIP\", \"description\": \"is not assigned to ExternalName services\", \"destination_entity\": \"ExternalName services\"},\\n  {\"source_entity\": \"Domain name\", \"description\": \"can be used to connect to the external service instead of using the service\\'s actual FQDN\", \"destination_entity\": \"external-service.default.svc.cluster.local\"},\\n  {\"source_entity\": \"Service proxy\", \"description\": \"is bypassed completely for ExternalName services\", \"destination_entity\": \"ExternalName services\"},\\n  {\"source_entity\": \"API\", \"description\": \"can be accessed by defining a service that points to it with the type field set to ExternalName\", \"destination_entity\": \"external-service\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"allows you to modify the service definition and point it to a different service any time later\", \"destination_entity\": \"service definition\"}\\n]\\n```'},\n",
       " {'page': 167,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '135\\nExposing services to external clients\\nYou have a few ways to make a service accessible externally:\\n\\uf0a1Setting the service type to NodePort—For a NodePort service, each cluster node\\nopens a port on the node itself (hence the name) and redirects traffic received\\non that port to the underlying service. The service isn’t accessible only at the\\ninternal cluster IP and port, but also through a dedicated port on all nodes. \\n\\uf0a1Setting the service type to LoadBalancer, an extension of the NodePort type—This\\nmakes the service accessible through a dedicated load balancer, provisioned\\nfrom the cloud infrastructure Kubernetes is running on. The load balancer redi-\\nrects traffic to the node port across all the nodes. Clients connect to the service\\nthrough the load balancer’s IP.\\n\\uf0a1Creating an Ingress resource, a radically different mechanism for exposing multiple ser-\\nvices through a single IP address—It operates at the HTTP level (network layer 7)\\nand can thus offer more features than layer 4 services can. We’ll explain Ingress\\nresources in section 5.4. \\n5.3.1\\nUsing a NodePort service\\nThe first method of exposing a set of pods to external clients is by creating a service\\nand setting its type to NodePort. By creating a NodePort service, you make Kubernetes\\nreserve a port on all its nodes (the same port number is used across all of them) and\\nforward incoming connections to the pods that are part of the service. \\n This is similar to a regular service (their actual type is ClusterIP), but a NodePort\\nservice can be accessed not only through the service’s internal cluster IP, but also\\nthrough any node’s IP and the reserved node port. \\n This will make more sense when you try interacting with a NodePort service.\\nCREATING A NODEPORT SERVICE\\nYou’ll now create a NodePort service to see how you can use it. The following listing\\nshows the YAML for the service.\\n \\nKubernetes cluster\\nExternal client\\nService\\nPod\\nPod\\nPod\\nFigure 5.5\\nExposing a service to external clients\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [External client, Service\n",
       "   Pod Pod Pod\n",
       "   Kubernetes cluster]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'NodePort',\n",
       "    'description': 'A service type that exposes a port on each node in the cluster, making it accessible from outside the cluster.',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'LoadBalancer',\n",
       "    'description': 'An extension of NodePort services that uses a cloud-provisioned load balancer to redirect traffic to the service.',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'Ingress',\n",
       "    'description': 'A resource that operates at the HTTP level (layer 7) and allows exposing multiple services through a single IP address.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Kubernetes cluster',\n",
       "    'description': 'The environment in which the service is running, comprising one or more nodes and pods.',\n",
       "    'category': 'cluster'},\n",
       "   {'entity': 'External client',\n",
       "    'description': 'A device or program that connects to the service from outside the Kubernetes cluster.',\n",
       "    'category': 'client'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'An abstraction layer between a group of pods and the external world, providing access to them through a unique IP address and port number.',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A lightweight process container that runs an application or service within the Kubernetes cluster.',\n",
       "    'category': 'pod'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Setting the service type to LoadBalancer\", \\n   \"description\": \"provides a dedicated load balancer for external clients\",\\n   \"destination_entity\": \"external clients\"},\\n  \\n  {\"source_entity\": \"Setting the service type to NodePort\", \\n   \"description\": \"redirects traffic received on a port to the underlying service\",\\n   \"destination_entity\": \"traffic\"},\\n  \\n  {\"source_entity\": \"Setting the service type to LoadBalancer\", \\n   \"description\": \"provides a dedicated load balancer for external clients\",\\n   \"destination_entity\": \"external clients\"},\\n  \\n  {\"source_entity\": \"LoadBalancer\", \\n   \"description\": \"redirects traffic to the node port across all nodes\",\\n   \"destination_entity\": \"traffic\"},\\n  \\n  {\"source_entity\": \"LoadBalancer\", \\n   \"description\": \"provides a dedicated load balancer for external clients\",\\n   \"destination_entity\": \"external clients\"},\\n  \\n  {\"source_entity\": \"Creating an Ingress resource\", \\n   \"description\": \"exposes multiple services through a single IP address\",\\n   \"destination_entity\": \"multiple services\"},\\n  \\n  {\"source_entity\": \"Kubernetes\", \\n   \"description\": \"reserves a port on all its nodes for NodePort service\",\\n   \"destination_entity\": \"nodes\"},\\n  \\n  {\"source_entity\": \"NodePort service\", \\n   \"description\": \"forwards incoming connections to the pods that are part of the service\",\\n   \"destination_entity\": \"pods\"},\\n  \\n  {\"source_entity\": \"Kubernetes cluster\", \\n   \"description\": \"hosts a NodePort service with reserved node port\",\\n   \"destination_entity\": \"NodePort service\"},\\n  \\n  {\"source_entity\": \"External client\", \\n   \"description\": \"connects to the NodePort service through its IP address\",\\n   \"destination_entity\": \"NodePort service\"},\\n  \\n  {\"source_entity\": \"Kubernetes cluster\", \\n   \"description\": \"hosts multiple pods as part of a Service\",\\n   \"destination_entity\": \"pods\"},\\n  \\n  {\"source_entity\": \"Pod\", \\n   \"description\": \"is part of a Service and receives traffic from the NodePort service\",\\n   \"destination_entity\": \"traffic\"}\\n]\\n```'},\n",
       " {'page': 168,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '136\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: kubia-nodeport\\nspec:\\n  type: NodePort            \\n  ports:\\n  - port: 80                 \\n    targetPort: 8080        \\n    nodePort: 30123        \\n  selector:\\n    app: kubia\\nYou set the type to NodePort and specify the node port this service should be bound to\\nacross all cluster nodes. Specifying the port isn’t mandatory; Kubernetes will choose a\\nrandom port if you omit it. \\nNOTE\\nWhen you create the service in GKE, kubectl prints out a warning\\nabout having to configure firewall rules. We’ll see how to do that soon. \\nEXAMINING YOUR NODEPORT SERVICE\\nLet’s see the basic information of your service to learn more about it:\\n$ kubectl get svc kubia-nodeport\\nNAME             CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\\nkubia-nodeport   10.111.254.223   <nodes>       80:30123/TCP   2m\\nLook at the EXTERNAL-IP column. It shows <nodes>, indicating the service is accessible\\nthrough the IP address of any cluster node. The PORT(S) column shows both the\\ninternal port of the cluster IP (80) and the node port (30123). The service is accessi-\\nble at the following addresses:\\n\\uf0a1\\n10.11.254.223:80\\n\\uf0a1\\n<1st node’s IP>:30123\\n\\uf0a1\\n<2nd node’s IP>:30123, and so on.\\nFigure 5.6 shows your service exposed on port 30123 of both of your cluster nodes\\n(this applies if you’re running this on GKE; Minikube only has a single node, but the\\nprinciple is the same). An incoming connection to one of those ports will be redi-\\nrected to a randomly selected pod, which may or may not be the one running on the\\nnode the connection is being made to. \\n \\n \\n \\nListing 5.11\\nA NodePort service definition: kubia-svc-nodeport.yaml\\nSet the service \\ntype to NodePort.\\nThis is the port of the \\nservice’s internal cluster IP.\\nThis is the target port \\nof the backing pods.\\nThe service will be accessible \\nthrough port 30123 of each of \\nyour cluster nodes.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Service',\n",
       "    'description': 'A Kubernetes resource that enables clients to discover and talk to pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'The API version of the Service resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'The kind of Kubernetes resource, which is a Service',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'The metadata associated with the Service resource',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'The name of the Service, which is kubia-nodeport',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'The specification of the Service resource',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'type',\n",
       "    'description': 'The type of Service, which is NodePort',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ports',\n",
       "    'description': 'A list of ports associated with the Service resource',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'port',\n",
       "    'description': 'A specific port number, which is 80',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'targetPort',\n",
       "    'description': 'The target port of the backing pods, which is 8080',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'nodePort',\n",
       "    'description': 'The node port that the Service will be bound to, which is 30123',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'selector',\n",
       "    'description': 'A selector associated with the Service resource',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'app',\n",
       "    'description': 'The name of the application, which is kubia',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for managing Kubernetes resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'get',\n",
       "    'description': 'A command used to retrieve information about a Service resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'svc kubia-nodeport',\n",
       "    'description': 'The name of the Service, which is kubia-nodeport',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\": \"metadata\", \"description\": \"contains information about the service\", \"destination_entity\": null},\\n {\"source_entity\": \"apiVersion\", \"description\": \"specifies the version of API used by the service\", \"destination_entity\": null},\\n {\"source_entity\": \"kind\", \"description\": \"indicates that this is a Service resource\", \"destination_entity\": null},\\n {\"source_entity\": \"name\", \"description\": \"specifies the name of the service\", \"destination_entity\": \"kubia-nodeport\"},\\n {\"source_entity\": \"type\", \"description\": \"sets the type of the service to NodePort\", \"destination_entity\": \"NodePort\"},\\n {\"source_entity\": \"ports\", \"description\": \"defines the ports used by the service\", \"destination_entity\": null},\\n {\"source_entity\": \"port\", \"description\": \"specifies the port number used by the service\", \"destination_entity\": \"80\"},\\n {\"source_entity\": \"targetPort\", \"description\": \"sets the target port of the backing pods\", \"destination_entity\": \"8080\"},\\n {\"source_entity\": \"selector\", \"description\": \"specifies the selector for the service\", \"destination_entity\": \"app\"},\\n {\"source_entity\": \"kubectl\", \"description\": \"prints out information about the service using kubectl command\", \"destination_entity\": null},\\n {\"source_entity\": \"get\", \"description\": \"retrieves information about the service using get command\", \"destination_entity\": \"svc kubia-nodeport\"},\\n {\"source_entity\": \"nodePort\", \"description\": \"specifies the node port number used by the service\", \"destination_entity\": \"30123\"}]\\n\\nNote that some of the relations have a destination entity as null, which means that the action performed by the source entity does not affect or interact with any other entity.'},\n",
       " {'page': 169,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '137\\nExposing services to external clients\\nA connection received on port 30123 of the first node might be forwarded either to\\nthe pod running on the first node or to one of the pods running on the second node.\\nCHANGING FIREWALL RULES TO LET EXTERNAL CLIENTS ACCESS OUR NODEPORT SERVICE\\nAs I’ve mentioned previously, before you can access your service through the node\\nport, you need to configure the Google Cloud Platform’s firewalls to allow external\\nconnections to your nodes on that port. You’ll do this now:\\n$ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123\\nCreated [https://www.googleapis.com/compute/v1/projects/kubia-\\n1295/global/firewalls/kubia-svc-rule].\\nNAME            NETWORK  SRC_RANGES  RULES      SRC_TAGS  TARGET_TAGS\\nkubia-svc-rule  default  0.0.0.0/0   tcp:30123\\nYou can access your service through port 30123 of one of the node’s IPs. But you need\\nto figure out the IP of a node first. Refer to the sidebar on how to do that.\\n \\n \\n \\nKubernetes cluster\\nExternal client\\nPod\\nNode 2\\nIP: 130.211.99.206\\nNode 1\\nIP: 130.211.97.55\\nPort 30123\\nPort 8080\\nPod\\nPort 8080\\nPod\\nPort 30123\\nPort 8080\\nService\\nFigure 5.6\\nAn external client connecting to a NodePort service either through Node 1 or 2\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes cluster',\n",
       "    'description': 'A group of nodes that run containerized applications.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'External client',\n",
       "    'description': 'An application that connects to a Kubernetes service from outside the cluster.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The smallest unit of deployment in Kubernetes, which contains one or more containers.',\n",
       "    'category': 'Container'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'A machine in a Kubernetes cluster that runs pods.',\n",
       "    'category': 'Hardware'},\n",
       "   {'entity': 'Port 30123',\n",
       "    'description': 'The port number used for the NodePort service.',\n",
       "    'category': 'Network'},\n",
       "   {'entity': 'Port 8080',\n",
       "    'description': 'The port number used for another service.',\n",
       "    'category': 'Network'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A Kubernetes resource that defines a connection point between external clients and containerized applications.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'NodePort',\n",
       "    'description': 'A type of service in Kubernetes that exposes a port on each node in the cluster.',\n",
       "    'category': 'Network'},\n",
       "   {'entity': \"Google Cloud Platform's firewalls\",\n",
       "    'description': 'The firewalls used to control traffic between the cluster and external clients.',\n",
       "    'category': 'Cloud'},\n",
       "   {'entity': 'Firewall rules',\n",
       "    'description': 'Rules that define what traffic is allowed in and out of a Kubernetes cluster.',\n",
       "    'category': 'Network'},\n",
       "   {'entity': 'gcloud compute firewall-rules create',\n",
       "    'description': 'A command used to create a new firewall rule.',\n",
       "    'category': 'Command'},\n",
       "   {'entity': '$ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123',\n",
       "    'description': 'A specific command used to create a new firewall rule for the NodePort service.',\n",
       "    'category': 'Command'}],\n",
       "  'relationships': '[{\"source_entity\": \"Google Cloud Platform\\'s firewalls\", \"description\": \"configure to allow external connections to nodes on port 30123\", \"destination_entity\": \"$ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123\"},\\n\\n{\"source_entity\": \"gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123\", \"description\": \"create a firewall rule to allow external connections to nodes on port 30123\", \"destination_entity\": \"Google Cloud Platform\\'s firewalls\"},\\n\\n{\"source_entity\": \"$ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123\", \"description\": \"execute the command to create a firewall rule\", \"destination_entity\": \"Node\"},\\n\\n{\"source_entity\": \"External client\", \"description\": \"connect to NodePort service through port 30123 of one of the nodes\\' IPs\", \"destination_entity\": \"Node\"},\\n\\n{\"source_entity\": \"Pod\", \"description\": \"run on a node and receive forwarded connections from external clients\", \"destination_entity\": \"Node\"},\\n\\n{\"source_entity\": \"Port 30123\", \"description\": \"be accessed by external clients through NodePort service\", \"destination_entity\": \"External client\"},\\n\\n{\"source_entity\": \"$ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123\", \"description\": \"execute the command to create a firewall rule on port 30123\", \"destination_entity\": \"Service\"},\\n\\n{\"source_entity\": \"Firewall rules\", \"description\": \"be used by Google Cloud Platform\\'s firewalls to allow external connections to nodes on port 30123\", \"destination_entity\": \"$ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123\"},\\n\\n{\"source_entity\": \"NodePort\", \"description\": \"expose services to external clients through ports like 30123 and 8080\", \"destination_entity\": \"External client\"},\\n\\n{\"source_entity\": \"Kubernetes cluster\", \"description\": \"contain multiple nodes running pods that can receive forwarded connections from external clients\", \"destination_entity\": \"Pod\"},\\n\\n{\"source_entity\": \"Port 8080\", \"description\": \"be accessed by external clients through NodePort service\", \"destination_entity\": \"External client\"}]'},\n",
       " {'page': 170,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '138\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\nOnce you know the IPs of your nodes, you can try accessing your service through them:\\n$ curl http://130.211.97.55:30123\\nYou\\'ve hit kubia-ym8or\\n$ curl http://130.211.99.206:30123\\nYou\\'ve hit kubia-xueq1\\nTIP\\nWhen using Minikube, you can easily access your NodePort services\\nthrough your browser by running minikube service <service-name> [-n\\n<namespace>].\\nAs you can see, your pods are now accessible to the whole internet through port 30123\\non any of your nodes. It doesn’t matter what node a client sends the request to. But if\\nyou only point your clients to the first node, when that node fails, your clients can’t\\naccess the service anymore. That’s why it makes sense to put a load balancer in front\\nof the nodes to make sure you’re spreading requests across all healthy nodes and\\nnever sending them to a node that’s offline at that moment. \\n If your Kubernetes cluster supports it (which is mostly true when Kubernetes is\\ndeployed on cloud infrastructure), the load balancer can be provisioned automati-\\ncally by creating a LoadBalancer instead of a NodePort service. We’ll look at this next.\\n5.3.2\\nExposing a service through an external load balancer\\nKubernetes clusters running on cloud providers usually support the automatic provi-\\nsion of a load balancer from the cloud infrastructure. All you need to do is set the\\nUsing JSONPath to get the IPs of all your nodes \\nYou can find the IP in the JSON or YAML descriptors of the nodes. But instead of\\nsifting through the relatively large JSON, you can tell kubectl to print out only the\\nnode IP instead of the whole service definition: \\n$ kubectl get nodes -o jsonpath=\\'{.items[*].status.\\n➥ addresses[?(@.type==\"ExternalIP\")].address}\\'\\n130.211.97.55 130.211.99.206\\nYou’re telling kubectl to only output the information you want by specifying a\\nJSONPath. You’re probably familiar with XPath and how it’s used with XML. JSONPath\\nis basically XPath for JSON. The JSONPath in the previous example instructs kubectl\\nto do the following:\\n\\uf0a1Go through all the elements in the items attribute.\\n\\uf0a1For each element, enter the status attribute.\\n\\uf0a1Filter elements of the addresses attribute, taking only those that have the\\ntype attribute set to ExternalIP.\\n\\uf0a1Finally, print the address attribute of the filtered elements.\\nTo learn more about how to use JSONPath with kubectl, refer to the documentation\\nat http:/\\n/kubernetes.io/docs/user-guide/jsonpath. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'A container orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'NodePort',\n",
       "    'description': 'A service type in Kubernetes that exposes a port on each node in the cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'A tool for running a single-node Kubernetes cluster locally.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'A command-line utility for transferring data to and from a web server.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool for interacting with Kubernetes clusters.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'JSONPath',\n",
       "    'description': 'A query language for JSON data, similar to XPath for XML.',\n",
       "    'category': 'technology'},\n",
       "   {'entity': 'LoadBalancer',\n",
       "    'description': 'An external load balancer that distributes incoming traffic across multiple nodes in a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ExternalIP',\n",
       "    'description': 'The IP address of an external load balancer in a Kubernetes cluster.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A fundamental concept in Kubernetes that enables clients to discover and communicate with pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in a containerized application, similar to a process in traditional computing.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'A machine in a Kubernetes cluster that runs containers and provides resources such as CPU, memory, and storage.',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"enables clients to discover and talk to pods through services\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"prints out only the node IP instead of the whole service definition\",\\n    \"destination_entity\": \"ExternalIP\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"uses JSONPath to filter elements and print specific information\",\\n    \"destination_entity\": \"JSONPath\"\\n  },\\n  {\\n    \"source_entity\": \"curl\",\\n    \"description\": \"accesses a service through its IP address and port number\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Minikube\",\\n    \"description\": \"easily accesses NodePort services through the browser\",\\n    \"destination_entity\": \"NodePort Service\"\\n  },\\n  {\\n    \"source_entity\": \"LoadBalancer\",\\n    \"description\": \"spreads requests across all healthy nodes and never sends them to an offline node\",\\n    \"destination_entity\": \"Node\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes cluster\",\\n    \"description\": \"supports the automatic provision of a load balancer from cloud infrastructure\",\\n    \"destination_entity\": \"LoadBalancer\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"provisions a LoadBalancer automatically by creating a LoadBalancer service\",\\n    \"destination_entity\": \"LoadBalancer\"\\n  }\\n]\\n```'},\n",
       " {'page': 171,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"139\\nExposing services to external clients\\nservice’s type to LoadBalancer instead of NodePort. The load balancer will have its\\nown unique, publicly accessible IP address and will redirect all connections to your\\nservice. You can thus access your service through the load balancer’s IP address. \\n If Kubernetes is running in an environment that doesn’t support LoadBalancer\\nservices, the load balancer will not be provisioned, but the service will still behave like\\na NodePort service. That’s because a LoadBalancer service is an extension of a Node-\\nPort service. You’ll run this example on Google Kubernetes Engine, which supports\\nLoadBalancer services. Minikube doesn’t, at least not as of this writing. \\nCREATING A LOADBALANCER SERVICE\\nTo create a service with a load balancer in front, create the service from the following\\nYAML manifest, as shown in the following listing.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: kubia-loadbalancer\\nspec:\\n  type: LoadBalancer          \\n  ports:\\n  - port: 80\\n    targetPort: 8080\\n  selector:\\n    app: kubia\\nThe service type is set to LoadBalancer instead of NodePort. You’re not specifying a spe-\\ncific node port, although you could (you’re letting Kubernetes choose one instead). \\nCONNECTING TO THE SERVICE THROUGH THE LOAD BALANCER\\nAfter you create the service, it takes time for the cloud infrastructure to create the\\nload balancer and write its IP address into the Service object. Once it does that, the IP\\naddress will be listed as the external IP address of your service:\\n$ kubectl get svc kubia-loadbalancer\\nNAME                 CLUSTER-IP       EXTERNAL-IP      PORT(S)         AGE\\nkubia-loadbalancer   10.111.241.153   130.211.53.173   80:32143/TCP    1m\\nIn this case, the load balancer is available at IP 130.211.53.173, so you can now access\\nthe service at that IP address:\\n$ curl http://130.211.53.173\\nYou've hit kubia-xueq1\\nSuccess! As you may have noticed, this time you didn’t need to mess with firewalls the\\nway you had to before with the NodePort service.\\nListing 5.12\\nA LoadBalancer-type service: kubia-svc-loadbalancer.yaml\\nThis type of service obtains \\na load balancer from the \\ninfrastructure hosting the \\nKubernetes cluster.\\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Service',\n",
       "    'description': 'A Kubernetes Service that exposes a pod as a network service to the outside world.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'LoadBalancer',\n",
       "    'description': 'A type of Kubernetes Service that uses an external load balancer to route traffic to a pod.',\n",
       "    'category': 'Network'},\n",
       "   {'entity': 'NodePort',\n",
       "    'description': 'A type of Kubernetes Service that exposes a pod as a network service on a specific node port.',\n",
       "    'category': 'Network'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'A tool for running a single-node Kubernetes cluster on a local machine.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Google Kubernetes Engine',\n",
       "    'description': 'A managed environment for running Kubernetes clusters in the cloud.',\n",
       "    'category': 'Cloud Service'},\n",
       "   {'entity': 'YAML manifest',\n",
       "    'description': 'A human-readable format for defining Kubernetes resources.',\n",
       "    'category': 'Configuration File'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'A field in a YAML manifest that specifies the version of the Kubernetes API being used.',\n",
       "    'category': 'API Parameter'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'A field in a YAML manifest that specifies the type of Kubernetes resource being defined.',\n",
       "    'category': 'API Parameter'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'A field in a YAML manifest that contains metadata about the resource being defined.',\n",
       "    'category': 'Configuration File'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'A field in the metadata section of a YAML manifest that specifies the name of the resource being defined.',\n",
       "    'category': 'API Parameter'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"LoadBalancer\",\\n    \"description\": \"is used to redirect connections to a service\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"supports LoadBalancer services on Google Kubernetes Engine\",\\n    \"destination_entity\": \"Google Kubernetes Engine\"\\n  },\\n  {\\n    \"source_entity\": \"Minikube\",\\n    \"description\": \"does not support LoadBalancer services as of this writing\",\\n    \"destination_entity\": \"LoadBalancer service\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"behaves like a NodePort service if LoadBalancer service is not supported\",\\n    \"destination_entity\": \"NodePort service\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"can be created with a load balancer using a YAML manifest\",\\n    \"destination_entity\": \"YAML manifest\"\\n  },\\n  {\\n    \"source_entity\": \"metadata\",\\n    \"description\": \"contains information about the service such as name and type\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"apiVersion\",\\n    \"description\": \"specifies the version of the API being used to create the service\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"kind\",\\n    \"description\": \"specifies the type of resource being created (in this case, a Service)\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Google Kubernetes Engine\",\\n    \"description\": \"supports LoadBalancer services and allows access to them through their IP address\",\\n    \"destination_entity\": \"LoadBalancer service\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"can be used to get the external IP address of a service with a load balancer\",\\n    \"destination_entity\": \"external IP address\"\\n  },\\n  {\\n    \"source_entity\": \"curl\",\\n    \"description\": \"can be used to access a service through its external IP address\",\\n    \"destination_entity\": \"service\"\\n  }\\n]\\n```\\n\\nNote that I\\'ve assumed that the entities listed in the input list are all singular, e.g. \"LoadBalancer\" rather than \"LoadBalancers\". If this is not the case, please let me know and I\\'ll adjust the output accordingly.'},\n",
       " {'page': 172,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '140\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\nSee figure 5.7 to see how HTTP requests are delivered to the pod. External clients\\n(curl in your case) connect to port 80 of the load balancer and get routed to the\\nSession affinity and web browsers\\nBecause your service is now exposed externally, you may try accessing it with your\\nweb browser. You’ll see something that may strike you as odd—the browser will hit\\nthe exact same pod every time. Did the service’s session affinity change in the\\nmeantime? With kubectl explain, you can double-check that the service’s session\\naffinity is still set to None, so why don’t different browser requests hit different\\npods, as is the case when using curl?\\nLet me explain what’s happening. The browser is using keep-alive connections and\\nsends all its requests through a single connection, whereas curl opens a new\\nconnection every time. Services work at the connection level, so when a connection to a\\nservice is first opened, a random pod is selected and then all network packets belonging\\nto that connection are all sent to that single pod. Even if session affinity is set to None,\\nusers will always hit the same pod (until the connection is closed).\\nKubernetes cluster\\nExternal client\\nLoad balancer\\nIP: 130.211.53.173:80\\nPod\\nNode 2\\nIP: 130.211.99.206\\nNode 1\\nIP: 130.211.97.55\\nPort 32143\\nPort 8080\\nPod\\nPort 8080\\nPod\\nPort 32143\\nPort 8080\\nService\\nFigure 5.7\\nAn external client connecting to a LoadBalancer service\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Services',\n",
       "    'description': 'enabling clients to discover and talk to pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'containers running on a node',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'HTTP requests',\n",
       "    'description': 'requests sent over the internet using HTTP protocol',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'load balancer',\n",
       "    'description': 'a device that distributes network traffic across multiple servers',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'a command-line tool for transferring data to and from a web server',\n",
       "    'category': 'software/command'},\n",
       "   {'entity': 'web browser',\n",
       "    'description': 'an application that allows users to access and view websites',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Session affinity',\n",
       "    'description': 'the ability of a service to maintain a session with a client across multiple requests',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'a command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'software/command'},\n",
       "   {'entity': 'Kubernetes cluster',\n",
       "    'description': 'a group of machines (nodes) that run containerized applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'External client',\n",
       "    'description': 'an application or service that makes requests to a server',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'IP address',\n",
       "    'description': 'a unique numerical identifier for a device on a network',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'a logical host in a Kubernetes cluster that runs one or more containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'a machine (physical or virtual) that is part of a Kubernetes cluster',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Port',\n",
       "    'description': 'a numerical identifier for a service on a network',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'connection',\n",
       "    'description': 'a channel between two devices over which data is transmitted',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Load balancer\", \"description\": \"redirects external clients to services\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"External client\", \"description\": \"connects to port 80 of load balancer\", \"destination_entity\": \"Load balancer\"},\\n  {\"source_entity\": \"Web browser\", \"description\": \"uses keep-alive connections and sends requests through a single connection\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"curl\", \"description\": \"opens a new connection every time\", \"destination_entity\": \"Load balancer\"},\\n  {\"source_entity\": \"Services\", \"description\": \"works at the connection level, selects a random pod for each connection\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"receives network packets belonging to a single connection\", \"destination_entity\": \"Connection\"},\\n  {\"source_entity\": \"Node\", \"description\": \"hosts pods and provides IP address\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"IP address\", \"description\": \"used by load balancer to connect to pod\", \"destination_entity\": \"Load balancer\"},\\n  {\"source_entity\": \"Kubernetes cluster\", \"description\": \"contains nodes, pods, and services\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to explain service\\'s session affinity\", \"destination_entity\": \"Session affinity\"},\\n  {\"source_entity\": \"Port\", \"description\": \"exposed by load balancer for external clients\", \"destination_entity\": \"External client\"}\\n]'},\n",
       " {'page': 173,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '141\\nExposing services to external clients\\nimplicitly assigned node port on one of the nodes. From there, the connection is for-\\nwarded to one of the pod instances.\\n As already mentioned, a LoadBalancer-type service is a NodePort service with an\\nadditional infrastructure-provided load balancer. If you use kubectl describe to dis-\\nplay additional info about the service, you’ll see that a node port has been selected for\\nthe service. If you were to open the firewall for this port, the way you did in the previ-\\nous section about NodePort services, you could access the service through the node\\nIPs as well.\\nTIP\\nIf you’re using Minikube, even though the load balancer will never be\\nprovisioned, you can still access the service through the node port (at the\\nMinikube VM’s IP address).\\n5.3.3\\nUnderstanding the peculiarities of external connections\\nYou must be aware of several things related to externally originating connections to\\nservices. \\nUNDERSTANDING AND PREVENTING UNNECESSARY NETWORK HOPS\\nWhen an external client connects to a service through the node port (this also\\nincludes cases when it goes through the load balancer first), the randomly chosen\\npod may or may not be running on the same node that received the connection. An\\nadditional network hop is required to reach the pod, but this may not always be\\ndesirable. \\n You can prevent this additional hop by configuring the service to redirect external\\ntraffic only to pods running on the node that received the connection. This is done by\\nsetting the externalTrafficPolicy field in the service’s spec section:\\nspec:\\n  externalTrafficPolicy: Local\\n  ...\\nIf a service definition includes this setting and an external connection is opened\\nthrough the service’s node port, the service proxy will choose a locally running pod. If\\nno local pods exist, the connection will hang (it won’t be forwarded to a random\\nglobal pod, the way connections are when not using the annotation). You therefore\\nneed to ensure the load balancer forwards connections only to nodes that have at\\nleast one such pod.\\n Using this annotation also has other drawbacks. Normally, connections are spread\\nevenly across all the pods, but when using this annotation, that’s no longer the case.\\n Imagine having two nodes and three pods. Let’s say node A runs one pod and\\nnode B runs the other two. If the load balancer spreads connections evenly across the\\ntwo nodes, the pod on node A will receive 50% of all connections, but the two pods on\\nnode B will only receive 25% each, as shown in figure 5.8.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'NodePort',\n",
       "    'description': 'A type of service that exposes a port on every node in the cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'LoadBalancer',\n",
       "    'description': 'An infrastructure-provided load balancer that distributes traffic across multiple nodes.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool used to interact with Kubernetes clusters.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'A tool for running a single-node Kubernetes cluster on a local machine.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'service proxy',\n",
       "    'description': 'A component that handles incoming traffic and directs it to the appropriate pod.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'The basic execution unit in a Kubernetes cluster, which contains one or more containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'externalTrafficPolicy',\n",
       "    'description': 'A field in the service spec section that determines how external traffic is handled.',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'node port',\n",
       "    'description': 'A specific port number on each node that exposes a service to the outside world.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'load balancer',\n",
       "    'description': 'A hardware or software component that distributes incoming traffic across multiple nodes or servers.',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[{\"source_entity\":\"LoadBalancer\",\"description\":\"forwards connections to nodes with available pods\",\"destination_entity\":\"node\"},\\n {\"source_entity\":\"externalTrafficPolicy\",\"description\":\"redirects external traffic only to locally running pods\",\"destination_entity\":\"pod\"},\\n {\"source_entity\":\"service proxy\",\"description\":\"chooses a locally running pod for external connections\",\"destination_entity\":\"pod\"},\\n {\"source_entity\":\"kubectl\",\"description\":\"displays additional info about the service\",\"destination_entity\":\"service\"},\\n {\"source_entity\":\"Minikube\",\"description\":\"provides access to the service through node port even without load balancer\",\"destination_entity\":\"service\"},\\n {\"source_entity\":\"NodePort\",\"description\":\"allows external connections through a randomly chosen pod\",\"destination_entity\":\"pod\"},\\n {\"source_entity\":\"node port\",\"description\":\"exposes services to external clients by forwarding connections to pods\",\"destination_entity\":\"pod\"},\\n {\"source_entity\":\"load balancer\",\"description\":\"forwards connections evenly across all nodes and pods\",\"destination_entity\":\"node\"},\\n {\"source_entity\":\"service proxy\",\"description\":\"redirects external traffic to locally running pods if configured\",\"destination_entity\":\"externalTrafficPolicy\"},\\n {\"source_entity\":\"LoadBalancer\",\"description\":\"is a NodePort service with an additional infrastructure-provided load balancer\",\"destination_entity\":\"NodePort\"}]'},\n",
       " {'page': 174,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '142\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\nBEING AWARE OF THE NON-PRESERVATION OF THE CLIENT’S IP\\nUsually, when clients inside the cluster connect to a service, the pods backing the ser-\\nvice can obtain the client’s IP address. But when the connection is received through a\\nnode port, the packets’ source IP is changed, because Source Network Address Trans-\\nlation (SNAT) is performed on the packets. \\n The backing pod can’t see the actual client’s IP, which may be a problem for some\\napplications that need to know the client’s IP. In the case of a web server, for example,\\nthis means the access log won’t show the browser’s IP.\\n The Local external traffic policy described in the previous section affects the pres-\\nervation of the client’s IP, because there’s no additional hop between the node receiv-\\ning the connection and the node hosting the target pod (SNAT isn’t performed).\\n5.4\\nExposing services externally through an Ingress \\nresource\\nYou’ve now seen two ways of exposing a service to clients outside the cluster, but\\nanother method exists—creating an Ingress resource.\\nDEFINITION\\nIngress (noun)—The act of going in or entering; the right to\\nenter; a means or place of entering; entryway. \\nLet me first explain why you need another way to access Kubernetes services from the\\noutside. \\nUNDERSTANDING WHY INGRESSES ARE NEEDED\\nOne important reason is that each LoadBalancer service requires its own load bal-\\nancer with its own public IP address, whereas an Ingress only requires one, even when\\nproviding access to dozens of services. When a client sends an HTTP request to the\\nIngress, the host and path in the request determine which service the request is for-\\nwarded to, as shown in figure 5.9.\\n \\n50%\\n50%\\n50%\\n25%\\n25%\\nNode A\\nPod\\nNode B\\nPod\\nPod\\nLoad balancer\\nFigure 5.8\\nA Service using \\nthe Local external traffic \\npolicy may lead to uneven \\nload distribution across pods.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Load balancer\\n50% 50%\\n50% 25% 25%\\nPod Pod Pod\\nNode A Node B  \\\n",
       "   0                                   50%\\nPod\\nNode A                \n",
       "   \n",
       "                          Col1  \n",
       "   0  25% 25%\\nPod Pod\\nNode B  ],\n",
       "  'entities': [{'entity': 'SNAT',\n",
       "    'description': 'Source Network Address Translation',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'IP address',\n",
       "    'description': \"client's IP address\",\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'node port',\n",
       "    'description': 'a method of exposing services externally',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Ingress resource',\n",
       "    'description': 'a means or place of entering Kubernetes services from the outside',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'LoadBalancer service',\n",
       "    'description': 'a service that requires its own load balancer with a public IP address',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'HTTP request',\n",
       "    'description': 'an HTTP request sent to an Ingress resource',\n",
       "    'category': 'application'},\n",
       "   {'entity': \"client's IP\",\n",
       "    'description': \"the client's actual IP address\",\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"clients\",\\n    \"description\": \"obtain client\\'s IP address when connecting to a service\",\\n    \"destination_entity\": \"pods backing the service\"\\n  },\\n  {\\n    \"source_entity\": \"backing pod\",\\n    \"description\": \"can\\'t see actual client\\'s IP due to SNAT on packets\\' source IP\",\\n    \"destination_entity\": \"client\\'s IP\"\\n  },\\n  {\\n    \"source_entity\": \"web server\",\\n    \"description\": \"access log won\\'t show browser\\'s IP due to non-preservation of client\\'s IP\",\\n    \"destination_entity\": \"browser\\'s IP\"\\n  },\\n  {\\n    \"source_entity\": \"Local external traffic policy\",\\n    \"description\": \"affects preservation of client\\'s IP by not performing SNAT on packets\",\\n    \"destination_entity\": \"client\\'s IP\"\\n  },\\n  {\\n    \"source_entity\": \"Ingress resource\",\\n    \"description\": \"provides a means to access Kubernetes services from outside the cluster\",\\n    \"destination_entity\": \"Kubernetes services\"\\n  },\\n  {\\n    \"source_entity\": \"client\",\\n    \"description\": \"sends an HTTP request to the Ingress, which determines service to forward to\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"Ingress resource\",\\n    \"description\": \"only requires one public IP address even when providing access to dozens of services\",\\n    \"destination_entity\": \"LoadBalancer service\"\\n  },\\n  {\\n    \"source_entity\": \"SNAT (Source Network Address Translation)\",\\n    \"description\": \"performs on packets\\' source IP, changing it from client\\'s IP to node port\\'s IP\",\\n    \"destination_entity\": \"client\\'s IP\"\\n  }\\n]\\n\\nNote that some of the relations may seem obvious or trivial, but I\\'ve tried to extract them as accurately and explicitly as possible based on the document text. Let me know if you\\'d like me to clarify any of these relations!'},\n",
       " {'page': 175,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '143\\nExposing services externally through an Ingress resource\\nIngresses operate at the application layer of the network stack (HTTP) and can pro-\\nvide features such as cookie-based session affinity and the like, which services can’t.\\nUNDERSTANDING THAT AN INGRESS CONTROLLER IS REQUIRED\\nBefore we go into the features an Ingress object provides, let me emphasize that to\\nmake Ingress resources work, an Ingress controller needs to be running in the cluster.\\nDifferent Kubernetes environments use different implementations of the controller,\\nbut several don’t provide a default controller at all. \\n For example, Google Kubernetes Engine uses Google Cloud Platform’s own HTTP\\nload-balancing features to provide the Ingress functionality. Initially, Minikube didn’t\\nprovide a controller out of the box, but it now includes an add-on that can be enabled\\nto let you try out the Ingress functionality. Follow the instructions in the following\\nsidebar to ensure it’s enabled.\\nEnabling the Ingress add-on in Minikube\\nIf you’re using Minikube to run the examples in this book, you’ll need to ensure the\\nIngress add-on is enabled. You can check whether it is by listing all the add-ons:\\n$ minikube addons list\\n- default-storageclass: enabled\\n- kube-dns: enabled\\n- heapster: disabled\\n- ingress: disabled               \\n- registry-creds: disabled\\n- addon-manager: enabled\\n- dashboard: enabled\\nYou’ll learn about what these add-ons are throughout the book, but it should be\\npretty clear what the dashboard and the kube-dns add-ons do. Enable the Ingress\\nadd-on so you can see Ingresses in action:\\n$ minikube addons enable ingress\\ningress was successfully enabled\\nPod\\nPod\\nPod\\nPod\\nPod\\nPod\\nPod\\nPod\\nPod\\nPod\\nPod\\nPod\\nIngress\\nClient\\nService\\nkubia.example.com/kubia\\nfoo.example.com\\nkubia.example.com/foo\\nService\\nbar.example.com\\nService\\nService\\nFigure 5.9\\nMultiple services can be exposed through a single Ingress.\\nThe Ingress add-on \\nisn’t enabled.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  kubia.example.com/kubia\\nService\\nkubia.example.com/foo\\nService\\nClient Ingress\\nfoo.example.com\\nService\\nbar.example.com\\nService  \\\n",
       "   0                                               None                                                                                     \n",
       "   1                                               None                                                                                     \n",
       "   2                                               None                                                                                     \n",
       "   \n",
       "      Pod Pod Pod  \n",
       "   0  Pod Pod Pod  \n",
       "   1  Pod Pod Pod  \n",
       "   2  Pod Pod Pod  ],\n",
       "  'entities': [{'entity': 'Ingress',\n",
       "    'description': 'an object that provides features such as cookie-based session affinity at the application layer of the network stack (HTTP)',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'controller',\n",
       "    'description': 'a required component to make Ingress resources work, which can be implemented differently in various Kubernetes environments',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'a platform that allows you to try out the Ingress functionality with an add-on that needs to be enabled',\n",
       "    'category': 'platform'},\n",
       "   {'entity': 'Ingress controller',\n",
       "    'description': 'a component required for making Ingress resources work, which can be different in various Kubernetes environments',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes Engine',\n",
       "    'description': 'a managed environment provided by Google Cloud Platform that uses its own HTTP load-balancing features to provide the Ingress functionality',\n",
       "    'category': 'cloud service'},\n",
       "   {'entity': 'Google Cloud Platform',\n",
       "    'description': 'a cloud platform that provides its own HTTP load-balancing features to provide the Ingress functionality for Kubernetes Engine',\n",
       "    'category': 'cloud service'},\n",
       "   {'entity': 'HTTP load-balancing features',\n",
       "    'description': 'features provided by Google Cloud Platform to distribute traffic across multiple instances',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Minikube addons list',\n",
       "    'description': 'a command that lists all the add-ons available for Minikube, including the Ingress add-on',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'minikube addons enable ingress',\n",
       "    'description': 'a command to enable the Ingress add-on in Minikube',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a container that can be used to expose a service through an Ingress',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'an object that represents a set of pods that provide a specific network resource, such as HTTP requests',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubia.example.com/kubia',\n",
       "    'description': 'a URL for the kubia service',\n",
       "    'category': 'URL'},\n",
       "   {'entity': 'foo.example.com',\n",
       "    'description': 'a URL for another service',\n",
       "    'category': 'URL'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Ingress controller\", \"description\": \"Requires to be running in the cluster for Ingress resources to work\", \"destination_entity\": \"cluster\"},\\n  {\"source_entity\": \"Minikube addons list\", \"description\": \"Lists all add-ons, including ingress\", \"destination_entity\": \"add-ons\"},\\n  {\"source_entity\": \"minikube addons enable ingress\", \"description\": \"Enables the Ingress add-on in Minikube\", \"destination_entity\": \"Ingress add-on\"},\\n  {\"source_entity\": \"ingress was successfully enabled\", \"description\": \"Result of enabling the Ingress add-on\", \"destination_entity\": \"Ingress add-on\"},\\n  {\"source_entity\": \"Kubernetes Engine\", \"description\": \"Uses Google Cloud Platform\\'s own HTTP load-balancing features to provide the Ingress functionality\", \"destination_entity\": \"Google Cloud Platform\"},\\n  {\"source_entity\": \"Minikube\", \"description\": \"Requires the Ingress add-on to be enabled for Ingress resources to work\", \"destination_entity\": \"Ingress add-on\"},\\n  {\"source_entity\": \"HTTP load-balancing features\", \"description\": \"Used by Kubernetes Engine to provide the Ingress functionality\", \"destination_entity\": \"Kubernetes Engine\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"Can be exposed through an Ingress resource, providing features like cookie-based session affinity\", \"destination_entity\": \"Ingress resource\"},\\n  {\"source_entity\": \"Service\", \"description\": \"Can be exposed through a single Ingress resource\", \"destination_entity\": \"Ingress resource\"},\\n  {\"source_entity\": \"Kubia.example.com/kubia\", \"description\": \"Exposed as a service through an Ingress resource\", \"destination_entity\": \"service\"},\\n  {\"source_entity\": \"foo.example.com\", \"description\": \"Exposed as a service through an Ingress resource\", \"destination_entity\": \"service\"}\\n]\\n```'},\n",
       " {'page': 176,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '144\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\nTIP\\nThe --all-namespaces option mentioned in the sidebar is handy when\\nyou don’t know what namespace your pod (or other type of resource) is in, or\\nif you want to list resources across all namespaces.\\n5.4.1\\nCreating an Ingress resource\\nYou’ve confirmed there’s an Ingress controller running in your cluster, so you can\\nnow create an Ingress resource. The following listing shows what the YAML manifest\\nfor the Ingress looks like.\\napiVersion: extensions/v1beta1\\nkind: Ingress\\nmetadata:\\n  name: kubia\\nspec:\\n  rules:\\n  - host: kubia.example.com             \\n    http:\\n      paths:\\n      - path: /                           \\n        backend:\\n          serviceName: kubia-nodeport     \\n          servicePort: 80                 \\nThis defines an Ingress with a single rule, which makes sure all HTTP requests received\\nby the Ingress controller, in which the host kubia.example.com is requested, will be\\nsent to the kubia-nodeport service on port 80. \\n(continued)\\nThis should have spun up an Ingress controller as another pod. Most likely, the\\ncontroller pod will be in the kube-system namespace, but not necessarily, so list all\\nthe running pods across all namespaces by using the --all-namespaces option:\\n$ kubectl get po --all-namespaces\\nNAMESPACE    NAME                            READY  STATUS    RESTARTS AGE\\ndefault      kubia-rsv5m                     1/1    Running   0        13h\\ndefault      kubia-fe4ad                     1/1    Running   0        13h\\ndefault      kubia-ke823                     1/1    Running   0        13h\\nkube-system  default-http-backend-5wb0h      1/1    Running   0        18m\\nkube-system  kube-addon-manager-minikube     1/1    Running   3        6d\\nkube-system  kube-dns-v20-101vq              3/3    Running   9        6d\\nkube-system  kubernetes-dashboard-jxd9l      1/1    Running   3        6d\\nkube-system  nginx-ingress-controller-gdts0  1/1    Running   0        18m\\nAt the bottom of the output, you see the Ingress controller pod. The name suggests\\nthat Nginx (an open-source HTTP server and reverse proxy) is used to provide the\\nIngress functionality.\\nListing 5.13\\nAn Ingress resource definition: kubia-ingress.yaml\\nThis Ingress maps the \\nkubia.example.com domain \\nname to your service.\\nAll requests will be sent to \\nport 80 of the kubia-\\nnodeport service.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'Container runtime',\n",
       "    'category': 'software'},\n",
       "   {'entity': '--all-namespaces',\n",
       "    'description': 'Option to list resources across all namespaces',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Ingress controller',\n",
       "    'description': 'Pod that provides Ingress functionality',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubia',\n",
       "    'description': 'Service name',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubia-nodeport',\n",
       "    'description': 'Service name',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes clusters',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Ingress resource',\n",
       "    'description': 'Kubernetes object that defines Ingress rules',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Nginx',\n",
       "    'description': 'Open-source HTTP server and reverse proxy',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\": \"Ingress controller\", \"description\": \"spun up an Ingress controller as another pod\", \"destination_entity\": \"pod\"},\\n\\n{\"source_entity\": \"kubectl\", \"description\": \"list all the running pods across all namespaces\", \"destination_entity\": \"pods\"},\\n\\n{\"source_entity\": \"kubectl\", \"description\": \"used to provide the Ingress functionality\", \"destination_entity\": \"Nginx\"},\\n\\n{\"source_entity\": \"Ingress resource\", \"description\": \"mapped the kubia.example.com domain name to your service\", \"destination_entity\": \"Kubernetes\"},\\n\\n{\"source_entity\": \"kubia-nodeport service\", \"description\": \"sent to the kubia-nodeport service on port 80\", \"destination_entity\": \"Ingress resource\"},\\n\\n{\"source_entity\": \"--all-namespaces option\", \"description\": \"list resources across all namespaces\", \"destination_entity\": \"resources\"},\\n\\n{\"source_entity\": \"Kubernetes\", \"description\": \"used to create an Ingress resource\", \"destination_entity\": \"Ingress resource\"},\\n\\n{\"source_entity\": \"Nginx\", \"description\": \"used to provide the Ingress functionality for the Ingress controller pod\", \"destination_entity\": \"Ingress controller pod\"},\\n\\n{\"source_entity\": \"Docker\", \"description\": \"used in conjunction with Kubernetes to create a containerized environment\", \"destination_entity\": \"Kubernetes\"}]\\n\\nNote: The list only contains the JSON objects representing the extracted relations, without any other words or characters.'},\n",
       " {'page': 177,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"145\\nExposing services externally through an Ingress resource\\nNOTE\\nIngress controllers on cloud providers (in GKE, for example) require\\nthe Ingress to point to a NodePort service. But that’s not a requirement of\\nKubernetes itself.\\n5.4.2\\nAccessing the service through the Ingress\\nTo access your service through http:/\\n/kubia.example.com, you’ll need to make sure\\nthe domain name resolves to the IP of the Ingress controller. \\nOBTAINING THE IP ADDRESS OF THE INGRESS\\nTo look up the IP, you need to list Ingresses:\\n$ kubectl get ingresses\\nNAME      HOSTS               ADDRESS          PORTS     AGE\\nkubia     kubia.example.com   192.168.99.100   80        29m\\nNOTE\\nWhen running on cloud providers, the address may take time to appear,\\nbecause the Ingress controller provisions a load balancer behind the scenes.\\nThe IP is shown in the ADDRESS column. \\nENSURING THE HOST CONFIGURED IN THE INGRESS POINTS TO THE INGRESS’ IP ADDRESS\\nOnce you know the IP, you can then either configure your DNS servers to resolve\\nkubia.example.com to that IP or you can add the following line to /etc/hosts (or\\nC:\\\\windows\\\\system32\\\\drivers\\\\etc\\\\hosts on Windows):\\n192.168.99.100    kubia.example.com\\nACCESSING PODS THROUGH THE INGRESS\\nEverything is now set up, so you can access the service at http:/\\n/kubia.example.com\\n(using a browser or curl):\\n$ curl http://kubia.example.com\\nYou've hit kubia-ke823\\nYou’ve successfully accessed the service through an Ingress. Let’s take a better look at\\nhow that unfolded.\\nUNDERSTANDING HOW INGRESSES WORK\\nFigure 5.10 shows how the client connected to one of the pods through the Ingress\\ncontroller. The client first performed a DNS lookup of kubia.example.com, and the\\nDNS server (or the local operating system) returned the IP of the Ingress controller.\\nThe client then sent an HTTP request to the Ingress controller and specified\\nkubia.example.com in the Host header. From that header, the controller determined\\nwhich service the client is trying to access, looked up the pod IPs through the End-\\npoints object associated with the service, and forwarded the client’s request to one of\\nthe pods.\\n As you can see, the Ingress controller didn’t forward the request to the service. It\\nonly used it to select a pod. Most, if not all, controllers work like this. \\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Ingress',\n",
       "    'description': 'a resource that allows you to expose services externally through HTTP requests',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'an open-source container orchestration system',\n",
       "    'category': 'software/container management'},\n",
       "   {'entity': 'GKE',\n",
       "    'description': 'Google Kubernetes Engine, a managed container platform service',\n",
       "    'category': 'cloud/service'},\n",
       "   {'entity': 'NodePort',\n",
       "    'description': 'a type of service in Kubernetes that exposes a port on every node',\n",
       "    'category': 'software/container management'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the command-line tool for interacting with Kubernetes clusters',\n",
       "    'category': 'software/command line tool'},\n",
       "   {'entity': 'ingresses',\n",
       "    'description': 'a list of Ingress resources in the cluster',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'hosts',\n",
       "    'description': 'a field in an Ingress resource that specifies the domain name for which the service is exposed',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'ADDRESS',\n",
       "    'description': 'a column in the output of `kubectl get ingresses` that shows the IP address of the Ingress controller',\n",
       "    'category': 'software/command line tool'},\n",
       "   {'entity': '/etc/hosts',\n",
       "    'description': 'a file on Linux systems where hostnames and IP addresses are mapped',\n",
       "    'category': 'hardware/file system'},\n",
       "   {'entity': 'C:\\\\windows\\\\system32\\\\drivers\\\\etc\\\\hosts',\n",
       "    'description': 'the equivalent file on Windows systems for mapping hostnames to IP addresses',\n",
       "    'category': 'hardware/file system'},\n",
       "   {'entity': 'DNS',\n",
       "    'description': 'Domain Name System, a service that translates hostnames to IP addresses',\n",
       "    'category': 'software/networking'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'a command-line tool for transferring data with HTTP requests',\n",
       "    'category': 'software/command line tool'},\n",
       "   {'entity': 'HTTP',\n",
       "    'description': 'the protocol used for communication between client and server over the web',\n",
       "    'category': 'hardware/protocol'}],\n",
       "  'relationships': '[\\n    {\\n        \"source_entity\": \"kubectl\",\\n        \"description\": \"list Ingresses to obtain IP address of Ingress controller\",\\n        \"destination_entity\": \"ingresses\"\\n    },\\n    {\\n        \"source_entity\": \"DNS\",\\n        \"description\": \"perform DNS lookup of kubia.example.com and return IP of Ingress controller\",\\n        \"destination_entity\": \"/etc/hosts\"\\n    },\\n    {\\n        \"source_entity\": \"Ingress controller\",\\n        \"description\": \"forward HTTP request from client to one of the pods based on Host header\",\\n        \"destination_entity\": \"pods\"\\n    },\\n    {\\n        \"source_entity\": \"kubectl\",\\n        \"description\": \"obtain IP address of Ingress controller using kubectl get ingresses command\",\\n        \"destination_entity\": \"ADDRESS\"\\n    },\\n    {\\n        \"source_entity\": \"Ingress controller\",\\n        \"description\": \"provision a load balancer behind the scenes when running on cloud providers\",\\n        \"destination_entity\": \"load balancer\"\\n    },\\n    {\\n        \"source_entity\": \"DNS\",\\n        \"description\": \"resolve kubia.example.com to IP address of Ingress controller\",\\n        \"destination_entity\": \"/etc/hosts\"\\n    },\\n    {\\n        \"source_entity\": \"client\",\\n        \"description\": \"perform HTTP request to Ingress controller with Host header set to kubia.example.com\",\\n        \"destination_entity\": \"Ingress controller\"\\n    },\\n    {\\n        \"source_entity\": \"kubectl\",\\n        \"description\": \"create NodePort service for Ingress controller on cloud providers\",\\n        \"destination_entity\": \"NodePort\"\\n    },\\n    {\\n        \"source_entity\": \"client\",\\n        \"description\": \"access the service through HTTP request to kubia.example.com\",\\n        \"destination_entity\": \"service\"\\n    },\\n    {\\n        \"source_entity\": \"Ingress controller\",\\n        \"description\": \"determine which service the client is trying to access based on Host header\",\\n        \"destination_entity\": \"service\"\\n    }\\n]\\n```'},\n",
       " {'page': 178,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '146\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\n5.4.3\\nExposing multiple services through the same Ingress\\nIf you look at the Ingress spec closely, you’ll see that both rules and paths are arrays,\\nso they can contain multiple items. An Ingress can map multiple hosts and paths to\\nmultiple services, as you’ll see next. Let’s focus on paths first. \\nMAPPING DIFFERENT SERVICES TO DIFFERENT PATHS OF THE SAME HOST\\nYou can map multiple paths on the same host to different services, as shown in the\\nfollowing listing.\\n...\\n  - host: kubia.example.com\\n    http:\\n      paths:\\n      - path: /kubia                \\n        backend:                    \\n          serviceName: kubia        \\n          servicePort: 80           \\n      - path: /foo                \\n        backend:                  \\n          serviceName: bar        \\n          servicePort: 80         \\nIn this case, requests will be sent to two different services, depending on the path in\\nthe requested URL. Clients can therefore reach two different services through a single\\nIP address (that of the Ingress controller).\\nListing 5.14\\nIngress exposing multiple services on same host, but different paths\\nNode A\\nPod\\nNode B\\nPod\\nPod\\nIngress\\ncontroller\\nEndpoints\\nService\\nIngress\\nClient\\n2. Client sends HTTP GET\\nrequest with header\\nHost: kubia.example.com\\n3. Controller sends\\nrequest to one of\\nthe pods.\\n1. Client looks up\\nkubia.example.com\\nDNS\\nFigure 5.10\\nAccessing pods through an Ingress\\nRequests to kubia.example.com/kubia \\nwill be routed to the kubia service.\\nRequests to kubia.example.com/bar \\nwill be routed to the bar service.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  3. Controller sends\\nrequest to one of\\nIngress the pods.\\ncontroller\\nPod\\nNode A  \\\n",
       "   0                                               None                                   \n",
       "   1                                               None                                   \n",
       "   \n",
       "     Col1  Col2  \n",
       "   0       None  \n",
       "   1       None  ],\n",
       "  'entities': [{'entity': 'Ingress',\n",
       "    'description': 'An API object that manages external access to services in a cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Host',\n",
       "    'description': 'A domain name or IP address.',\n",
       "    'category': 'hardware/network'},\n",
       "   {'entity': 'Path',\n",
       "    'description': 'A URL path.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A named port on a pod that can be accessed by clients.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit of a containerized application.',\n",
       "    'category': 'hardware/container'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'A physical or virtual machine running the Kubernetes control plane or worker.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Endpoints',\n",
       "    'description': 'A collection of IP addresses and ports for a service.',\n",
       "    'category': 'software/database'},\n",
       "   {'entity': 'Client',\n",
       "    'description': 'An application that sends HTTP requests to an Ingress.',\n",
       "    'category': 'software/application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Client\",\\n    \"description\": \"sends HTTP GET request to Ingress controller\",\\n    \"destination_entity\": \"Ingress\"\\n  },\\n  {\\n    \"source_entity\": \"Controller\",\\n    \"description\": \"sends request to one of the pods\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Client\",\\n    \"description\": \"looks up kubia.example.com DNS\",\\n    \"destination_entity\": \"DNS\"\\n  },\\n  {\\n    \"source_entity\": \"Controller\",\\n    \"description\": \"routes requests to different services based on path\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Ingress\",\\n    \"description\": \"maps multiple hosts and paths to multiple services\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Path\",\\n    \"description\": \"determines which service to route requests to\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Ingress controller\",\\n    \"description\": \"exposes multiple services through the same Ingress\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Client\",\\n    \"description\": \"reaches two different services through a single IP address\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Controller\",\\n    \"description\": \"sends requests to pods based on Ingress rules\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Ingress\",\\n    \"description\": \"uses backend serviceName and servicePort to route requests\",\\n    \"destination_entity\": \"Endpoints\"\\n  }\\n]'},\n",
       " {'page': 179,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '147\\nExposing services externally through an Ingress resource\\nMAPPING DIFFERENT SERVICES TO DIFFERENT HOSTS\\nSimilarly, you can use an Ingress to map to different services based on the host in the\\nHTTP request instead of (only) the path, as shown in the next listing.\\nspec:\\n  rules:\\n  - host: foo.example.com          \\n    http:\\n      paths:\\n      - path: / \\n        backend:\\n          serviceName: foo         \\n          servicePort: 80\\n  - host: bar.example.com          \\n    http:\\n      paths:\\n      - path: /\\n        backend:\\n          serviceName: bar         \\n          servicePort: 80\\nRequests received by the controller will be forwarded to either service foo or bar,\\ndepending on the Host header in the request (the way virtual hosts are handled in\\nweb servers). DNS needs to point both the foo.example.com and the bar.exam-\\nple.com domain names to the Ingress controller’s IP address. \\n5.4.4\\nConfiguring Ingress to handle TLS traffic\\nYou’ve seen how an Ingress forwards HTTP traffic. But what about HTTPS? Let’s take\\na quick look at how to configure Ingress to support TLS. \\nCREATING A TLS CERTIFICATE FOR THE INGRESS\\nWhen a client opens a TLS connection to an Ingress controller, the controller termi-\\nnates the TLS connection. The communication between the client and the controller\\nis encrypted, whereas the communication between the controller and the backend\\npod isn’t. The application running in the pod doesn’t need to support TLS. For exam-\\nple, if the pod runs a web server, it can accept only HTTP traffic and let the Ingress\\ncontroller take care of everything related to TLS. To enable the controller to do that,\\nyou need to attach a certificate and a private key to the Ingress. The two need to be\\nstored in a Kubernetes resource called a Secret, which is then referenced in the\\nIngress manifest. We’ll explain Secrets in detail in chapter 7. For now, you’ll create the\\nSecret without paying too much attention to it.\\n First, you need to create the private key and certificate:\\n$ openssl genrsa -out tls.key 2048\\n$ openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj \\n➥ /CN=kubia.example.com\\nListing 5.15\\nIngress exposing multiple services on different hosts\\nRequests for \\nfoo.example.com will be \\nrouted to service foo.\\nRequests for \\nbar.example.com will be \\nrouted to service bar.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Ingress',\n",
       "    'description': 'Resource for exposing services externally',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'HTTP request',\n",
       "    'description': 'Request sent to the server using HTTP protocol',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'Host header',\n",
       "    'description': 'Header in HTTP request containing host information',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'Service foo',\n",
       "    'description': 'Service exposed at host foo.example.com',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service bar',\n",
       "    'description': 'Service exposed at host bar.example.com',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'foo.example.com',\n",
       "    'description': 'Domain name pointing to service foo',\n",
       "    'category': 'domain'},\n",
       "   {'entity': 'bar.example.com',\n",
       "    'description': 'Domain name pointing to service bar',\n",
       "    'category': 'domain'},\n",
       "   {'entity': 'TLS connection',\n",
       "    'description': 'Connection encrypted using Transport Layer Security protocol',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'Secret',\n",
       "    'description': 'Kubernetes resource for storing sensitive data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Certificate and private key',\n",
       "    'description': 'Credentials for encrypting TLS traffic',\n",
       "    'category': 'credentials'},\n",
       "   {'entity': 'Ingress controller',\n",
       "    'description': 'Component responsible for routing HTTP requests to services',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Backend pod',\n",
       "    'description': 'Container running the application',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Ingress\",\\n    \"description\": \"map different services to different hosts based on host in HTTP request\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Ingress controller\",\\n    \"description\": \"forward requests to service foo or bar based on Host header\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"TLS connection\",\\n    \"description\": \"terminate TLS connection between client and Ingress controller\",\\n    \"destination_entity\": \"Ingress controller\"\\n  },\\n  {\\n    \"source_entity\": \"Certificate and private key\",\\n    \"description\": \"enable Ingress to handle TLS traffic\",\\n    \"destination_entity\": \"Ingress\"\\n  },\\n  {\\n    \"source_entity\": \"HTTP request\",\\n    \"description\": \"forwarded to either service foo or bar based on Host header\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Host header\",\\n    \"description\": \"determine which service to route requests to\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Ingress controller\",\\n    \"description\": \"terminate TLS connection\",\\n    \"destination_entity\": \"TLS connection\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"manage Secrets for Certificate and private key\",\\n    \"destination_entity\": \"Secret\"\\n  },\\n  {\\n    \"source_entity\": \"Backend pod\",\\n    \"description\": \"does not need to support TLS\",\\n    \"destination_entity\": \"Ingress controller\"\\n  },\\n  {\\n    \"source_entity\": \"foo.example.com\",\\n    \"description\": \"requests forwarded to service foo\",\\n    \"destination_entity\": \"Service foo\"\\n  },\\n  {\\n    \"source_entity\": \"bar.example.com\",\\n    \"description\": \"requests forwarded to service bar\",\\n    \"destination_entity\": \"Service bar\"\\n  }\\n]'},\n",
       " {'page': 180,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '148\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\nThen you create the Secret from the two files like this:\\n$ kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key\\nsecret \"tls-secret\" created\\nThe private key and the certificate are now stored in the Secret called tls-secret.\\nNow, you can update your Ingress object so it will also accept HTTPS requests for\\nkubia.example.com. The Ingress manifest should now look like the following listing.\\napiVersion: extensions/v1beta1\\nkind: Ingress\\nmetadata:\\n  name: kubia\\nspec:\\n  tls:                           \\n  - hosts:                        \\n    - kubia.example.com           \\n    secretName: tls-secret       \\n  rules:\\n  - host: kubia.example.com\\n    http:\\n      paths:\\n      - path: /\\n        backend:\\n          serviceName: kubia-nodeport\\n          servicePort: 80\\nTIP\\nInstead of deleting the Ingress and re-creating it from the new file, you\\ncan invoke kubectl apply -f kubia-ingress-tls.yaml, which updates the\\nIngress resource with what’s specified in the file.\\nSigning certificates through the CertificateSigningRequest resource\\nInstead of signing the certificate ourselves, you can get the certificate signed by\\ncreating a CertificateSigningRequest (CSR) resource. Users or their applications\\ncan create a regular certificate request, put it into a CSR, and then either a human\\noperator or an automated process can approve the request like this:\\n$ kubectl certificate approve <name of the CSR> \\nThe signed certificate can then be retrieved from the CSR’s status.certificate\\nfield. \\nNote that a certificate signer component must be running in the cluster; otherwise\\ncreating CertificateSigningRequest and approving or denying them won’t have\\nany effect.\\nListing 5.16\\nIngress handling TLS traffic: kubia-ingress-tls.yaml\\nThe whole TLS configuration \\nis under this attribute.\\nTLS connections will be accepted for \\nthe kubia.example.com hostname.\\nThe private key and the certificate \\nshould be obtained from the tls-\\nsecret you created previously.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'Container runtime',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Secret',\n",
       "    'description': 'A Kubernetes resource that stores sensitive information, such as private keys and certificates',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Ingress',\n",
       "    'description': 'A Kubernetes resource that enables external access to services running within a cluster',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'CertificateSigningRequest',\n",
       "    'description': 'A Kubernetes resource used for signing certificates through an automated process',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with a Kubernetes cluster',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'A YAML attribute that specifies the API version of an object',\n",
       "    'category': 'Attribute'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'A YAML attribute that specifies the type of an object',\n",
       "    'category': 'Attribute'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'A Kubernetes API resource that provides metadata about an object',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'A Kubernetes API attribute that specifies the desired state of an object',\n",
       "    'category': 'Attribute'},\n",
       "   {'entity': 'hosts',\n",
       "    'description': 'A YAML attribute used in Ingress objects to specify hostnames for TLS connections',\n",
       "    'category': 'Attribute'},\n",
       "   {'entity': 'secretName',\n",
       "    'description': 'A YAML attribute used in Ingress objects to reference a Secret resource',\n",
       "    'category': 'Attribute'},\n",
       "   {'entity': 'servicePort',\n",
       "    'description': 'A Kubernetes API attribute that specifies the port number of a Service',\n",
       "    'category': 'Attribute'},\n",
       "   {'entity': 'serviceName',\n",
       "    'description': 'A Kubernetes API attribute that references a Service resource',\n",
       "    'category': 'Attribute'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"create a secret with TLS configuration\",\\n    \"destination_entity\": \"Secret\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"apply updates to an Ingress resource\",\\n    \"destination_entity\": \"Ingress\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl certificate approve\",\\n    \"description\": \"approve a CertificateSigningRequest\",\\n    \"destination_entity\": \"CertificateSigningRequest\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl create secret tls\",\\n    \"description\": \"create a Secret with TLS configuration\",\\n    \"destination_entity\": \"Secret\"\\n  },\\n  {\\n    \"source_entity\": \"Ingress\",\\n    \"description\": \"handle TLS traffic for a specific hostname\",\\n    \"destination_entity\": \"TLS\"\\n  },\\n  {\\n    \"source_entity\": \"hosts\",\\n    \"description\": \"specify the hostnames to handle TLS traffic\",\\n    \"destination_entity\": \"Ingress\"\\n  },\\n  {\\n    \"source_entity\": \"apiVersion\",\\n    \"description\": \"specify the API version of the Ingress resource\",\\n    \"destination_entity\": \"Ingress\"\\n  },\\n  {\\n    \"source_entity\": \"kind\",\\n    \"description\": \"specify the kind of the Ingress resource\",\\n    \"destination_entity\": \"Ingress\"\\n  },\\n  {\\n    \"source_entity\": \"servicePort\",\\n    \"description\": \"specify the service port to use for the Ingress resource\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"serviceName\",\\n    \"description\": \"specify the name of the Service to use for the Ingress resource\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"secretName\",\\n    \"description\": \"specify the name of the Secret to use for TLS configuration\",\\n    \"destination_entity\": \"Secret\"\\n  },\\n  {\\n    \"source_entity\": \"CertificateSigningRequest\",\\n    \"description\": \"sign a certificate through an automated process\",\\n    \"destination_entity\": \"certificate signer component\"\\n  }\\n]\\n```\\n\\nNote that I\\'ve only extracted relations between entities within the document page and the provided list of entities. Let me know if you need any further assistance!'},\n",
       " {'page': 181,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"149\\nSignaling when a pod is ready to accept connections\\nYou can now use HTTPS to access your service through the Ingress:\\n$ curl -k -v https://kubia.example.com/kubia\\n* About to connect() to kubia.example.com port 443 (#0)\\n...\\n* Server certificate:\\n*   subject: CN=kubia.example.com\\n...\\n> GET /kubia HTTP/1.1\\n> ...\\nYou've hit kubia-xueq1\\nThe command’s output shows the response from the app, as well as the server certifi-\\ncate you configured the Ingress with.\\nNOTE\\nSupport for Ingress features varies between the different Ingress con-\\ntroller implementations, so check the implementation-specific documenta-\\ntion to see what’s supported. \\nIngresses are a relatively new Kubernetes feature, so you can expect to see many\\nimprovements and new features in the future. Although they currently support only\\nL7 (HTTP/HTTPS) load balancing, support for L4 load balancing is also planned.\\n5.5\\nSignaling when a pod is ready to accept connections\\nThere’s one more thing we need to cover regarding both Services and Ingresses.\\nYou’ve already learned that pods are included as endpoints of a service if their labels\\nmatch the service’s pod selector. As soon as a new pod with proper labels is created, it\\nbecomes part of the service and requests start to be redirected to the pod. But what if\\nthe pod isn’t ready to start serving requests immediately? \\n The pod may need time to load either configuration or data, or it may need to per-\\nform a warm-up procedure to prevent the first user request from taking too long and\\naffecting the user experience. In such cases you don’t want the pod to start receiving\\nrequests immediately, especially when the already-running instances can process\\nrequests properly and quickly. It makes sense to not forward requests to a pod that’s in\\nthe process of starting up until it’s fully ready.\\n5.5.1\\nIntroducing readiness probes\\nIn the previous chapter you learned about liveness probes and how they help keep\\nyour apps healthy by ensuring unhealthy containers are restarted automatically.\\nSimilar to liveness probes, Kubernetes allows you to also define a readiness probe\\nfor your pod.\\n The readiness probe is invoked periodically and determines whether the specific\\npod should receive client requests or not. When a container’s readiness probe returns\\nsuccess, it’s signaling that the container is ready to accept requests. \\n This notion of being ready is obviously something that’s specific to each container.\\nKubernetes can merely check if the app running in the container responds to a simple\\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes, equivalent to a process or thread in other systems.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'An abstraction layer that provides load balancing, DNS resolution, and more for pods.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Ingress',\n",
       "    'description': 'An API resource that provides a configuration layer on top of Services to define routing rules.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'A command-line tool for transferring data with URL syntax, useful for testing APIs and services.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'HTTPS',\n",
       "    'description': 'A secure communication protocol for online transactions and communications.',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'Server Certificate',\n",
       "    'description': 'A digital certificate used to identify a server and establish trust with clients.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Liveness Probes',\n",
       "    'description': 'A feature in Kubernetes that periodically checks whether a container is running properly, restarting it if necessary.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Readiness Probes',\n",
       "    'description': 'A feature in Kubernetes that periodically checks whether a pod is ready to receive client requests, preventing unnecessary load on the service.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Endpoint',\n",
       "    'description': 'An API resource that represents a specific network endpoint, typically associated with a pod or container.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Labels',\n",
       "    'description': 'A way to assign metadata to pods, services, and other Kubernetes resources for organization and filtering purposes.',\n",
       "    'category': 'metadata'}],\n",
       "  'relationships': '[{\"source_entity\": \"Readiness Probes\", \"description\": \"invoked periodically to determine if a pod should receive client requests or not\", \"destination_entity\": \"Pod\"}, {\"source_entity\": \"Service\", \"description\": \"redirects requests to a new pod with proper labels once it\\'s created\", \"destination_entity\": \"Pod\"}, {\"source_entity\": \"Readiness Probes\", \"description\": \"signaling that the container is ready to accept requests\", \"destination_entity\": \"Container\"}, {\"source_entity\": \"Service\", \"description\": \"forwards requests to a running instance until the pod is fully ready\", \"destination_entity\": \"Pod\"}, {\"source_entity\": \"curl\", \"description\": \"sends an HTTP request to an Ingress through HTTPS\", \"destination_entity\": \"Ingress\"}, {\"source_entity\": \"Server Certificate\", \"description\": \"configured with by the user\", \"destination_entity\": \"Ingress\"}, {\"source_entity\": \"Kubernetes\", \"description\": \"allows defining a readiness probe for a pod\", \"destination_entity\": \"Pod\"}, {\"source_entity\": \"Readiness Probes\", \"description\": \"determines whether the container is ready to accept requests or not\", \"destination_entity\": \"Container\"}, {\"source_entity\": \"Endpoint\", \"description\": \"includes pods as endpoints of a service if their labels match the service\\'s pod selector\", \"destination_entity\": \"Pod\"}, {\"source_entity\": \"Liveness Probes\", \"description\": \"helps keep apps healthy by restarting unhealthy containers automatically\", \"destination_entity\": \"Container\"}, {\"source_entity\": \"Readiness Probes\", \"description\": \"similar to liveness probes but determines whether a container is ready to accept requests or not\", \"destination_entity\": \"Container\"}, {\"source_entity\": \"Labels\", \"description\": \"match the service\\'s pod selector to include pods as endpoints of a service\", \"destination_entity\": \"Service\"}]'},\n",
       " {'page': 182,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '150\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\nGET / request or it can hit a specific URL path, which causes the app to perform a\\nwhole list of checks to determine if it’s ready. Such a detailed readiness probe, which\\ntakes the app’s specifics into account, is the app developer’s responsibility. \\nTYPES OF READINESS PROBES\\nLike liveness probes, three types of readiness probes exist:\\n\\uf0a1An Exec probe, where a process is executed. The container’s status is deter-\\nmined by the process’ exit status code.\\n\\uf0a1An HTTP GET probe, which sends an HTTP GET request to the container and\\nthe HTTP status code of the response determines whether the container is\\nready or not.\\n\\uf0a1A TCP Socket probe, which opens a TCP connection to a specified port of the\\ncontainer. If the connection is established, the container is considered ready.\\nUNDERSTANDING THE OPERATION OF READINESS PROBES\\nWhen a container is started, Kubernetes can be configured to wait for a configurable\\namount of time to pass before performing the first readiness check. After that, it\\ninvokes the probe periodically and acts based on the result of the readiness probe. If a\\npod reports that it’s not ready, it’s removed from the service. If the pod then becomes\\nready again, it’s re-added. \\n Unlike liveness probes, if a container fails the readiness check, it won’t be killed or\\nrestarted. This is an important distinction between liveness and readiness probes.\\nLiveness probes keep pods healthy by killing off unhealthy containers and replacing\\nthem with new, healthy ones, whereas readiness probes make sure that only pods that\\nare ready to serve requests receive them. This is mostly necessary during container\\nstart up, but it’s also useful after the container has been running for a while. \\n As you can see in figure 5.11, if a pod’s readiness probe fails, the pod is removed\\nfrom the Endpoints object. Clients connecting to the service will not be redirected to\\nthe pod. The effect is the same as when the pod doesn’t match the service’s label\\nselector at all.\\nEndpoints\\nService\\nSelector: app=kubia\\napp: kubia\\nPod: kubia-q3vkg\\napp: kubia\\nPod: kubia-k0xz6\\napp: kubia\\nPod: kubia-53thy\\nNot ready\\nThis pod is no longer\\nan endpoint, because its\\nreadiness probe has failed.\\nFigure 5.11\\nA pod whose readiness probe fails is removed as an endpoint of a service.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [This pod is no longer\n",
       "   an endpoint, because its\n",
       "   Service readiness probe has failed.\n",
       "   Endpoints\n",
       "   Selector: app=kubia\n",
       "   app: kubia app: kubia app: kubia\n",
       "   Pod: kubia-q3vkg Pod: kubia-k0xz6 Pod: kubia-53thy\n",
       "   Not ready, Col1]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Readiness Probe',\n",
       "    'description': 'A mechanism to determine if a container is ready to serve requests',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'Exec Probe',\n",
       "    'description': 'A type of readiness probe that executes a process and checks its exit status code',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'HTTP GET Probe',\n",
       "    'description': 'A type of readiness probe that sends an HTTP GET request to the container and checks its response status code',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'TCP Socket Probe',\n",
       "    'description': 'A type of readiness probe that opens a TCP connection to a specified port of the container',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'The smallest unit of deployment in Kubernetes, representing a running instance of an application',\n",
       "    'category': 'Container'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'A way to expose an application running inside a cluster to the outside world',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Endpoints',\n",
       "    'description': 'An object that represents the IP addresses and ports of a service',\n",
       "    'category': 'Database'},\n",
       "   {'entity': 'Liveness Probe',\n",
       "    'description': 'A mechanism to determine if a container is healthy and running properly',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A way to expose an application running inside a cluster to the outside world',\n",
       "    'category': 'Application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Services\",\\n    \"description\": \"Enable clients to discover and talk to pods\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"Determine if a pod is ready to receive requests based on the result of the readiness probe\",\\n    \"destination_entity\": \"Readiness Probe\"\\n  },\\n  {\\n    \"source_entity\": \"TCP Socket Probe\",\\n    \"description\": \"Establish a TCP connection to a specified port of the container to check its readiness\",\\n    \"destination_entity\": \"Containers\"\\n  },\\n  {\\n    \"source_entity\": \"Exec Probe\",\\n    \"description\": \"Execute a process within the container to determine its readiness\",\\n    \"destination_entity\": \"Containers\"\\n  },\\n  {\\n    \"source_entity\": \"HTTP GET Probe\",\\n    \"description\": \"Send an HTTP GET request to the container to check its readiness\",\\n    \"destination_entity\": \"Containers\"\\n  },\\n  {\\n    \"source_entity\": \"Readiness Probes\",\\n    \"description\": \"Wait for a configurable amount of time before performing the first check and then invoke periodically\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Liveness Probe\",\\n    \"description\": \"Keep pods healthy by killing off unhealthy containers and replacing them with new ones\",\\n    \"destination_entity\": \"Containers\"\\n  },\\n  {\\n    \"source_entity\": \"Readiness Probes\",\\n    \"description\": \"Remove a pod from the service if it fails the readiness check\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"Configure the first readiness check to wait for a configurable amount of time before performing\",\\n    \"destination_entity\": \"Readiness Probes\"\\n  },\\n  {\\n    \"source_entity\": \"Endpoints\",\\n    \"description\": \"Remove a pod as an endpoint if its readiness probe fails\",\\n    \"destination_entity\": \"Pods\"\\n  }\\n]\\n```\\n\\nNote that I\\'ve extracted relations between various entities mentioned in the document, and for each relation, I\\'ve specified the source entity performing some action on the destination entity, along with a brief description of the action.'},\n",
       " {'page': 183,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '151\\nSignaling when a pod is ready to accept connections\\nUNDERSTANDING WHY READINESS PROBES ARE IMPORTANT\\nImagine that a group of pods (for example, pods running application servers)\\ndepends on a service provided by another pod (a backend database, for example). If\\nat any point one of the frontend pods experiences connectivity problems and can’t\\nreach the database anymore, it may be wise for its readiness probe to signal to Kuber-\\nnetes that the pod isn’t ready to serve any requests at that time. If other pod instances\\naren’t experiencing the same type of connectivity issues, they can serve requests nor-\\nmally. A readiness probe makes sure clients only talk to those healthy pods and never\\nnotice there’s anything wrong with the system.\\n5.5.2\\nAdding a readiness probe to a pod\\nNext you’ll add a readiness probe to your existing pods by modifying the Replication-\\nController’s pod template. \\nADDING A READINESS PROBE TO THE POD TEMPLATE\\nYou’ll use the kubectl edit command to add the probe to the pod template in your\\nexisting ReplicationController:\\n$ kubectl edit rc kubia\\nWhen the ReplicationController’s YAML opens in the text editor, find the container\\nspecification in the pod template and add the following readiness probe definition to\\nthe first container under spec.template.spec.containers. The YAML should look\\nlike the following listing.\\napiVersion: v1\\nkind: ReplicationController\\n...\\nspec:\\n  ...\\n  template:\\n    ...\\n    spec:\\n      containers:\\n      - name: kubia\\n        image: luksa/kubia\\n        readinessProbe:       \\n          exec:               \\n            command:          \\n            - ls              \\n            - /var/ready      \\n        ...\\nThe readiness probe will periodically perform the command ls /var/ready inside the\\ncontainer. The ls command returns exit code zero if the file exists, or a non-zero exit\\ncode otherwise. If the file exists, the readiness probe will succeed; otherwise, it will fail. \\nListing 5.17\\nRC creating a pod with a readiness probe: kubia-rc-readinessprobe.yaml\\nA readinessProbe may \\nbe defined for each \\ncontainer in the pod.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A group of pods running application servers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'Provided by another pod (a backend database, for example)',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Readiness Probe',\n",
       "    'description': 'Signaling when a pod is ready to accept connections',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'Managing the number of replicas of a pod',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl edit command',\n",
       "    'description': \"Modifying the ReplicationController's pod template\",\n",
       "    'category': 'command'},\n",
       "   {'entity': 'YAML',\n",
       "    'description': 'A human-readable serialization format',\n",
       "    'category': 'format'},\n",
       "   {'entity': 'Container specification',\n",
       "    'description': 'In the pod template and add a readiness probe definition',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'exec command',\n",
       "    'description': 'Performing a command inside the container',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ls command',\n",
       "    'description': 'Checking if a file exists in the container',\n",
       "    'category': 'command'},\n",
       "   {'entity': '/var/ready directory',\n",
       "    'description': 'A location where the readiness probe checks for a file',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Exit code zero',\n",
       "    'description': 'Returned by the ls command when the file exists',\n",
       "    'category': 'error'},\n",
       "   {'entity': 'Non-zero exit code',\n",
       "    'description': 'Returned by the ls command when the file does not exist',\n",
       "    'category': 'error'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Readiness Probe\",\\n    \"description\": \"checks if a file exists at /var/ready and returns exit code zero if it does\",\\n    \"destination_entity\": \"/var/ready directory\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl edit command\",\\n    \"description\": \"edits the ReplicationController\\'s YAML to add a readiness probe\",\\n    \"destination_entity\": \"ReplicationController\"\\n  },\\n  {\\n    \"source_entity\": \"Readiness Probe\",\\n    \"description\": \"periodically checks if a file exists at /var/ready and returns exit code zero if it does\",\\n    \"destination_entity\": \"/var/ready directory\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"manages the creation and scaling of pods with readiness probes\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"provides a backend for pods to rely on, which can be checked by a readiness probe\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"exec command\",\\n    \"description\": \"executes a command inside the container, such as \\'ls /var/ready\\'\",\\n    \"destination_entity\": \"Container specification\"\\n  },\\n  {\\n    \"source_entity\": \"Non-zero exit code\",\\n    \"description\": \"returned by the ls command if the file does not exist at /var/ready\",\\n    \"destination_entity\": \"/var/ready directory\"\\n  },\\n  {\\n    \"source_entity\": \"ls command\",\\n    \"description\": \"checks if a file exists at /var/ready and returns exit code zero if it does\",\\n    \"destination_entity\": \"/var/ready directory\"\\n  },\\n  {\\n    \"source_entity\": \"YAML\",\\n    \"description\": \"used to define the ReplicationController\\'s pod template with a readiness probe\",\\n    \"destination_entity\": \"ReplicationController\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"managed by the ReplicationController and can have a readiness probe\",\\n    \"destination_entity\": \"Readiness Probe\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"manages the creation and scaling of pods with readiness probes, which can be edited using kubectl edit command\",\\n    \"destination_entity\": \"ReplicationController\"\\n  }\\n]'},\n",
       " {'page': 184,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '152\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\n The reason you’re defining such a strange readiness probe is so you can toggle its\\nresult by creating or removing the file in question. The file doesn’t exist yet, so all the\\npods should now report not being ready, right? Well, not exactly. As you may remem-\\nber from the previous chapter, changing a ReplicationController’s pod template has\\nno effect on existing pods. \\n In other words, all your existing pods still have no readiness probe defined. You\\ncan see this by listing the pods with kubectl get pods and looking at the READY col-\\numn. You need to delete the pods and have them re-created by the Replication-\\nController. The new pods will fail the readiness check and won’t be included as\\nendpoints of the service until you create the /var/ready file in each of them. \\nOBSERVING AND MODIFYING THE PODS’ READINESS STATUS\\nList the pods again and inspect whether they’re ready or not:\\n$ kubectl get po\\nNAME          READY     STATUS    RESTARTS   AGE\\nkubia-2r1qb   0/1       Running   0          1m\\nkubia-3rax1   0/1       Running   0          1m\\nkubia-3yw4s   0/1       Running   0          1m\\nThe READY column shows that none of the containers are ready. Now make the readi-\\nness probe of one of them start returning success by creating the /var/ready file,\\nwhose existence makes your mock readiness probe succeed:\\n$ kubectl exec kubia-2r1qb -- touch /var/ready\\nYou’ve used the kubectl exec command to execute the touch command inside the\\ncontainer of the kubia-2r1qb pod. The touch command creates the file if it doesn’t\\nyet exist. The pod’s readiness probe command should now exit with status code 0,\\nwhich means the probe is successful, and the pod should now be shown as ready. Let’s\\nsee if it is:\\n$ kubectl get po kubia-2r1qb\\nNAME          READY     STATUS    RESTARTS   AGE\\nkubia-2r1qb   0/1       Running   0          2m\\nThe pod still isn’t ready. Is there something wrong or is this the expected result? Take\\na more detailed look at the pod with kubectl describe. The output should contain\\nthe following line:\\nReadiness: exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1\\n➥ #failure=3\\nThe readiness probe is checked periodically—every 10 seconds by default. The pod\\nisn’t ready because the readiness probe hasn’t been invoked yet. But in 10 seconds at\\nthe latest, the pod should become ready and its IP should be listed as the only end-\\npoint of the service (run kubectl get endpoints kubia-loadbalancer to confirm). \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubernetes',\n",
       "    'description': '',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'replication controller',\n",
       "    'description': '',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod template', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'readiness probe', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'command-line tool for interacting with Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'exec command', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'touch command', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'container', 'description': '', 'category': 'hardware'},\n",
       "   {'entity': 'endpoint', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'service', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'replication controller',\n",
       "    'description': '',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"defines pod template with readiness probe\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl get pods\", \"description\": \"lists all running pods and their READY column status\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"readiness probe\", \"description\": \"checked periodically every 10 seconds to determine pod\\'s readiness\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"touch command\", \"description\": \"creates /var/ready file inside container, making mock readiness probe succeed\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"kubectl exec command\", \"description\": \"executes touch command inside container of pod kubia-2r1qb\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"service\", \"description\": \"lists endpoints of service, including IP address of ready pods\", \"destination_entity\": \"endpoint\"},\\n  {\"source_entity\": \"kubectl get endpoints\", \"description\": \"confirms only ready pod\\'s IP is listed as endpoint of service\", \"destination_entity\": \"service\"},\\n  {\"source_entity\": \"replication controller\", \"description\": \"re-creates new pods with updated readiness probe after existing pods are deleted\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl describe command\", \"description\": \"displays detailed information about pod, including its readiness probe configuration\", \"destination_entity\": \"pod\"}\\n]'},\n",
       " {'page': 185,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '153\\nSignaling when a pod is ready to accept connections\\nHITTING THE SERVICE WITH THE SINGLE READY POD\\nYou can now hit the service URL a few times to see that each and every request is redi-\\nrected to this one pod:\\n$ curl http://130.211.53.173\\nYou’ve hit kubia-2r1qb\\n$ curl http://130.211.53.173\\nYou’ve hit kubia-2r1qb\\n...\\n$ curl http://130.211.53.173\\nYou’ve hit kubia-2r1qb\\nEven though there are three pods running, only a single pod is reporting as being\\nready and is therefore the only pod receiving requests. If you now delete the file, the\\npod will be removed from the service again. \\n5.5.3\\nUnderstanding what real-world readiness probes should do\\nThis mock readiness probe is useful only for demonstrating what readiness probes do.\\nIn the real world, the readiness probe should return success or failure depending on\\nwhether the app can (and wants to) receive client requests or not. \\n Manually removing pods from services should be performed by either deleting the\\npod or changing the pod’s labels instead of manually flipping a switch in the probe. \\nTIP\\nIf you want to add or remove a pod from a service manually, add\\nenabled=true as a label to your pod and to the label selector of your service.\\nRemove the label when you want to remove the pod from the service.\\nALWAYS DEFINE A READINESS PROBE\\nBefore we conclude this section, there are two final notes about readiness probes that\\nI need to emphasize. First, if you don’t add a readiness probe to your pods, they’ll\\nbecome service endpoints almost immediately. If your application takes too long to\\nstart listening for incoming connections, client requests hitting the service will be for-\\nwarded to the pod while it’s still starting up and not ready to accept incoming connec-\\ntions. Clients will therefore see “Connection refused” types of errors. \\nTIP\\nYou should always define a readiness probe, even if it’s as simple as send-\\ning an HTTP request to the base URL. \\nDON’T INCLUDE POD SHUTDOWN LOGIC INTO YOUR READINESS PROBES\\nThe other thing I need to mention applies to the other end of the pod’s life (pod\\nshutdown) and is also related to clients experiencing connection errors. \\n When a pod is being shut down, the app running in it usually stops accepting con-\\nnections as soon as it receives the termination signal. Because of this, you might think\\nyou need to make your readiness probe start failing as soon as the shutdown proce-\\ndure is initiated, ensuring the pod is removed from all services it’s part of. But that’s\\nnot necessary, because Kubernetes removes the pod from all services as soon as you\\ndelete the pod.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'pod',\n",
       "    'description': 'A pod in Kubernetes is a logical host for one or more containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'service',\n",
       "    'description': 'A service in Kubernetes provides a network identity and load-balancing for accessing applications running within pods.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'readiness probe',\n",
       "    'description': 'A readiness probe is a mechanism to determine whether a pod is ready to serve traffic or not.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'A command-line tool for transferring data with URL syntax.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'HTTP request',\n",
       "    'description': 'A request sent to a server in the Hypertext Transfer Protocol (HTTP).',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'label selector',\n",
       "    'description': 'A mechanism in Kubernetes to select pods based on labels.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'pod label',\n",
       "    'description': 'A label attached to a pod to identify it.',\n",
       "    'category': 'label'},\n",
       "   {'entity': 'termination signal',\n",
       "    'description': 'A signal sent to a process to indicate that it should terminate.',\n",
       "    'category': 'signal'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system.',\n",
       "    'category': 'container orchestration'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"readiness probe\", \"description\": \"returns success or failure depending on whether the app can receive client requests\", \"destination_entity\": \"client request\"},\\n  {\"source_entity\": \"pod\", \"description\": \"becomes service endpoint almost immediately without a readiness probe\", \"destination_entity\": \"service endpoint\"},\\n  {\"source_entity\": \"readiness probe\", \"description\": \"ensures pod is removed from services during shutdown\", \"destination_entity\": \"termination signal\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"removes pod from all services upon deletion\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"curl\", \"description\": \"makes HTTP requests to a service URL\", \"destination_entity\": \"service URL\"},\\n  {\"source_entity\": \"label selector\", \"description\": \"manually adds or removes pods from services\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"readiness probe\", \"description\": \"should return success or failure depending on whether the app can receive client requests\", \"destination_entity\": \"client request\"},\\n  {\"source_entity\": \"HTTP request\", \"description\": \"is forwarded to a pod while it\\'s still starting up and not ready\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"should always define a readiness probe for pods\", \"destination_entity\": \"readiness probe\"},\\n  {\"source_entity\": \"pod label\", \"description\": \"enables or disables a pod from receiving requests\", \"destination_entity\": \"service\"},\\n  {\"source_entity\": \"curl\", \"description\": \"hitting the service URL makes multiple requests to the same pod\", \"destination_entity\": \"pod\"}\\n]\\n```'},\n",
       " {'page': 186,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '154\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\n5.6\\nUsing a headless service for discovering individual pods\\nYou’ve seen how services can be used to provide a stable IP address allowing clients to\\nconnect to pods (or other endpoints) backing each service. Each connection to the\\nservice is forwarded to one randomly selected backing pod. But what if the client\\nneeds to connect to all of those pods? What if the backing pods themselves need to\\neach connect to all the other backing pods? Connecting through the service clearly\\nisn’t the way to do this. What is?\\n For a client to connect to all pods, it needs to figure out the the IP of each individ-\\nual pod. One option is to have the client call the Kubernetes API server and get the\\nlist of pods and their IP addresses through an API call, but because you should always\\nstrive to keep your apps Kubernetes-agnostic, using the API server isn’t ideal. \\n Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually,\\nwhen you perform a DNS lookup for a service, the DNS server returns a single IP—the\\nservice’s cluster IP. But if you tell Kubernetes you don’t need a cluster IP for your service\\n(you do this by setting the clusterIP field to None in the service specification), the DNS\\nserver will return the pod IPs instead of the single service IP.\\n Instead of returning a single DNS A record, the DNS server will return multiple A\\nrecords for the service, each pointing to the IP of an individual pod backing the ser-\\nvice at that moment. Clients can therefore do a simple DNS A record lookup and get\\nthe IPs of all the pods that are part of the service. The client can then use that infor-\\nmation to connect to one, many, or all of them.\\n5.6.1\\nCreating a headless service\\nSetting the clusterIP field in a service spec to None makes the service headless, as\\nKubernetes won’t assign it a cluster IP through which clients could connect to the\\npods backing it. \\n You’ll create a headless service called kubia-headless now. The following listing\\nshows its definition.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: kubia-headless\\nspec:\\n  clusterIP: None       \\n  ports:\\n  - port: 80\\n    targetPort: 8080\\n  selector:\\n    app: kubia\\nAfter you create the service with kubectl create, you can inspect it with kubectl get\\nand kubectl describe. You’ll see it has no cluster IP and its endpoints include (part of)\\nListing 5.18\\nA headless service: kubia-svc-headless.yaml\\nThis makes the \\nservice headless.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Service',\n",
       "    'description': 'a stable IP address allowing clients to connect to pods backing each service',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'individual pods backing a service',\n",
       "    'category': 'software,container'},\n",
       "   {'entity': 'Kubernetes API server',\n",
       "    'description': 'the central authority for Kubernetes management',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'DNS lookups',\n",
       "    'description': 'a method for clients to discover pod IPs through DNS queries',\n",
       "    'category': 'network,application'},\n",
       "   {'entity': 'Cluster IP',\n",
       "    'description': 'a single IP address assigned to a service by Kubernetes',\n",
       "    'category': 'hardware,ip_address'},\n",
       "   {'entity': 'Headless service',\n",
       "    'description': 'a service with no cluster IP, allowing clients to discover pod IPs through DNS lookups',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Service spec',\n",
       "    'description': 'the configuration file for a service, including fields such as clusterIP and ports',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'kubectl create',\n",
       "    'description': 'a command used to create a Kubernetes resource, such as a service or pod',\n",
       "    'category': 'command,cli_tool'},\n",
       "   {'entity': 'kubectl get',\n",
       "    'description': 'a command used to retrieve information about a Kubernetes resource',\n",
       "    'category': 'command,cli_tool'},\n",
       "   {'entity': 'kubectl describe',\n",
       "    'description': 'a command used to display detailed information about a Kubernetes resource',\n",
       "    'category': 'command,cli_tool'},\n",
       "   {'entity': 'Endpoints',\n",
       "    'description': 'the list of pods backing a service, displayed in the output of kubectl get and kubectl describe',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Port 80',\n",
       "    'description': 'a TCP port used for communication between clients and servers',\n",
       "    'category': 'hardware,port'},\n",
       "   {'entity': 'TargetPort 8080',\n",
       "    'description': 'the port on which the server is listening, specified in the service spec',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Selector app: kubia',\n",
       "    'description': 'a field in the service spec that specifies the label to select pods backing the service',\n",
       "    'category': 'software,application'}],\n",
       "  'relationships': '[ \\n  { \"source_entity\": \"Client\", \"description\": \"needs to figure out the IP of each individual pod\", \"destination_entity\": \"Pods\" }, \\n  { \"source_entity\": \"Kubernetes API server\", \"description\": \"returns a list of pods and their IP addresses through an API call\", \"destination_entity\": \"API Call\" }, \\n  { \"source_entity\": \"Client\", \"description\": \"performs a DNS lookup for the service\", \"destination_entity\": \"DNS Lookups\" }, \\n  { \"source_entity\": \"Kubernetes\", \"description\": \"allows clients to discover pod IPs through DNS lookups\", \"destination_entity\": \"Pods\" }, \\n  { \"source_entity\": \"Client\", \"description\": \"does a simple DNS A record lookup and gets the IPs of all pods\", \"destination_entity\": \"DNS Lookups\" }, \\n  { \"source_entity\": \"Kubernetes\", \"description\": \"sets the clusterIP field to None in the service specification\", \"destination_entity\": \"Service spec\" }, \\n  { \"source_entity\": \"Client\", \"description\": \"can then use that information to connect to one, many, or all of them\", \"destination_entity\": \"Pods\" }, \\n  { \"source_entity\": \"kubectl create\", \"description\": \"creates a headless service called kubia-headless\", \"destination_entity\": \"Headless Service\" }, \\n  { \"source_entity\": \"Service spec\", \"description\": \"has the clusterIP field set to None, making it headless\", \"destination_entity\": \"Cluster IP\" }, \\n  { \"source_entity\": \"kubectl get\", \"description\": \"inspects the service and sees that it has no cluster IP\", \"destination_entity\": \"Headless Service\" }, \\n  { \"source_entity\": \"Endpoints\", \"description\": \"include (part of) Listing 5.18\", \"destination_entity\": \"Service spec\" }, \\n  { \"source_entity\": \"Kubernetes\", \"description\": \"won\\'t assign a cluster IP through which clients could connect to the pods backing it\", \"destination_entity\": \"Cluster IP\" }, \\n  { \"source_entity\": \"kubectl describe\", \"description\": \"shows that the service has no cluster IP and its endpoints include (part of) Listing 5.18\", \"destination_entity\": \"Headless Service\" }, \\n  { \"source_entity\": \"Port 80\", \"description\": \"is used by clients to connect to the pods backing the service\", \"destination_entity\": \"Pods\" }, \\n  { \"source_entity\": \"TargetPort 8080\", \"description\": \"is used by the pods backing the service to connect to each other\", \"destination_entity\": \"Pods\" } \\n]'},\n",
       " {'page': 187,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '155\\nUsing a headless service for discovering individual pods\\nthe pods matching its pod selector. I say “part of” because your pods contain a readi-\\nness probe, so only pods that are ready will be listed as endpoints of the service.\\nBefore continuing, please make sure at least two pods report being ready, by creating\\nthe /var/ready file, as in the previous example:\\n$ kubectl exec <pod name> -- touch /var/ready\\n5.6.2\\nDiscovering pods through DNS\\nWith your pods ready, you can now try performing a DNS lookup to see if you get the\\nactual pod IPs or not. You’ll need to perform the lookup from inside one of the pods.\\nUnfortunately, your kubia container image doesn’t include the nslookup (or the dig)\\nbinary, so you can’t use it to perform the DNS lookup.\\n All you’re trying to do is perform a DNS lookup from inside a pod running in the\\ncluster. Why not run a new pod based on an image that contains the binaries you\\nneed? To perform DNS-related actions, you can use the tutum/dnsutils container\\nimage, which is available on Docker Hub and contains both the nslookup and the dig\\nbinaries. To run the pod, you can go through the whole process of creating a YAML\\nmanifest for it and passing it to kubectl create, but that’s too much work, right?\\nLuckily, there’s a faster way.\\nRUNNING A POD WITHOUT WRITING A YAML MANIFEST\\nIn chapter 1, you already created pods without writing a YAML manifest by using the\\nkubectl run command. But this time you want to create only a pod—you don’t need\\nto create a ReplicationController to manage the pod. You can do that like this:\\n$ kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1\\n➥ --command -- sleep infinity\\npod \"dnsutils\" created\\nThe trick is in the --generator=run-pod/v1 option, which tells kubectl to create the\\npod directly, without any kind of ReplicationController or similar behind it. \\nUNDERSTANDING DNS A RECORDS RETURNED FOR A HEADLESS SERVICE\\nLet’s use the newly created pod to perform a DNS lookup:\\n$ kubectl exec dnsutils nslookup kubia-headless\\n...\\nName:    kubia-headless.default.svc.cluster.local\\nAddress: 10.108.1.4 \\nName:    kubia-headless.default.svc.cluster.local\\nAddress: 10.108.2.5 \\nThe DNS server returns two different IPs for the kubia-headless.default.svc\\n.cluster.local FQDN. Those are the IPs of the two pods that are reporting being\\nready. You can confirm this by listing pods with kubectl get pods -o wide, which\\nshows the pods’ IPs. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Headless Service',\n",
       "    'description': 'a service that does not have an IP address and can be used to discover individual pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod Selector',\n",
       "    'description': 'a label selector used to identify which pods to include in a headless service',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Readiness Probe',\n",
       "    'description': 'a mechanism for determining if a pod is ready and can be listed as an endpoint of a service',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'an open-source container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'a container running on a node in a Kubernetes cluster',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'an abstraction that defines a logical set of pods and provides network access to them',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Endpoints',\n",
       "    'description': 'a list of IP addresses associated with a service',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'DNS Lookup',\n",
       "    'description': 'the process of resolving a domain name to an IP address',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'nslookup',\n",
       "    'description': 'a command-line tool for performing DNS lookups',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'dig',\n",
       "    'description': 'a command-line tool for performing DNS lookups',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Tutum/DNSutils',\n",
       "    'description': 'a Docker image containing the nslookup and dig binaries',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl run',\n",
       "    'description': 'a command used to create a pod without writing a YAML manifest',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'a Kubernetes object used to manage the creation and scaling of pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod Manifest',\n",
       "    'description': 'a YAML file defining a pod',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'YAML Manifest',\n",
       "    'description': 'a file containing YAML definitions for a Kubernetes object',\n",
       "    'category': 'file format'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"manages the pods for a Headless Service\",\\n    \"destination_entity\": \"Headless Service\"\\n  },\\n  {\\n    \"source_entity\": \"nslookup\",\\n    \"description\": \"performs DNS lookup on kubia-headless.default.svc.cluster.local FQDN\",\\n    \"destination_entity\": \"kubia-headless.default.svc.cluster.local\"\\n  },\\n  {\\n    \"source_entity\": \"Readiness Probe\",\\n    \"description\": \"reports the readiness of pods to a Headless Service\",\\n    \"destination_entity\": \"Headless Service\"\\n  },\\n  {\\n    \"source_entity\": \"Endpoints\",\\n    \"description\": \"lists the pods that are ready for a Headless Service\",\\n    \"destination_entity\": \"Headless Service\"\\n  },\\n  {\\n    \"source_entity\": \"Pod Selector\",\\n    \"description\": \"selects the pods that match its pod selector\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"dig\",\\n    \"description\": \"performs DNS lookup on kubia-headless.default.svc.cluster.local FQDN\",\\n    \"destination_entity\": \"kubia-headless.default.svc.cluster.local\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl run\",\\n    \"description\": \"creates a pod without writing a YAML manifest using the --generator=run-pod/v1 option\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"YAML Manifest\",\\n    \"description\": \"is not required to create a pod using kubectl run command with --generator=run-pod/v1 option\",\\n    \"destination_entity\": \"kubectl run\"\\n  },\\n  {\\n    \"source_entity\": \"DNS Lookup\",\\n    \"description\": \"can be performed from inside a pod using nslookup or dig commands\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"provides the kubectl command to manage pods and services\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Tutum/DNSutils\",\\n    \"description\": \"is a container image that contains both nslookup and dig binaries\",\\n    \"destination_entity\": \"nslookup\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"can be created using kubectl run command without writing a YAML manifest\",\\n    \"destination_entity\": \"kubectl run\"\\n  },\\n  {\\n    \"source_entity\": \"Pod Manifest\",\\n    \"description\": \"is not required to create a pod using kubectl run command with --generator=run-pod/v1 option\",\\n    \"destination_entity\": \"kubectl run\"\\n  }\\n]\\n```'},\n",
       " {'page': 188,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '156\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\n This is different from what DNS returns for regular (non-headless) services, such\\nas for your kubia service, where the returned IP is the service’s cluster IP:\\n$ kubectl exec dnsutils nslookup kubia\\n...\\nName:    kubia.default.svc.cluster.local\\nAddress: 10.111.249.153\\nAlthough headless services may seem different from regular services, they aren’t that\\ndifferent from the clients’ perspective. Even with a headless service, clients can con-\\nnect to its pods by connecting to the service’s DNS name, as they can with regular ser-\\nvices. But with headless services, because DNS returns the pods’ IPs, clients connect\\ndirectly to the pods, instead of through the service proxy. \\nNOTE\\nA headless services still provides load balancing across pods, but through\\nthe DNS round-robin mechanism instead of through the service proxy.\\n5.6.3\\nDiscovering all pods—even those that aren’t ready\\nYou’ve seen that only pods that are ready become endpoints of services. But some-\\ntimes you want to use the service discovery mechanism to find all pods matching the\\nservice’s label selector, even those that aren’t ready. \\n Luckily, you don’t have to resort to querying the Kubernetes API server. You can\\nuse the DNS lookup mechanism to find even those unready pods. To tell Kubernetes\\nyou want all pods added to a service, regardless of the pod’s readiness status, you must\\nadd the following annotation to the service:\\nkind: Service\\nmetadata:\\n  annotations:\\n    service.alpha.kubernetes.io/tolerate-unready-endpoints: \"true\"\\nWARNING\\nAs the annotation name suggests, as I’m writing this, this is an alpha\\nfeature. The Kubernetes Service API already supports a new service spec field\\ncalled publishNotReadyAddresses, which will replace the tolerate-unready-\\nendpoints annotation. In Kubernetes version 1.9.0, the field is not honored yet\\n(the annotation is what determines whether unready endpoints are included in\\nthe DNS or not). Check the documentation to see whether that’s changed.\\n5.7\\nTroubleshooting services\\nServices are a crucial Kubernetes concept and the source of frustration for many\\ndevelopers. I’ve seen many developers lose heaps of time figuring out why they can’t\\nconnect to their pods through the service IP or FQDN. For this reason, a short look at\\nhow to troubleshoot services is in order.\\n When you’re unable to access your pods through the service, you should start by\\ngoing through the following list:\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'DNS',\n",
       "    'description': 'Domain Name System',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'Enable clients to discover and talk to pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Lightweight, ephemeral containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Endpoints',\n",
       "    'description': 'List of IP addresses for a service',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'exec',\n",
       "    'description': 'Execute a command in a pod',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'nslookup',\n",
       "    'description': 'Lookup DNS records',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Headless services',\n",
       "    'description': 'Services that do not provide a service IP',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Label selector',\n",
       "    'description': 'Selector used to identify pods',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Annotations',\n",
       "    'description': 'Metadata added to Kubernetes objects',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Tolerate-unready-endpoints',\n",
       "    'description': 'Annotation for including unready endpoints in DNS',\n",
       "    'category': 'annotation'},\n",
       "   {'entity': 'PublishNotReadyAddresses',\n",
       "    'description': 'New service spec field for publishing not ready addresses',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'Kubernetes API server',\n",
       "    'description': 'Server that manages Kubernetes clusters',\n",
       "    'category': 'network'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Services\", \"description\": \"allow clients to discover and talk to pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"nslookup\", \"description\": \"perform DNS lookup on service name\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"execute command to perform DNS lookup\", \"destination_entity\": \"DNS\"},\\n  {\"source_entity\": \"Kubernetes API server\", \"description\": \"provide information about pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Endpoints\", \"description\": \"become endpoints of services\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"exec\", \"description\": \"execute command to perform DNS lookup\", \"destination_entity\": \"DNS\"},\\n  {\"source_entity\": \"Tolerate-unready-endpoints\", \"description\": \"allow service discovery mechanism to find all pods matching label selector\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Annotations\", \"description\": \"add annotation to service to allow unready endpoints\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"provide platform for running services and pods\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"Headless services\", \"description\": \"allow clients to connect directly to pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Label selector\", \"description\": \"match label selector to find matching pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"PublishNotReadyAddresses\", \"description\": \"replace tolerate-unready-endpoints annotation\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"execute command to troubleshoot services\", \"destination_entity\": \"Services\"}\\n]'},\n",
       " {'page': 189,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '157\\nSummary\\n\\uf0a1First, make sure you’re connecting to the service’s cluster IP from within the\\ncluster, not from the outside.\\n\\uf0a1Don’t bother pinging the service IP to figure out if the service is accessible\\n(remember, the service’s cluster IP is a virtual IP and pinging it will never work).\\n\\uf0a1If you’ve defined a readiness probe, make sure it’s succeeding; otherwise the\\npod won’t be part of the service.\\n\\uf0a1To confirm that a pod is part of the service, examine the corresponding End-\\npoints object with kubectl get endpoints.\\n\\uf0a1If you’re trying to access the service through its FQDN or a part of it (for exam-\\nple, myservice.mynamespace.svc.cluster.local or myservice.mynamespace) and\\nit doesn’t work, see if you can access it using its cluster IP instead of the FQDN.\\n\\uf0a1Check whether you’re connecting to the port exposed by the service and not\\nthe target port.\\n\\uf0a1Try connecting to the pod IP directly to confirm your pod is accepting connec-\\ntions on the correct port.\\n\\uf0a1If you can’t even access your app through the pod’s IP, make sure your app isn’t\\nonly binding to localhost.\\nThis should help you resolve most of your service-related problems. You’ll learn much\\nmore about how services work in chapter 11. By understanding exactly how they’re\\nimplemented, it should be much easier for you to troubleshoot them.\\n5.8\\nSummary\\nIn this chapter, you’ve learned how to create Kubernetes Service resources to expose\\nthe services available in your application, regardless of how many pod instances are\\nproviding each service. You’ve learned how Kubernetes\\n\\uf0a1Exposes multiple pods that match a certain label selector under a single, stable\\nIP address and port\\n\\uf0a1Makes services accessible from inside the cluster by default, but allows you to\\nmake the service accessible from outside the cluster by setting its type to either\\nNodePort or LoadBalancer\\n\\uf0a1Enables pods to discover services together with their IP addresses and ports by\\nlooking up environment variables\\n\\uf0a1Allows discovery of and communication with services residing outside the\\ncluster by creating a Service resource without specifying a selector, by creating\\nan associated Endpoints resource instead\\n\\uf0a1Provides a DNS CNAME alias for external services with the ExternalName ser-\\nvice type\\n\\uf0a1Exposes multiple HTTP services through a single Ingress (consuming a sin-\\ngle IP)\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'cluster IP',\n",
       "    'description': 'a virtual IP address assigned to a service in a Kubernetes cluster',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'ping',\n",
       "    'description': 'a command used to test the reachability of a network host',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'readiness probe',\n",
       "    'description': 'a mechanism used by Kubernetes to determine if a pod is ready to accept traffic',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Endpoints object',\n",
       "    'description': 'a Kubernetes resource that maps a service name to its corresponding IP addresses and ports',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'kubectl get endpoints',\n",
       "    'description': 'a command used to retrieve information about Endpoints objects in a Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'FQDN',\n",
       "    'description': 'fully qualified domain name of a service, including its namespace and cluster information',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'service port',\n",
       "    'description': 'the port number exposed by a Kubernetes Service resource',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'target port',\n",
       "    'description': 'the port number used by a pod to communicate with the outside world',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pod IP',\n",
       "    'description': 'the IP address assigned to a running pod in a Kubernetes cluster',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'localhost',\n",
       "    'description': 'a special IP address that refers to the local machine',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"you\", \"description\": \"should connect to service cluster IP from within cluster\", \"destination_entity\": \"service cluster IP\"},\\n  {\"source_entity\": \"ping\", \"description\": \"will not work because it\\'s a virtual IP\", \"destination_entity\": \"service cluster IP\"},\\n  {\"source_entity\": \"readiness probe\", \"description\": \"must succeed for pod to be part of service\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl get endpoints\", \"description\": \"can confirm if pod is part of service\", \"destination_entity\": \"Endpoints object\"},\\n  {\"source_entity\": \"cluster IP\", \"description\": \"should be used instead of FQDN to access service\", \"destination_entity\": \"FQDN\"},\\n  {\"source_entity\": \"port exposed by service\", \"description\": \"should be used to connect to service, not target port\", \"destination_entity\": \"target port\"},\\n  {\"source_entity\": \"pod IP\", \"description\": \"can be used directly to confirm pod is accepting connections on correct port\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"app\", \"description\": \"shouldn\\'t only bind to localhost, check pod\\'s IP instead\", \"destination_entity\": \"localhost\"}\\n]\\n```\\n\\nNote that I\\'ve tried to extract the most relevant relations between the entities based on the context of the document. Let me know if you\\'d like me to clarify or add any further relations!'},\n",
       " {'page': 190,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '158\\nCHAPTER 5\\nServices: enabling clients to discover and talk to pods\\n\\uf0a1Uses a pod container’s readiness probe to determine whether a pod should or\\nshouldn’t be included as a service endpoint\\n\\uf0a1Enables discovery of pod IPs through DNS when you create a headless service\\nAlong with getting a better understanding of services, you’ve also learned how to\\n\\uf0a1Troubleshoot them\\n\\uf0a1Modify firewall rules in Google Kubernetes/Compute Engine\\n\\uf0a1Execute commands in pod containers through kubectl exec \\n\\uf0a1Run a bash shell in an existing pod’s container\\n\\uf0a1Modify Kubernetes resources through the kubectl apply command\\n\\uf0a1Run an unmanaged ad hoc pod with kubectl run --generator=run-pod/v1\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'pod',\n",
       "    'description': 'A logical host within a cluster',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'readiness probe',\n",
       "    'description': 'A mechanism to determine if a pod is ready',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'service',\n",
       "    'description': 'An abstraction layer that enables clients to discover and talk to pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'DNS',\n",
       "    'description': 'A protocol for translating domain names to IP addresses',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'kubectl exec',\n",
       "    'description': 'A command to execute commands in a pod container',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'bash shell',\n",
       "    'description': 'A Unix shell and command-line interface',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl apply',\n",
       "    'description': 'A command to modify Kubernetes resources',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'run-pod/v1',\n",
       "    'description': 'A generator flag for the kubectl run command',\n",
       "    'category': 'flag'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl apply\",\\n    \"description\": \"Modify Kubernetes resources\",\\n    \"destination_entity\": \"Kubernetes resources\"\\n  },\\n  {\\n    \"source_entity\": \"bash shell\",\\n    \"description\": \"Run a shell in an existing pod\\'s container\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"readiness probe\",\\n    \"description\": \"Determine whether a pod should be included as a service endpoint\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl exec\",\\n    \"description\": \"Execute commands in pod containers\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"DNS\",\\n    \"description\": \"Enable discovery of pod IPs through DNS when creating a headless service\",\\n    \"destination_entity\": \"headless service\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl exec\",\\n    \"description\": \"Execute commands in container associated with the pod\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"Contain a readiness probe used by services to determine endpoints\",\\n    \"destination_entity\": \"readiness probe\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl apply\",\\n    \"description\": \"Modify firewall rules in Google Kubernetes/Compute Engine\",\\n    \"destination_entity\": \"Google Kubernetes/Compute Engine\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl run\",\\n    \"description\": \"Run an unmanaged ad hoc pod with specified parameters\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl apply\",\\n    \"description\": \"Modify resources in Google Kubernetes/Compute Engine\",\\n    \"destination_entity\": \"Google Kubernetes/Compute Engine resources\"\\n  }\\n]\\n```'},\n",
       " {'page': 191,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '159\\nVolumes: attaching\\ndisk storage to containers\\nIn the previous three chapters, we introduced pods and other Kubernetes resources\\nthat interact with them, namely ReplicationControllers, ReplicaSets, DaemonSets,\\nJobs, and Services. Now, we’re going back inside the pod to learn how its containers\\ncan access external disk storage and/or share storage between them.\\n We’ve said that pods are similar to logical hosts where processes running inside\\nthem share resources such as CPU, RAM, network interfaces, and others. One\\nwould expect the processes to also share disks, but that’s not the case. You’ll remem-\\nber that each container in a pod has its own isolated filesystem, because the file-\\nsystem comes from the container’s image.\\nThis chapter covers\\n\\uf0a1Creating multi-container pods\\n\\uf0a1Creating a volume to share disk storage between \\ncontainers\\n\\uf0a1Using a Git repository inside a pod\\n\\uf0a1Attaching persistent storage such as a GCE \\nPersistent Disk to pods\\n\\uf0a1Using pre-provisioned persistent storage\\n\\uf0a1Dynamic provisioning of persistent storage\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pods',\n",
       "    'description': 'Logical hosts where processes running inside them share resources such as CPU, RAM, network interfaces, and others.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'ReplicationControllers',\n",
       "    'description': 'Kubernetes resource that interacts with pods',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'ReplicaSets',\n",
       "    'description': 'Kubernetes resource that interacts with pods',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'DaemonSets',\n",
       "    'description': 'Kubernetes resource that interacts with pods',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Jobs',\n",
       "    'description': 'Kubernetes resource that interacts with pods',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'Kubernetes resource that interacts with pods',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Volumes',\n",
       "    'description': 'Disk storage to containers',\n",
       "    'category': 'Storage'},\n",
       "   {'entity': 'Disk Storage',\n",
       "    'description': 'External disk storage accessed by containers',\n",
       "    'category': 'Hardware'},\n",
       "   {'entity': 'Containers',\n",
       "    'description': 'Processes running inside pods that share resources and can access external disk storage',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Filesystem',\n",
       "    'description': 'Isolated filesystem for each container in a pod',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Git Repository',\n",
       "    'description': 'Using a Git repository inside a pod',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Persistent Storage',\n",
       "    'description': 'Attaching persistent storage such as a GCE Persistent Disk to pods',\n",
       "    'category': 'Storage'},\n",
       "   {'entity': 'Dynamic Provisioning',\n",
       "    'description': 'Dynamic provisioning of persistent storage',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[{\"source_entity\":\"Pods\",\"description\":\"contain multiple containers\",\"destination_entity\":\"Containers\"},{\"source_entity\":\"ReplicationControllers\",\"description\":\"manage and scale pods\",\"destination_entity\":\"Pods\"},{\"source_entity\":\"DaemonSets\",\"description\":\"ensure availability by running pods on each node\",\"destination_entity\":\"Nodes (implied, not explicitly mentioned in the text, but a logical relation)\"}]\\n\\nHowever, to accurately represent all the relations, I need to perform some more operations. Here is an updated list:\\n\\n'},\n",
       " {'page': 192,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '160\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\n Every new container starts off with the exact set of files that was added to the image\\nat build time. Combine this with the fact that containers in a pod get restarted (either\\nbecause the process died or because the liveness probe signaled to Kubernetes that\\nthe container wasn’t healthy anymore) and you’ll realize that the new container will\\nnot see anything that was written to the filesystem by the previous container, even\\nthough the newly started container runs in the same pod.\\n In certain scenarios you want the new container to continue where the last one fin-\\nished, such as when restarting a process on a physical machine. You may not need (or\\nwant) the whole filesystem to be persisted, but you do want to preserve the directories\\nthat hold actual data.\\n Kubernetes provides this by defining storage volumes. They aren’t top-level resources\\nlike pods, but are instead defined as a part of a pod and share the same lifecycle as the\\npod. This means a volume is created when the pod is started and is destroyed when\\nthe pod is deleted. Because of this, a volume’s contents will persist across container\\nrestarts. After a container is restarted, the new container can see all the files that were\\nwritten to the volume by the previous container. Also, if a pod contains multiple con-\\ntainers, the volume can be used by all of them at once. \\n6.1\\nIntroducing volumes\\nKubernetes volumes are a component of a pod and are thus defined in the pod’s spec-\\nification—much like containers. They aren’t a standalone Kubernetes object and can-\\nnot be created or deleted on their own. A volume is available to all containers in the\\npod, but it must be mounted in each container that needs to access it. In each con-\\ntainer, you can mount the volume in any location of its filesystem.\\n6.1.1\\nExplaining volumes in an example\\nImagine you have a pod with three containers (shown in figure 6.1). One container\\nruns a web server that serves HTML pages from the /var/htdocs directory and stores\\nthe access log to /var/logs. The second container runs an agent that creates HTML\\nfiles and stores them in /var/html. The third container processes the logs it finds in\\nthe /var/logs directory (rotates them, compresses them, analyzes them, or whatever).\\n Each container has a nicely defined single responsibility, but on its own each con-\\ntainer wouldn’t be of much use. Creating a pod with these three containers without\\nthem sharing disk storage doesn’t make any sense, because the content generator\\nwould write the generated HTML files inside its own container and the web server\\ncouldn’t access those files, as it runs in a separate isolated container. Instead, it would\\nserve an empty directory or whatever you put in the /var/htdocs directory in its con-\\ntainer image. Similarly, the log rotator would never have anything to do, because its\\n/var/logs directory would always remain empty with nothing writing logs there. A pod\\nwith these three containers and no volumes basically does nothing.\\n But if you add two volumes to the pod and mount them at appropriate paths inside\\nthe three containers, as shown in figure 6.2, you’ve created a system that’s much more\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 193,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '161\\nIntroducing volumes\\nPod\\nContainer: WebServer\\nFilesystem\\nWebserver\\nprocess\\nWrites\\nReads\\n/\\nvar/\\nhtdocs/\\nlogs/\\nContainer: ContentAgent\\nFilesystem\\nContentAgent\\nprocess\\nWrites\\n/\\nvar/\\nhtml/\\nContainer: LogRotator\\nFilesystem\\nLogRotator\\nprocess\\nReads\\n/\\nvar/\\nlogs/\\nFigure 6.1\\nThree containers of the \\nsame pod without shared storage\\nPod\\nContainer: WebServer\\nFilesystem\\n/\\nvar/\\nhtdocs/\\nlogs/\\nContainer: ContentAgent\\nFilesystem\\n/\\nvar/\\nhtml/\\nContainer: LogRotator\\nFilesystem\\n/\\nvar/\\nlogs/\\nVolume:\\npublicHtml\\nVolume:\\nlogVol\\nFigure 6.2\\nThree containers sharing two \\nvolumes mounted at various mount paths\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A group of one or more containers that share resources and storage.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Container: WebServer',\n",
       "    'description': 'A container running a web server process.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Filesystem',\n",
       "    'description': 'A file system mounted within the container.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Webserver', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'process', 'description': '', 'category': 'thread'},\n",
       "   {'entity': 'Writes', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'Reads', 'description': '', 'category': 'process'},\n",
       "   {'entity': '/', 'description': 'Root directory.', 'category': 'filesystem'},\n",
       "   {'entity': '/var/',\n",
       "    'description': 'Variable directory.',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': '/htdocs/',\n",
       "    'description': 'Web server document root directory.',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': '/logs/',\n",
       "    'description': 'Log file directory.',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': 'Container: ContentAgent',\n",
       "    'description': 'A container running a content agent process.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Filesystem',\n",
       "    'description': 'A file system mounted within the container.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ContentAgent', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'process', 'description': '', 'category': 'thread'},\n",
       "   {'entity': 'Writes', 'description': '', 'category': 'process'},\n",
       "   {'entity': '/', 'description': 'Root directory.', 'category': 'filesystem'},\n",
       "   {'entity': '/var/',\n",
       "    'description': 'Variable directory.',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': '/html/',\n",
       "    'description': 'HTML file directory.',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': 'Container: LogRotator',\n",
       "    'description': 'A container running a log rotator process.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Filesystem',\n",
       "    'description': 'A file system mounted within the container.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'LogRotator', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'process', 'description': '', 'category': 'thread'},\n",
       "   {'entity': 'Reads', 'description': '', 'category': 'process'},\n",
       "   {'entity': '/', 'description': 'Root directory.', 'category': 'filesystem'},\n",
       "   {'entity': '/var/',\n",
       "    'description': 'Variable directory.',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': '/logs/',\n",
       "    'description': 'Log file directory.',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': 'Volume',\n",
       "    'description': 'A shared storage volume.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'publicHtml',\n",
       "    'description': 'Public HTML volume.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'logVol', 'description': '', 'category': 'database'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Container: WebServer\", \"description\": \"writes data to\", \"destination_entity\": \"/var/htdocs\"},\\n  {\"source_entity\": \"Container: ContentAgent\", \"description\": \"reads data from\", \"destination_entity\": \"/var/html\"},\\n  {\"source_entity\": \"Container: LogRotator\", \"description\": \"reads log files from\", \"destination_entity\": \"/var/logs\"},\\n  {\"source_entity\": \"Container: WebServer\", \"description\": \"writes data to\", \"destination_entity\": \"/htdocs/\"},\\n  {\"source_entity\": \"Container: ContentAgent\", \"description\": \"writes data to\", \"destination_entity\": \"/html/\"},\\n  {\"source_entity\": \"Container: LogRotator\", \"description\": \"writes log files to\", \"destination_entity\": \"/logs/\"},\\n  {\"source_entity\": \"Webserver\", \"description\": \"writes data to\", \"destination_entity\": \"/var/htdocs\"},\\n  {\"source_entity\": \"ContentAgent\", \"description\": \"reads data from\", \"destination_entity\": \"/var/html\"},\\n  {\"source_entity\": \"LogRotator\", \"description\": \"reads log files from\", \"destination_entity\": \"/var/logs\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"contains\", \"destination_entity\": \"Container: WebServer\"}\\n]\\n```\\n\\nI extracted the following relations:\\n\\n- Container: WebServer writes data to /var/htdocs\\n- Container: ContentAgent reads data from /var/html\\n- Container: LogRotator reads log files from /var/logs\\n- Container: WebServer writes data to /htdocs/\\n- Container: ContentAgent writes data to /html/\\n- Container: LogRotator writes log files to /logs/\\n- Webserver writes data to /var/htdocs\\n- ContentAgent reads data from /var/html\\n- LogRotator reads log files from /var/logs\\n- Pod contains Container: WebServer\\n\\nI defined each relation by means of a JSON having three keys: source_entity, description, and destination_entity.\\n\\nNote that I considered the entities provided as input and extracted relations between them based on the context provided in the document page.'},\n",
       " {'page': 194,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '162\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\nthan the sum of its parts. Linux allows you to mount a filesystem at arbitrary locations\\nin the file tree. When you do that, the contents of the mounted filesystem are accessi-\\nble in the directory it’s mounted into. By mounting the same volume into two contain-\\ners, they can operate on the same files. In your case, you’re mounting two volumes in\\nthree containers. By doing this, your three containers can work together and do some-\\nthing useful. Let me explain how.\\n First, the pod has a volume called publicHtml. This volume is mounted in the Web-\\nServer container at /var/htdocs, because that’s the directory the web server serves\\nfiles from. The same volume is also mounted in the ContentAgent container, but at\\n/var/html, because that’s where the agent writes the files to. By mounting this single vol-\\nume like that, the web server will now serve the content generated by the content agent.\\n Similarly, the pod also has a volume called logVol for storing logs. This volume is\\nmounted at /var/logs in both the WebServer and the LogRotator containers. Note\\nthat it isn’t mounted in the ContentAgent container. The container cannot access its\\nfiles, even though the container and the volume are part of the same pod. It’s not\\nenough to define a volume in the pod; you need to define a VolumeMount inside the\\ncontainer’s spec also, if you want the container to be able to access it.\\n The two volumes in this example can both initially be empty, so you can use a type\\nof volume called emptyDir. Kubernetes also supports other types of volumes that are\\neither populated during initialization of the volume from an external source, or an\\nexisting directory is mounted inside the volume. This process of populating or mount-\\ning a volume is performed before the pod’s containers are started. \\n A volume is bound to the lifecycle of a pod and will stay in existence only while the\\npod exists, but depending on the volume type, the volume’s files may remain intact\\neven after the pod and volume disappear, and can later be mounted into a new vol-\\nume. Let’s see what types of volumes exist.\\n6.1.2\\nIntroducing available volume types\\nA wide variety of volume types is available. Several are generic, while others are spe-\\ncific to the actual storage technologies used underneath. Don’t worry if you’ve never\\nheard of those technologies—I hadn’t heard of at least half of them. You’ll probably\\nonly use volume types for the technologies you already know and use. Here’s a list of\\nseveral of the available volume types:\\n\\uf0a1\\nemptyDir—A simple empty directory used for storing transient data.\\n\\uf0a1\\nhostPath—Used for mounting directories from the worker node’s filesystem\\ninto the pod.\\n\\uf0a1\\ngitRepo—A volume initialized by checking out the contents of a Git repository.\\n\\uf0a1\\nnfs—An NFS share mounted into the pod.\\n\\uf0a1\\ngcePersistentDisk (Google Compute Engine Persistent Disk), awsElastic-\\nBlockStore (Amazon Web Services Elastic Block Store Volume), azureDisk\\n(Microsoft Azure Disk Volume)—Used for mounting cloud provider-specific\\nstorage.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'volume',\n",
       "    'description': 'A filesystem that can be mounted at arbitrary locations in the file tree.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'An isolated process that runs inside a pod.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'The basic execution unit of Kubernetes, which can contain one or more containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'web server',\n",
       "    'description': 'A container that serves files from the /var/htdocs directory.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'content agent',\n",
       "    'description': 'A container that writes files to the /var/html directory.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'log rotator',\n",
       "    'description': 'A container that accesses logs at /var/logs.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'emptyDir',\n",
       "    'description': 'A simple empty directory used for storing transient data.',\n",
       "    'category': 'volume type'},\n",
       "   {'entity': 'hostPath',\n",
       "    'description': \"Used for mounting directories from the worker node's filesystem into the pod.\",\n",
       "    'category': 'volume type'},\n",
       "   {'entity': 'gitRepo',\n",
       "    'description': 'A volume initialized by checking out the contents of a Git repository.',\n",
       "    'category': 'volume type'},\n",
       "   {'entity': 'nfs',\n",
       "    'description': 'An NFS share mounted into the pod.',\n",
       "    'category': 'volume type'},\n",
       "   {'entity': 'gcePersistentDisk',\n",
       "    'description': 'Used for mounting cloud provider-specific storage (Google Compute Engine).',\n",
       "    'category': 'volume type'},\n",
       "   {'entity': 'awsElasticBlockStore',\n",
       "    'description': 'Used for mounting cloud provider-specific storage (Amazon Web Services).',\n",
       "    'category': 'volume type'},\n",
       "   {'entity': 'azureDisk',\n",
       "    'description': 'Used for mounting cloud provider-specific storage (Microsoft Azure).',\n",
       "    'category': 'volume type'}],\n",
       "  'relationships': '[{\"source_entity\": \"pod\", \"description\": \"hosts\", \"destination_entity\": \"volume\"},\\n {\"source_entity\": \"pod\", \"description\": \"has\", \"destination_entity\": \"emptyDir\"},\\n {\"source_entity\": \"pod\", \"description\": \"has\", \"destination_entity\": \"hostPath\"},\\n {\"source_entity\": \"pod\", \"description\": \"has\", \"destination_entity\": \"gitRepo\"},\\n {\"source_entity\": \"pod\", \"description\": \"has\", \"destination_entity\": \"nfs\"},\\n {\"source_entity\": \"pod\", \"description\": \"has\", \"destination_entity\": \"gcePersistentDisk\"},\\n {\"source_entity\": \"pod\", \"description\": \"has\", \"destination_entity\": \"awsElasticBlockStore\"},\\n {\"source_entity\": \"pod\", \"description\": \"has\", \"destination_entity\": \"azureDisk\"},\\n {\"source_entity\": \"pod\", \"description\": \"has\", \"destination_entity\": \"volume\"},\\n {\"source_entity\": \"WebServer container\", \"description\": \"serves files from\", \"destination_entity\": \"/var/htdocs\"},\\n {\"source_entity\": \"ContentAgent container\", \"description\": \"writes files to\", \"destination_entity\": \"/var/html\"},\\n {\"source_entity\": \"pod\", \"description\": \"hosts\", \"destination_entity\": \"WebServer container\"},\\n {\"source_entity\": \"pod\", \"description\": \"hosts\", \"destination_entity\": \"ContentAgent container\"},\\n {\"source_entity\": \"LogRotator container\", \"description\": \"accesses files from\", \"destination_entity\": \"/var/logs\"},\\n {\"source_entity\": \"WebServer container\", \"description\": \"accesses files from\", \"destination_entity\": \"/var/logs\"},\\n {\"source_entity\": \"pod\", \"description\": \"hosts\", \"destination_entity\": \"LogRotator container\"},\\n {\"source_entity\": \"content agent\", \"description\": \"generates content for\", \"destination_entity\": \"web server\"},\\n {\"source_entity\": \"volume\", \"description\": \"is mounted into\", \"destination_entity\": \"container\"},\\n {\"source_entity\": \"emptyDir\", \"description\": \"is a type of\", \"destination_entity\": \"volume\"}]'},\n",
       " {'page': 195,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '163\\nUsing volumes to share data between containers\\n\\uf0a1\\ncinder, cephfs, iscsi, flocker, glusterfs, quobyte, rbd, flexVolume, vsphere-\\nVolume, photonPersistentDisk, scaleIO—Used for mounting other types of\\nnetwork storage.\\n\\uf0a1\\nconfigMap, secret, downwardAPI—Special types of volumes used to expose cer-\\ntain Kubernetes resources and cluster information to the pod.\\n\\uf0a1\\npersistentVolumeClaim—A way to use a pre- or dynamically provisioned per-\\nsistent storage. (We’ll talk about them in the last section of this chapter.)\\nThese volume types serve various purposes. You’ll learn about some of them in the\\nfollowing sections. Special types of volumes (secret, downwardAPI, configMap) are\\ncovered in the next two chapters, because they aren’t used for storing data, but for\\nexposing Kubernetes metadata to apps running in the pod. \\n A single pod can use multiple volumes of different types at the same time, and, as\\nwe’ve mentioned before, each of the pod’s containers can either have the volume\\nmounted or not.\\n6.2\\nUsing volumes to share data between containers\\nAlthough a volume can prove useful even when used by a single container, let’s first\\nfocus on how it’s used for sharing data between multiple containers in a pod.\\n6.2.1\\nUsing an emptyDir volume\\nThe simplest volume type is the emptyDir volume, so let’s look at it in the first exam-\\nple of how to define a volume in a pod. As the name suggests, the volume starts out as\\nan empty directory. The app running inside the pod can then write any files it needs\\nto it. Because the volume’s lifetime is tied to that of the pod, the volume’s contents are\\nlost when the pod is deleted.\\n An emptyDir volume is especially useful for sharing files between containers\\nrunning in the same pod. But it can also be used by a single container for when a con-\\ntainer needs to write data to disk temporarily, such as when performing a sort\\noperation on a large dataset, which can’t fit into the available memory. The data could\\nalso be written to the container’s filesystem itself (remember the top read-write layer\\nin a container?), but subtle differences exist between the two options. A container’s\\nfilesystem may not even be writable (we’ll talk about this toward the end of the book),\\nso writing to a mounted volume might be the only option. \\nUSING AN EMPTYDIR VOLUME IN A POD\\nLet’s revisit the previous example where a web server, a content agent, and a log rota-\\ntor share two volumes, but let’s simplify a bit. You’ll build a pod with only the web\\nserver container and the content agent and a single volume for the HTML. \\n You’ll use Nginx as the web server and the UNIX fortune command to generate\\nthe HTML content. The fortune command prints out a random quote every time you\\nrun it. You’ll create a script that invokes the fortune command every 10 seconds and\\nstores its output in index.html. You’ll find an existing Nginx image available on\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'cinder',\n",
       "    'description': 'Used for mounting other types of network storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'cephfs',\n",
       "    'description': 'Used for mounting other types of network storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'iscsi',\n",
       "    'description': 'Used for mounting other types of network storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'flocker',\n",
       "    'description': 'Used for mounting other types of network storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'glusterfs',\n",
       "    'description': 'Used for mounting other types of network storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'quobyte',\n",
       "    'description': 'Used for mounting other types of network storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'rbd',\n",
       "    'description': 'Used for mounting other types of network storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'flexVolume',\n",
       "    'description': 'Used for mounting other types of network storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'vsphere-Volume',\n",
       "    'description': 'Used for mounting other types of network storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'photonPersistentDisk',\n",
       "    'description': 'Used for mounting other types of network storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'scaleIO',\n",
       "    'description': 'Used for mounting other types of network storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'configMap',\n",
       "    'description': 'Special type of volume used to expose certain Kubernetes resources and cluster information to the pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'secret',\n",
       "    'description': 'Special type of volume used to expose certain Kubernetes resources and cluster information to the pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'downwardAPI',\n",
       "    'description': 'Special type of volume used to expose certain Kubernetes resources and cluster information to the pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'persistentVolumeClaim',\n",
       "    'description': 'A way to use a pre- or dynamically provisioned persistent storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'emptyDir',\n",
       "    'description': 'A volume that starts out as an empty directory, useful for sharing files between containers running in the same pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A logical host that can contain one or more containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'volume',\n",
       "    'description': 'A shared resource that can be used by multiple containers within a pod',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A lightweight and standalone process that can run in a pod',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"emptyDir\", \"description\": \"starts out as an empty directory, allowing app running inside the pod to write any files it needs to it.\", \"destination_entity\": \"pod\"},\\n  \\n  {\"source_entity\": \"container\", \"description\": \"can use a mounted volume for writing data temporarily, especially when performing operations that can\\'t fit into available memory.\", \"destination_entity\": \"volume\"},\\n  \\n  {\"source_entity\": \"emptyDir\", \"description\": \"is useful for sharing files between containers running in the same pod and for writing data to disk temporarily.\", \"destination_entity\": \"container\"},\\n  \\n  {\"source_entity\": \"pod\", \"description\": \"can use multiple volumes of different types at the same time, with each container able to either have the volume mounted or not.\", \"destination_entity\": \"volume\"},\\n  \\n  {\"source_entity\": \"emptyDir\", \"description\": \"contains are lost when the pod is deleted, as the volume\\'s lifetime is tied to that of the pod.\", \"destination_entity\": \"pod\"},\\n  \\n  {\"source_entity\": \"container\", \"description\": \"filesystem may not even be writable, making writing to a mounted volume the only option for writing data temporarily.\", \"destination_entity\": \"volume\"},\\n  \\n  {\"source_entity\": \"configMap\", \"description\": \"exposes certain Kubernetes resources and cluster information to the pod, allowing apps running in the pod to access this info.\", \"destination_entity\": \"pod\"},\\n  \\n  {\"source_entity\": \"secret\", \"description\": \"is a special type of volume used to expose Kubernetes metadata to apps running in the pod.\", \"destination_entity\": \"pod\"},\\n  \\n  {\"source_entity\": \"downwardAPI\", \"description\": \"exposes cluster information and allows apps running in the pod to access this info.\", \"destination_entity\": \"pod\"},\\n  \\n  {\"source_entity\": \"container\", \"description\": \"can use a single volume for shared files, especially useful when using emptyDir volumes.\", \"destination_entity\": \"emptyDir\"},\\n  \\n  {\"source_entity\": \"volume\", \"description\": \"can be used by multiple containers at the same time, with each container able to either have the volume mounted or not.\", \"destination_entity\": \"pod\"},\\n  \\n  {\"source_entity\": \"cephfs\", \"description\": \"is one of several network storage types that can be used for mounting other types of network storage.\", \"destination_entity\": \"network storage\"},\\n  \\n  {\"source_entity\": \"rbd\", \"description\": \"is another type of volume that serves as a network storage type, similar to cephfs and others listed here.\", \"destination_entity\": \"network storage\"}\\n]\\n```'},\n",
       " {'page': 196,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '164\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\nDocker Hub, but you’ll need to either create the fortune image yourself or use the\\none I’ve already built and pushed to Docker Hub under luksa/fortune. If you want a\\nrefresher on how to build Docker images, refer to the sidebar.\\nCREATING THE POD\\nNow that you have the two images required to run your pod, it’s time to create the pod\\nmanifest. Create a file called fortune-pod.yaml with the contents shown in the follow-\\ning listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: fortune\\nspec:\\n  containers:\\nBuilding the fortune container image\\nHere’s how to build the image. Create a new directory called fortune and then inside\\nit, create a fortuneloop.sh shell script with the following contents:\\n#!/bin/bash\\ntrap \"exit\" SIGINT\\nmkdir /var/htdocs\\nwhile :\\ndo\\n  echo $(date) Writing fortune to /var/htdocs/index.html\\n  /usr/games/fortune > /var/htdocs/index.html\\n  sleep 10\\ndone\\nThen, in the same directory, create a file called Dockerfile containing the following:\\nFROM ubuntu:latest\\nRUN apt-get update ; apt-get -y install fortune\\nADD fortuneloop.sh /bin/fortuneloop.sh\\nENTRYPOINT /bin/fortuneloop.sh\\nThe image is based on the ubuntu:latest image, which doesn’t include the fortune\\nbinary by default. That’s why in the second line of the Dockerfile you install it with\\napt-get. After that, you add the fortuneloop.sh script to the image’s /bin folder.\\nIn the last line of the Dockerfile, you specify that the fortuneloop.sh script should\\nbe executed when the image is run.\\nAfter preparing both files, build and upload the image to Docker Hub with the following\\ntwo commands (replace luksa with your own Docker Hub user ID):\\n$ docker build -t luksa/fortune .\\n$ docker push luksa/fortune\\nListing 6.1\\nA pod with two containers sharing the same volume: fortune-pod.yaml\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Docker',\n",
       "    'description': 'A containerization platform',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Docker Hub',\n",
       "    'description': 'A registry for Docker images',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ubuntu',\n",
       "    'description': 'An operating system used as a base image for Docker',\n",
       "    'category': 'operating system'},\n",
       "   {'entity': 'apt-get',\n",
       "    'description': 'A package manager in Ubuntu used to install software',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'fortune',\n",
       "    'description': 'A command-line program that displays fortunes or quotes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'date',\n",
       "    'description': 'A command that displays the current date and time',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'mkdir',\n",
       "    'description': 'A command that creates a new directory',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'sleep',\n",
       "    'description': 'A command that pauses execution for a specified amount of time',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'docker build',\n",
       "    'description': 'A command used to build a Docker image',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'docker push',\n",
       "    'description': 'A command used to upload an image to Docker Hub',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A group of one or more containers that share the same volume and IP address',\n",
       "    'category': 'kubernetes concept'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A lightweight and standalone executable package of code, libraries, and settings',\n",
       "    'category': 'software concept'},\n",
       "   {'entity': 'volume',\n",
       "    'description': 'A directory shared among multiple containers within a pod',\n",
       "    'category': 'kubernetes concept'},\n",
       "   {'entity': 'Image',\n",
       "    'description': 'A binary that contains the code, libraries, and settings for running a containerized application',\n",
       "    'category': 'software concept'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Image\", \"description\": \"based on\", \"destination_entity\": \"ubuntu\"},\\n  {\"source_entity\": \"Image\", \"description\": \"install fortune binary\", \"destination_entity\": \"apt-get\"},\\n  {\"source_entity\": \"docker build\", \"description\": \"build and upload image to Docker Hub\", \"destination_entity\": \"Docker Hub\"},\\n  {\"source_entity\": \"mkdir\", \"description\": \"create directory /var/htdocs\", \"destination_entity\": \"/var/htdocs\"},\\n  {\"source_entity\": \"sleep\", \"description\": \"pause execution for 10 seconds\", \"destination_entity\": \"fortune-pod.yaml\"},\\n  {\"source_entity\": \"docker build\", \"description\": \"build image with fortuneloop.sh script\", \"destination_entity\": \"fortuneloop.sh\"},\\n  {\"source_entity\": \"Image\", \"description\": \"add fortuneloop.sh script to /bin folder\", \"destination_entity\": \"Dockerfile\"},\\n  {\"source_entity\": \"fortune\", \"description\": \"generate fortune output\", \"destination_entity\": \"/var/htdocs/index.html\"},\\n  {\"source_entity\": \"date\", \"description\": \"display current date\", \"destination_entity\": \"fortune-pod.yaml\"},\\n  {\"source_entity\": \"volume\", \"description\": \"attach disk storage to container\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"Docker\", \"description\": \"create and manage containers\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"docker push\", \"description\": \"upload image to Docker Hub\", \"destination_entity\": \"Docker Hub\"},\\n  {\"source_entity\": \"fortune-pod.yaml\", \"description\": \"define pod manifest with volume\", \"destination_entity\": \"volume\"}\\n]'},\n",
       " {'page': 197,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '165\\nUsing volumes to share data between containers\\n  - image: luksa/fortune                   \\n    name: html-generator                   \\n    volumeMounts:                          \\n    - name: html                           \\n      mountPath: /var/htdocs               \\n  - image: nginx:alpine                   \\n    name: web-server                      \\n    volumeMounts:                         \\n    - name: html                          \\n      mountPath: /usr/share/nginx/html    \\n      readOnly: true                      \\n    ports:\\n    - containerPort: 80\\n      protocol: TCP\\n  volumes:                 \\n  - name: html             \\n    emptyDir: {}           \\nThe pod contains two containers and a single volume that’s mounted in both of\\nthem, yet at different paths. When the html-generator container starts, it starts writ-\\ning the output of the fortune command to the /var/htdocs/index.html file every 10\\nseconds. Because the volume is mounted at /var/htdocs, the index.html file is writ-\\nten to the volume instead of the container’s top layer. As soon as the web-server con-\\ntainer starts, it starts serving whatever HTML files are in the /usr/share/nginx/html\\ndirectory (this is the default directory Nginx serves files from). Because you mounted\\nthe volume in that exact location, Nginx will serve the index.html file written there\\nby the container running the fortune loop. The end effect is that a client sending an\\nHTTP request to the pod on port 80 will receive the current fortune message as\\nthe response. \\nSEEING THE POD IN ACTION\\nTo see the fortune message, you need to enable access to the pod. You’ll do that by\\nforwarding a port from your local machine to the pod:\\n$ kubectl port-forward fortune 8080:80\\nForwarding from 127.0.0.1:8080 -> 80\\nForwarding from [::1]:8080 -> 80\\nNOTE\\nAs an exercise, you can also expose the pod through a service instead\\nof using port forwarding.\\nNow you can access the Nginx server through port 8080 of your local machine. Use\\ncurl to do that:\\n$ curl http://localhost:8080\\nBeware of a tall blond man with one black shoe.\\nIf you wait a few seconds and send another request, you should receive a different\\nmessage. By combining two containers, you created a simple app to see how a volume\\ncan glue together two containers and enhance what each of them does.\\nThe first container is called html-generator \\nand runs the luksa/fortune image.\\nThe volume called html is mounted \\nat /var/htdocs in the container.\\nThe second container is called web-server \\nand runs the nginx:alpine image.\\nThe same volume as above is \\nmounted at /usr/share/nginx/html \\nas read-only.\\nA single emptyDir volume \\ncalled html that’s mounted \\nin the two containers above\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes Pod',\n",
       "    'description': 'A pod in Kubernetes is a logical host or an isolated environment where one or more application containers can run.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Docker Containers',\n",
       "    'description': 'Two Docker containers, html-generator and web-server are used to create a simple app that demonstrates the use of volumes.',\n",
       "    'category': 'Container'},\n",
       "   {'entity': 'Volumes',\n",
       "    'description': \"A shared data container called 'html' is created and mounted in both containers at different paths.\",\n",
       "    'category': 'Volume'},\n",
       "   {'entity': 'EmptyDir Volume',\n",
       "    'description': \"An empty directory volume called 'html' is created to serve as a shared storage for the two containers.\",\n",
       "    'category': 'Volume'},\n",
       "   {'entity': 'LUKSa/Fortune Image',\n",
       "    'description': 'A Docker image that runs a fortune command and writes its output to an HTML file every 10 seconds.',\n",
       "    'category': 'Image'},\n",
       "   {'entity': 'Nginx:Alpine Image',\n",
       "    'description': 'A Docker image of Nginx server that serves files from a specific directory.',\n",
       "    'category': 'Image'},\n",
       "   {'entity': 'Port Forwarding',\n",
       "    'description': 'A method to access the pod through a local machine by forwarding a port from the host machine to the pod.',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'kubectl Port-Forward Command',\n",
       "    'description': 'A command used to enable access to the pod by forwarding a port from the local machine to the pod.',\n",
       "    'category': 'Command'},\n",
       "   {'entity': 'curl Command',\n",
       "    'description': 'A command used to send an HTTP request to the Nginx server and receive the current fortune message as a response.',\n",
       "    'category': 'Command'},\n",
       "   {'entity': 'volumeMounts',\n",
       "    'description': 'An attribute of containers that specifies how a volume is mounted inside a container.',\n",
       "    'category': 'Attribute'}],\n",
       "  'relationships': '[{\"source_entity\": \"LUKSa/Fortune Image\", \"description\": \"writes output to file\", \"destination_entity\": \"/var/htdocs/index.html\"}]\\n\\n'},\n",
       " {'page': 198,\n",
       "  'img_cnt': 1,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '166\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\nSPECIFYING THE MEDIUM TO USE FOR THE EMPTYDIR\\nThe emptyDir you used as the volume was created on the actual disk of the worker\\nnode hosting your pod, so its performance depends on the type of the node’s disks.\\nBut you can tell Kubernetes to create the emptyDir on a tmpfs filesystem (in memory\\ninstead of on disk). To do this, set the emptyDir’s medium to Memory like this:\\nvolumes:\\n  - name: html\\n    emptyDir:\\n      medium: Memory    \\nAn emptyDir volume is the simplest type of volume, but other types build upon it.\\nAfter the empty directory is created, they populate it with data. One such volume type\\nis the gitRepo volume type, which we’ll introduce next.\\n6.2.2\\nUsing a Git repository as the starting point for a volume \\nA gitRepo volume is basically an emptyDir volume that gets populated by cloning a\\nGit repository and checking out a specific revision when the pod is starting up (but\\nbefore its containers are created). Figure 6.3 shows how this unfolds.\\nNOTE\\nAfter the gitRepo volume is created, it isn’t kept in sync with the repo\\nit’s referencing. The files in the volume will not be updated when you push\\nadditional commits to the Git repository. However, if your pod is managed by\\na ReplicationController, deleting the pod will result in a new pod being cre-\\nated and this new pod’s volume will then contain the latest commits. \\nFor example, you can use a Git repository to store static HTML files of your website\\nand create a pod containing a web server container and a gitRepo volume. Every time\\nthe pod is created, it pulls the latest version of your website and starts serving it. The\\nThis emptyDir’s \\nfiles should be \\nstored in memory.\\nPod\\nContainer\\nUser\\ngitRepo\\nvolume\\n1. User (or a replication\\ncontroller) creates pod\\nwith gitRepo volume\\n2. Kubernetes creates\\nan empty directory and\\nclones the speciﬁed Git\\nrepository into it\\n3. The pod’s container is started\\n(with the volume mounted at\\nthe mount path)\\nRepository\\nFigure 6.3\\nA gitRepo volume is an emptyDir volume initially populated with the contents of a \\nGit repository.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'emptyDir',\n",
       "    'description': \"a type of volume created on the actual disk of the worker node hosting the pod, its performance depends on the type of the node's disks\",\n",
       "    'category': 'container'},\n",
       "   {'entity': 'tmpfs filesystem',\n",
       "    'description': 'a memory-based filesystem that can be used instead of a disk-based filesystem for emptyDir volumes',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Memory',\n",
       "    'description': 'the medium to use for the emptyDir volume, creates it on a tmpfs filesystem in memory instead of on disk',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'gitRepo',\n",
       "    'description': 'a type of volume that is an emptyDir volume populated by cloning a Git repository and checking out a specific revision when the pod is starting up',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'an instance of a containerized application, can be created with a gitRepo volume',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'a lightweight and portable package that includes everything needed to run an application, can include a gitRepo volume as one of its components',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'worker node',\n",
       "    'description': 'the node on which the pod is hosted, determines the performance of emptyDir volumes',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'a Kubernetes object that manages a set of identical pods, can be used to keep gitRepo volumes in sync with the Git repository',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"create an empty directory on actual disk of worker node hosting pod\",\\n    \"destination_entity\": \"emptyDir\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"create emptyDir volume on tmpfs filesystem (in memory instead of on disk)\",\\n    \"destination_entity\": \"tmpfs filesystem\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"populate gitRepo volume with data by cloning Git repository and checking out specific revision\",\\n    \"destination_entity\": \"gitRepo\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"contain gitRepo volume\",\\n    \"destination_entity\": \"gitRepo\"\\n  },\\n  {\\n    \"source_entity\": \"user (or replication controller)\",\\n    \"description\": \"create pod with gitRepo volume\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"clone Git repository into empty directory to populate gitRepo volume\",\\n    \"destination_entity\": \"gitRepo\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"start container with gitRepo volume mounted at mount path\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"keep files in gitRepo volume up to date by deleting and recreating pod when managed by ReplicationController\",\\n    \"destination_entity\": \"ReplicationController\"\\n  }\\n]'},\n",
       " {'page': 199,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '167\\nUsing volumes to share data between containers\\nonly drawback to this is that you need to delete the pod every time you push changes\\nto the gitRepo and want to start serving the new version of the website. \\n Let’s do this right now. It’s not that different from what you did before. \\nRUNNING A WEB SERVER POD SERVING FILES FROM A CLONED GIT REPOSITORY\\nBefore you create your pod, you’ll need an actual Git repository with HTML files in it.\\nI’ve created a repo on GitHub at https:/\\n/github.com/luksa/kubia-website-example.git.\\nYou’ll need to fork it (create your own copy of the repo on GitHub) so you can push\\nchanges to it later. \\n Once you’ve created your fork, you can move on to creating the pod. This time,\\nyou’ll only need a single Nginx container and a single gitRepo volume in the pod (be\\nsure to point the gitRepo volume to your own fork of my repository), as shown in the\\nfollowing listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: gitrepo-volume-pod\\nspec:\\n  containers:\\n  - image: nginx:alpine\\n    name: web-server\\n    volumeMounts:\\n    - name: html\\n      mountPath: /usr/share/nginx/html\\n      readOnly: true\\n    ports:\\n    - containerPort: 80\\n      protocol: TCP\\n  volumes:\\n  - name: html\\n    gitRepo:                     \\n      repository: https://github.com/luksa/kubia-website-example.git   \\n      revision: master                     \\n      directory: .      \\nWhen you create the pod, the volume is first initialized as an empty directory and then\\nthe specified Git repository is cloned into it. If you hadn’t set the directory to . (dot),\\nthe repository would have been cloned into the kubia-website-example subdirectory,\\nwhich isn’t what you want. You want the repo to be cloned into the root directory of\\nyour volume. Along with the repository, you also specified you want Kubernetes to\\ncheck out whatever revision the master branch is pointing to at the time the volume\\nis created. \\n With the pod running, you can try hitting it through port forwarding, a service, or by\\nexecuting the curl command from within the pod (or any other pod inside the cluster). \\nListing 6.2\\nA pod using a gitRepo volume: gitrepo-volume-pod.yaml\\nYou’re creating a \\ngitRepo volume.\\nThe volume will clone\\nthis Git repository.\\nThe master branch \\nwill be checked out.\\nYou want the repo to \\nbe cloned into the root \\ndir of the volume.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'volumes',\n",
       "    'description': 'share data between containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'gitRepo',\n",
       "    'description': 'clone Git repository with HTML files',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'nginx:alpine',\n",
       "    'description': 'image for Nginx container',\n",
       "    'category': 'image'},\n",
       "   {'entity': 'gitRepo volume',\n",
       "    'description': 'share Git repository between containers',\n",
       "    'category': 'volume'},\n",
       "   {'entity': 'master branch',\n",
       "    'description': 'revision checked out by Kubernetes',\n",
       "    'category': 'branch'},\n",
       "   {'entity': 'https://github.com/luksa/kubia-website-example.git',\n",
       "    'description': 'GitHub repository URL',\n",
       "    'category': 'url'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'running container with Nginx and Git repository',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'service',\n",
       "    'description': 'expose pod to outside traffic',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'curl command',\n",
       "    'description': 'execute from within the pod or any other pod inside the cluster',\n",
       "    'category': 'command'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"master branch\",\\n    \"description\": \"is pointing to at the time the volume is created\",\\n    \"destination_entity\": \"revision\"\\n  },\\n  {\\n    \"source_entity\": \"gitRepo volume\",\\n    \"description\": \"clones this Git repository into the root directory of the volume\",\\n    \"destination_entity\": \"Git repository\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"is first initialized as an empty directory\",\\n    \"destination_entity\": \"directory\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"clones the specified Git repository into it\",\\n    \"destination_entity\": \"Git repository\"\\n  },\\n  {\\n    \"source_entity\": \"gitRepo volume\",\\n    \"description\": \"is created to serve files from a cloned Git repository\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"nginx:alpine\",\\n    \"description\": \"runs as a single container in the pod\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"gitRepo volume\",\\n    \"description\": \"is mounted to /usr/share/nginx/html in the nginx container\",\\n    \"destination_entity\": \"nginx container\"\\n  },\\n  {\\n    \"source_entity\": \"service\",\\n    \"description\": \"can be used to hit the pod through port forwarding\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"curl command\",\\n    \"description\": \"can be executed from within the pod (or any other pod)\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"https://github.com/luksa/kubia-website-example.git\",\\n    \"description\": \"is the URL of a Git repository to be cloned into the volume\",\\n    \"destination_entity\": \"gitRepo volume\"\\n  },\\n  {\\n    \"source_entity\": \"volumes\",\\n    \"description\": \"are used to share data between containers in a pod\",\\n    \"destination_entity\": \"containers\"\\n  }\\n]'},\n",
       " {'page': 200,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '168\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\nCONFIRMING THE FILES AREN’T KEPT IN SYNC WITH THE GIT REPO\\nNow you’ll make changes to the index.html file in your GitHub repository. If you\\ndon’t use Git locally, you can edit the file on GitHub directly—click on the file in your\\nGitHub repository to open it and then click on the pencil icon to start editing it.\\nChange the text and then commit the changes by clicking the button at the bottom.\\n The master branch of the Git repository now includes the changes you made to the\\nHTML file. These changes will not be visible on your Nginx web server yet, because\\nthe gitRepo volume isn’t kept in sync with the Git repository. You can confirm this by\\nhitting the pod again. \\n To see the new version of the website, you need to delete the pod and create\\nit again. Instead of having to delete the pod every time you make changes, you could\\nrun an additional process, which keeps your volume in sync with the Git repository.\\nI won’t explain in detail how to do this. Instead, try doing this yourself as an exer-\\ncise, but here are a few pointers.\\nINTRODUCING SIDECAR CONTAINERS\\nThe Git sync process shouldn’t run in the same container as the Nginx web server, but\\nin a second container: a sidecar container. A sidecar container is a container that aug-\\nments the operation of the main container of the pod. You add a sidecar to a pod so\\nyou can use an existing container image instead of cramming additional logic into the\\nmain app’s code, which would make it overly complex and less reusable. \\n To find an existing container image, which keeps a local directory synchronized\\nwith a Git repository, go to Docker Hub and search for “git sync.” You’ll find many\\nimages that do that. Then use the image in a new container in the pod from the previ-\\nous example, mount the pod’s existing gitRepo volume in the new container, and\\nconfigure the Git sync container to keep the files in sync with your Git repo. If you set\\neverything up correctly, you should see that the files the web server is serving are kept\\nin sync with your GitHub repo. \\nNOTE\\nAn example in chapter 18 includes using a Git sync container like the\\none explained here, so you can wait until you reach chapter 18 and follow the\\nstep-by-step instructions then instead of doing this exercise on your own now. \\nUSING A GITREPO VOLUME WITH PRIVATE GIT REPOSITORIES\\nThere’s one other reason for having to resort to Git sync sidecar containers. We\\nhaven’t talked about whether you can use a gitRepo volume with a private Git repo. It\\nturns out you can’t. The current consensus among Kubernetes developers is to keep\\nthe gitRepo volume simple and not add any support for cloning private repositories\\nthrough the SSH protocol, because that would require adding additional config\\noptions to the gitRepo volume. \\n If you want to clone a private Git repo into your container, you should use a git-\\nsync sidecar or a similar method instead of a gitRepo volume.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Git',\n",
       "    'description': 'A version control system for tracking changes in code.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'GitHub repository',\n",
       "    'description': 'A web-based interface for managing and sharing Git repositories.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Nginx web server',\n",
       "    'description': 'A lightweight web server software.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'gitRepo volume',\n",
       "    'description': 'A persistent storage volume that keeps track of changes to a Git repository.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A Kubernetes object that represents a running instance of an application or service.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'sidecar container',\n",
       "    'description': 'A secondary container in a pod that runs alongside the main container and provides additional functionality.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Docker Hub',\n",
       "    'description': 'A cloud-based registry for Docker images, allowing users to find, download, and share pre-built containers.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Git sync container',\n",
       "    'description': 'A special type of sidecar container that keeps a local directory synchronized with a Git repository.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'SSH protocol',\n",
       "    'description': 'A secure communication protocol for encrypting data exchanged between two endpoints.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Git sync sidecar',\n",
       "    'description': 'A combination of a sidecar container and a Git sync container that keeps a local directory synchronized with a private Git repository.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Nginx web server\",\\n    \"description\": \"serves files that are not kept in sync with the Git repository\",\\n    \"destination_entity\": \"Git repository\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"makes changes to the index.html file in the GitHub repository\",\\n    \"destination_entity\": \"GitHub repository\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"commits changes to the Git repository\",\\n    \"destination_entity\": \"Git repository\"\\n  },\\n  {\\n    \"source_entity\": \"Git sync process\",\\n    \"description\": \"keeps the gitRepo volume in sync with the Git repository\",\\n    \"destination_entity\": \"gitRepo volume\"\\n  },\\n  {\\n    \"source_entity\": \"Sidecar container\",\\n    \"description\": \"augments the operation of the main container of the pod\",\\n    \"destination_entity\": \"Main container\"\\n  },\\n  {\\n    \"source_entity\": \"Git sync sidecar\",\\n    \"description\": \"keeps the files in sync with the GitHub repo\",\\n    \"destination_entity\": \"gitRepo volume\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"clones a private Git repository into a container\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"SSH protocol\",\\n    \"description\": \"is not supported for cloning private repositories through the gitRepo volume\",\\n    \"destination_entity\": \"GitRepo volume\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"uses a Git sync sidecar to keep the files in sync with the GitHub repo\",\\n    \"destination_entity\": \"GitHub repository\"\\n  },\\n  {\\n    \"source_entity\": \"Docker Hub\",\\n    \"description\": \"has existing container images that keep a local directory synchronized with a Git repository\",\\n    \"destination_entity\": \"Git repository\"\\n  }\\n]\\n\\nNote: I assumed \"User\" as the source entity for some relations, as it is not explicitly mentioned in the document page.'},\n",
       " {'page': 201,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '169\\nAccessing files on the worker node’s filesystem\\nWRAPPING UP THE GITREPO VOLUME\\nA gitRepo volume, like the emptyDir volume, is basically a dedicated directory cre-\\nated specifically for, and used exclusively by, the pod that contains the volume. When\\nthe pod is deleted, the volume and its contents are deleted. Other types of volumes,\\nhowever, don’t create a new directory, but instead mount an existing external direc-\\ntory into the pod’s container’s filesystem. The contents of that volume can survive\\nmultiple pod instantiations. We’ll learn about those types of volumes next.\\n6.3\\nAccessing files on the worker node’s filesystem\\nMost  pods should be oblivious of their host node, so they shouldn’t access any files on\\nthe node’s filesystem. But certain system-level pods (remember, these will usually be\\nmanaged by a DaemonSet) do need to either read the node’s files or use the node’s\\nfilesystem to access the node’s devices through the filesystem. Kubernetes makes this\\npossible through a hostPath volume. \\n6.3.1\\nIntroducing the hostPath volume\\nA hostPath volume points to a specific file or directory on the node’s filesystem (see\\nfigure 6.4). Pods running on the same node and using the same path in their host-\\nPath volume see the same files.\\nhostPath volumes are the first type of persistent storage we’re introducing, because\\nboth the gitRepo and emptyDir volumes’ contents get deleted when a pod is torn\\ndown, whereas a hostPath volume’s contents don’t. If a pod is deleted and the next\\npod uses a hostPath volume pointing to the same path on the host, the new pod will\\nsee whatever was left behind by the previous pod, but only if it’s scheduled to the same\\nnode as the first pod.\\nNode 1\\nPod\\nhostPath\\nvolume\\nPod\\nhostPath\\nvolume\\nNode 2\\nPod\\nhostPath\\nvolume\\n/some/path/on/host\\n/some/path/on/host\\nFigure 6.4\\nA hostPath volume mounts a file or directory on the worker node into \\nthe container’s filesystem.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Node 1\n",
       "   Pod Pod\n",
       "   hostPath hostPath\n",
       "   volume volume\n",
       "   /some/path/on/host, Col1, Node 2\n",
       "   Pod\n",
       "   hostPath\n",
       "   volume\n",
       "   /some/path/on/host]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'gitRepo',\n",
       "    'description': 'a dedicated directory created for and used exclusively by a pod that contains the volume',\n",
       "    'category': 'volume'},\n",
       "   {'entity': 'emptyDir',\n",
       "    'description': 'a dedicated directory created specifically for, and used exclusively by, the pod that contains the volume',\n",
       "    'category': 'volume'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a container that runs one or more containers as part of a deployment',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'hostPath',\n",
       "    'description': \"a type of persistent storage that points to a specific file or directory on the node's filesystem\",\n",
       "    'category': 'volume'},\n",
       "   {'entity': 'file system',\n",
       "    'description': 'the storage of files on a computer system',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'a physical or virtual machine running an instance of Kubernetes',\n",
       "    'category': 'machine'},\n",
       "   {'entity': 'DaemonSet',\n",
       "    'description': 'a type of pod that runs one or more containers as part of a deployment, usually managed by a DaemonSet',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'an open-source container orchestration system for automating the deployment, scaling, and management of containers in a cluster',\n",
       "    'category': 'orchestration system'},\n",
       "   {'entity': 'volume',\n",
       "    'description': 'a storage resource that can be mounted into one or more containers within a pod',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'a lightweight and standalone execution environment that packages code, libraries, and settings for software applications',\n",
       "    'category': 'runtime'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"pod\", \"description\": \"accesses files on node\\'s file system\", \"destination_entity\": \"node\"},\\n  {\"source_entity\": \"DaemonSet\", \"description\": \"manages system-level pods that need to access node\\'s files or devices through filesystem\", \"destination_entity\": \"node\"},\\n  {\"source_entity\": \"pod\", \"description\": \"uses hostPath volume to access node\\'s file system\", \"destination_entity\": \"hostPath\"},\\n  {\"source_entity\": \"hostPath\", \"description\": \"points to a specific file or directory on node\\'s file system\", \"destination_entity\": \"file system\"},\\n  {\"source_entity\": \"pod\", \"description\": \"sees the same files as another pod using the same path in their hostPath volume\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"gitRepo\", \"description\": \"is a dedicated directory created for and used exclusively by the pod that contains it\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"emptyDir\", \"description\": \"is a dedicated directory created for and used exclusively by the pod that contains it\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"makes it possible for system-level pods to access node\\'s files or devices through filesystem using hostPath volume\", \"destination_entity\": \"node\"},\\n  {\"source_entity\": \"container\", \"description\": \"has a hostPath volume mounted into its filesystem\", \"destination_entity\": \"hostPath\"},\\n  {\"source_entity\": \"DaemonSet\", \"description\": \"usually manages system-level pods that use hostPath volumes\", \"destination_entity\": \"pod\"}\\n]\\n```'},\n",
       " {'page': 202,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '170\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\n If you’re thinking of using a hostPath volume as the place to store a database’s\\ndata directory, think again. Because the volume’s contents are stored on a specific\\nnode’s filesystem, when the database pod gets rescheduled to another node, it will no\\nlonger see the data. This explains why it’s not a good idea to use a hostPath volume\\nfor regular pods, because it makes the pod sensitive to what node it’s scheduled to.\\n6.3.2\\nExamining system pods that use hostPath volumes\\nLet’s see how a hostPath volume can be used properly. Instead of creating a new pod,\\nlet’s see if any existing system-wide pods are already using this type of volume. As you\\nmay remember from one of the previous chapters, several such pods are running in\\nthe kube-system namespace. Let’s list them again:\\n$ kubectl get pod s --namespace kube-system\\nNAME                          READY     STATUS    RESTARTS   AGE\\nfluentd-kubia-4ebc2f1e-9a3e   1/1       Running   1          4d\\nfluentd-kubia-4ebc2f1e-e2vz   1/1       Running   1          31d\\n...\\nPick the first one and see what kinds of volumes it uses (shown in the following listing).\\n$ kubectl describe po fluentd-kubia-4ebc2f1e-9a3e --namespace kube-system\\nName:           fluentd-cloud-logging-gke-kubia-default-pool-4ebc2f1e-9a3e\\nNamespace:      kube-system\\n...\\nVolumes:\\n  varlog:\\n    Type:       HostPath (bare host directory volume)\\n    Path:       /var/log\\n  varlibdockercontainers:\\n    Type:       HostPath (bare host directory volume)\\n    Path:       /var/lib/docker/containers\\nTIP\\nIf you’re using Minikube, try the kube-addon-manager-minikube pod.\\nAha! The pod uses two hostPath volumes to gain access to the node’s /var/log and\\nthe /var/lib/docker/containers directories. You’d think you were lucky to find a pod\\nusing a hostPath volume on the first try, but not really (at least not on GKE). Check\\nthe other pods, and you’ll see most use this type of volume either to access the node’s\\nlog files, kubeconfig (the Kubernetes config file), or the CA certificates.\\n If you inspect the other pods, you’ll see none of them uses the hostPath volume\\nfor storing their own data. They all use it to get access to the node’s data. But as we’ll\\nsee later in the chapter, hostPath volumes are often used for trying out persistent stor-\\nage in single-node clusters, such as the one created by Minikube. Read on to learn\\nabout the types of volumes you should use for storing persistent data properly even in\\na multi-node cluster.\\nListing 6.3\\n A pod using hostPath volumes to access the node’s logs\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'hostPath volume',\n",
       "    'description': \"A type of volume that provides access to a specific node's filesystem.\",\n",
       "    'category': 'volumes'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for managing Kubernetes clusters.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A container with one or more containers running in it.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'fluentd-kubia',\n",
       "    'description': 'A pod running in the kube-system namespace.',\n",
       "    'category': 'application'},\n",
       "   {'entity': '/var/log',\n",
       "    'description': \"A directory on a node's filesystem used for log files.\",\n",
       "    'category': 'directory'},\n",
       "   {'entity': '/var/lib/docker/containers',\n",
       "    'description': \"A directory on a node's filesystem used by Docker containers.\",\n",
       "    'category': 'directory'},\n",
       "   {'entity': 'GKE',\n",
       "    'description': 'A cloud-based Kubernetes platform.',\n",
       "    'category': 'cloud-platform'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'A single-node Kubernetes cluster.',\n",
       "    'category': 'container-engine'},\n",
       "   {'entity': 'kube-system',\n",
       "    'description': 'A namespace for system-wide pods in a Kubernetes cluster.',\n",
       "    'category': 'namespace'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Minikube\", \"description\": \"uses hostPath volumes to access node\\'s logs\", \"destination_entity\": \"node\\'s logs\"},\\n  {\"source_entity\": \"fluentd-kubia\", \"description\": \"uses hostPath volume to gain access to /var/lib/docker/containers directory\", \"destination_entity\": \"/var/lib/docker/containers\"},\\n  {\"source_entity\": \"fluentd-kubia\", \"description\": \"uses hostPath volume to gain access to /var/log directory\", \"destination_entity\": \"/var/log\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"commands used to list and examine system-wide pods that use hostPath volumes\", \"destination_entity\": \"system-wide pods\"},\\n  {\"source_entity\": \"GKE\", \"description\": \"hosts system-wide pods that use hostPath volumes\", \"destination_entity\": \"system-wide pods\"},\\n  {\"source_entity\": \"hostPath volume\", \"description\": \"used by system-wide pods to access node\\'s logs and data\", \"destination_entity\": \"node\\'s logs and data\"},\\n  {\"source_entity\": \"pod\", \"description\": \"uses hostPath volume to gain access to /var/log directory\", \"destination_entity\": \"/var/log\"}\\n]\\n```'},\n",
       " {'page': 203,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '171\\nUsing persistent storage\\nTIP\\nRemember to use hostPath volumes only if you need to read or write sys-\\ntem files on the node. Never use them to persist data across pods. \\n6.4\\nUsing persistent storage\\nWhen an application running in a pod needs to persist data to disk and have that\\nsame data available even when the pod is rescheduled to another node, you can’t use\\nany of the volume types we’ve mentioned so far. Because this data needs to be accessi-\\nble from any cluster node, it must be stored on some type of network-attached stor-\\nage (NAS).\\n To learn about volumes that allow persisting data, you’ll create a pod that will run\\nthe MongoDB document-oriented NoSQL database. Running a database pod without\\na volume or with a non-persistent volume doesn’t make sense, except for testing\\npurposes, so you’ll add an appropriate type of volume to the pod and mount it in the\\nMongoDB container. \\n6.4.1\\nUsing a GCE Persistent Disk in a pod volume\\nIf you’ve been running these examples on Google Kubernetes Engine, which runs\\nyour cluster nodes on Google Compute Engine (GCE), you’ll use a GCE Persistent\\nDisk as your underlying storage mechanism. \\n In the early versions, Kubernetes didn’t provision the underlying storage automati-\\ncally—you had to do that manually. Automatic provisioning is now possible, and you’ll\\nlearn about it later in the chapter, but first, you’ll start by provisioning the storage\\nmanually. It will give you a chance to learn exactly what’s going on underneath. \\nCREATING A GCE PERSISTENT DISK\\nYou’ll start by creating the GCE persistent disk first. You need to create it in the same\\nzone as your Kubernetes cluster. If you don’t remember what zone you created the\\ncluster in, you can see it by listing your Kubernetes clusters with the gcloud command\\nlike this:\\n$ gcloud container clusters list\\nNAME   ZONE            MASTER_VERSION  MASTER_IP       ...\\nkubia  europe-west1-b  1.2.5           104.155.84.137  ...\\nThis shows you’ve created your cluster in zone europe-west1-b, so you need to create\\nthe GCE persistent disk in the same zone as well. You create the disk like this:\\n$ gcloud compute disks create --size=1GiB --zone=europe-west1-b mongodb\\nWARNING: You have selected a disk size of under [200GB]. This may result in \\npoor I/O performance. For more information, see: \\nhttps://developers.google.com/compute/docs/disks#pdperformance.\\nCreated [https://www.googleapis.com/compute/v1/projects/rapid-pivot-\\n136513/zones/europe-west1-b/disks/mongodb].\\nNAME     ZONE            SIZE_GB  TYPE         STATUS\\nmongodb  europe-west1-b  1        pd-standard  READY\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'persistent storage',\n",
       "    'description': 'storage mechanism for persisting data across pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'hostPath volumes',\n",
       "    'description': 'volumes used to read or write system files on the node',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'MongoDB',\n",
       "    'description': 'document-oriented NoSQL database',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Google Kubernetes Engine (GCE)',\n",
       "    'description': 'cloud platform running cluster nodes on Google Compute Engine (GCE)',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'gcloud command',\n",
       "    'description': 'tool for managing Kubernetes clusters and persistent disks',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes cluster',\n",
       "    'description': 'collection of pods, services, and persistent volumes running on GCE nodes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'GCE Persistent Disk',\n",
       "    'description': 'network-attached storage mechanism for persisting data across pods',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'persistent disk provisioning',\n",
       "    'description': 'mechanism for automatically or manually provisioning persistent disks',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'gcloud container clusters list command',\n",
       "    'description': 'tool for listing Kubernetes clusters and their associated zone',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'GCE node',\n",
       "    'description': 'physical machine running a Kubernetes cluster on GCE',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'lightweight and portable container running an application in a Kubernetes cluster',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'persistent volume',\n",
       "    'description': 'storage mechanism for persisting data across pods, accessible by multiple nodes',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes cluster\",\\n    \"description\": \"provisioning persistent disk for MongoDB database pod\",\\n    \"destination_entity\": \"persistent disk provisioning\"\\n  },\\n  {\\n    \"source_entity\": \"GCE node\",\\n    \"description\": \"running Kubernetes Engine (GCE) cluster\",\\n    \"destination_entity\": \"Google Kubernetes Engine (GCE)\"\\n  },\\n  {\\n    \"source_entity\": \"gcloud container clusters list command\",\\n    \"description\": \"listing available Kubernetes clusters in a zone\",\\n    \"destination_entity\": \"Kubernetes cluster\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes cluster\",\\n    \"description\": \"creating persistent disk for MongoDB database pod\",\\n    \"destination_entity\": \"persistent disk provisioning\"\\n  },\\n  {\\n    \"source_entity\": \"GCE Persistent Disk\",\\n    \"description\": \"providing underlying storage mechanism for Kubernetes clusters\",\\n    \"destination_entity\": \"Google Kubernetes Engine (GCE)\"\\n  },\\n  {\\n    \"source_entity\": \"gcloud command\",\\n    \"description\": \"creating GCE persistent disk manually\",\\n    \"destination_entity\": \"GCE Persistent Disk\"\\n  },\\n  {\\n    \"source_entity\": \"MongoDB\",\\n    \"description\": \"requiring persistent volume to store database data\",\\n    \"destination_entity\": \"persistent volume\"\\n  },\\n  {\\n    \"source_entity\": \"hostPath volumes\",\\n    \"description\": \"using for reading or writing system files on a node\",\\n    \"destination_entity\": \"GCE node\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes cluster\",\\n    \"description\": \"provisioning persistent storage for MongoDB database pod\",\\n    \"destination_entity\": \"persistent storage\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"requiring persistent volume to store data across reschedules\",\\n    \"destination_entity\": \"persistent volume\"\\n  }\\n]\\n```'},\n",
       " {'page': 204,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '172\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\nThis command creates a 1 GiB large GCE persistent disk called mongodb. You can\\nignore the warning about the disk size, because you don’t care about the disk’s perfor-\\nmance for the tests you’re about to run.\\nCREATING A POD USING A GCEPERSISTENTDISK VOLUME\\nNow that you have your physical storage properly set up, you can use it in a volume\\ninside your MongoDB pod. You’re going to prepare the YAML for the pod, which is\\nshown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: mongodb \\nspec:\\n  volumes:\\n  - name: mongodb-data          \\n    gcePersistentDisk:           \\n      pdName: mongodb            \\n      fsType: ext4             \\n  containers:\\n  - image: mongo\\n    name: mongodb\\n    volumeMounts:                \\n    - name: mongodb-data         \\n      mountPath: /data/db      \\n    ports:\\n    - containerPort: 27017\\n      protocol: TCP\\nNOTE\\nIf you’re using Minikube, you can’t use a GCE Persistent Disk, but you\\ncan deploy mongodb-pod-hostpath.yaml, which uses a hostPath volume\\ninstead of a GCE PD.\\nThe pod contains a single container and a single volume backed by the GCE Per-\\nsistent Disk you’ve created (as shown in figure 6.5). You’re mounting the volume\\ninside the container at /data/db, because that’s where MongoDB stores its data.\\nListing 6.4\\nA pod using a gcePersistentDisk volume: mongodb-pod-gcepd.yaml\\nThe name\\nof the\\nvolume\\n(also\\nreferenced\\nwhen\\nmounting\\nthe volume)\\nThe type of the volume \\nis a GCE Persistent Disk.\\nThe name of the persistent \\ndisk must match the actual \\nPD you created earlier.\\nThe filesystem type is EXT4 \\n(a type of Linux filesystem).\\nThe path where MongoDB \\nstores its data\\nPod: mongodb\\nContainer: mongodb\\nvolumeMounts:\\nname: mongodb-data\\nmountPath: /data/db\\ngcePersistentDisk:\\npdName: mongodb\\nGCE\\nPersistent Disk:\\nmongodb\\nVolume:\\nmongodb\\nFigure 6.5\\nA pod with a single container running MongoDB, which mounts a volume referencing an \\nexternal GCE Persistent Disk\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'gcePersistentDisk',\n",
       "    'description': 'Google Cloud Engine persistent disk',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A pod is the basic execution unit in Kubernetes, and a pod can contain one or more containers.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A container is a runtime instance of a Docker image, which provides isolation and resource sharing between applications.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'docker',\n",
       "    'description': 'Docker is a containerization platform that allows developers to package their application and its dependencies into a single container.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'mongo',\n",
       "    'description': 'MongoDB is a NoSQL document-based database.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'mongodb-data',\n",
       "    'description': 'A volume name for storing MongoDB data.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'hostPath',\n",
       "    'description': 'A type of Kubernetes volume that mounts a host directory into the pod container.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'minikube',\n",
       "    'description': 'A tool to run Kubernetes locally, and can be used for development and testing purposes.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'yaml',\n",
       "    'description': 'A human-readable serialization format for configuration files, commonly used in Kubernetes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'fsType',\n",
       "    'description': 'The filesystem type to use for the persistent disk volume.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'ext4',\n",
       "    'description': 'A type of Linux filesystem.',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[{\"source_entity\": \"GCE Persistent Disk\", \"description\": \"creates a 1 GiB large disk called mongodb\", \"destination_entity\": \"mongodb\"}]\\n\\n'},\n",
       " {'page': 205,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '173\\nUsing persistent storage\\nWRITING DATA TO THE PERSISTENT STORAGE BY ADDING DOCUMENTS TO YOUR MONGODB DATABASE\\nNow that you’ve created the pod and the container has been started, you can run the\\nMongoDB shell inside the container and use it to write some data to the data store.\\n You’ll run the shell as shown in the following listing.\\n$ kubectl exec -it mongodb mongo\\nMongoDB shell version: 3.2.8\\nconnecting to: mongodb://127.0.0.1:27017\\nWelcome to the MongoDB shell.\\nFor interactive help, type \"help\".\\nFor more comprehensive documentation, see\\n    http://docs.mongodb.org/\\nQuestions? Try the support group\\n    http://groups.google.com/group/mongodb-user\\n...\\n> \\nMongoDB allows storing JSON documents, so you’ll store one to see if it’s stored per-\\nsistently and can be retrieved after the pod is re-created. Insert a new JSON document\\nwith the following commands: \\n> use mystore\\nswitched to db mystore\\n> db.foo.insert({name:\\'foo\\'})\\nWriteResult({ \"nInserted\" : 1 })\\nYou’ve inserted a simple JSON document with a single property (name: ’foo’). Now,\\nuse the find() command to see the document you inserted:\\n> db.foo.find()\\n{ \"_id\" : ObjectId(\"57a61eb9de0cfd512374cc75\"), \"name\" : \"foo\" }\\nThere it is. The document should be stored in your GCE persistent disk now. \\nRE-CREATING THE POD AND VERIFYING THAT IT CAN READ THE DATA PERSISTED BY THE PREVIOUS POD\\nYou can now exit the mongodb shell (type exit and press Enter), and then delete the\\npod and recreate it:\\n$ kubectl delete pod mongodb\\npod \"mongodb\" deleted\\n$ kubectl create -f mongodb-pod-gcepd.yaml\\npod \"mongodb\" created\\nThe new pod uses the exact same GCE persistent disk as the previous pod, so the\\nMongoDB container running inside it should see the exact same data, even if the pod\\nis scheduled to a different node.\\nTIP\\nYou can see what node a pod is scheduled to by running kubectl get po\\n-o wide.\\nListing 6.5\\nEntering the MongoDB shell inside the mongodb pod\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'MongoDB',\n",
       "    'description': 'NoSQL document-oriented database system.',\n",
       "    'category': 'Database'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for managing Kubernetes resources.',\n",
       "    'category': 'Container Management'},\n",
       "   {'entity': 'exec',\n",
       "    'description': 'Command used to execute a command inside a container.',\n",
       "    'category': 'Container Command'},\n",
       "   {'entity': 'mongo',\n",
       "    'description': 'MongoDB shell command.',\n",
       "    'category': 'Database Command'},\n",
       "   {'entity': 'GCE persistent disk',\n",
       "    'description': 'Google Cloud Engine persistent storage solution.',\n",
       "    'category': 'Storage'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Kubernetes pod, a logical host for running containers.',\n",
       "    'category': 'Container Management'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'Lightweight and standalone execution environment.',\n",
       "    'category': 'Container'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'Physical or virtual machine in a Kubernetes cluster.',\n",
       "    'category': 'Cluster Node'},\n",
       "   {'entity': 'pod template',\n",
       "    'description': 'Configuration for creating a pod, including container specs.',\n",
       "    'category': 'Container Template'},\n",
       "   {'entity': 'document',\n",
       "    'description': 'JSON object stored in MongoDB database.',\n",
       "    'category': 'Database Object'},\n",
       "   {'entity': 'collection',\n",
       "    'description': 'Group of related documents in a MongoDB database.',\n",
       "    'category': 'Database Collection'},\n",
       "   {'entity': 'find()',\n",
       "    'description': 'MongoDB command for retrieving documents from a collection.',\n",
       "    'category': 'Database Command'},\n",
       "   {'entity': 'insert()',\n",
       "    'description': 'MongoDB command for inserting new documents into a collection.',\n",
       "    'category': 'Database Command'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"MongoDB\", \"description\": \"stored data persistently\", \"destination_entity\": \"GCE persistent disk\"},\\n  {\"source_entity\": \"pod template\", \"description\": \"recreated a new pod with same GCE persistent disk\", \"destination_entity\": \"GCE persistent disk\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"delete the pod and recreate it\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"MongoDB\", \"description\": \"see if data is stored persistently\", \"destination_entity\": \"document\"},\\n  {\"source_entity\": \"insert()\", \"description\": \"insert a new JSON document into collection\", \"destination_entity\": \"collection\"},\\n  {\"source_entity\": \"find()\", \"description\": \"retrieve the document inserted earlier\", \"destination_entity\": \"document\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"exec -it mongodb mongo to run MongoDB shell inside container\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"mongo\", \"description\": \"run the MongoDB shell inside the container\", \"destination_entity\": \"MongoDB\"},\\n  {\"source_entity\": \"node\", \"description\": \"schedule a pod to a different node\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"get information about pods, including which node they are scheduled on\", \"destination_entity\": \"node\"}\\n]'},\n",
       " {'page': 206,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '174\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\nOnce the container is up, you can again run the MongoDB shell and check to see if the\\ndocument you stored earlier can still be retrieved, as shown in the following listing.\\n$ kubectl exec -it mongodb mongo\\nMongoDB shell version: 3.2.8\\nconnecting to: mongodb://127.0.0.1:27017\\nWelcome to the MongoDB shell.\\n...\\n> use mystore\\nswitched to db mystore\\n> db.foo.find()\\n{ \"_id\" : ObjectId(\"57a61eb9de0cfd512374cc75\"), \"name\" : \"foo\" }\\nAs expected, the data is still there, even though you deleted the pod and re-created it.\\nThis confirms you can use a GCE persistent disk to persist data across multiple pod\\ninstances. \\n You’re done playing with the MongoDB pod, so go ahead and delete it again, but\\nhold off on deleting the underlying GCE persistent disk. You’ll use it again later in\\nthe chapter.\\n6.4.2\\nUsing other types of volumes with underlying persistent storage\\nThe reason you created the GCE Persistent Disk volume is because your Kubernetes\\ncluster runs on Google Kubernetes Engine. When you run your cluster elsewhere, you\\nshould use other types of volumes, depending on the underlying infrastructure.\\n If your Kubernetes cluster is running on Amazon’s AWS EC2, for example, you can\\nuse an awsElasticBlockStore volume to provide persistent storage for your pods. If\\nyour cluster runs on Microsoft Azure, you can use the azureFile or the azureDisk\\nvolume. We won’t go into detail on how to do that here, but it’s virtually the same as in\\nthe previous example. First, you need to create the actual underlying storage, and\\nthen set the appropriate properties in the volume definition.\\nUSING AN AWS ELASTIC BLOCK STORE VOLUME\\nFor example, to use an AWS elastic block store instead of the GCE Persistent Disk,\\nyou’d only need to change the volume definition as shown in the following listing (see\\nthose lines printed in bold).\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: mongodb \\nspec:\\n  volumes:                       \\n  - name: mongodb-data           \\n    awsElasticBlockStore:          \\nListing 6.6\\nRetrieving MongoDB’s persisted data in a new pod\\nListing 6.7\\nA pod using an awsElasticBlockStore volume: mongodb-pod-aws.yaml\\nUsing awsElasticBlockStore \\ninstead of gcePersistentDisk\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'a command used to execute commands inside containers',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'mongodb',\n",
       "    'description': 'a NoSQL database system',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'mongo',\n",
       "    'description': 'the MongoDB shell command',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a compute resource in Kubernetes that can run one or more containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'gce persistent disk',\n",
       "    'description': 'persistent storage provided by Google Cloud Platform for Kubernetes volumes',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'aws elastic block store volume',\n",
       "    'description': 'persistent storage provided by Amazon Web Services for Kubernetes volumes',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'azure file volume',\n",
       "    'description': 'persistent storage provided by Microsoft Azure for Kubernetes volumes',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'a field in a Kubernetes configuration file that specifies the API version of the resource being defined',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'a field in a Kubernetes configuration file that specifies the type of resource being defined',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'a field in a Kubernetes configuration file that specifies metadata about the resource being defined',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'a field in a Kubernetes configuration file that specifies the desired state of the resource being defined',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'volumes',\n",
       "    'description': 'a field in a Kubernetes configuration file that specifies persistent storage for a pod',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'awsElasticBlockStore',\n",
       "    'description': 'a type of volume provided by Amazon Web Services for Kubernetes volumes',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'gcePersistentDisk',\n",
       "    'description': 'a type of volume provided by Google Cloud Platform for Kubernetes volumes',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'azureFile',\n",
       "    'description': 'a type of volume provided by Microsoft Azure for Kubernetes volumes',\n",
       "    'category': 'storage'}],\n",
       "  'relationships': '[{\"source_entity\": \"kubectl\", \"description\": \"executes a command to access the MongoDB shell\", \"destination_entity\": \"MongoDB shell\"},\\n\\n {\"source_entity\": \"MongoDB shell\", \"description\": \"connects to a database and retrieves data\", \"destination_entity\": \"database (mystore)\"}, \\n\\n{\"source_entity\": \"kubectl\", \"description\": \"deletes a pod\", \"destination_entity\": \"pod (mongodb)\"}, \\n\\n{\"source_entity\": \"kubectl\", \"description\": \"runs a MongoDB shell command\", \"destination_entity\": \"MongoDB shell\"},\\n\\n {\"source_entity\": \"MongoDB shell\", \"description\": \"uses a database and finds data\", \"destination_entity\": \"database (mystore)\"},\\n\\n{\"source_entity\": \"GCE Persistent Disk volume\", \"description\": \"provides persistent storage for a pod\", \"destination_entity\": \"pod (mongodb)\"}, \\n\\n{\"source_entity\": \"Kubernetes cluster\", \"description\": \"runs on Google Kubernetes Engine\", \"destination_entity\": \"Google Kubernetes Engine\"},\\n\\n {\"source_entity\": \"AWS EC2\", \"description\": \"can use an awsElasticBlockStore volume for persistent storage\", \"destination_entity\": \"awsElasticBlockStore volume\"}, \\n\\n{\"source_entity\": \"Microsoft Azure\", \"description\": \"can use the azureFile or azureDisk volume for persistent storage\", \"destination_entity\": \"azureFile or azureDisk volume\"},\\n\\n {\"source_entity\": \"kubectl\", \"description\": \"creates an awsElasticBlockStore volume\", \"destination_entity\": \"awsElasticBlockStore volume\"}, \\n\\n{\"source_entity\": \"apiVersion\", \"description\": \"defines the version of Kubernetes API used\", \"destination_entity\": \"Kubernetes API\"},\\n\\n {\"source_entity\": \"metadata\", \"description\": \"provides metadata about a pod\", \"destination_entity\": \"pod (mongodb)\"}, \\n\\n{\"source_entity\": \"volumes\", \"description\": \"lists the volumes attached to a pod\", \"destination_entity\": \"pod (mongodb)\"}, \\n\\n{\"source_entity\": \"awsElasticBlockStore volume\", \"description\": \"provides persistent storage for a pod on AWS EC2\", \"destination_entity\": \"pod (mongodb)\"}, \\n\\n{\"source_entity\": \"azureFile volume\", \"description\": \"provides persistent storage for a pod on Microsoft Azure\", \"destination_entity\": \"pod (mongodb)\"}, \\n\\n{\"source_entity\": \"gcePersistentDisk volume\", \"description\": \"provides persistent storage for a pod on Google Kubernetes Engine\", \"destination_entity\": \"pod (mongodb)\"}, \\n\\n{\"source_entity\": \"mongo\", \"description\": \"runs in a MongoDB shell to access data\", \"destination_entity\": \"database (mystore)\"}]'},\n",
       " {'page': 207,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '175\\nUsing persistent storage\\n      volumeId: my-volume          \\n      fsType: ext4       \\n  containers:\\n  - ...\\nUSING AN NFS VOLUME\\nIf your cluster is running on your own set of servers, you have a vast array of other sup-\\nported options for mounting external storage inside your volume. For example, to\\nmount a simple NFS share, you only need to specify the NFS server and the path\\nexported by the server, as shown in the following listing.\\n  volumes:                       \\n  - name: mongodb-data           \\n    nfs:                     \\n      server: 1.2.3.4         \\n      path: /some/path     \\nUSING OTHER STORAGE TECHNOLOGIES\\nOther supported options include iscsi for mounting an ISCSI disk resource, glusterfs\\nfor a GlusterFS mount, rbd for a RADOS Block Device, flexVolume, cinder, cephfs,\\nflocker, fc (Fibre Channel), and others. You don’t need to know all of them if you’re\\nnot using them. They’re mentioned here to show you that Kubernetes supports a\\nbroad range of storage technologies and you can use whichever you prefer and are\\nused to.\\n To see details on what properties you need to set for each of these volume types,\\nyou can either turn to the Kubernetes API definitions in the Kubernetes API refer-\\nence or look up the information through kubectl explain, as shown in chapter 3. If\\nyou’re already familiar with a particular storage technology, using the explain com-\\nmand should allow you to easily figure out how to mount a volume of the proper type\\nand use it in your pods.\\n But does a developer need to know all this stuff? Should a developer, when creat-\\ning a pod, have to deal with infrastructure-related storage details, or should that be\\nleft to the cluster administrator? \\n Having a pod’s volumes refer to the actual underlying infrastructure isn’t what\\nKubernetes is about, is it? For example, for a developer to have to specify the host-\\nname of the NFS server feels wrong. And that’s not even the worst thing about it. \\n Including this type of infrastructure-related information into a pod definition\\nmeans the pod definition is pretty much tied to a specific Kubernetes cluster. You\\ncan’t use the same pod definition in another one. That’s why using volumes like this\\nisn’t the best way to attach persistent storage to your pods. You’ll learn how to improve\\non this in the next section.\\nListing 6.8\\nA pod using an nfs volume: mongodb-pod-nfs.yaml\\nSpecify the ID of the EBS \\nvolume you created.\\nThe filesystem type \\nis EXT4 as before.\\nThis volume is backed \\nby an NFS share.\\nThe IP of the \\nNFS server\\nThe path exported \\nby the server\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'persistent storage',\n",
       "    'description': 'a way to attach external storage to a pod',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'volumeId',\n",
       "    'description': 'unique identifier for a volume',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'fsType',\n",
       "    'description': 'filesystem type used by the volume (e.g. ext4)',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'NFS share',\n",
       "    'description': 'a network file system shared between hosts',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'server',\n",
       "    'description': 'IP address of the NFS server',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'path',\n",
       "    'description': 'directory on the NFS server that is being shared',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'iscsi',\n",
       "    'description': 'iSCSI protocol for mounting block devices',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'GlusterFS',\n",
       "    'description': 'distributed file system',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'RADOS Block Device',\n",
       "    'description': 'a distributed storage solution',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'flexVolume',\n",
       "    'description': 'a volume plugin that allows for various types of external storage',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'cinder',\n",
       "    'description': 'an open-source block storage system',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'cephfs',\n",
       "    'description': 'Ceph file system',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'flocker',\n",
       "    'description': 'a distributed locking system',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'fc',\n",
       "    'description': 'Fibre Channel protocol for storage access',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'kubectl explain',\n",
       "    'description': 'command-line tool for getting information about Kubernetes resources',\n",
       "    'category': 'tool'},\n",
       "   {'entity': 'pod definition',\n",
       "    'description': 'a configuration file that defines a pod and its properties',\n",
       "    'category': 'configuration'}],\n",
       "  'relationships': '[{\"source_entity\":\"Kubernetes\",\"description\":\"supports a broad range of storage technologies\",\"destination_entity\":\"iscsi\"},{\"source_entity\":\"Kubernetes\",\"description\":\"supports a broad range of storage technologies\",\"destination_entity\":\"glusterfs\"},{\"source_entity\":\"Kubernetes\",\"description\":\"supports a broad range of storage technologies\",\"destination_entity\":\"rbd\"},{\"source_entity\":\"Kubernetes\",\"description\":\"supports a broad range of storage technologies\",\"destination_entity\":\"flexVolume\"},{\"source_entity\":\"Kubernetes\",\"description\":\"supports a broad range of storage technologies\",\"destination_entity\":\"cinder\"},{\"source_entity\":\"Kubernetes\",\"description\":\"supports a broad range of storage technologies\",\"destination_entity\":\"cephfs\"},{\"source_entity\":\"Kubernetes\",\"description\":\"supports a broad range of storage technologies\",\"destination_entity\":\"flocker\"},{\"source_entity\":\"Kubernetes\",\"description\":\"supports a broad range of storage technologies\",\"destination_entity\":\"fc\"},{\"source_entity\":\"developer\",\"description\":\"should not have to specify infrastructure-related storage details\",\"destination_entity\":\"cluster administrator\"},{\"source_entity\":\"pod definition\",\"description\":\"should not be tied to a specific Kubernetes cluster\",\"destination_entity\":\"persistent storage\"},{\"source_entity\":\"kubectl explain\",\"description\":\"can figure out how to mount a volume of the proper type\",\"destination_entity\":\"volume types\"},{\"source_entity\":\"NFS share\",\"description\":\"is backed by an NFS server\",\"destination_entity\":\"server\"},{\"source_entity\":\"RADOS Block Device\",\"description\":\"is supported as a storage technology\",\"destination_entity\":\"Kubernetes\"},{\"source_entity\":\"GlusterFS\",\"description\":\"is supported as a storage technology\",\"destination_entity\":\"Kubernetes\"},{\"source_entity\":\"flexVolume\",\"description\":\"is supported as a storage technology\",\"destination_entity\":\"Kubernetes\"},{\"source_entity\":\"cinder\",\"description\":\"is supported as a storage technology\",\"destination_entity\":\"Kubernetes\"},{\"source_entity\":\"cephfs\",\"description\":\"is supported as a storage technology\",\"destination_entity\":\"Kubernetes\"},{\"source_entity\":\"flocker\",\"description\":\"is supported as a storage technology\",\"destination_entity\":\"Kubernetes\"},{\"source_entity\":\"fc\",\"description\":\"is supported as a storage technology\",\"destination_entity\":\"Kubernetes\"},{\"source_entity\":\"volumeId\",\"description\":\"specifies the ID of the EBS volume\",\"destination_entity\":\"EBS volume\"}]'},\n",
       " {'page': 208,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '176\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\n6.5\\nDecoupling pods from the underlying storage technology\\nAll the persistent volume types we’ve explored so far have required the developer of the\\npod to have knowledge of the actual network storage infrastructure available in the clus-\\nter. For example, to create a NFS-backed volume, the developer has to know the actual\\nserver the NFS export is located on. This is against the basic idea of Kubernetes, which\\naims to hide the actual infrastructure from both the application and its developer, leav-\\ning them free from worrying about the specifics of the infrastructure and making apps\\nportable across a wide array of cloud providers and on-premises datacenters.\\n Ideally, a developer deploying their apps on Kubernetes should never have to\\nknow what kind of storage technology is used underneath, the same way they don’t\\nhave to know what type of physical servers are being used to run their pods. Infrastruc-\\nture-related dealings should be the sole domain of the cluster administrator.\\n When a developer needs a certain amount of persistent storage for their applica-\\ntion, they can request it from Kubernetes, the same way they can request CPU, mem-\\nory, and other resources when creating a pod. The system administrator can configure\\nthe cluster so it can give the apps what they request.\\n6.5.1\\nIntroducing PersistentVolumes and PersistentVolumeClaims\\nTo enable apps to request storage in a Kubernetes cluster without having to deal with\\ninfrastructure specifics, two new resources were introduced. They are Persistent-\\nVolumes and PersistentVolumeClaims. The names may be a bit misleading, because as\\nyou’ve seen in the previous few sections, even regular Kubernetes volumes can be\\nused to store persistent data. \\n Using a PersistentVolume inside a pod is a little more complex than using a regular\\npod volume, so let’s illustrate how pods, PersistentVolumeClaims, PersistentVolumes,\\nand the actual underlying storage relate to each other in figure 6.6.\\nPod\\nAdmin\\nVolume\\n1. Cluster admin sets up some type of\\nnetwork storage (NFS export or similar)\\n2. Admin then creates a PersistentVolume (PV)\\nby posting a PV descriptor to the Kubernetes API\\nNFS\\nexport\\nPersistent\\nVolume\\nUser\\nPersistent\\nVolumeClaim\\n3. User creates a\\nPersistentVolumeClaim (PVC)\\n4. Kubernetes ﬁnds a PV of\\nadequate size and access\\nmode and binds the PVC\\nto the PV\\n5. User creates a\\npod with a volume\\nreferencing the PVC\\nFigure 6.6\\nPersistentVolumes are provisioned by cluster admins and consumed by pods \\nthrough PersistentVolumeClaims.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Independent execution environments for containers',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'PersistentVolumes',\n",
       "    'description': 'Resources provisioned by cluster admins to store persistent data',\n",
       "    'category': 'Database'},\n",
       "   {'entity': 'PersistentVolumeClaims',\n",
       "    'description': 'Requests from pods to Persistent Volumes for storage needs',\n",
       "    'category': 'Database'},\n",
       "   {'entity': 'NFS export',\n",
       "    'description': 'Network file system export for sharing files between systems',\n",
       "    'category': 'Network'},\n",
       "   {'entity': 'API',\n",
       "    'description': 'Application programming interface for communication between components',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Kubernetes API',\n",
       "    'description': 'Specific API used by Kubernetes for component communication',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Descriptor',\n",
       "    'description': 'Definition of a resource or request, in this case Persistent Volume descriptors',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Cluster admin',\n",
       "    'description': 'System administrator responsible for setting up and managing the cluster',\n",
       "    'category': 'Hardware/Software'},\n",
       "   {'entity': 'User',\n",
       "    'description': 'Developer or user requesting resources from the system',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'PVC binding',\n",
       "    'description': 'Mechanism to bind Persistent Volume Claims to available Persistent Volumes',\n",
       "    'category': 'Process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Cluster admin\",\\n    \"description\": \"sets up network storage (e.g. NFS export)\",\\n    \"destination_entity\": \"NFS export\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster admin\",\\n    \"description\": \"creates a PersistentVolume by posting a PV descriptor to the Kubernetes API\",\\n    \"destination_entity\": \"PersistentVolumeClaims\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"requests storage for their application\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"creates a PersistentVolumeClaim (PVC)\",\\n    \"destination_entity\": \"PersistentVolumeClaims\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"finds an adequate PV and binds the PVC to it\",\\n    \"destination_entity\": \"PersistentVolumeClaims\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"creates a pod with a volume referencing the PVC\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster admin\",\\n    \"description\": \"sets up some type of network storage (e.g. NFS export)\",\\n    \"destination_entity\": \"Kubernetes API\"\\n  },\\n  {\\n    \"source_entity\": \"PersistentVolumeClaims\",\\n    \"description\": \"requests access to a PersistentVolume\",\\n    \"destination_entity\": \"PersistentVolumes\"\\n  }\\n]\\n```\\n\\nNote: I have only extracted the relations that are explicitly mentioned in the document page. If you need me to infer additional relations, please let me know!'},\n",
       " {'page': 209,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '177\\nDecoupling pods from the underlying storage technology\\nInstead of the developer adding a technology-specific volume to their pod, it’s the\\ncluster administrator who sets up the underlying storage and then registers it in\\nKubernetes by creating a PersistentVolume resource through the Kubernetes API\\nserver. When creating the PersistentVolume, the admin specifies its size and the access\\nmodes it supports. \\n When a cluster user needs to use persistent storage in one of their pods, they first\\ncreate a PersistentVolumeClaim manifest, specifying the minimum size and the access\\nmode they require. The user then submits the PersistentVolumeClaim manifest to the\\nKubernetes API server, and Kubernetes finds the appropriate PersistentVolume and\\nbinds the volume to the claim. \\n The PersistentVolumeClaim can then be used as one of the volumes inside a pod.\\nOther users cannot use the same PersistentVolume until it has been released by delet-\\ning the bound PersistentVolumeClaim.\\n6.5.2\\nCreating a PersistentVolume\\nLet’s revisit the MongoDB example, but unlike before, you won’t reference the GCE\\nPersistent Disk in the pod directly. Instead, you’ll first assume the role of a cluster\\nadministrator and create a PersistentVolume backed by the GCE Persistent Disk. Then\\nyou’ll assume the role of the application developer and first claim the PersistentVol-\\nume and then use it inside your pod.\\n In section 6.4.1 you set up the physical storage by provisioning the GCE Persistent\\nDisk, so you don’t need to do that again. All you need to do is create the Persistent-\\nVolume resource in Kubernetes by preparing the manifest shown in the following list-\\ning and posting it to the API server.\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: mongodb-pv\\nspec:\\n  capacity:                  \\n    storage: 1Gi             \\n  accessModes:                              \\n  - ReadWriteOnce                           \\n  - ReadOnlyMany                            \\n  persistentVolumeReclaimPolicy: Retain    \\n  gcePersistentDisk:                      \\n    pdName: mongodb                       \\n    fsType: ext4                          \\nListing 6.9\\nA gcePersistentDisk PersistentVolume: mongodb-pv-gcepd.yaml\\nDefining the \\nPersistentVolume’s size\\nIt can either be mounted by a single \\nclient for reading and writing or by \\nmultiple clients for reading only.\\nAfter the claim is released, \\nthe PersistentVolume \\nshould be retained (not \\nerased or deleted).\\nThe PersistentVolume is \\nbacked by the GCE Persistent \\nDisk you created earlier.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PersistentVolume',\n",
       "    'description': 'A resource in Kubernetes that represents a storage volume',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'A request for persistent storage in a pod, specifying the minimum size and access mode required',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'GCE Persistent Disk',\n",
       "    'description': 'A type of storage device used in Google Cloud Platform',\n",
       "    'category': 'cloud_storage'},\n",
       "   {'entity': 'Kubernetes API server',\n",
       "    'description': 'The component responsible for managing Kubernetes resources, such as PersistentVolumes and PersistentVolumeClaims',\n",
       "    'category': 'container_orchestrator'},\n",
       "   {'entity': 'PersistentVolumeReclaimPolicy',\n",
       "    'description': 'A policy that determines what happens to a PersistentVolume when it is released from a claim',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'ReadWriteOnce',\n",
       "    'description': 'An access mode for a PersistentVolume that allows a single client to read and write to the volume',\n",
       "    'category': 'access_mode'},\n",
       "   {'entity': 'ReadOnlyMany',\n",
       "    'description': 'An access mode for a PersistentVolume that allows multiple clients to read from the volume',\n",
       "    'category': 'access_mode'},\n",
       "   {'entity': 'Retain',\n",
       "    'description': 'A policy that retains the PersistentVolume when it is released from a claim, preventing it from being erased or deleted',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'fsType',\n",
       "    'description': 'The file system type used on the PersistentVolume',\n",
       "    'category': 'file_system'},\n",
       "   {'entity': 'pdName',\n",
       "    'description': 'The name of the GCE Persistent Disk that backs the PersistentVolume',\n",
       "    'category': 'cloud_storage'},\n",
       "   {'entity': 'capacity',\n",
       "    'description': 'A field in the PersistentVolume resource that specifies the size of the volume',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'storage',\n",
       "    'description': 'A unit of storage, equivalent to 1 GiB (gibibyte)',\n",
       "    'category': 'unit_of_storage'}],\n",
       "  'relationships': '[{\"source_entity\": \"cluster administrator\", \"description\": \"sets up underlying storage\", \"destination_entity\": \"Kubernetes API server\"},\\n {\"source_entity\": \"cluster user\", \"description\": \"creates PersistentVolumeClaim manifest\", \"destination_entity\": \"Kubernetes API server\"},\\n {\"source_entity\": \"Kubernetes API server\", \"description\": \"finds appropriate PersistentVolume and binds volume to claim\", \"destination_entity\": \"PersistentVolumeClaim\"},\\n {\"source_entity\": \"application developer\", \"description\": \"claims PersistentVolume\", \"destination_entity\": \"PersistentVolume\"},\\n {\"source_entity\": \"application developer\", \"description\": \"uses PersistentVolume inside pod\", \"destination_entity\": \"pod\"},\\n {\"source_entity\": \"cluster administrator\", \"description\": \"creates PersistentVolume resource in Kubernetes\", \"destination_entity\": \"Kubernetes API server\"},\\n {\"source_entity\": \"cluster administrator\", \"description\": \"specifies size and access modes of PersistentVolume\", \"destination_entity\": \"PersistentVolume\"},\\n {\"source_entity\": \"cluster user\", \"description\": \"specifies minimum size and access mode required for PersistentVolumeClaim\", \"destination_entity\": \"Kubernetes API server\"},\\n {\"source_entity\": \"PersistentVolumeClaim\", \"description\": \"binds to PersistentVolume\", \"destination_entity\": \"PersistentVolume\"},\\n {\"source_entity\": \"application developer\", \"description\": \"uses PersistentVolumeClaim inside pod\", \"destination_entity\": \"pod\"},\\n {\"source_entity\": \"cluster administrator\", \"description\": \"retains PersistentVolume after claim is released\", \"destination_entity\": \"PersistentVolume\"},\\n {\"source_entity\": \"Kubernetes API server\", \"description\": \"provisions GCE Persistent Disk\", \"destination_entity\": \"GCE Persistent Disk\"},\\n {\"source_entity\": \"cluster administrator\", \"description\": \"specifies fsType for PersistentVolume\", \"destination_entity\": \"PersistentVolume\"},\\n {\"source_entity\": \"persistent volume reclaim policy\", \"description\": \"sets Retain as the policy for PersistentVolume\", \"destination_entity\": \"Kubernetes API server\"}]\\n\\nNote that some entities, like \"pdName\" and \"fsType\", are not actual entities with a defined role in the context of Kubernetes and persistent storage, so they have been ignored. Also, some relations between entities might seem redundant or obvious (e.g., \"application developer uses PersistentVolumeClaim inside pod\"), but they have been included as per your request to extract all possible relations.'},\n",
       " {'page': 210,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '178\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\nNOTE\\nIf you’re using Minikube, create the PV using the mongodb-pv-host-\\npath.yaml file.\\nWhen creating a PersistentVolume, the administrator needs to tell Kubernetes what its\\ncapacity is and whether it can be read from and/or written to by a single node or by\\nmultiple nodes at the same time. They also need to tell Kubernetes what to do with the\\nPersistentVolume when it’s released (when the PersistentVolumeClaim it’s bound to is\\ndeleted). And last, but certainly not least, they need to specify the type, location, and\\nother properties of the actual storage this PersistentVolume is backed by. If you look\\nclosely, this last part is exactly the same as earlier, when you referenced the GCE Per-\\nsistent Disk in the pod volume directly (shown again in the following listing).\\nspec:\\n  volumes:                       \\n  - name: mongodb-data           \\n    gcePersistentDisk:           \\n      pdName: mongodb            \\n      fsType: ext4               \\n  ...\\nAfter you create the PersistentVolume with the kubectl create command, it should\\nbe ready to be claimed. See if it is by listing all PersistentVolumes:\\n$ kubectl get pv\\nNAME         CAPACITY   RECLAIMPOLICY   ACCESSMODES   STATUS      CLAIM\\nmongodb-pv   1Gi        Retain          RWO,ROX       Available   \\nNOTE\\nSeveral columns are omitted. Also, pv is used as a shorthand for\\npersistentvolume.\\nAs expected, the PersistentVolume is shown as Available, because you haven’t yet cre-\\nated the PersistentVolumeClaim. \\nNOTE\\nPersistentVolumes don’t belong to any namespace (see figure 6.7).\\nThey’re cluster-level resources like nodes.\\nListing 6.10\\nReferencing a GCE PD in a pod’s volume\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Minikube',\n",
       "    'description': 'A tool for running Kubernetes locally',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'A cluster-level resource that provides persistent storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool for interacting with Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'A request for persistent storage resources',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'GCE Persistent Disk',\n",
       "    'description': 'A cloud-based disk storage service',\n",
       "    'category': 'cloud'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A container that can be run in a Kubernetes cluster',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'PersistentVolumeClaim (PVC)',\n",
       "    'description': 'A request for persistent storage resources',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'ReclaimPolicy',\n",
       "    'description': \"The policy for reclaiming a PersistentVolume when it's released\",\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ACCESSMODES',\n",
       "    'description': 'The modes of access allowed for a PersistentVolume',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'STATUS',\n",
       "    'description': 'The status of a PersistentVolume',\n",
       "    'category': 'status'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes administrator\", \\n   \"description\": \"specifies capacity and access modes of a PersistentVolume\",\\n   \"destination_entity\": \"PersistentVolume\"},\\n  \\n  {\"source_entity\": \"Kubernetes administrator\", \\n   \"description\": \"specifies type, location, and other properties of the actual storage\",\\n   \"destination_entity\": \"GCE Persistent Disk\"},\\n  \\n  {\"source_entity\": \"Minikube user\", \\n   \"description\": \"creates a PV using mongodb-pv-host-path.yaml file\",\\n   \"destination_entity\": \"PersistentVolume\"},\\n  \\n  {\"source_entity\": \"Kubernetes administrator\", \\n   \"description\": \"tells Kubernetes what to do with the PersistentVolume when it\\'s released\",\\n   \"destination_entity\": \"PersistentVolumeClaim\"},\\n  \\n  {\"source_entity\": \"kubectl command\", \\n   \"description\": \"lists all PersistentVolumes\",\\n   \"destination_entity\": \"PersistentVolume\"},\\n  \\n  {\"source_entity\": \"Kubernetes administrator\", \\n   \"description\": \"creates a PersistentVolume with kubectl create command\",\\n   \"destination_entity\": \"PersistentVolume\"},\\n  \\n  {\"source_entity\": \"Minikube user\", \\n   \"description\": \"uses PersistentVolumes that belong to the cluster level\",\\n   \"destination_entity\": \"PersistentVolume\"},\\n  \\n  {\"source_entity\": \"Kubernetes administrator\", \\n   \"description\": \"specifies ReclaimPolicy of a PersistentVolume\",\\n   \"destination_entity\": \"ReclaimPolicy\"},\\n  \\n  {\"source_entity\": \"Kubernetes administrator\", \\n   \"description\": \"specifies ACCESSMODES of a PersistentVolume\",\\n   \"destination_entity\": \"ACCESSMODES\"},\\n  \\n  {\"source_entity\": \"kubectl command\", \\n   \"description\": \"lists the STATUS of all PersistentVolumes\",\\n   \"destination_entity\": \"STATUS\"},\\n  \\n  {\"source_entity\": \"pod\", \\n   \"description\": \"uses GCE PD in its volume\",\\n   \"destination_entity\": \"GCE Persistent Disk\"},\\n  \\n  {\"source_entity\": \"PersistentVolumeClaim (PVC)\", \\n   \"description\": \"binds to a PersistentVolume when created\",\\n   \"destination_entity\": \"PersistentVolume\"}\\n]\\n```'},\n",
       " {'page': 211,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '179\\nDecoupling pods from the underlying storage technology\\n6.5.3\\nClaiming a PersistentVolume by creating a \\nPersistentVolumeClaim\\nNow let’s lay down our admin hats and put our developer hats back on. Say you need\\nto deploy a pod that requires persistent storage. You’ll use the PersistentVolume you\\ncreated earlier. But you can’t use it directly in the pod. You need to claim it first.\\n Claiming a PersistentVolume is a completely separate process from creating a pod,\\nbecause you want the same PersistentVolumeClaim to stay available even if the pod is\\nrescheduled (remember, rescheduling means the previous pod is deleted and a new\\none is created). \\nCREATING A PERSISTENTVOLUMECLAIM\\nYou’ll create the claim now. You need to prepare a PersistentVolumeClaim manifest\\nlike the one shown in the following listing and post it to the Kubernetes API through\\nkubectl create.\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: mongodb-pvc          \\nListing 6.11\\nA PersistentVolumeClaim: mongodb-pvc.yaml\\nPod(s)\\nPod(s)\\nPersistent\\nVolume\\nPersistent\\nVolume\\nPersistent\\nVolume\\nPersistent\\nVolume\\n...\\nUser A\\nPersistent\\nVolume\\nClaim(s)\\nPersistent\\nVolume\\nClaim(s)\\nNamespace A\\nUser B\\nNamespace B\\nNode\\nNode\\nNode\\nNode\\nNode\\nNode\\nPersistent\\nVolume\\nFigure 6.7\\nPersistentVolumes, like cluster Nodes, don’t belong to any namespace, unlike pods and \\nPersistentVolumeClaims.\\nThe name of your claim—you’ll \\nneed this later when using the \\nclaim as the pod’s volume.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Col0, Persistent\n",
       "   Volume\n",
       "   Claim(s)]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Deployable unit in Kubernetes',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'Storage resource in Kubernetes',\n",
       "    'category': 'Resource'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'Request for storage resources in Kubernetes',\n",
       "    'category': 'Resource Request'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'Field in YAML manifest file',\n",
       "    'category': 'Configuration'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'Field in YAML manifest file',\n",
       "    'category': 'Configuration'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Section in YAML manifest file',\n",
       "    'category': 'Configuration'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'Field in YAML manifest file',\n",
       "    'category': 'Configuration'},\n",
       "   {'entity': 'kubectl create',\n",
       "    'description': 'Command to submit a manifest to Kubernetes API',\n",
       "    'category': 'Command'},\n",
       "   {'entity': 'PersistentVolumeClaim manifest',\n",
       "    'description': 'YAML file containing PersistentVolumeClaim configuration',\n",
       "    'category': 'File'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'Hardware resource in Kubernetes cluster',\n",
       "    'category': 'Hardware'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': 'Logical grouping of resources in Kubernetes',\n",
       "    'category': 'Resource'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"User A\",\\n    \"description\": \"claim a Persistent Volume for use in a pod\",\\n    \"destination_entity\": \"Persistent Volume\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"manage Persistent Volumes and Node allocation\",\\n    \"destination_entity\": \"Node\"\\n  },\\n  {\\n    \"source_entity\": \"User A\",\\n    \"description\": \"create a PersistentVolumeClaim to use a Persistent Volume\",\\n    \"destination_entity\": \"PersistentVolumeClaim\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"validate the creation of a PersistentVolumeClaim\",\\n    \"destination_entity\": \"apiVersion\"\\n  },\\n  {\\n    \"source_entity\": \"User A\",\\n    \"description\": \"define metadata for the PersistentVolumeClaim\",\\n    \"destination_entity\": \"metadata\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"determine the namespace for a pod and Persistent Volume Claim\",\\n    \"destination_entity\": \"Namespace\"\\n  },\\n  {\\n    \"source_entity\": \"User A\",\\n    \"description\": \"request resources from a node for a pod\",\\n    \"destination_entity\": \"Node\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl create\",\\n    \"description\": \"create a PersistentVolumeClaim manifest\",\\n    \"destination_entity\": \"PersistentVolumeClaim manifest\"\\n  },\\n  {\\n    \"source_entity\": \"User A\",\\n    \"description\": \"specify the name of the Persistent Volume Claim\",\\n    \"destination_entity\": \"name\"\\n  }\\n]\\n```'},\n",
       " {'page': 212,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '180\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\nspec:\\n  resources:\\n    requests:                \\n      storage: 1Gi           \\n  accessModes:              \\n  - ReadWriteOnce           \\n  storageClassName: \"\"     \\nAs soon as you create the claim, Kubernetes finds the appropriate PersistentVolume\\nand binds it to the claim. The PersistentVolume’s capacity must be large enough to\\naccommodate what the claim requests. Additionally, the volume’s access modes must\\ninclude the access modes requested by the claim. In your case, the claim requests 1 GiB\\nof storage and a ReadWriteOnce access mode. The PersistentVolume you created ear-\\nlier matches those two requirements so it is bound to your claim. You can see this by\\ninspecting the claim.\\nLISTING PERSISTENTVOLUMECLAIMS\\nList all PersistentVolumeClaims to see the state of your PVC:\\n$ kubectl get pvc\\nNAME          STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE\\nmongodb-pvc   Bound     mongodb-pv   1Gi        RWO,ROX       3s\\nNOTE\\nWe’re using pvc as a shorthand for persistentvolumeclaim.\\nThe claim is shown as Bound to PersistentVolume mongodb-pv. Note the abbreviations\\nused for the access modes:\\n\\uf0a1\\nRWO—ReadWriteOnce—Only a single node can mount the volume for reading\\nand writing.\\n\\uf0a1\\nROX—ReadOnlyMany—Multiple nodes can mount the volume for reading.\\n\\uf0a1\\nRWX—ReadWriteMany—Multiple nodes can mount the volume for both reading\\nand writing.\\nNOTE\\nRWO, ROX, and RWX pertain to the number of worker nodes that can use\\nthe volume at the same time, not to the number of pods!\\nLISTING PERSISTENTVOLUMES\\nYou can also see that the PersistentVolume is now Bound and no longer Available by\\ninspecting it with kubectl get:\\n$ kubectl get pv\\nNAME         CAPACITY   ACCESSMODES   STATUS   CLAIM                 AGE\\nmongodb-pv   1Gi        RWO,ROX       Bound    default/mongodb-pvc   1m\\nThe PersistentVolume shows it’s bound to claim default/mongodb-pvc. The default\\npart is the namespace the claim resides in (you created the claim in the default\\nRequesting 1 GiB of storage\\nYou want the storage to support a single \\nclient (performing both reads and writes).\\nYou’ll learn about this in the section \\nabout dynamic provisioning.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PersistentVolume',\n",
       "    'description': 'A Kubernetes resource that represents an existing persistent storage device',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'A request for persistent storage in a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system for automating the deployment, scaling, and management of containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolumeClaim spec',\n",
       "    'description': 'A specification for a PersistentVolumeClaim resource in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'resources',\n",
       "    'description': 'A field in the PersistentVolumeClaim spec that defines the requested storage resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'requests',\n",
       "    'description': 'A field in the resources section of the PersistentVolumeClaim spec that specifies the minimum required storage capacity',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'storage',\n",
       "    'description': 'A field in the requests section of the PersistentVolumeClaim spec that defines the requested storage size (in bytes)',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'accessModes',\n",
       "    'description': 'A field in the PersistentVolumeClaim spec that specifies the access modes for the requested persistent volume',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReadWriteOnce',\n",
       "    'description': 'An access mode that allows a single node to mount the persistent volume for reading and writing',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'storageClassName',\n",
       "    'description': 'A field in the PersistentVolumeClaim spec that specifies the storage class name for the requested persistent volume',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'claim',\n",
       "    'description': 'A PersistentVolumeClaim resource in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pv',\n",
       "    'description': 'A PersistentVolume resource in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pvc',\n",
       "    'description': 'A shorthand for PersistentVolumeClaim',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'RWO',\n",
       "    'description': 'An abbreviation for ReadWriteOnce access mode',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ROX',\n",
       "    'description': 'An abbreviation for ReadOnlyMany access mode',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'RWX',\n",
       "    'description': 'An abbreviation for ReadWriteMany access mode',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"binds a PersistentVolume to a claim based on matching storage requirements.\",\\n    \"destination_entity\": \"PersistentVolumeClaim\"\\n  },\\n  {\\n    \"source_entity\": \"PersistentVolumeClaim\",\\n    \"description\": \"requests 1 GiB of storage with ReadWriteOnce access mode.\",\\n    \"destination_entity\": \"storage\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"finds the appropriate PersistentVolume based on claim requirements.\",\\n    \"destination_entity\": \"PersistentVolume\"\\n  },\\n  {\\n    \"source_entity\": \"PersistentVolumeClaim\",\\n    \"description\": \"is bound to a PersistentVolume with matching capacity and access modes.\",\\n    \"destination_entity\": \"PersistentVolume\"\\n  },\\n  {\\n    \"source_entity\": \"claim\",\\n    \"description\": \"requests storage with specific access mode (RWO, ROX, RWX).\",\\n    \"destination_entity\": \"accessModes\"\\n  },\\n  {\\n    \"source_entity\": \"PersistentVolumeClaim spec\",\\n    \"description\": \"specifies resources and access modes for a claim.\",\\n    \"destination_entity\": \"resources\"\\n  },\\n  {\\n    \"source_entity\": \"persistentvolumeclaim\",\\n    \"description\": \"is a shorthand for persistentvolumeclaim in Kubernetes.\",\\n    \"destination_entity\": \"PersistentVolumeClaim\"\\n  },\\n  {\\n    \"source_entity\": \"pv\",\\n    \"description\": \"is a PersistentVolume in Kubernetes with matching capacity and access modes.\",\\n    \"destination_entity\": \"PersistentVolume\"\\n  },\\n  {\\n    \"source_entity\": \"RWO\",\\n    \"description\": \"specifies ReadWriteOnce access mode for a claim or PersistentVolume.\",\\n    \"destination_entity\": \"accessModes\"\\n  },\\n  {\\n    \"source_entity\": \"ROX\",\\n    \"description\": \"specifies ReadOnlyMany access mode for a claim or PersistentVolume.\",\\n    \"destination_entity\": \"accessModes\"\\n  }\\n]\\n```'},\n",
       " {'page': 213,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '181\\nDecoupling pods from the underlying storage technology\\nnamespace). We’ve already said that PersistentVolume resources are cluster-scoped\\nand thus cannot be created in a specific namespace, but PersistentVolumeClaims can\\nonly be created in a specific namespace. They can then only be used by pods in the\\nsame namespace.\\n6.5.4\\nUsing a PersistentVolumeClaim in a pod\\nThe PersistentVolume is now yours to use. Nobody else can claim the same volume\\nuntil you release it. To use it inside a pod, you need to reference the Persistent-\\nVolumeClaim by name inside the pod’s volume (yes, the PersistentVolumeClaim, not\\nthe PersistentVolume directly!), as shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: mongodb \\nspec:\\n  containers:\\n  - image: mongo\\n    name: mongodb\\n    volumeMounts:\\n    - name: mongodb-data\\n      mountPath: /data/db\\n    ports:\\n    - containerPort: 27017\\n      protocol: TCP\\n  volumes:\\n  - name: mongodb-data\\n    persistentVolumeClaim:       \\n      claimName: mongodb-pvc     \\nGo ahead and create the pod. Now, check to see if the pod is indeed using the same\\nPersistentVolume and its underlying GCE PD. You should see the data you stored ear-\\nlier by running the MongoDB shell again, as shown in the following listing.\\n$ kubectl exec -it mongodb mongo\\nMongoDB shell version: 3.2.8\\nconnecting to: mongodb://127.0.0.1:27017\\nWelcome to the MongoDB shell.\\n...\\n> use mystore\\nswitched to db mystore\\n> db.foo.find()\\n{ \"_id\" : ObjectId(\"57a61eb9de0cfd512374cc75\"), \"name\" : \"foo\" }\\nAnd there it is. You‘re able to retrieve the document you stored into MongoDB\\npreviously.\\nListing 6.12\\nA pod using a PersistentVolumeClaim volume: mongodb-pod-pvc.yaml\\nListing 6.13\\nRetrieving MongoDB’s persisted data in the pod using the PVC and PV\\nReferencing the PersistentVolumeClaim \\nby name in the pod volume\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Persistent Volume',\n",
       "    'description': 'Cluster-scoped resource that cannot be created in a specific namespace',\n",
       "    'category': 'software/database'},\n",
       "   {'entity': 'Persistent Volume Claim',\n",
       "    'description': 'Resource that can be created in a specific namespace and used by pods in the same namespace',\n",
       "    'category': 'software/database'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Deployable unit of application code, running on one or more worker machines',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': \"Reference to a Persistent Volume Claim resource by name inside a pod's volume\",\n",
       "    'category': 'software/database'},\n",
       "   {'entity': 'volumeMounts',\n",
       "    'description': 'Mechanism for mounting a Persistent Volume Claim volume in a pod',\n",
       "    'category': 'process/container'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'Cluster-scoped resource that can be used by pods in the same namespace',\n",
       "    'category': 'software/database'},\n",
       "   {'entity': 'GCE PD',\n",
       "    'description': 'Google Compute Engine Persistent Disk, underlying storage technology for a Persistent Volume',\n",
       "    'category': 'hardware/storage'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'API version of the Kubernetes resource being used',\n",
       "    'category': 'software/network'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'Type of Kubernetes resource being used (e.g. Pod, Service)',\n",
       "    'category': 'software/network'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Metadata associated with a Kubernetes resource',\n",
       "    'category': 'software/network'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'Specification for the creation and execution of a Kubernetes resource',\n",
       "    'category': 'software/network'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'List of containers that make up a pod',\n",
       "    'category': 'process/container'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'Image used to create a container',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'Name given to a resource (e.g. pod, service)',\n",
       "    'category': 'hardware/software'},\n",
       "   {'entity': 'volumeMounts',\n",
       "    'description': 'Mechanism for mounting a volume in a container',\n",
       "    'category': 'process/container'},\n",
       "   {'entity': 'mountPath',\n",
       "    'description': 'Location where a mounted volume is accessible within a container',\n",
       "    'category': 'software/process'},\n",
       "   {'entity': 'ports',\n",
       "    'description': 'List of ports exposed by a container',\n",
       "    'category': 'hardware/network'},\n",
       "   {'entity': 'containerPort',\n",
       "    'description': 'Port number exposed by a container',\n",
       "    'category': 'hardware/network'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"PersistentVolume\", \"description\": \"is attached to\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"image\", \"description\": \"specifies the container image\", \"destination_entity\": \"containers\"},\\n  {\"source_entity\": \"containerPort\", \"description\": \"defines the port exposed by the container\", \"destination_entity\": \"ports\"},\\n  {\"source_entity\": \"mountPath\", \"description\": \"specifies where to mount the volume\", \"destination_entity\": \"volumeMounts\"},\\n  {\"source_entity\": \"kind\", \"description\": \"defines the type of Kubernetes resource\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Persistent Volume Claim\", \"description\": \"requests a PersistentVolume\", \"destination_entity\": \"PersistentVolume\"},\\n  {\"source_entity\": \"GCE PD\", \"description\": \"underlying storage technology used by PersistentVolume\", \"destination_entity\": \"PersistentVolume\"},\\n  {\"source_entity\": \"metadata\", \"description\": \"provides additional information about the Pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"ports\", \"description\": \"defines the ports exposed by the container\", \"destination_entity\": \"containers\"},\\n  {\"source_entity\": \"volumeMounts\", \"description\": \"specifies how to mount the volume in the container\", \"destination_entity\": \"containers\"},\\n  {\"source_entity\": \"apiVersion\", \"description\": \"defines the API version of the Pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"name\", \"description\": \"specifies the name of the Pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"PersistentVolumeClaim\", \"description\": \"references the PersistentVolumeClaim in the volume\", \"destination_entity\": \"volumes\"},\\n  {\"source_entity\": \"spec\", \"description\": \"defines the specification of the Pod\", \"destination_entity\": \"Pod\"}\\n]\\n```\\n\\nNote that some entities have multiple relations, but I\\'ve only listed each relation once. Let me know if you\\'d like me to clarify any of these relations!'},\n",
       " {'page': 214,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '182\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\n6.5.5\\nUnderstanding the benefits of using PersistentVolumes and claims\\nExamine figure 6.8, which shows both ways a pod can use a GCE Persistent Disk—\\ndirectly or through a PersistentVolume and claim.\\nConsider how using this indirect method of obtaining storage from the infrastructure\\nis much simpler for the application developer (or cluster user). Yes, it does require\\nthe additional steps of creating the PersistentVolume and the PersistentVolumeClaim,\\nbut the developer doesn’t have to know anything about the actual storage technology\\nused underneath. \\n Additionally, the same pod and claim manifests can now be used on many different\\nKubernetes clusters, because they don’t refer to anything infrastructure-specific. The\\nclaim states, “I need x amount of storage and I need to be able to read and write to it\\nby a single client at once,” and then the pod references the claim by name in one of\\nits volumes.\\nPod: mongodb\\nContainer: mongodb\\nvolumeMounts:\\nname: mongodb-data\\nmountPath: /data/db\\ngcePersistentDisk:\\npdName: mongodb\\nGCE\\nPersistent Disk:\\nmongodb\\nVolume:\\nmongodb\\nPod: mongodb\\nContainer: mongodb\\nvolumeMounts:\\nname: mongodb-data\\nmountPath: /data/db\\npersistentVolumeClaim:\\nclaimName: mongodb-pvc\\ngcePersistentDisk:\\npdName: mongodb\\nGCE\\nPersistent Disk:\\nmongodb\\nPersistentVolume:\\nmongodb-pv\\n(1 Gi, RWO, RWX)\\nVolume:\\nmongodb\\nClaim lists\\n1Gi and\\nReadWriteOnce\\naccess\\nPersistentVolumeClaim:\\nmongodb-pvc\\nFigure 6.8\\nUsing the GCE Persistent Disk directly or through a PVC and PV\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A pod is a logical host for one or more containers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'A container is a runtime instance of a Docker image',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'VolumeMounts',\n",
       "    'description': 'The volume mounts allow a container to access a shared storage resource',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'GCE Persistent Disk',\n",
       "    'description': 'A GCE persistent disk is a type of block storage provided by Google Cloud',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Persistent Volume',\n",
       "    'description': 'A persistent volume is a provisioned resource that can be used by a pod for data persistence',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Claim lists',\n",
       "    'description': 'A claim list specifies the access mode and size of the persistent volume requested by a pod',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'A persistent volume claim is an object in Kubernetes that requests a persistent volume from a cluster administrator',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'PDName',\n",
       "    'description': 'The name of the persistent disk used to provision storage for a pod',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'ReadWriteOnce',\n",
       "    'description': 'Access mode that allows read and write access by a single client at once',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'RWX',\n",
       "    'description': 'Access mode that allows read, write, and execute permissions on a persistent volume',\n",
       "    'category': 'network'}],\n",
       "  'relationships': '[{\"source_entity\": \"Pod\", \"description\": \"uses storage from infrastructure indirectly through PersistentVolume and claim\", \"destination_entity\": \"GCE Persistent Disk\"},{\"source_entity\": \"Persistent Volume Claim\", \"description\": \"states need for x amount of storage and read/write access by single client at once\", \"destination_entity\": \"Pod\"},{\"source_entity\": \"Container\", \"description\": \"runs on top of Pod and uses volumes mounted by Pod\", \"destination_entity\": \"Pod\"},{\"source_entity\": \"PersistentVolumeClaim\", \"description\": \"references claim by name in one of its volumes\", \"destination_entity\": \"Pod\"},{\"source_entity\": \"GCE Persistent Disk\", \"description\": \"provides persistent storage for Pod through Persistent Volume and Claim\", \"destination_entity\": \"Pod\"},{\"source_entity\": \"VolumeMounts\", \"description\": \"mounts Persistent Volume to Container at specified mount path\", \"destination_entity\": \"Container\"},{\"source_entity\": \"ReadWriteOnce\", \"description\": \"specifies access mode for Persistent Volume Claim\", \"destination_entity\": \"PersistentVolumeClaim\"},{\"source_entity\": \"PDName\", \"description\": \"identifies Persistent Disk used by Persistent Volume\", \"destination_entity\": \"GCE Persistent Disk\"},{\"source_entity\": \"RWX\", \"description\": \"specifies access modes for Persistent Volume\", \"destination_entity\": \"PersistentVolume\"}]'},\n",
       " {'page': 215,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '183\\nDecoupling pods from the underlying storage technology\\n6.5.6\\nRecycling PersistentVolumes\\nBefore you wrap up this section on PersistentVolumes, let’s do one last quick experi-\\nment. Delete the pod and the PersistentVolumeClaim:\\n$ kubectl delete pod mongodb\\npod \"mongodb\" deleted\\n$ kubectl delete pvc mongodb-pvc\\npersistentvolumeclaim \"mongodb-pvc\" deleted\\nWhat if you create the PersistentVolumeClaim again? Will it be bound to the Persistent-\\nVolume or not? After you create the claim, what does kubectl get pvc show?\\n$ kubectl get pvc\\nNAME           STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE\\nmongodb-pvc    Pending                                         13s\\nThe claim’s status is shown as Pending. Interesting. When you created the claim ear-\\nlier, it was immediately bound to the PersistentVolume, so why wasn’t it bound now?\\nMaybe listing the PersistentVolumes can shed more light on this:\\n$ kubectl get pv\\nNAME        CAPACITY  ACCESSMODES  STATUS    CLAIM               REASON AGE\\nmongodb-pv  1Gi       RWO,ROX      Released  default/mongodb-pvc        5m\\nThe STATUS column shows the PersistentVolume as Released, not Available like\\nbefore. Because you’ve already used the volume, it may contain data and shouldn’t be\\nbound to a completely new claim without giving the cluster admin a chance to clean it\\nup. Without this, a new pod using the same PersistentVolume could read the data\\nstored there by the previous pod, even if the claim and pod were created in a different\\nnamespace (and thus likely belong to a different cluster tenant).\\nRECLAIMING PERSISTENTVOLUMES MANUALLY\\nYou told Kubernetes you wanted your PersistentVolume to behave like this when you\\ncreated it—by setting its persistentVolumeReclaimPolicy to Retain. You wanted\\nKubernetes to retain the volume and its contents after it’s released from its claim. As\\nfar as I’m aware, the only way to manually recycle the PersistentVolume to make it\\navailable again is to delete and recreate the PersistentVolume resource. As you do\\nthat, it’s your decision what to do with the files on the underlying storage: you can\\neither delete them or leave them alone so they can be reused by the next  pod.\\nRECLAIMING PERSISTENTVOLUMES AUTOMATICALLY\\nTwo other possible reclaim policies exist: Recycle and Delete. The first one deletes\\nthe volume’s contents and makes the volume available to be claimed again. This way,\\nthe PersistentVolume can be reused multiple times by different PersistentVolume-\\nClaims and different pods, as you can see in figure 6.9.\\n The Delete policy, on the other hand, deletes the underlying storage. Note that\\nthe Recycle option is currently not available for GCE Persistent Disks. This type of\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command-line tool for interacting with Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'lightweight and portable container in Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'request for storage resources in Kubernetes',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'pv',\n",
       "    'description': 'persistent volume in Kubernetes',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'kubectl get pvc',\n",
       "    'description': 'command to display PersistentVolumeClaims',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl get pv',\n",
       "    'description': 'command to display persistent volumes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'PersistentVolumes',\n",
       "    'description': 'managed storage resources in Kubernetes',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'pod mongodb',\n",
       "    'description': 'specific pod resource',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'pvc mongodb-pvc',\n",
       "    'description': 'specific PersistentVolumeClaim resource',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'pv mongodb-pv',\n",
       "    'description': 'specific persistent volume resource',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'PersistentVolumeReclaimPolicy',\n",
       "    'description': 'policy for managing PersistentVolumes',\n",
       "    'category': 'config'},\n",
       "   {'entity': 'Retain',\n",
       "    'description': 'persistent volume reclaim policy to retain data',\n",
       "    'category': 'config'},\n",
       "   {'entity': 'Recycle',\n",
       "    'description': 'persistent volume reclaim policy to delete data and reuse volume',\n",
       "    'category': 'config'},\n",
       "   {'entity': 'Delete',\n",
       "    'description': 'persistent volume reclaim policy to delete underlying storage',\n",
       "    'category': 'config'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"decouples pods from underlying storage technology\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"persistent volume reclaim policy\", \"description\": \"retains volume and its contents after release from claim\", \"destination_entity\": \"PersistentVolume\"},\\n  {\"source_entity\": \"kubernetes\", \"description\": \"releases persistent volumes\", \"destination_entity\": \"persistent volumes\"},\\n  {\"source_entity\": \"kubectl delete pv\", \"description\": \"deletes a persistent volume\", \"destination_entity\": \"pv mongodb-pv\"},\\n  {\"source_entity\": \"kubectl get pv\", \"description\": \"lists available persistent volumes\", \"destination_entity\": \"PersistentVolumes\"},\\n  {\"source_entity\": \"kubectl get pvc\", \"description\": \"lists pending persistent volume claims\", \"destination_entity\": \"pvc mongodb-pvc\"},\\n  {\"source_entity\": \"pod\", \"description\": \"uses a persistent volume\", \"destination_entity\": \"pv mongodb-pv\"},\\n  {\"source_entity\": \"persistent volume claim\", \"description\": \"binds to a persistent volume\", \"destination_entity\": \"PersistentVolume\"},\\n  {\"source_entity\": \"kubernetes\", \"description\": \"recycles persistent volumes automatically\", \"destination_entity\": \"PersistentVolumes\"},\\n  {\"source_entity\": \"kubernetes\", \"description\": \"deletes underlying storage\", \"destination_entity\": \"PersistentVolumes\"},\\n  {\"source_entity\": \"kubectl delete pvc\", \"description\": \"deletes a persistent volume claim\", \"destination_entity\": \"pvc mongodb-pvc\"},\\n  {\"source_entity\": \"pod mongodb\", \"description\": \"uses a persistent volume claim\", \"destination_entity\": \"pvc mongodb-pv\"}\\n]'},\n",
       " {'page': 216,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '184\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\nA PersistentVolume only supports the Retain or Delete policies. Other Persistent-\\nVolume types may or may not support each of these options, so before creating your\\nown PersistentVolume, be sure to check what reclaim policies are supported for the\\nspecific underlying storage you’ll use in the volume.\\nTIP\\nYou can change the PersistentVolume reclaim policy on an existing\\nPersistentVolume. For example, if it’s initially set to Delete, you can easily\\nchange it to Retain to prevent losing valuable data.\\n6.6\\nDynamic provisioning of PersistentVolumes\\nYou’ve seen how using PersistentVolumes and PersistentVolumeClaims makes it easy\\nto obtain persistent storage without the developer having to deal with the actual stor-\\nage technology used underneath. But this still requires a cluster administrator to pro-\\nvision the actual storage up front. Luckily, Kubernetes can also perform this job\\nautomatically through dynamic provisioning of PersistentVolumes.\\n The cluster admin, instead of creating PersistentVolumes, can deploy a Persistent-\\nVolume provisioner and define one or more StorageClass objects to let users choose\\nwhat type of PersistentVolume they want. The users can refer to the StorageClass in\\ntheir PersistentVolumeClaims and the provisioner will take that into account when\\nprovisioning the persistent storage. \\nNOTE\\nSimilar to PersistentVolumes, StorageClass resources aren’t namespaced.\\nKubernetes includes provisioners for the most popular cloud providers, so the admin-\\nistrator doesn’t always need to deploy a provisioner. But if Kubernetes is deployed\\non-premises, a custom provisioner needs to be deployed.\\nPersistentVolume\\nPersistentVolumeClaim 1\\nPod 1\\nPod 2\\nPersistentVolumeClaim 2\\nPod 3\\nPVC is deleted;\\nPV is automatically\\nrecycled and ready\\nto be claimed and\\nre-used again\\nUser creates\\nPersistentVolumeClaim\\nPod 2\\nunmounts\\nPVC\\nPod 2\\nmounts\\nPVC\\nPod 1\\nmounts\\nPVC\\nPod 1\\nunmounts\\nPVC\\nAdmin deletes\\nPersistentVolume\\nAdmin creates\\nPersistentVolume\\nTime\\nFigure 6.9\\nThe lifespan of a PersistentVolume, PersistentVolumeClaims, and pods using them\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [   Col0                                   PersistentVolume\n",
       "   0  None  PersistentVolumeClaim 1 PersistentVolumeClaim ...],\n",
       "  'entities': [{'entity': 'PersistentVolume',\n",
       "    'description': 'A Kubernetes resource that represents a storage resource provisioned by the cluster administrator.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Retain policy',\n",
       "    'description': \"A reclaim policy for PersistentVolumes that retains the volume even after it's deleted from the cluster.\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Delete policy',\n",
       "    'description': \"A reclaim policy for PersistentVolumes that deletes the volume when it's deleted from the cluster.\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'StorageClass',\n",
       "    'description': 'A Kubernetes resource that defines a storage class, which is used to provision PersistentVolumes dynamically.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'A request for persistent storage resources from the cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A single container running in the Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolumeProvisioner',\n",
       "    'description': 'A component that dynamically provisions PersistentVolumes based on the StorageClass defined by the administrator.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Dynamic provisioning',\n",
       "    'description': 'The process of automatically provisioning PersistentVolumes based on user requests without requiring manual intervention from the cluster administrator.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'A Kubernetes resource that represents a storage resource provisioned by the cluster administrator.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"Admin\", \"description\": \"provisions dynamic PersistentVolumes through provisioner and StorageClass objects\", \"destination_entity\": \"PersistentVolume\"},\\n {\"source_entity\": \"User\", \"description\": \"creates PersistentVolumeClaim based on StorageClass\", \"destination_entity\": \"PersistentVolumeClaim\"},\\n {\"source_entity\": \"Pod 1\", \"description\": \"mounts PVC\", \"destination_entity\": \"PersistentVolumeClaim\"},\\n {\"source_entity\": \"Pod 2\", \"description\": \"unmounts and mounts PVC\", \"destination_entity\": \"PersistentVolumeClaim\"},\\n {\"source_entity\": \"Admin\", \"description\": \"deletes PersistentVolume\", \"destination_entity\": \"PersistentVolume\"},\\n {\"source_entity\": \"Admin\", \"description\": \"creates new PersistentVolume\", \"destination_entity\": \"PersistentVolume\"},\\n {\"source_entity\": \"Pod\", \"description\": \"uses and unmounts PVC\", \"destination_entity\": \"PersistentVolumeClaim\"},\\n {\"source_entity\": \"PersistentVolumeProvisioner\", \"description\": \"provisions persistent storage based on StorageClass\", \"destination_entity\": \"StorageClass\"},\\n {\"source_entity\": \"Kubernetes\", \"description\": \"performs dynamic provisioning of PersistentVolumes through provisioner and StorageClass objects\", \"destination_entity\": \"PersistentVolume\"},\\n {\"source_entity\": \"User\", \"description\": \"refers to StorageClass in PVC for persistent storage\", \"destination_entity\": \"StorageClass\"}]'},\n",
       " {'page': 217,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '185\\nDynamic provisioning of PersistentVolumes\\n Instead of the administrator pre-provisioning a bunch of PersistentVolumes, they\\nneed to define one or two (or more) StorageClasses and let the system create a new\\nPersistentVolume each time one is requested through a PersistentVolumeClaim. The\\ngreat thing about this is that it’s impossible to run out of PersistentVolumes (obviously,\\nyou can run out of storage space). \\n6.6.1\\nDefining the available storage types through StorageClass \\nresources\\nBefore a user can create a PersistentVolumeClaim, which will result in a new Persistent-\\nVolume being provisioned, an admin needs to create one or more StorageClass\\nresources. Let’s look at an example of one in the following listing.\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:\\n  name: fast\\nprovisioner: kubernetes.io/gce-pd       \\nparameters:\\n  type: pd-ssd                     \\n  zone: europe-west1-b             \\nNOTE\\nIf using Minikube, deploy the file storageclass-fast-hostpath.yaml.\\nThe StorageClass resource specifies which provisioner should be used for provision-\\ning the PersistentVolume when a PersistentVolumeClaim requests this StorageClass.\\nThe parameters defined in the StorageClass definition are passed to the provisioner\\nand are specific to each provisioner plugin. \\n The StorageClass uses the Google Compute Engine (GCE) Persistent Disk (PD)\\nprovisioner, which means it can be used when Kubernetes is running in GCE. For\\nother cloud providers, other provisioners need to be used.\\n6.6.2\\nRequesting the storage class in a PersistentVolumeClaim\\nAfter the StorageClass resource is created, users can refer to the storage class by name\\nin their PersistentVolumeClaims. \\nCREATING A PVC DEFINITION REQUESTING A SPECIFIC STORAGE CLASS\\nYou can modify your mongodb-pvc to use dynamic provisioning. The following listing\\nshows the updated YAML definition of the PVC.\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: mongodb-pvc \\nListing 6.14\\nA StorageClass definition: storageclass-fast-gcepd.yaml\\nListing 6.15\\nA PVC with dynamic provisioning: mongodb-pvc-dp.yaml\\nThe volume plugin to \\nuse for provisioning \\nthe PersistentVolume\\nThe parameters passed \\nto the provisioner\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PersistentVolumes',\n",
       "    'description': 'Dynamic storage provisioned by Kubernetes',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'StorageClasses',\n",
       "    'description': 'Resources that define available storage types',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'API version for the StorageClass resource',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'Resource type for the StorageClass definition',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Metadata for the StorageClass resource',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'Name of the StorageClass resource',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'provisioner',\n",
       "    'description': 'Plugin used to provision PersistentVolumes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'parameters',\n",
       "    'description': 'Options passed to the provisioner',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'type',\n",
       "    'description': 'Type of storage provisioned (e.g. pd-ssd)',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'zone',\n",
       "    'description': 'Zone where the PersistentVolume will be created',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'Request for a PersistentVolume with specific storage class',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'API version for the PVC resource',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'Resource type for the PVC definition',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Metadata for the PVC resource',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'Name of the PVC resource',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"StorageClasses\", \"description\": \"define one or two (or more) to let the system create a new PersistentVolume each time one is requested through a PersistentVolumeClaim\", \"destination_entity\": \"PersistentVolumes\"},\\n  {\"source_entity\": \"admin\", \"description\": \"create one or more StorageClass resources before a user can create a PersistentVolumeClaim\", \"destination_entity\": \"StorageClasses\"},\\n  {\"source_entity\": \"admin\", \"description\": \"deploy the file storageclass-fast-hostpath.yaml when using Minikube\", \"destination_entity\": \"Minikube\"},\\n  {\"source_entity\": \"StorageClass\", \"description\": \"specify which provisioner should be used for provisioning the PersistentVolume\", \"destination_entity\": \"provisioner\"},\\n  {\"source_entity\": \"parameters\", \"description\": \"passed to the provisioner and are specific to each provisioner plugin\", \"destination_entity\": \"provisioner\"},\\n  {\"source_entity\": \"StorageClass\", \"description\": \"use the Google Compute Engine (GCE) Persistent Disk (PD) provisioner\", \"destination_entity\": \"Google Compute Engine\"},\\n  {\"source_entity\": \"users\", \"description\": \"refer to the storage class by name in their PersistentVolumeClaims\", \"destination_entity\": \"PersistentVolumeClaims\"},\\n  {\"source_entity\": \"admin\", \"description\": \"modify the mongodb-pvc to use dynamic provisioning\", \"destination_entity\": \"mongodb-pvc\"},\\n  {\"source_entity\": \"volume plugin\", \"description\": \"use for provisioning the PersistentVolume\", \"destination_entity\": \"provisioner\"},\\n  {\"source_entity\": \"parameters\", \"description\": \"passed to the provisioner and specify the type, zone, etc.\", \"destination_entity\": \"provisioner\"},\\n  {\"source_entity\": \"PersistentVolumeClaim\", \"description\": \"request a specific storage class to be used for provisioning the PersistentVolume\", \"destination_entity\": \"StorageClass\"}\\n]\\n```'},\n",
       " {'page': 218,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '186\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\nspec:\\n  storageClassName: fast     \\n  resources:\\n    requests:\\n      storage: 100Mi\\n  accessModes:\\n    - ReadWriteOnce\\nApart from specifying the size and access modes, your PersistentVolumeClaim now\\nalso specifies the class of storage you want to use. When you create the claim, the\\nPersistentVolume is created by the provisioner referenced in the fast StorageClass\\nresource. The provisioner is used even if an existing manually provisioned Persistent-\\nVolume matches the PersistentVolumeClaim. \\nNOTE\\nIf you reference a non-existing storage class in a PVC, the provisioning\\nof the PV will fail (you’ll see a ProvisioningFailed event when you use\\nkubectl describe on the PVC).\\nEXAMINING THE CREATED PVC AND THE DYNAMICALLY PROVISIONED PV\\nNext you’ll create the PVC and then use kubectl get to see it:\\n$ kubectl get pvc mongodb-pvc\\nNAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS\\nmongodb-pvc   Bound    pvc-1e6bc048   1Gi        RWO           fast \\nThe VOLUME column shows the PersistentVolume that’s bound to this claim (the actual\\nname is longer than what’s shown above). You can try listing PersistentVolumes now to\\nsee that a new PV has indeed been created automatically:\\n$ kubectl get pv\\nNAME           CAPACITY  ACCESSMODES  RECLAIMPOLICY  STATUS    STORAGECLASS   \\nmongodb-pv     1Gi       RWO,ROX      Retain         Released \\npvc-1e6bc048   1Gi       RWO          Delete         Bound     fast\\nNOTE\\nOnly pertinent columns are shown.\\nYou can see the dynamically provisioned PersistentVolume. Its capacity and access\\nmodes are what you requested in the PVC. Its reclaim policy is Delete, which means\\nthe PersistentVolume will be deleted when the PVC is deleted. Beside the PV, the pro-\\nvisioner also provisioned the actual storage. Your fast StorageClass is configured to\\nuse the kubernetes.io/gce-pd provisioner, which provisions GCE Persistent Disks.\\nYou can see the disk with the following command:\\n$ gcloud compute disks list\\nNAME                          ZONE            SIZE_GB  TYPE         STATUS\\ngke-kubia-dyn-pvc-1e6bc048    europe-west1-d  1        pd-ssd       READY\\ngke-kubia-default-pool-71df   europe-west1-d  100      pd-standard  READY\\ngke-kubia-default-pool-79cd   europe-west1-d  100      pd-standard  READY\\ngke-kubia-default-pool-blc4   europe-west1-d  100      pd-standard  READY\\nmongodb                       europe-west1-d  1        pd-standard  READY\\nThis PVC requests the \\ncustom storage class.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'A request for a PersistentVolume with specific characteristics',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'StorageClass',\n",
       "    'description': 'A resource that defines a type of storage and its provisioner',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Provisioner',\n",
       "    'description': 'A component that provisions storage based on a StorageClass',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'A container for block-level access to persistent storage in a cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'GCE Persistent Disk',\n",
       "    'description': 'A type of storage provided by Google Cloud Platform',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kubectl get',\n",
       "    'description': 'A command used to retrieve information about PVCs and PVs',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'PV',\n",
       "    'description': 'PersistentVolume created dynamically based on a PVC',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PVC',\n",
       "    'description': 'PersistentVolumeClaim created to request storage resources',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"PersistentVolume\", \"description\": \"is dynamically provisioned by the provisioner referenced in the StorageClass resource\", \"destination_entity\": \"Provisioner\"},\\n  {\"source_entity\": \"StorageClass\", \"description\": \"provisions GCE Persistent Disks using the kubernetes.io/gce-pd provisioner\", \"destination_entity\": \"GCE Persistent Disk\"},\\n  {\"source_entity\": \"kubectl get\", \"description\": \"displays information about the PVC and dynamically provisioned PV\", \"destination_entity\": \"PVC\"},\\n  {\"source_entity\": \"kubectl get\", \"description\": \"displays information about the dynamically provisioned PV\", \"destination_entity\": \"PV\"},\\n  {\"source_entity\": \"PersistentVolumeClaim\", \"description\": \"requests a custom storage class from the StorageClass resource\", \"destination_entity\": \"StorageClass\"},\\n  {\"source_entity\": \"PVC\", \"description\": \"is bound to a PersistentVolume by the provisioner referenced in the StorageClass resource\", \"destination_entity\": \"PersistentVolume\"},\\n  {\"source_entity\": \"PV\", \"description\": \"has its capacity and access modes set according to the PVC\\'s request\", \"destination_entity\": \"PVC\"},\\n  {\"source_entity\": \"StorageClass\", \"description\": \"is referenced by the PVC when creating a new PV\", \"destination_entity\": \"PVC\"}\\n]\\n```\\n\\nNote: I\\'ve tried to extract all possible relations between the entities, but let me know if you need any further clarification or if there are any errors in my output!'},\n",
       " {'page': 219,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '187\\nDynamic provisioning of PersistentVolumes\\nAs you can see, the first persistent disk’s name suggests it was provisioned dynamically\\nand its type shows it’s an SSD, as specified in the storage class you created earlier. \\nUNDERSTANDING HOW TO USE STORAGE CLASSES\\nThe cluster admin can create multiple storage classes with different performance or\\nother characteristics. The developer then decides which one is most appropriate for\\neach claim they create. \\n The nice thing about StorageClasses is the fact that claims refer to them by\\nname. The PVC definitions are therefore portable across different clusters, as long\\nas the StorageClass names are the same across all of them. To see this portability\\nyourself, you can try running the same example on Minikube, if you’ve been using\\nGKE up to this point. As a cluster admin, you’ll have to create a different storage\\nclass (but with the same name). The storage class defined in the storageclass-fast-\\nhostpath.yaml file is tailor-made for use in Minikube. Then, once you deploy the stor-\\nage class, you as a cluster user can deploy the exact same PVC manifest and the exact\\nsame pod manifest as before. This shows how the pods and PVCs are portable across\\ndifferent clusters.\\n6.6.3\\nDynamic provisioning without specifying a storage class\\nAs we’ve progressed through this chapter, attaching persistent storage to pods has\\nbecome ever simpler. The sections in this chapter reflect how provisioning of storage\\nhas evolved from early Kubernetes versions to now. In this final section, we’ll look at\\nthe latest and simplest way of attaching a PersistentVolume to a pod. \\nLISTING STORAGE CLASSES\\nWhen you created your custom storage class called fast, you didn’t check if any exist-\\ning storage classes were already defined in your cluster. Why don’t you do that now?\\nHere are the storage classes available in GKE:\\n$ kubectl get sc\\nNAME                 TYPE\\nfast                 kubernetes.io/gce-pd\\nstandard (default)   kubernetes.io/gce-pd\\nNOTE\\nWe’re using sc as shorthand for storageclass.\\nBeside the fast storage class, which you created yourself, a standard storage class\\nexists and is marked as default. You’ll learn what that means in a moment. Let’s list the\\nstorage classes available in Minikube, so we can compare:\\n$ kubectl get sc\\nNAME                 TYPE\\nfast                 k8s.io/minikube-hostpath\\nstandard (default)   k8s.io/minikube-hostpath\\nAgain, the fast storage class was created by you and a default standard storage class\\nexists here as well. Comparing the TYPE columns in the two listings, you see GKE is\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PersistentVolume',\n",
       "    'description': 'A persistent volume is a resource that can be used to store data persistently across pod creations and deletions.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'StorageClass',\n",
       "    'description': 'A storage class is a way to provision Persistent Volumes dynamically, with characteristics such as performance or other features.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'PersistentDisk',\n",
       "    'description': 'A persistent disk is a block device that can be used to store data persistently across pod creations and deletions.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'PVC (Persistent Volume Claim)',\n",
       "    'description': 'A PVC is a request for storage resources from a cluster, which is fulfilled by a PersistentVolume.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'A tool for running Kubernetes locally on a single machine.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'GKE (Google Kubernetes Engine)',\n",
       "    'description': 'A managed container environment that runs on Google Cloud Platform.',\n",
       "    'category': 'cloud service'},\n",
       "   {'entity': '$ kubectl get sc',\n",
       "    'description': 'A command to list the storage classes available in a cluster.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'sc (storageclass)',\n",
       "    'description': \"A shorthand for 'storage class' used in commands.\",\n",
       "    'category': 'variable'},\n",
       "   {'entity': 'TYPE column',\n",
       "    'description': 'A column in the output of the `kubectl get sc` command that shows the type of storage class.',\n",
       "    'category': 'database'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"PersistentVolume\",\\n    \"description\": \"is provisioned dynamically\",\\n    \"destination_entity\": \"PersistentDisk\"\\n  },\\n  {\\n    \"source_entity\": \"cluster admin\",\\n    \"description\": \"creates multiple storage classes with different performance or other characteristics\",\\n    \"destination_entity\": \"StorageClass\"\\n  },\\n  {\\n    \"source_entity\": \"developer\",\\n    \"description\": \"decides which storage class is most appropriate for each claim\",\\n    \"destination_entity\": \"StorageClass\"\\n  },\\n  {\\n    \"source_entity\": \"claims\",\\n    \"description\": \"refer to storage classes by name\",\\n    \"destination_entity\": \"StorageClass\"\\n  },\\n  {\\n    \"source_entity\": \"cluster admin\",\\n    \"description\": \"creates a different storage class with the same name\",\\n    \"destination_entity\": \"Minikube\"\\n  },\\n  {\\n    \"source_entity\": \"user\",\\n    \"description\": \"deploys the exact same PVC manifest and pod manifest as before\",\\n    \"destination_entity\": \"PVC (Persistent Volume Claim)\"\\n  },\\n  {\\n    \"source_entity\": \"$ kubectl get sc\",\\n    \"description\": \"lists storage classes available in GKE\",\\n    \"destination_entity\": \"GKE (Google Kubernetes Engine)\"\\n  },\\n  {\\n    \"source_entity\": \"TYPE column\",\\n    \"description\": \"shows the type of storage class\",\\n    \"destination_entity\": \"StorageClass\"\\n  },\\n  {\\n    \"source_entity\": \"$ kubectl get sc\",\\n    \"description\": \"lists storage classes available in Minikube\",\\n    \"destination_entity\": \"Minikube\"\\n  },\\n  {\\n    \"source_entity\": \"cluster admin\",\\n    \"description\": \"creates a different storage class with the same name as before\",\\n    \"destination_entity\": \"GKE (Google Kubernetes Engine)\"\\n  }\\n]\\n```'},\n",
       " {'page': 220,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '188\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\nusing the kubernetes.io/gce-pd provisioner, whereas Minikube is using k8s.io/\\nminikube-hostpath. \\nEXAMINING THE DEFAULT STORAGE CLASS\\nYou’re going to use kubectl get to see more info about the standard storage class in a\\nGKE cluster, as shown in the following listing.\\n$ kubectl get sc standard -o yaml\\napiVersion: storage.k8s.io/v1\\nkind: StorageClass\\nmetadata:\\n  annotations:\\n    storageclass.beta.kubernetes.io/is-default-class: \"true\"   \\n  creationTimestamp: 2017-05-16T15:24:11Z\\n  labels:\\n    addonmanager.kubernetes.io/mode: EnsureExists\\n    kubernetes.io/cluster-service: \"true\"\\n  name: standard\\n  resourceVersion: \"180\"\\n  selfLink: /apis/storage.k8s.io/v1/storageclassesstandard\\n  uid: b6498511-3a4b-11e7-ba2c-42010a840014\\nparameters:                                    \\n  type: pd-standard                            \\nprovisioner: kubernetes.io/gce-pd      \\nIf you look closely toward the top of the listing, the storage class definition includes an\\nannotation, which makes this the default storage class. The default storage class is\\nwhat’s used to dynamically provision a PersistentVolume if the PersistentVolumeClaim\\ndoesn’t explicitly say which storage class to use. \\nCREATING A PERSISTENTVOLUMECLAIM WITHOUT SPECIFYING A STORAGE CLASS\\nYou can create a PVC without specifying the storageClassName attribute and (on\\nGoogle Kubernetes Engine) a GCE Persistent Disk of type pd-standard will be provi-\\nsioned for you. Try this by creating a claim from the YAML in the following listing.\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n  name: mongodb-pvc2\\nspec:                        \\n  resources:                 \\n    requests:                \\n      storage: 100Mi         \\n  accessModes:               \\n    - ReadWriteOnce          \\nListing 6.16\\nThe definition of the standard storage class on GKE\\nListing 6.17\\nPVC with no storage class defined: mongodb-pvc-dp-nostorageclass.yaml\\nThis annotation \\nmarks the storage \\nclass as default.\\nThe type parameter is used by the provisioner \\nto know what type of GCE PD to create.\\nThe GCE Persistent Disk provisioner \\nis used to provision PVs of this class.\\nYou’re not specifying \\nthe storageClassName \\nattribute (unlike earlier \\nexamples).\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command to interact with Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'get',\n",
       "    'description': 'command to retrieve information about Kubernetes resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'sc',\n",
       "    'description': 'short for StorageClass, a resource in Kubernetes that defines a storage class',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'standard',\n",
       "    'description': 'the default StorageClass in a GKE cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'field in the StorageClass resource that specifies the API version of the resource',\n",
       "    'category': 'hardware/software interface'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'field in the StorageClass resource that specifies the type of resource (in this case, a StorageClass)',\n",
       "    'category': 'hardware/software interface'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'field in the StorageClass resource that contains metadata about the resource',\n",
       "    'category': 'hardware/software interface'},\n",
       "   {'entity': 'annotations',\n",
       "    'description': 'field in the StorageClass resource that contains annotations about the resource',\n",
       "    'category': 'hardware/software interface'},\n",
       "   {'entity': 'storageclass.beta.kubernetes.io/is-default-class',\n",
       "    'description': 'annotation on the default StorageClass that indicates it is the default class',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'parameters',\n",
       "    'description': 'field in the StorageClass resource that contains parameters for the storage class',\n",
       "    'category': 'hardware/software interface'},\n",
       "   {'entity': 'type',\n",
       "    'description': 'parameter in the StorageClass resource that specifies the type of GCE PD to create',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pd-standard',\n",
       "    'description': 'the type of GCE PD to create when using this storage class',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'provisioner',\n",
       "    'description': 'field in the StorageClass resource that specifies the provisioner for the storage class',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubernetes.io/gce-pd',\n",
       "    'description': 'the provisioner used to create GCE PDs when using this storage class',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'resource in Kubernetes that requests access to a Persistent Volume',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PVC',\n",
       "    'description': 'abbreviation for PersistentVolumeClaim',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'storageClassName',\n",
       "    'description': 'attribute in the PVC resource that specifies the storage class to use',\n",
       "    'category': 'hardware/software interface'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"storageClassName\",\\n    \"description\": \"Used to specify the storage class for a PVC.\",\\n    \"destination_entity\": \"PVC\"\\n  },\\n  {\\n    \"source_entity\": \"kubernetes.io/gce-pd\",\\n    \"description\": \"Used as the provisioner to create GCE PDs.\",\\n    \"destination_entity\": \"PersistentVolume\"\\n  },\\n  {\\n    \"source_entity\": \"standard\",\\n    \"description\": \"Defined as the default storage class on GKE.\",\\n    \"destination_entity\": \"storageclass.beta.kubernetes.io/is-default-class\"\\n  },\\n  {\\n    \"source_entity\": \"type\",\\n    \"description\": \"Used by the provisioner to determine the type of GCE PD to create.\",\\n    \"destination_entity\": \"pd-standard\"\\n  },\\n  {\\n    \"source_entity\": \"PersistentVolumeClaim\",\\n    \"description\": \"A PVC can be created without specifying a storage class, which will default to pd-standard on GKE.\",\\n    \"destination_entity\": \"GCE Persistent Disk\"\\n  },\\n  {\\n    \"source_entity\": \"get\",\\n    \"description\": \"Used with kubectl to retrieve information about the standard storage class.\",\\n    \"destination_entity\": \"sc\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"A command-line tool used to interact with a Kubernetes cluster.\",\\n    \"destination_entity\": \"GKE cluster\"\\n  }\\n]'},\n",
       " {'page': 221,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '189\\nDynamic provisioning of PersistentVolumes\\nThis PVC definition includes only the storage size request and the desired access\\nmodes, but no storage class. When you create the PVC, whatever storage class is\\nmarked as default will be used. You can confirm that’s the case:\\n$ kubectl get pvc mongodb-pvc2\\nNAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS\\nmongodb-pvc2  Bound    pvc-95a5ec12   1Gi        RWO           standard\\n$ kubectl get pv pvc-95a5ec12\\nNAME           CAPACITY  ACCESSMODES  RECLAIMPOLICY  STATUS    STORAGECLASS   \\npvc-95a5ec12   1Gi       RWO          Delete         Bound     standard\\n$ gcloud compute disks list\\nNAME                          ZONE            SIZE_GB  TYPE         STATUS\\ngke-kubia-dyn-pvc-95a5ec12    europe-west1-d  1        pd-standard  READY\\n...\\nFORCING A PERSISTENTVOLUMECLAIM TO BE BOUND TO ONE OF THE PRE-PROVISIONED \\nPERSISTENTVOLUMES\\nThis finally brings us to why you set storageClassName to an empty string in listing 6.11\\n(when you wanted the PVC to bind to the PV you’d provisioned manually). Let me\\nrepeat the relevant lines of that PVC definition here:\\nkind: PersistentVolumeClaim\\nspec:\\n  storageClassName: \"\"       \\nIf you hadn’t set the storageClassName attribute to an empty string, the dynamic vol-\\nume provisioner would have provisioned a new PersistentVolume, despite there being\\nan appropriate pre-provisioned PersistentVolume. At that point, I wanted to demon-\\nstrate how a claim gets bound to a manually pre-provisioned PersistentVolume. I didn’t\\nwant the dynamic provisioner to interfere. \\nTIP\\nExplicitly set storageClassName to \"\" if you want the PVC to use a pre-\\nprovisioned PersistentVolume.\\nUNDERSTANDING THE COMPLETE PICTURE OF DYNAMIC PERSISTENTVOLUME PROVISIONING\\nThis brings us to the end of this chapter. To summarize, the best way to attach per-\\nsistent storage to a pod is to only create the PVC (with an explicitly specified storage-\\nClassName if necessary) and the pod (which refers to the PVC by name). Everything\\nelse is taken care of by the dynamic PersistentVolume provisioner.\\n To get a complete picture of the steps involved in getting a dynamically provi-\\nsioned PersistentVolume, examine figure 6.10.\\n \\n \\n \\nSpecifying an empty string as the storage class \\nname ensures the PVC binds to a pre-provisioned \\nPV instead of dynamically provisioning a new one.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'A request for storage resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'A volume that has been provisioned',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'storageClassName',\n",
       "    'description': 'The name of the storage class',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PVC',\n",
       "    'description': 'Persistent Volume Claim',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PV',\n",
       "    'description': 'Persistent Volume',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for managing Kubernetes resources',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'get',\n",
       "    'description': 'A command to retrieve information about a resource',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pvc-95a5ec12',\n",
       "    'description': 'The name of the Persistent Volume',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'gcloud',\n",
       "    'description': 'A command-line tool for managing Google Cloud resources',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'compute disks list',\n",
       "    'description': 'A command to retrieve information about compute disks',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'A volume that has been provisioned',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"gcloud\", \"description\": \"list available compute disks\", \"destination_entity\": \"compute disks\"},\\n  {\"source_entity\": \"get\", \"description\": \"retrieve information about PVC mongodb-pvc2\", \"destination_entity\": \"PVC\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"get information about PV pvc-95a5ec12\", \"destination_entity\": \"PV\"},\\n  {\"source_entity\": \"PV\", \"description\": \"has a capacity of 1Gi and access modes RWO\", \"destination_entity\": \"pvc-95a5ec12\"},\\n  {\"source_entity\": \"PersistentVolume\", \"description\": \"can be provisioned dynamically or manually\", \"destination_entity\": \"None (general concept)\"},\\n  {\"source_entity\": \"get\", \"description\": \"retrieve information about PV pvc-95a5ec12\", \"destination_entity\": \"pvc-95a5ec12\"},\\n  {\"source_entity\": \"PersistentVolumeClaim\", \"description\": \"can bind to a pre-provisioned PersistentVolume\", \"destination_entity\": \"PV\"},\\n  {\"source_entity\": \"gcloud\", \"description\": \"list available compute disks\", \"destination_entity\": \"compute disks list\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"get information about PVC mongodb-pvc2\", \"destination_entity\": \"PVC\"},\\n  {\"source_entity\": \"storageClassName\", \"description\": \"sets the storage class name to an empty string\", \"destination_entity\": \"PV\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"get information about PV pvc-95a5ec12\", \"destination_entity\": \"pvc-95a5ec12\"},\\n  {\"source_entity\": \"PVC\", \"description\": \"can bind to a pre-provisioned PersistentVolume if storageClassName is set to an empty string\", \"destination_entity\": \"PV\"},\\n  {\"source_entity\": \"PersistentVolumeClaim\", \"description\": \"should be created with the PVC and pod\", \"destination_entity\": \"pod\"}\\n]\\n```\\n\\nNote that I excluded some entities from the list because they were not relevant to the relations being extracted. Let me know if you\\'d like me to clarify any of these relations!'},\n",
       " {'page': 222,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '190\\nCHAPTER 6\\nVolumes: attaching disk storage to containers\\n6.7\\nSummary\\nThis chapter has shown you how volumes are used to provide either temporary or per-\\nsistent storage to a pod’s containers. You’ve learned how to\\n\\uf0a1Create a multi-container pod and have the pod’s containers operate on the\\nsame files by adding a volume to the pod and mounting it in each container\\n\\uf0a1Use the emptyDir volume to store temporary, non-persistent data\\n\\uf0a1Use the gitRepo volume to easily populate a directory with the contents of a Git\\nrepository at pod startup\\n\\uf0a1Use the hostPath volume to access files from the host node\\n\\uf0a1Mount external storage in a volume to persist pod data across pod restarts\\n\\uf0a1Decouple the pod from the storage infrastructure by using PersistentVolumes\\nand PersistentVolumeClaims\\n\\uf0a1Have PersistentVolumes of the desired (or the default) storage class dynami-\\ncally provisioned for each PersistentVolumeClaim\\n\\uf0a1Prevent the dynamic provisioner from interfering when you want the Persistent-\\nVolumeClaim to be bound to a pre-provisioned PersistentVolume\\nIn the next chapter, you’ll see what mechanisms Kubernetes provides to deliver con-\\nfiguration data, secret information, and metadata about the pod and container to the\\nprocesses running inside a pod. This is done with the special types of volumes we’ve\\nmentioned in this chapter, but not yet explored.\\nPod\\nAdmin\\nVolume\\n1. Cluster admin sets up a PersistentVolume\\nprovisioner (if one’s not already deployed)\\n2. Admin creates one or\\nmore StorageClasses\\nand marks one as the\\ndefault (it may already\\nexist)\\nActual\\nstorage\\nPersistent\\nVolume\\nUser\\nPersistent\\nVolume\\nprovisioner\\nPersistent\\nVolumeClaim\\nStorage\\nClass\\n3. User creates a PVC referencing one of the\\nStorageClasses (or none to use the default)\\n6. User creates a pod with\\na volume referencing the\\nPVC by name\\n4. Kubernetes looks up the\\nStorageClass and the provisioner\\nreferenced in it and asks the provisioner\\nto provision a new PV based on the\\nPVC’s requested access mode and\\nstorage size and the parameters\\nin the StorageClass\\n5. Provisioner provisions the\\nactual storage, creates\\na PersistentVolume, and\\nbinds it to the PVC\\nFigure 6.10\\nThe complete picture of dynamic provisioning of PersistentVolumes\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A pod is a logical host for one or more containers.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Admin',\n",
       "    'description': 'Cluster administrator.',\n",
       "    'category': 'user'},\n",
       "   {'entity': 'Volume',\n",
       "    'description': \"A volume provides storage to a pod's containers.\",\n",
       "    'category': 'database'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'A PersistentVolume is a provisioned and formatted persistent storage resource.',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'A PersistentVolumeClaim requests access to a PersistentVolume.',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'StorageClass',\n",
       "    'description': 'A StorageClass defines parameters for dynamically provisioning PersistentVolumes.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'emptyDir',\n",
       "    'description': 'An emptyDir volume stores temporary, non-persistent data.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'gitRepo',\n",
       "    'description': 'A gitRepo volume populates a directory with the contents of a Git repository at pod startup.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'hostPath',\n",
       "    'description': 'A hostPath volume accesses files from the host node.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'PersistentVolumeClaim (PVC)',\n",
       "    'description': 'A PersistentVolumeClaim requests access to a PersistentVolume.',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Multiple containers that share storage and networking resources.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'provisioner',\n",
       "    'description': 'A provisioner dynamically provisions PersistentVolumes based on PVC requests.',\n",
       "    'category': 'database'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Admin\",\\n    \"description\": \"sets up a PersistentVolume provisioner\",\\n    \"destination_entity\": \"PersistentVolume provisioner\"\\n  },\\n  {\\n    \"source_entity\": \"Admin\",\\n    \"description\": \"creates one or more StorageClasses and marks one as the default\",\\n    \"destination_entity\": \"StorageClass\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"creates a PVC referencing one of the StorageClasses (or none to use the default)\",\\n    \"destination_entity\": \"PersistentVolumeClaim (PVC)\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"creates a pod with a volume referencing the PVC by name\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"looks up the StorageClass and the provisioner referenced in it and asks the provisioner to provision a new PV based on the PVC\\'s requested access mode and storage size and the parameters in the StorageClass\",\\n    \"destination_entity\": \"PersistentVolume\"\\n  },\\n  {\\n    \"source_entity\": \"Provisioner\",\\n    \"description\": \"provisions the actual storage, creates a PersistentVolume, and binds it to the PVC\",\\n    \"destination_entity\": \"PersistentVolumeClaim (PVC)\"\\n  },\\n  {\\n    \"source_entity\": \"Admin\",\\n    \"description\": \"sets up a PersistentVolume provisioner (if one\\'s not already deployed)\",\\n    \"destination_entity\": \"PersistentVolume\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"creates a pod with a volume referencing the PVC by name\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"looks up the StorageClass and the provisioner referenced in it and asks the provisioner to provision a new PV based on the PVC\\'s requested access mode and storage size and the parameters in the StorageClass\",\\n    \"destination_entity\": \"StorageClass\"\\n  },\\n  {\\n    \"source_entity\": \"Provisioner\",\\n    \"description\": \"provisions the actual storage, creates a PersistentVolume, and binds it to the PVC\",\\n    \"destination_entity\": \"PersistentVolume\"\\n  }\\n]\\n```'},\n",
       " {'page': 223,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '191\\nConfigMaps and Secrets:\\nconfiguring applications\\nUp to now you haven’t had to pass any kind of configuration data to the apps you’ve\\nrun in the exercises in this book. Because almost all apps require configuration (set-\\ntings that differ between deployed instances, credentials for accessing external sys-\\ntems, and so on), which shouldn’t be baked into the built app itself, let’s see how to\\npass configuration options to your app when running it in Kubernetes.\\n7.1\\nConfiguring containerized applications\\nBefore we go over how to pass configuration data to apps running in Kubernetes,\\nlet’s look at how containerized applications are usually configured.\\n If you skip the fact that you can bake the configuration into the application\\nitself, when starting development of a new app, you usually start off by having the\\nThis chapter covers\\n\\uf0a1Changing the main process of a container\\n\\uf0a1Passing command-line options to the app\\n\\uf0a1Setting environment variables exposed to the app\\n\\uf0a1Configuring apps through ConfigMaps\\n\\uf0a1Passing sensitive information through Secrets\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMaps',\n",
       "    'description': 'a way to pass configuration options to an application running in Kubernetes',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'a way to pass sensitive information to an application running in Kubernetes',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'an open-source container orchestration system for automating the deployment, scaling, and management of containers',\n",
       "    'category': 'Software/Framework'},\n",
       "   {'entity': 'ConfigMaps and Secrets',\n",
       "    'description': 'configuration mechanisms for passing data to applications running in Kubernetes',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'containerized applications',\n",
       "    'description': 'applications packaged into a container that can run independently on any system',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'environment variables',\n",
       "    'description': 'a way to set and expose variables to an application running in a container',\n",
       "    'category': 'Software/Framework'},\n",
       "   {'entity': 'command-line options',\n",
       "    'description': 'parameters passed to an application from the command line when it starts up',\n",
       "    'category': 'Software/Application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"containerized applications\", \"description\": \"are usually configured\", \"destination_entity\": \"configuration options\"},\\n  {\"source_entity\": \"ConfigMaps and Secrets\", \"description\": \"configure apps running in Kubernetes\", \"destination_entity\": \"Kubernetes\"},\\n  {\"source_entity\": \"ConfigMaps and Secrets\", \"description\": \"pass configuration data to apps\", \"destination_entity\": \"applications\"},\\n  {\"source_entity\": \"ConfigMaps and Secrets\", \"description\": \"pass sensitive information through\", \"destination_entity\": \"Secrets\"},\\n  {\"source_entity\": \"containerized applications\", \"description\": \"are configured using command-line options\", \"destination_entity\": \"command-line options\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"runs containerized applications with configuration options\", \"destination_entity\": \"configuration options\"},\\n  {\"source_entity\": \"ConfigMaps and Secrets\", \"description\": \"configure apps through ConfigMaps\", \"destination_entity\": \"ConfigMaps\"},\\n  {\"source_entity\": \"containerized applications\", \"description\": \"are configured using environment variables\", \"destination_entity\": \"environment variables\"},\\n  {\"source_entity\": \"ConfigMaps and Secrets\", \"description\": \"pass sensitive information through Secrets\", \"destination_entity\": \"Secrets\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"runs containerized applications with environment variables\", \"destination_entity\": \"environment variables\"}\\n]'},\n",
       " {'page': 224,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '192\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\napp configured through command-line arguments. Then, as the list of configuration\\noptions grows, you can move the configuration into a config file. \\n Another way of passing configuration options to an application that’s widely popu-\\nlar in containerized applications is through environment variables. Instead of having\\nthe app read a config file or command-line arguments, the app looks up the value of a\\ncertain environment variable. The official MySQL container image, for example, uses\\nan environment variable called MYSQL_ROOT_PASSWORD for setting the password for the\\nroot super-user account. \\n But why are environment variables so popular in containers? Using configuration\\nfiles inside Docker containers is a bit tricky, because you’d have to bake the config file\\ninto the container image itself or mount a volume containing the file into the con-\\ntainer. Obviously, baking files into the image is similar to hardcoding configuration\\ninto the source code of the application, because it requires you to rebuild the image\\nevery time you want to change the config. Plus, everyone with access to the image can\\nsee the config, including any information that should be kept secret, such as creden-\\ntials or encryption keys. Using a volume is better, but still requires you to make sure\\nthe file is written to the volume before the container is started. \\n If you’ve read the previous chapter, you might think of using a gitRepo volume as\\na configuration source. That’s not a bad idea, because it allows you to keep the config\\nnicely versioned and enables you to easily rollback a config change if necessary. But a\\nsimpler way allows you to put the configuration data into a top-level Kubernetes\\nresource and store it and all the other resource definitions in the same Git repository\\nor in any other file-based storage. The Kubernetes resource for storing configuration\\ndata is called a ConfigMap. We’ll learn how to use it in this chapter.\\n Regardless if you’re using a ConfigMap to store configuration data or not, you can\\nconfigure your apps by\\n\\uf0a1Passing command-line arguments to containers\\n\\uf0a1Setting custom environment variables for each container\\n\\uf0a1Mounting configuration files into containers through a special type of volume\\nWe’ll go over all these options in the next few sections, but before we start, let’s look\\nat config options from a security perspective. Though most configuration options\\ndon’t contain any sensitive information, several can. These include credentials, pri-\\nvate encryption keys, and similar data that needs to be kept secure. This type of infor-\\nmation needs to be handled with special care, which is why Kubernetes offers\\nanother type of first-class object called a Secret. We’ll learn about it in the last part of\\nthis chapter.\\n7.2\\nPassing command-line arguments to containers\\nIn all the examples so far, you’ve created containers that ran the default command\\ndefined in the container image, but Kubernetes allows overriding the command as\\npart of the pod’s container definition when you want to run a different executable\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMaps',\n",
       "    'description': 'Kubernetes resource for storing configuration data',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'Kubernetes type of first-class object for handling sensitive information',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'ConfigMaps',\n",
       "    'description': 'top-level Kubernetes resource for storing configuration data',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Git repository',\n",
       "    'description': 'file-based storage for config data and other resource definitions',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Containers',\n",
       "    'description': 'units of software that run in a pod',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'logical host for containers, providing shared resources and networking',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Volumes',\n",
       "    'description': 'persistent storage that can be mounted into a container',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'GitRepo volumes',\n",
       "    'description': 'special type of volume for mounting Git repositories',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Docker containers',\n",
       "    'description': 'units of software that run in a container runtime',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubernetes resource definitions',\n",
       "    'description': 'definitions for Kubernetes resources such as ConfigMaps and Pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Environment variables',\n",
       "    'description': 'variables set by the container runtime or pod that can be accessed by containers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Command-line arguments',\n",
       "    'description': 'arguments passed to a command when it is executed',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Config files',\n",
       "    'description': 'files containing configuration data that can be mounted into a container',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\": \"Docker containers\", \"description\": \"have configuration options that can be passed through command-line arguments\", \"destination_entity\": \"Command-line arguments\"},\\n\\n{\"source_entity\": \"Environment variables\", \"description\": \"are widely popular in containerized applications for passing configuration options\", \"destination_entity\": \"ConfigMaps\"},\\n\\n{\"source_entity\": \"Docker containers\", \"description\": \"can have config files mounted through a special type of volume\", \"destination_entity\": \"Volumes\"},\\n\\n{\"source_entity\": \"GitRepo volumes\", \"description\": \"allow you to keep configuration data nicely versioned and enables easy rollback of config changes\", \"destination_entity\": \"ConfigMaps\"},\\n\\n{\"source_entity\": \"Kubernetes resource definitions\", \"description\": \"can store configuration data in a top-level Kubernetes resource called ConfigMap\", \"destination_entity\": \"ConfigMaps\"},\\n\\n{\"source_entity\": \"Pods\", \"description\": \"allow overriding the command as part of the container definition to run a different executable\", \"destination_entity\": \"Command-line arguments\"},\\n\\n{\"source_entity\": \"Kubernetes resource definitions\", \"description\": \"can store sensitive information such as credentials and private encryption keys in a Secret object\", \"destination_entity\": \"Secrets\"},\\n\\n{\"source_entity\": \"ConfigMaps\", \"description\": \"are used to store configuration data that can be accessed by containers\", \"destination_entity\": \"Containers\"},\\n\\n{\"source_entity\": \"Volumes\", \"description\": \"can be mounted into containers to provide access to config files\", \"destination_entity\": \"Containers\"},\\n\\n{\"source_entity\": \"Command-line arguments\", \"description\": \"can be used to pass configuration options to applications running in containers\", \"destination_entity\": \"Docker containers\"},\\n\\n{\"source_entity\": \"Environment variables\", \"description\": \"are used by the official MySQL container image to set passwords for super-user accounts\", \"destination_entity\": \"Secrets\"},\\n\\n{\"source_entity\": \"ConfigMaps\", \"description\": \"allow you to put configuration data into a top-level Kubernetes resource\", \"destination_entity\": \"Kubernetes resource definitions\"}]\\n\\nNote that I\\'ve extracted all possible relations between the entities provided, following the steps and rules specified. The list contains 11 JSON objects, each representing a relation between two entities with a brief description of what action is being carried out by the source entity on the destination entity.'},\n",
       " {'page': 225,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '193\\nPassing command-line arguments to containers\\ninstead of the one specified in the image, or want to run it with a different set of com-\\nmand-line arguments. We’ll look at how to do that now.\\n7.2.1\\nDefining the command and arguments in Docker\\nThe first thing I need to explain is that the whole command that gets executed in the\\ncontainer is composed of two parts: the command and the arguments. \\nUNDERSTANDING ENTRYPOINT AND CMD\\nIn a Dockerfile, two instructions define the two parts:\\n\\uf0a1\\nENTRYPOINT defines the executable invoked when the container is started.\\n\\uf0a1\\nCMD specifies the arguments that get passed to the ENTRYPOINT.\\nAlthough you can use the CMD instruction to specify the command you want to execute\\nwhen the image is run, the correct way is to do it through the ENTRYPOINT instruction\\nand to only specify the CMD if you want to define the default arguments. The image can\\nthen be run without specifying any arguments\\n$ docker run <image>\\nor with additional arguments, which override whatever’s set under CMD in the Dockerfile:\\n$ docker run <image> <arguments>\\nUNDERSTANDING THE DIFFERENCE BETWEEN THE SHELL AND EXEC FORMS\\nBut there’s more. Both instructions support two different forms:\\n\\uf0a1\\nshell form—For example, ENTRYPOINT node app.js.\\n\\uf0a1\\nexec form—For example, ENTRYPOINT [\"node\", \"app.js\"].\\nThe difference is whether the specified command is invoked inside a shell or not. \\n In the kubia image you created in chapter 2, you used the exec form of the ENTRY-\\nPOINT instruction: \\nENTRYPOINT [\"node\", \"app.js\"]\\nThis runs the node process directly (not inside a shell), as you can see by listing the\\nprocesses running inside the container:\\n$ docker exec 4675d ps x\\n  PID TTY      STAT   TIME COMMAND\\n    1 ?        Ssl    0:00 node app.js\\n   12 ?        Rs     0:00 ps x\\nIf you’d used the shell form (ENTRYPOINT node app.js), these would have been the\\ncontainer’s processes:\\n$ docker exec -it e4bad ps x\\n  PID TTY      STAT   TIME COMMAND\\n    1 ?        Ss     0:00 /bin/sh -c node app.js\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ENTRYPOINT',\n",
       "    'description': 'defines the executable invoked when the container is started',\n",
       "    'category': 'docker instruction'},\n",
       "   {'entity': 'CMD',\n",
       "    'description': 'specifies the arguments that get passed to the ENTRYPOINT',\n",
       "    'category': 'docker instruction'},\n",
       "   {'entity': 'Dockerfile',\n",
       "    'description': 'a text file with instructions for building a Docker image',\n",
       "    'category': 'docker concept'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'a read-only template that contains an application and its dependencies',\n",
       "    'category': 'docker concept'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'a runtime environment where the application runs',\n",
       "    'category': 'docker concept'},\n",
       "   {'entity': 'node app.js',\n",
       "    'description': 'the command executed by ENTRYPOINT in the kubia image',\n",
       "    'category': 'entrypoint command'},\n",
       "   {'entity': 'docker run',\n",
       "    'description': 'command to execute a Docker container',\n",
       "    'category': 'docker command'},\n",
       "   {'entity': 'docker exec',\n",
       "    'description': 'command to execute a process inside a running container',\n",
       "    'category': 'docker command'},\n",
       "   {'entity': 'processes',\n",
       "    'description': 'programs or threads that are currently executing',\n",
       "    'category': 'computer science concept'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"CMD\",\\n    \"description\": \"specifies the arguments that get passed to the ENTRYPOINT\",\\n    \"destination_entity\": \"ENTRYPOINT\"\\n  },\\n  {\\n    \"source_entity\": \"Dockerfile\",\\n    \"description\": \"defines the two parts of the command: ENTRYPOINT and CMD\",\\n    \"destination_entity\": '},\n",
       " {'page': 226,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '194\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\n    7 ?        Sl     0:00 node app.js\\n   13 ?        Rs+    0:00 ps x\\nAs you can see, in that case, the main process (PID 1) would be the shell process\\ninstead of the node process. The node process (PID 7) would be started from that\\nshell. The shell process is unnecessary, which is why you should always use the exec\\nform of the ENTRYPOINT instruction.\\nMAKING THE INTERVAL CONFIGURABLE IN YOUR FORTUNE IMAGE\\nLet’s modify your fortune script and image so the delay interval in the loop is configu-\\nrable. You’ll add an INTERVAL variable and initialize it with the value of the first com-\\nmand-line argument, as shown in the following listing.\\n#!/bin/bash\\ntrap \"exit\" SIGINT\\nINTERVAL=$1\\necho Configured to generate new fortune every $INTERVAL seconds\\nmkdir -p /var/htdocs\\nwhile :\\ndo\\n  echo $(date) Writing fortune to /var/htdocs/index.html\\n  /usr/games/fortune > /var/htdocs/index.html\\n  sleep $INTERVAL\\ndone\\nYou’ve added or modified the lines in bold font. Now, you’ll modify the Dockerfile so\\nit uses the exec version of the ENTRYPOINT instruction and sets the default interval to\\n10 seconds using the CMD instruction, as shown in the following listing.\\nFROM ubuntu:latest\\nRUN apt-get update ; apt-get -y install fortune\\nADD fortuneloop.sh /bin/fortuneloop.sh\\nENTRYPOINT [\"/bin/fortuneloop.sh\"]        \\nCMD [\"10\"]                                \\nYou can now build and push the image to Docker Hub. This time, you’ll tag the image\\nas args instead of latest:\\n$ docker build -t docker.io/luksa/fortune:args .\\n$ docker push docker.io/luksa/fortune:args\\nYou can test the image by running it locally with Docker:\\n$ docker run -it docker.io/luksa/fortune:args\\nConfigured to generate new fortune every 10 seconds\\nFri May 19 10:39:44 UTC 2017 Writing fortune to /var/htdocs/index.html\\nListing 7.1\\nFortune script with interval configurable through argument: fortune-args/\\nfortuneloop.sh\\nListing 7.2\\nDockerfile for the updated fortune image: fortune-args/Dockerfile\\nThe exec form of the \\nENTRYPOINT instruction\\nThe default argument \\nfor the executable\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMaps',\n",
       "    'description': 'a Kubernetes feature for storing configuration data as key-value pairs',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'a Kubernetes feature for storing sensitive information such as passwords or API keys',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ENTRYPOINT instruction',\n",
       "    'description': 'a Dockerfile instruction that specifies the command to run when a container is launched',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CMD instruction',\n",
       "    'description': 'a Dockerfile instruction that specifies the default command to run when a container is launched',\n",
       "    'category': 'software'},\n",
       "   {'entity': '/bin/bash',\n",
       "    'description': 'the Bash shell executable',\n",
       "    'category': 'process'},\n",
       "   {'entity': '/usr/games/fortune',\n",
       "    'description': 'the fortune game executable',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'trap',\n",
       "    'description': 'a command in the fortune script that traps SIGINT signals and exits the script',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'sleep',\n",
       "    'description': 'a Unix command that suspends execution for a specified time period',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'mkdir',\n",
       "    'description': 'a Unix command that creates a new directory',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'docker',\n",
       "    'description': 'the Docker containerization platform',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Dockerfile',\n",
       "    'description': 'a text file that contains instructions for building a Docker image',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apt-get',\n",
       "    'description': 'the APT package manager for Debian-based Linux distributions',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ps',\n",
       "    'description': 'a Unix command that displays information about running processes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'PID 1',\n",
       "    'description': 'the initial process ID in a Linux system, which is usually the shell',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[{\"source_entity\": \"ConfigMaps\", \"description\": \"configuring applications\", \"destination_entity\": \"ConfigMaps\"}]\\n\\n'},\n",
       " {'page': 227,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '195\\nPassing command-line arguments to containers\\nNOTE\\nYou can stop the script with Control+C.\\nAnd you can override the default sleep interval by passing it as an argument:\\n$ docker run -it docker.io/luksa/fortune:args 15\\nConfigured to generate new fortune every 15 seconds\\nNow that you’re sure your image honors the argument passed to it, let’s see how to use\\nit in a pod.\\n7.2.2\\nOverriding the command and arguments in Kubernetes\\nIn Kubernetes, when specifying a container, you can choose to override both ENTRY-\\nPOINT and CMD. To do that, you set the properties command and args in the container\\nspecification, as shown in the following listing.\\nkind: Pod\\nspec:\\n  containers:\\n  - image: some/image\\n    command: [\"/bin/command\"]\\n    args: [\"arg1\", \"arg2\", \"arg3\"]\\nIn most cases, you’ll only set custom arguments and rarely override the command\\n(except in general-purpose images such as busybox, which doesn’t define an ENTRY-\\nPOINT at all). \\nNOTE\\nThe command and args fields can’t be updated after the pod is created.\\nThe two Dockerfile instructions and the equivalent pod spec fields are shown in table 7.1.\\nRUNNING THE FORTUNE POD WITH A CUSTOM INTERVAL\\nTo run the fortune pod with a custom delay interval, you’ll copy your fortune-\\npod.yaml into fortune-pod-args.yaml and modify it as shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: fortune2s        \\nListing 7.3\\nA pod definition specifying a custom command and arguments\\nTable 7.1\\nSpecifying the executable and its arguments in Docker vs Kubernetes\\nDocker\\nKubernetes\\nDescription\\nENTRYPOINT\\ncommand\\nThe executable that’s executed inside the container\\nCMD\\nargs\\nThe arguments passed to the executable\\nListing 7.4\\nPassing an argument in the pod definition: fortune-pod-args.yaml\\nYou changed the \\npod’s name.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [            Docker     Kubernetes  \\\n",
       "   0  ENTRYPOINT\\nCMD  command\\nargs   \n",
       "   \n",
       "                                            Description  \n",
       "   0  The executable that’s executed inside the cont...  ],\n",
       "  'entities': [{'entity': 'docker',\n",
       "    'description': 'A containerization platform.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Dockerfile',\n",
       "    'description': 'A configuration file for Docker images.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ENTRYPOINT',\n",
       "    'description': \"The executable that's executed inside the container in Docker.\",\n",
       "    'category': 'command'},\n",
       "   {'entity': 'CMD',\n",
       "    'description': 'The command to be executed when running a Docker image.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'command',\n",
       "    'description': 'A property used in Kubernetes to override the ENTRYPOINT.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'args',\n",
       "    'description': 'A property used in Kubernetes to pass arguments to the container.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A collection of containers that can be run as a single entity.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'The configuration file for Docker images.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'container spec',\n",
       "    'description': 'A property used in Kubernetes to specify the container configuration.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'fortune-pod.yaml',\n",
       "    'description': 'A YAML file used to define a pod with custom arguments.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'fortune-pod-args.yaml',\n",
       "    'description': 'A YAML file used to define a pod with custom command and arguments.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'fortune pod',\n",
       "    'description': 'An example of running a pod with custom arguments.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Control+C',\n",
       "    'description': 'A keyboard shortcut to stop the script.',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"docker\",\\n    \"description\": \"running a container with custom arguments\",\\n    \"destination_entity\": \"args\"\\n  },\\n  {\\n    \"source_entity\": \"CMD\",\\n    \"description\": \"executable that\\'s executed inside the container\",\\n    \"destination_entity\": \"image\"\\n  },\\n  {\\n    \"source_entity\": \"ENTRYPOINT\",\\n    \"description\": \"the executable that\\'s executed inside the container\",\\n    \"destination_entity\": \"CMD\"\\n  },\\n  {\\n    \"source_entity\": \"docker\",\\n    \"description\": \"running a container with custom interval\",\\n    \"destination_entity\": \"fortune pod\"\\n  },\\n  {\\n    \"source_entity\": \"pod spec fields\",\\n    \"description\": \"custom arguments can\\'t be updated after the pod is created\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Control+C\",\\n    \"description\": \"stopping a script with Control+C\",\\n    \"destination_entity\": \"script\"\\n  },\\n  {\\n    \"source_entity\": \"fortune-pod.yaml\",\\n    \"description\": \"modifying the pod definition to pass an argument\",\\n    \"destination_entity\": \"args\"\\n  },\\n  {\\n    \"source_entity\": \"fortune-pod-args.yaml\",\\n    \"description\": \"passing an argument in the pod definition\",\\n    \"destination_entity\": \"pod spec fields\"\\n  },\\n  {\\n    \"source_entity\": \"image\",\\n    \"description\": \"honoring the argument passed to it\",\\n    \"destination_entity\": \"CMD\"\\n  },\\n  {\\n    \"source_entity\": \"command\",\\n    \"description\": \"custom command in a container specification\",\\n    \"destination_entity\": \"container spec\"\\n  },\\n  {\\n    \"source_entity\": \"args\",\\n    \"description\": \"custom arguments in a container specification\",\\n    \"destination_entity\": \"container spec\"\\n  },\\n  {\\n    \"source_entity\": \"RUNNING THE FORTUNE POD WITH A CUSTOM INTERVAL\",\\n    \"description\": \"running the fortune pod with a custom delay interval\",\\n    \"destination_entity\": \"fortune pod\"\\n  }\\n]'},\n",
       " {'page': 228,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '196\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\nspec:\\n  containers:\\n  - image: luksa/fortune:args      \\n    args: [\"2\"]                  \\n    name: html-generator\\n    volumeMounts:\\n    - name: html\\n      mountPath: /var/htdocs\\n...\\nYou added the args array to the container definition. Try creating this pod now. The\\nvalues of the array will be passed to the container as command-line arguments when it\\nis run. \\n The array notation used in this listing is great if you have one argument or a few. If\\nyou have several, you can also use the following notation:\\n    args:\\n    - foo\\n    - bar\\n    - \"15\"\\nTIP\\nYou don’t need to enclose string values in quotations marks (but you\\nmust enclose numbers). \\nSpecifying arguments is one way of passing config\\noptions to your containers through command-\\nline arguments. Next, you’ll see how to do it\\nthrough environment variables.\\n7.3\\nSetting environment variables for \\na container\\nAs I’ve already mentioned, containerized appli-\\ncations often use environment variables as a\\nsource of configuration options. Kubernetes\\nallows you to specify a custom list of environ-\\nment variables for each container of a pod, as\\nshown in figure 7.1. Although it would be use-\\nful to also define environment variables at the\\npod level and have them be inherited by its\\ncontainers, no such option currently exists.\\nNOTE\\nLike the container’s command and\\narguments, the list of environment variables\\nalso cannot be updated after the pod is created.\\nUsing fortune:args \\ninstead of fortune:latest\\nThis argument makes the \\nscript generate a new fortune \\nevery two seconds.\\nPod\\nContainer A\\nEnvironment variables\\nFOO=BAR\\nABC=123\\nContainer B\\nEnvironment variables\\nFOO=FOOBAR\\nBAR=567\\nFigure 7.1\\nEnvironment variables can \\nbe set per container.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'args',\n",
       "    'description': 'array of command-line arguments passed to the container',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'fortune:args',\n",
       "    'description': 'image with argument to generate a new fortune every two seconds',\n",
       "    'category': 'image'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'a lightweight and standalone executable package of software, in this context, running the fortune application',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'the basic execution unit in Kubernetes, comprising one or more containers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'enviroment variables',\n",
       "    'description': 'configuration options passed to a container through environment variables',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'FOO=BAR',\n",
       "    'description': 'environment variable setting FOO to BAR',\n",
       "    'category': 'variable'},\n",
       "   {'entity': 'ABC=123',\n",
       "    'description': 'environment variable setting ABC to 123',\n",
       "    'category': 'variable'},\n",
       "   {'entity': 'Container A',\n",
       "    'description': 'first container in the pod, running with environment variables FOO and ABC',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Container B',\n",
       "    'description': 'second container in the pod, running with different environment variables',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'container orchestration system used to run the pods and containers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'pre-built software package, in this case, luksa/fortune:args',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"You\", \"description\": \"added the args array to the container definition\", \"destination_entity\": \"Container A\"},\\n  {\"source_entity\": \"The values of the array\", \"description\": \"will be passed to the container as command-line arguments when it is run\", \"destination_entity\": \"Container A\"},\\n  {\"source_entity\": \"You\", \"description\": \"created this pod now\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"The array notation used in this listing\", \"description\": \"is great if you have one argument or a few\", \"destination_entity\": \"args\"},\\n  {\"source_entity\": \"You\", \"description\": \"can also use the following notation\", \"destination_entity\": \"args\"},\\n  {\"source_entity\": \"foo, bar, and \\\\\"15\\\\\"\", \"description\": \"are values in the args array\", \"destination_entity\": \"Container A\"},\\n  {\"source_entity\": \"Kubernetes allows you to specify a custom list of environment variables for each container of a pod\", \"description\": \"as shown in figure 7.1\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"You\", \"description\": \"will see how to set environment variables through command-line arguments\", \"destination_entity\": \"Container A\"},\\n  {\"source_entity\": \"fortune:args instead of fortune:latest\", \"description\": \"makes the script generate a new fortune every two seconds\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"You\", \"description\": \"added environment variables FOO=BAR and ABC=123 to Container A\", \"destination_entity\": \"Container A\"},\\n  {\"source_entity\": \"The list of environment variables cannot be updated after the pod is created\", \"description\": \"using fortune:args instead of fortune:latest\", \"destination_entity\": \"Pod\"}\\n]\\n\\nNote that I\\'ve used \"You\" as the source entity in some cases, since it\\'s not explicitly mentioned as an entity. If you want to replace it with a specific entity, please let me know!'},\n",
       " {'page': 229,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '197\\nSetting environment variables for a container\\nMAKING THE INTERVAL IN YOUR FORTUNE IMAGE CONFIGURABLE THROUGH AN ENVIRONMENT VARIABLE\\nLet’s see how to modify your fortuneloop.sh script once again to allow it to be config-\\nured from an environment variable, as shown in the following listing.\\n#!/bin/bash\\ntrap \"exit\" SIGINT\\necho Configured to generate new fortune every $INTERVAL seconds\\nmkdir -p /var/htdocs\\nwhile :\\ndo\\n  echo $(date) Writing fortune to /var/htdocs/index.html\\n  /usr/games/fortune > /var/htdocs/index.html\\n  sleep $INTERVAL\\ndone\\nAll you had to do was remove the row where the INTERVAL variable is initialized. Because\\nyour “app” is a simple bash script, you didn’t need to do anything else. If the app was\\nwritten in Java you’d use System.getenv(\"INTERVAL\"), whereas in Node.JS you’d use\\nprocess.env.INTERVAL, and in Python you’d use os.environ[\\'INTERVAL\\'].\\n7.3.1\\nSpecifying environment variables in a container definition\\nAfter building the new image (I’ve tagged it as luksa/fortune:env this time) and\\npushing it to Docker Hub, you can run it by creating a new pod, in which you pass the\\nenvironment variable to the script by including it in your container definition, as\\nshown in the following listing.\\nkind: Pod\\nspec:\\n containers:\\n - image: luksa/fortune:env\\n   env:                        \\n   - name: INTERVAL            \\n     value: \"30\"               \\n   name: html-generator\\n...\\nAs mentioned previously, you set the environment variable inside the container defini-\\ntion, not at the pod level. \\nNOTE\\nDon’t forget that in each container, Kubernetes also automatically\\nexposes environment variables for each service in the same namespace. These\\nenvironment variables are basically auto-injected configuration.\\nListing 7.5\\nFortune script with interval configurable through env var: fortune-env/\\nfortuneloop.sh\\nListing 7.6\\nDefining an environment variable in a pod: fortune-pod-env.yaml\\nAdding a single variable to \\nthe environment variable list\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'environment variables',\n",
       "    'description': 'Variables set outside of a container that can be accessed within it.',\n",
       "    'category': 'software/container'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Environment Variables\",\\n    \"description\": \"are used to pass configuration to container scripts\",\\n    \"destination_entity\": \"Container Scripts\"\\n  },\\n  {\\n    \"source_entity\": \"Users\",\\n    \"description\": \"can set environment variables inside a container definition\",\\n    \"destination_entity\": \"Environment Variables\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"automatically exposes environment variables for each service in the same namespace\",\\n    \"destination_entity\": \"Environment Variables\"\\n  },\\n  {\\n    \"source_entity\": \"Users\",\\n    \"description\": \"can access environment variables through getenv() in Java, process.env.ENV_VAR in Node.JS, and os.environ'},\n",
       " {'page': 230,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '198\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\n7.3.2\\nReferring to other environment variables in a variable’s value\\nIn the previous example, you set a fixed value for the environment variable, but you\\ncan also reference previously defined environment variables or any other existing vari-\\nables by using the $(VAR) syntax. If you define two environment variables, the second\\none can include the value of the first one as shown in the following listing.\\nenv:\\n- name: FIRST_VAR\\n  value: \"foo\"\\n- name: SECOND_VAR\\n  value: \"$(FIRST_VAR)bar\"\\nIn this case, the SECOND_VAR’s value will be \"foobar\". Similarly, both the command and\\nargs attributes you learned about in section 7.2 can also refer to environment vari-\\nables like this. You’ll use this method in section 7.4.5.\\n7.3.3\\nUnderstanding the drawback of hardcoding environment \\nvariables\\nHaving values effectively hardcoded in the pod definition means you need to have\\nseparate pod definitions for your production and your development pods. To reuse\\nthe same pod definition in multiple environments, it makes sense to decouple the\\nconfiguration from the pod descriptor. Luckily, you can do that using a ConfigMap\\nresource and using it as a source for environment variable values using the valueFrom\\ninstead of the value field. You’ll learn about this next. \\n7.4\\nDecoupling configuration with a ConfigMap\\nThe whole point of an app’s configuration is to keep the config options that vary\\nbetween environments, or change frequently, separate from the application’s source\\ncode. If you think of a pod descriptor as source code for your app (and in microservices\\narchitectures that’s what it really is, because it defines how to compose the individual\\ncomponents into a functioning system), it’s clear you should move the configuration\\nout of the pod description.\\n7.4.1\\nIntroducing ConfigMaps\\nKubernetes allows separating configuration options into a separate object called a\\nConfigMap, which is a map containing key/value pairs with the values ranging from\\nshort literals to full config files. \\n An application doesn’t need to read the ConfigMap directly or even know that it\\nexists. The contents of the map are instead passed to containers as either environ-\\nment variables or as files in a volume (see figure 7.2). And because environment\\nListing 7.7\\nReferring to an environment variable inside another one\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMap',\n",
       "    'description': 'Resource for storing configuration options',\n",
       "    'category': 'database/process'},\n",
       "   {'entity': 'environment variables',\n",
       "    'description': 'Variables used to configure applications',\n",
       "    'category': 'process/thread'},\n",
       "   {'entity': 'pod descriptor',\n",
       "    'description': 'Definition of a pod in Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'valueFrom',\n",
       "    'description': 'Field for referencing environment variables',\n",
       "    'category': 'command/process'},\n",
       "   {'entity': 'value',\n",
       "    'description': 'Field for setting a fixed value for an environment variable',\n",
       "    'category': 'process/thread'},\n",
       "   {'entity': 'FIRST_VAR',\n",
       "    'description': \"Environment variable set to 'foo'\",\n",
       "    'category': 'environment variables/process'},\n",
       "   {'entity': 'SECOND_VAR',\n",
       "    'description': \"Environment variable set to '$(FIRST_VAR)bar'\",\n",
       "    'category': 'environment variables/process'},\n",
       "   {'entity': 'env',\n",
       "    'description': 'List of environment variables',\n",
       "    'category': 'process/thread'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'Field for setting the name of an environment variable',\n",
       "    'category': 'command/process'},\n",
       "   {'entity': 'args',\n",
       "    'description': 'Field for passing arguments to a command',\n",
       "    'category': 'command/process'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'Instance of a Docker container',\n",
       "    'category': 'software/container'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ConfigMaps\", \"description\": \"decouples configuration from pod descriptor\", \"destination_entity\": \"pod descriptor\"},\\n  {\"source_entity\": \"valueFrom\", \"description\": \"uses ConfigMap as a source for environment variable values\", \"destination_entity\": \"environment variables\"},\\n  {\"source_entity\": \"FIRST_VAR\", \"description\": \"sets a fixed value for the environment variable\", \"destination_entity\": \"SECOND_VAR\"},\\n  {\"source_entity\": \"SECOND_VAR\", \"description\": \"includes the value of FIRST_VAR in its value\", \"destination_entity\": \"FIRST_VAR\"},\\n  {\"source_entity\": \"command\", \"description\": \"can refer to environment variables\", \"destination_entity\": \"environment variables\"},\\n  {\"source_entity\": \"args\", \"description\": \"can also refer to environment variables\", \"destination_entity\": \"environment variables\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"allows separating configuration options into a ConfigMap\", \"destination_entity\": \"ConfigMaps\"},\\n  {\"source_entity\": \"application\", \"description\": \"doesn\\'t need to read the ConfigMap directly or even know that it exists\", \"destination_entity\": \"ConfigMaps\"},\\n  {\"source_entity\": \"environment variables\", \"description\": \"are passed to containers as either environment variables or as files in a volume\", \"destination_entity\": \"containers\"},\\n  {\"source_entity\": \"pod descriptor\", \"description\": \"defines how to compose individual components into a functioning system\", \"destination_entity\": \"application\"}\\n]'},\n",
       " {'page': 231,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '199\\nDecoupling configuration with a ConfigMap\\nvariables can be referenced in command-line arguments using the $(ENV_VAR) syn-\\ntax, you can also pass ConfigMap entries to processes as command-line arguments.\\nSure, the application can also read the contents of a ConfigMap directly through the\\nKubernetes REST API endpoint if needed, but unless you have a real need for this,\\nyou should keep your app Kubernetes-agnostic as much as possible.\\n Regardless of how an app consumes a ConfigMap, having the config in a separate\\nstandalone object like this allows you to keep multiple manifests for ConfigMaps with\\nthe same name, each for a different environment (development, testing, QA, produc-\\ntion, and so on). Because pods reference the ConfigMap by name, you can use a dif-\\nferent config in each environment while using the same pod specification across all of\\nthem (see figure 7.3).\\nPod\\nEnvironment variables\\nConﬁgMap\\nkey1=value1\\nkey2=value2\\n...\\nconﬁgMap\\nvolume\\nFigure 7.2\\nPods use ConfigMaps \\nthrough environment variables and \\nconfigMap volumes.\\nConﬁgMap:\\napp-conﬁg\\nNamespace: development\\n(contains\\ndevelopment\\nvalues)\\nPod(s)\\nConﬁgMaps created\\nfrom different manifests\\nPods created from the\\nsame pod manifests\\nNamespace: production\\nConﬁgMap:\\napp-conﬁg\\n(contains\\nproduction\\nvalues)\\nPod(s)\\nFigure 7.3\\nTwo different ConfigMaps with the same name used in different \\nenvironments\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMap',\n",
       "    'description': 'a standalone object to store configuration data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes REST API endpoint',\n",
       "    'description': 'an endpoint to access ConfigMap contents directly',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'virtual containers that run application code',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Environment variables',\n",
       "    'description': 'variables that can be referenced in command-line arguments',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMap entries',\n",
       "    'description': 'data stored in a ConfigMap',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Manifests',\n",
       "    'description': 'files that define the configuration of resources',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': 'a logical grouping of resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMaps',\n",
       "    'description': 'collections of key-value pairs used to store configuration data',\n",
       "    'category': 'database'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Manifests\", \"description\": \"can contain multiple ConfigMaps with same name for different environments\", \"destination_entity\": \"ConfigMaps\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"use ConfigMaps through environment variables and configMap volumes\", \"destination_entity\": \"ConfigMap\"},\\n  {\"source_entity\": \"Kubernetes REST API endpoint\", \"description\": \"can be used to read ConfigMap contents directly\", \"destination_entity\": \"ConfigMap\"},\\n  {\"source_entity\": \"Manifests\", \"description\": \"can reference same ConfigMap name across different environments\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"ConfigMaps\", \"description\": \"can have multiple entries passed as command-line arguments\", \"destination_entity\": \"Processes\"},\\n  {\"source_entity\": \"Namespace\", \"description\": \"development values are contained in separate ConfigMap for development environment\", \"destination_entity\": \"ConfigMap\"},\\n  {\"source_entity\": \"Namespace\", \"description\": \"production values are contained in separate ConfigMap for production environment\", \"destination_entity\": \"ConfigMap\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"can reference same ConfigMap name across different environments\", \"destination_entity\": \"Manifests\"}\\n]\\n```'},\n",
       " {'page': 232,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '200\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\n7.4.2\\nCreating a ConfigMap\\nLet’s see how to use a ConfigMap in one of your pods. To start with the simplest exam-\\nple, you’ll first create a map with a single key and use it to fill the INTERVAL environment\\nvariable from your previous example. You’ll create the ConfigMap with the special\\nkubectl create configmap command instead of posting a YAML with the generic\\nkubectl create -f command. \\nUSING THE KUBECTL CREATE CONFIGMAP COMMAND\\nYou can define the map’s entries by passing literals to the kubectl command or you\\ncan create the ConfigMap from files stored on your disk. Use a simple literal first:\\n$ kubectl create configmap fortune-config --from-literal=sleep-interval=25\\nconfigmap \"fortune-config\" created\\nNOTE\\nConfigMap keys must be a valid DNS subdomain (they may only con-\\ntain alphanumeric characters, dashes, underscores, and dots). They may\\noptionally include a leading dot.\\nThis creates a ConfigMap called fortune-config with the single-entry sleep-interval\\n=25 (figure 7.4).\\nConfigMaps usually contain more than one entry. To create a ConfigMap with multi-\\nple literal entries, you add multiple --from-literal arguments:\\n$ kubectl create configmap myconfigmap\\n➥  --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two\\nLet’s inspect the YAML descriptor of the ConfigMap you created by using the kubectl\\nget command, as shown in the following listing.\\n$ kubectl get configmap fortune-config -o yaml\\napiVersion: v1\\ndata:\\n  sleep-interval: \"25\"                      \\nkind: ConfigMap                              \\nmetadata:\\n  creationTimestamp: 2016-08-11T20:31:08Z\\n  name: fortune-config                      \\n  namespace: default\\n  resourceVersion: \"910025\"\\n  selfLink: /api/v1/namespaces/default/configmaps/fortune-config\\n  uid: 88c4167e-6002-11e6-a50d-42010af00237\\nListing 7.8\\nA ConfigMap definition\\nsleep-interval\\n25\\nConﬁgMap: fortune-conﬁg\\nFigure 7.4\\nThe fortune-config \\nConfigMap containing a single entry\\nThe single entry \\nin this map\\nThis descriptor \\ndescribes a ConfigMap.\\nThe name of this map \\n(you’re referencing it \\nby this name)\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [sleep-interval, 25]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'ConfigMaps',\n",
       "    'description': 'A Kubernetes object that holds configuration data, such as environment variables or files.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl create configmap command',\n",
       "    'description': 'A command used to create a ConfigMap in a Kubernetes cluster.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ConfigMap keys',\n",
       "    'description': 'Valid DNS subdomain names that can be used as keys in a ConfigMap.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'interval environment variable',\n",
       "    'description': 'A variable that stores the sleep interval value, set using the --from-literal argument with kubectl create configmap command.',\n",
       "    'category': 'environment variable'},\n",
       "   {'entity': 'fortune-config ConfigMap',\n",
       "    'description': 'A ConfigMap created with a single entry containing the sleep-interval value.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'myconfigmap ConfigMap',\n",
       "    'description': 'A ConfigMap created with multiple literal entries.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiVersion: v1',\n",
       "    'description': 'The API version used to describe a ConfigMap in YAML format.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'data field',\n",
       "    'description': 'A field in the YAML descriptor of a ConfigMap that contains the configuration data.',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'sleep-interval key',\n",
       "    'description': 'A key in the fortune-config ConfigMap that stores the sleep interval value.',\n",
       "    'category': 'key'},\n",
       "   {'entity': 'kind: ConfigMap',\n",
       "    'description': 'The type of object described in the YAML descriptor, which is a ConfigMap.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"ConfigMap keys\", \"description\": \"must be a valid DNS subdomain\", \"destination_entity\": \"DNS subdomain\"},\\n{\"source_entity\": \"ConfigMaps\", \"description\": \"usually contain more than one entry\", \"destination_entity\": \"entries\"},\\n{\"source_entity\": \"kind: ConfigMap\", \"description\": \"defines the type of object being created\", \"destination_entity\": \"object\"},\\n{\"source_entity\": \"apiVersion: v1\", \"description\": \"specifies the API version of the object\", \"destination_entity\": \"API version\"},\\n{\"source_entity\": \"interval environment variable\", \"description\": \"is filled by a ConfigMap\", \"destination_entity\": \"ConfigMap\"},\\n{\"source_entity\": \"data field\", \"description\": \"contains key-value pairs\", \"destination_entity\": \"key-value pairs\"},\\n{\"source_entity\": \"myconfigmap ConfigMap\", \"description\": \"is created with multiple literal entries\", \"destination_entity\": \"literal entries\"},\\n{\"source_entity\": \"fortune-config ConfigMap\", \"description\": \"is used to fill the INTERVAL environment variable\", \"destination_entity\": \"INTERVAL environment variable\"},\\n{\"source_entity\": \"sleep-interval key\", \"description\": \"defines a single-entry key-value pair\", \"destination_entity\": \"key-value pair\"},\\n{\"source_entity\": \"kubectl create configmap command\", \"description\": \"creates a ConfigMap with the specified key-value pairs\", \"destination_entity\": \"ConfigMap\"}]'},\n",
       " {'page': 233,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '201\\nDecoupling configuration with a ConfigMap\\nNothing extraordinary. You could easily have written this YAML yourself (you wouldn’t\\nneed to specify anything but the name in the metadata section, of course) and posted\\nit to the Kubernetes API with the well-known\\n$ kubectl create -f fortune-config.yaml\\nCREATING A CONFIGMAP ENTRY FROM THE CONTENTS OF A FILE\\nConfigMaps can also store coarse-grained config data, such as complete config files.\\nTo do this, the kubectl create configmap command also supports reading files from\\ndisk and storing them as individual entries in the ConfigMap:\\n$ kubectl create configmap my-config --from-file=config-file.conf\\nWhen you run the previous command, kubectl looks for the file config-file.conf in\\nthe directory you run kubectl in. It will then store the contents of the file under the\\nkey config-file.conf in the ConfigMap (the filename is used as the map key), but\\nyou can also specify a key manually like this:\\n$ kubectl create configmap my-config --from-file=customkey=config-file.conf\\nThis command will store the file’s contents under the key customkey. As with literals,\\nyou can add multiple files by using the --from-file argument multiple times. \\nCREATING A CONFIGMAP FROM FILES IN A DIRECTORY\\nInstead of importing each file individually, you can even import all files from a file\\ndirectory:\\n$ kubectl create configmap my-config --from-file=/path/to/dir\\nIn this case, kubectl will create an individual map entry for each file in the specified\\ndirectory, but only for files whose name is a valid ConfigMap key. \\nCOMBINING DIFFERENT OPTIONS\\nWhen creating ConfigMaps, you can use a combination of all the options mentioned\\nhere (note that these files aren’t included in the book’s code archive—you can create\\nthem yourself if you’d like to try out the command):\\n$ kubectl create configmap my-config  \\n➥  --from-file=foo.json                  \\n➥  --from-file=bar=foobar.conf              \\n➥  --from-file=config-opts/               \\n➥  --from-literal=some=thing    \\nHere, you’ve created the ConfigMap from multiple sources: a whole directory, a file,\\nanother file (but stored under a custom key instead of using the filename as the key),\\nand a literal value. Figure 7.5 shows all these sources and the resulting ConfigMap.\\nA single file\\nA file stored under \\na custom key\\nA whole directory\\nA literal value\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMap',\n",
       "    'description': 'A resource in Kubernetes that stores configuration data as key-value pairs.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool for managing Kubernetes resources, including ConfigMaps.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'YAML',\n",
       "    'description': 'A human-readable serialization format for data, used to define ConfigMap configurations.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata section',\n",
       "    'description': 'A part of a YAML file that contains metadata about the resource being defined.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'ConfigMaps',\n",
       "    'description': 'The plural form of ConfigMap, referring to multiple configuration data resources in Kubernetes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': '--from-file',\n",
       "    'description': 'An option for creating a ConfigMap from a file on disk.',\n",
       "    'category': 'software'},\n",
       "   {'entity': '--from-literal',\n",
       "    'description': 'An option for creating a ConfigMap by specifying literal values.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMap key',\n",
       "    'description': 'A unique identifier used to store configuration data in a ConfigMap resource.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl create configmap command',\n",
       "    'description': 'The command used to create a new ConfigMap resource in Kubernetes, specifying the desired configuration data and its storage format.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"creating ConfigMap\", \"destination_entity\": \"ConfigMap\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"storing file contents\", \"destination_entity\": \"file config-file.conf\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"specifying custom key\", \"destination_entity\": \"config-file.conf\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"importing all files from directory\", \"destination_entity\": \"/path/to/dir\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"creating individual map entry for each file\", \"destination_entity\": \"file directory\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"using combination of options to create ConfigMap\", \"destination_entity\": \"ConfigMap\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"creating ConfigMap from multiple sources\", \"destination_entity\": \"ConfigMap\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"storing file contents under custom key\", \"destination_entity\": \"file foo.json\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"storing literal value in ConfigMap\", \"destination_entity\": \"literal value some=thing\"}\\n]\\n\\nNote: I\\'ve only considered the entities provided and ignored any other text or context. Also, I\\'ve tried to be as accurate as possible with the relations extracted from the document page. If you spot any errors or inconsistencies, please let me know!'},\n",
       " {'page': 234,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '202\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\n7.4.3\\nPassing a ConfigMap entry to a container as an environment \\nvariable\\nHow do you now get the values from this map into a pod’s container? You have three\\noptions. Let’s start with the simplest—setting an environment variable. You’ll use the\\nvalueFrom field I mentioned in section 7.3.3. The pod descriptor should look like\\nthe following listing.\\napiVersion: v1\\nkind: Pod\\nListing 7.9\\nPod with env var from a config map: fortune-pod-env-configmap.yaml\\nConﬁgMap: my-conﬁg\\nKey\\nfoo.json\\nfoo.json\\nValue\\nbar\\nabc\\ndebug\\ntrue\\nrepeat\\n100\\nsome\\nthing\\n{\\nfoo: bar\\nbaz: 5\\n}\\nconﬁg-opts directory\\nLiteral\\nsome=thing\\n{\\nfoo: bar\\nbaz: 5\\n}\\n--from-ﬁle=foo.json\\n--from-ﬁle=conﬁg-opts/\\n--from-literal=some=thing\\nfoobar.conf\\nabc\\ndebug\\ntrue\\nrepeat\\n100\\n--from-ﬁle=bar=foobar.conf\\nFigure 7.5\\nCreating a ConfigMap from individual files, a directory, and a literal value\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [        Key                   Value\n",
       "   0  foo.json  {\\nfoo: bar\\nbaz: 5\\n}\n",
       "   1       bar                     abc\n",
       "   2     debug                    true\n",
       "   3    repeat                     100\n",
       "   4      some                   thing],\n",
       "  'entities': [{'entity': 'ConfigMaps',\n",
       "    'description': 'A way to store configuration data as a Kubernetes object',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'A way to store sensitive information such as passwords or keys as a Kubernetes object',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Environment variables',\n",
       "    'description': 'Variables that can be set within a container to customize its behavior',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Pod descriptor',\n",
       "    'description': 'A YAML file that defines the configuration for a pod, including environment variables',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ConfigMap',\n",
       "    'description': 'A Kubernetes object that stores configuration data as key-value pairs',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Key-value pair',\n",
       "    'description': 'A basic unit of storage in ConfigMaps and Secrets',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ValueFrom field',\n",
       "    'description': 'A field in the pod descriptor that allows specifying a value from a ConfigMap or Secret',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'A field in YAML files that specifies the API version of the object being described',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'A field in YAML files that specifies the type of object being described (e.g. Pod, ConfigMap, etc.)',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ConfigMap key',\n",
       "    'description': 'The label used to identify a value within a ConfigMap',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'JSON data',\n",
       "    'description': 'A format for storing key-value pairs as a string',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'foo.json',\n",
       "    'description': 'An example file that contains JSON data',\n",
       "    'category': 'file'},\n",
       "   {'entity': '--from-file',\n",
       "    'description': 'A command-line option used to specify a value from a file',\n",
       "    'category': 'command'},\n",
       "   {'entity': '--from-literal',\n",
       "    'description': 'A command-line option used to specify a value as a literal string',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Literal value',\n",
       "    'description': 'A value that is specified directly in the pod descriptor, rather than being retrieved from another source',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ValueFrom field\",\\n    \"description\": \"used to get values from a ConfigMap or Secret into a pod\\'s container\",\\n    \"destination_entity\": \"ConfigMap key\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMap key\",\\n    \"description\": \"key used to access a value in a ConfigMap\",\\n    \"destination_entity\": \"foo.json\"\\n  },\\n  {\\n    \"source_entity\": \"--from-literal\",\\n    \"description\": \"used to set a literal value from a ConfigMap or Secret into a pod\\'s container\",\\n    \"destination_entity\": \"Key-value pair\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMap key\",\\n    \"description\": \"key used to access a value in a ConfigMap\",\\n    \"destination_entity\": \"Value\"\\n  },\\n  {\\n    \"source_entity\": \"--from-file\",\\n    \"description\": \"used to get values from a file or directory into a pod\\'s container\",\\n    \"destination_entity\": \"foo.json\"\\n  },\\n  {\\n    \"source_entity\": \"--from-literal\",\\n    \"description\": \"used to set a literal value from a ConfigMap or Secret into a pod\\'s container\",\\n    \"destination_entity\": \"Literal value\"\\n  },\\n  {\\n    \"source_entity\": \"apiVersion\",\\n    \"description\": \"header used in a Pod descriptor to specify the API version\",\\n    \"destination_entity\": \"Pod descriptor\"\\n  },\\n  {\\n    \"source_entity\": \"kind\",\\n    \"description\": \"header used in a Pod descriptor to specify the type of resource\",\\n    \"destination_entity\": \"Pod descriptor\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMap key\",\\n    \"description\": \"key used to access a value in a ConfigMap\",\\n    \"destination_entity\": \"JSON data\"\\n  },\\n  {\\n    \"source_entity\": \"--from-file\",\\n    \"description\": \"used to get values from a file or directory into a pod\\'s container\",\\n    \"destination_entity\": \"conﬁg-opts directory\"\\n  },\\n  {\\n    \"source_entity\": \"Literal value\",\\n    \"description\": \"value set using the --from-literal field in a Pod descriptor\",\\n    \"destination_entity\": \"Environment variables\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMap key\",\\n    \"description\": \"key used to access a value in a ConfigMap\",\\n    \"destination_entity\": \"ConfigMaps\"\\n  },\\n  {\\n    \"source_entity\": \"--from-file\",\\n    \"description\": \"used to get values from a file or directory into a pod\\'s container\",\\n    \"destination_entity\": \"foobar.conf\"\\n  }\\n]'},\n",
       " {'page': 235,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"203\\nDecoupling configuration with a ConfigMap\\nmetadata:\\n  name: fortune-env-from-configmap\\nspec:\\n  containers:\\n  - image: luksa/fortune:env\\n    env:                             \\n    - name: INTERVAL                 \\n      valueFrom:                       \\n        configMapKeyRef:               \\n          name: fortune-config      \\n          key: sleep-interval    \\n...\\nYou defined an environment variable called INTERVAL and set its value to whatever is\\nstored in the fortune-config ConfigMap under the key sleep-interval. When the\\nprocess running in the html-generator container reads the INTERVAL environment\\nvariable, it will see the value 25 (shown in figure 7.6).\\nREFERENCING NON-EXISTING CONFIGMAPS IN A POD\\nYou might wonder what happens if the referenced ConfigMap doesn’t exist when you\\ncreate the pod. Kubernetes schedules the pod normally and tries to run its containers.\\nThe container referencing the non-existing ConfigMap will fail to start, but the other\\ncontainer will start normally. If you then create the missing ConfigMap, the failed con-\\ntainer is started without requiring you to recreate the pod.\\nNOTE\\nYou can also mark a reference to a ConfigMap as optional (by setting\\nconfigMapKeyRef.optional: true). In that case, the container starts even if\\nthe ConfigMap doesn’t exist.\\nThis example shows you how to decouple the configuration from the pod specifica-\\ntion. This allows you to keep all the configuration options closely together (even for\\nmultiple pods) instead of having them splattered around the pod definition (or dupli-\\ncated across multiple pod manifests). \\nYou’re setting the environment \\nvariable called INTERVAL.\\nInstead of setting a fixed value, you're \\ninitializing it from a ConfigMap key.\\nThe name of the ConfigMap \\nyou're referencing\\nYou're setting the variable to whatever is\\nstored under this key in the ConfigMap.\\nConﬁgMap: fortune-conﬁg\\nsleep-interval\\n25\\nPod\\nContainer: web-server\\nContainer: html-generator\\nEnvironment variables\\nINTERVAL=25\\nfortuneloop.sh\\nprocess\\nFigure 7.6\\nPassing a ConfigMap entry as \\nan environment variable to a container\\n \\n\",\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [sleep-interval, 25]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'ConfigMap',\n",
       "    'description': 'a Kubernetes resource that stores configuration data as key-value pairs',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'the basic execution unit in the Kubernetes model, which can run one or more containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'a lightweight and portable runtime environment for software applications, such as a web server or html-generator',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'env',\n",
       "    'description': 'environment variables passed to a container for configuration purposes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'configMapKeyRef',\n",
       "    'description': 'a reference to a ConfigMap key that is used to initialize an environment variable',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'key',\n",
       "    'description': 'the name of the entry in the ConfigMap that stores the value of the environment variable',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'valueFrom',\n",
       "    'description': 'the mechanism by which an environment variable is initialized from a ConfigMap key',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'interval',\n",
       "    'description': 'an environment variable used to control the behavior of the html-generator container',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'sleep-interval',\n",
       "    'description': 'a specific entry in the ConfigMap that stores the value of the INTERVAL environment variable',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'an open-source container orchestration system for automating the deployment, scaling, and management of containers across clusters',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'configmap',\n",
       "    'description': 'an optional reference to a ConfigMap that can be used to initialize an environment variable',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ConfigMap\", \"description\": \"defines environment variable INTERVAL with value stored under key sleep-interval\", \"destination_entity\": \"env\"},\\n  {\"source_entity\": \"valueFrom\", \"description\": \"initializes environment variable from ConfigMapKeyRef\", \"destination_entity\": \"env\"},\\n  {\"source_entity\": \"configMapKeyRef\", \"description\": \"references ConfigMap for initialization of environment variable INTERVAL\", \"destination_entity\": \"valueFrom\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"creates a pod with container referencing non-existent ConfigMap\", \"destination_entity\": \"ConfigMap\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"schedules the pod normally and tries to run its containers\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"container\", \"description\": \"starts even if the referenced ConfigMap doesn’t exist\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"web-server\", \"description\": \"sets environment variable INTERVAL to value stored under key sleep-interval in ConfigMap fortune-config\", \"destination_entity\": \"env\"},\\n  {\"source_entity\": \"html-generator\", \"description\": \"reads the INTERVAL environment variable and sees the value 25\", \"destination_entity\": \"process\"},\\n  {\"source_entity\": \"configMapKeyRef\", \"description\": \"defines the name of the ConfigMap referenced by container\", \"destination_entity\": \"ConfigMap\"},\\n  {\"source_entity\": \"sleep-interval\", \"description\": \"defines the key under which environment variable INTERVAL is initialized from ConfigMap\", \"destination_entity\": \"env\"},\\n  {\"source_entity\": \"key\", \"description\": \"specifies the key under which environment variable INTERVAL is stored in ConfigMap\", \"destination_entity\": \"configMapKeyRef\"}\\n]\\n\\nNote that some of these relations might be implicit or inferred from the context, but I\\'ve tried to stick to the provided entities and context as much as possible.'},\n",
       " {'page': 236,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '204\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\n7.4.4\\nPassing all entries of a ConfigMap as environment variables \\nat once\\nWhen your ConfigMap contains more than just a few entries, it becomes tedious and\\nerror-prone to create environment variables from each entry individually. Luckily,\\nKubernetes version 1.6 provides a way to expose all entries of a ConfigMap as environ-\\nment variables. \\n Imagine having a ConfigMap with three keys called FOO, BAR, and FOO-BAR. You can\\nexpose them all as environment variables by using the envFrom attribute, instead of\\nenv the way you did in previous examples. The following listing shows an example.\\nspec:\\n  containers:\\n  - image: some-image\\n    envFrom:                \\n    - prefix: CONFIG_             \\n      configMapRef:              \\n        name: my-config-map      \\n...\\nAs you can see, you can also specify a prefix for the environment variables (CONFIG_ in\\nthis case). This results in the following two environment variables being present inside\\nthe container: CONFIG_FOO and CONFIG_BAR. \\nNOTE\\nThe prefix is optional, so if you omit it the environment variables will\\nhave the same name as the keys. \\nDid you notice I said two variables, but earlier, I said the ConfigMap has three entries\\n(FOO, BAR, and FOO-BAR)? Why is there no environment variable for the FOO-BAR\\nConfigMap entry?\\n The reason is that CONFIG_FOO-BAR isn’t a valid environment variable name\\nbecause it contains a dash. Kubernetes doesn’t convert the keys in any way (it doesn’t\\nconvert dashes to underscores, for example). If a ConfigMap key isn’t in the proper\\nformat, it skips the entry (but it does record an event informing you it skipped it).\\n7.4.5\\nPassing a ConfigMap entry as a command-line argument\\nNow, let’s also look at how to pass values from a ConfigMap as arguments to the main\\nprocess running in the container. You can’t reference ConfigMap entries directly in\\nthe pod.spec.containers.args field, but you can first initialize an environment vari-\\nable from the ConfigMap entry and then refer to the variable inside the arguments as\\nshown in figure 7.7.\\n Listing 7.11 shows an example of how to do this in the YAML.\\n \\nListing 7.10\\nPod with env vars from all entries of a ConfigMap\\nUsing envFrom instead of env\\nAll environment variables will \\nbe prefixed with CONFIG_.\\nReferencing the ConfigMap \\ncalled my-config-map\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMaps',\n",
       "    'description': 'a way to expose all entries of a ConfigMap as environment variables in Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'envFrom',\n",
       "    'description': 'an attribute used to expose all entries of a ConfigMap as environment variables',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMap',\n",
       "    'description': 'a Kubernetes object that stores configuration data',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'environment variables',\n",
       "    'description': 'variables set for the execution context in which a program runs',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'an open-source container orchestration system',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'a lightweight and standalone executable package of software',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'a pre-built binary that can be run directly on a Docker host without the need to compile it from source',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'the specification of a Kubernetes object',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'configMapRef',\n",
       "    'description': 'a reference to a ConfigMap in a Kubernetes object',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'prefix',\n",
       "    'description': 'an optional prefix for environment variables created from a ConfigMap',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'args',\n",
       "    'description': 'command-line arguments passed to the main process running in a container',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"containers\", \"description\": \"expose environment variables\", \"destination_entity\": \"environment variables\"},\\n  {\"source_entity\": \"configMapRef\", \"description\": \"reference ConfigMap entries\", \"destination_entity\": \"ConfigMap entries\"},\\n  {\"source_entity\": \"spec\", \"description\": \"configure containers\", \"destination_entity\": \"containers\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"expose environment variables with prefix\", \"destination_entity\": \"environment variables\"},\\n  {\"source_entity\": \"image\", \"description\": \"run in container\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"args\", \"description\": \"pass values from ConfigMap as arguments\", \"destination_entity\": \"ConfigMap entries\"},\\n  {\"source_entity\": \"envFrom\", \"description\": \"expose all entries of a ConfigMap\", \"destination_entity\": \"ConfigMap entries\"},\\n  {\"source_entity\": \"prefix\", \"description\": \"specify prefix for environment variables\", \"destination_entity\": \"environment variables\"},\\n  {\"source_entity\": \"ConfigMaps\", \"description\": \"configure applications\", \"destination_entity\": \"applications\"},\\n  {\"source_entity\": \"ConfigMap\", \"description\": \"store sensitive information\", \"destination_entity\": \"sensitive information\"}\\n]\\n```\\n\\nThese relations capture the connections between different entities mentioned in the document, such as how containers expose environment variables, how ConfigMaps reference entries, and how Kubernetes handles prefixes for environment variables.'},\n",
       " {'page': 237,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '205\\nDecoupling configuration with a ConfigMap\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: fortune-args-from-configmap\\nspec:\\n  containers:\\n  - image: luksa/fortune:args         \\n    env:                               \\n    - name: INTERVAL                   \\n      valueFrom:                       \\n        configMapKeyRef:               \\n          name: fortune-config         \\n          key: sleep-interval          \\n    args: [\"$(INTERVAL)\"]      \\n...\\nYou defined the environment variable exactly as you did before, but then you used the\\n$(ENV_VARIABLE_NAME) syntax to have Kubernetes inject the value of the variable into\\nthe argument. \\n7.4.6\\nUsing a configMap volume to expose ConfigMap entries as files\\nPassing configuration options as environment variables or command-line arguments\\nis usually used for short variable values. A ConfigMap, as you’ve seen, can also con-\\ntain whole config files. When you want to expose those to the container, you can use\\none of the special volume types I mentioned in the previous chapter, namely a\\nconfigMap volume.\\n A configMap volume will expose each entry of the ConfigMap as a file. The pro-\\ncess running in the container can obtain the entry’s value by reading the contents of\\nthe file.\\nListing 7.11\\nUsing ConfigMap entries as arguments: fortune-pod-args-configmap.yaml\\nConﬁgMap: fortune-conﬁg\\nsleep-interval\\n25\\nPod\\nContainer: web-server\\nContainer: html-generator\\nEnvironment variables\\nINTERVAL=25\\nfortuneloop.sh $(INTERVAL)\\nFigure 7.7\\nPassing a ConfigMap entry as a command-line argument\\nUsing the image that takes the \\ninterval from the first argument, \\nnot from an environment variable\\nDefining the \\nenvironment variable \\nexactly as before\\nReferencing the environment \\nvariable in the argument\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [sleep-interval, 25]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'ConfigMap',\n",
       "    'description': 'A ConfigMap is a Kubernetes resource used to store configuration data.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A Pod is a basic execution unit in Kubernetes.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'A Container is a lightweight and standalone executable software package.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Image',\n",
       "    'description': 'An Image is a read-only template for creating a container.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Environment Variable',\n",
       "    'description': 'A variable that can be set and used within an application or process.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Argument',\n",
       "    'description': 'A command-line argument passed to a program or process.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ConfigMapKeyRef',\n",
       "    'description': 'A Kubernetes API object that references a ConfigMap entry.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Volume',\n",
       "    'description': 'A block of storage space shared among containers in a pod.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'File',\n",
       "    'description': 'A permanent, named collection of data stored on a computer system.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Process',\n",
       "    'description': 'An instance of a running program or application.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMap entry',\n",
       "    'description': 'A single item within a ConfigMap that stores configuration data.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"ConfigMap\", \"description\": \"exposes entries as files\", \"destination_entity\": \"File\"},{\"source_entity\": \"Pod\", \"description\": \"uses ConfigMap entries as environment variables\", \"destination_entity\": \"Environment Variable\"},{\"source_entity\": \"Container\", \"description\": \"can obtain entry values by reading file contents\", \"destination_entity\": \"Process\"},{\"source_entity\": \"ConfigMap\", \"description\": \"contains config files and entries\", \"destination_entity\": \"Image\"},{\"source_entity\": \"Kubernetes\", \"description\": \"injects variable values into arguments\", \"destination_entity\": \"Argument\"},{\"source_entity\": \"ConfigMapKeyRef\", \"description\": \"references ConfigMap entry in environment variable\", \"destination_entity\": \"Environment Variable\"},{\"source_entity\": \"Pod\", \"description\": \"uses special volume types to expose ConfigMap entries\", \"destination_entity\": \"Volume\"},{\"source_entity\": \"Container\", \"description\": \"runs process that reads file contents for config values\", \"destination_entity\": \"Process\"},{\"source_entity\": \"Image\", \"description\": \"takes interval from first argument instead of environment variable\", \"destination_entity\": \"Argument\"},{\"source_entity\": \"ConfigMap entry\", \"description\": \"used as command-line argument in fortune-pod-args-configmap.yaml\", \"destination_entity\": \"Command-Line Argument\"},{\"source_entity\": \"Pod\", \"description\": \"uses ConfigMap entries to set environment variables\", \"destination_entity\": \"Environment Variable\"},{\"source_entity\": \"Kubernetes\", \"description\": \"exposes ConfigMap entries as files through volume types\", \"destination_entity\": \"File\"}]'},\n",
       " {'page': 238,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '206\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\n Although this method is mostly meant for passing large config files to the con-\\ntainer, nothing prevents you from passing short single values this way. \\nCREATING THE CONFIGMAP\\nInstead of modifying your fortuneloop.sh script once again, you’ll now try a different\\nexample. You’ll use a config file to configure the Nginx web server running inside the\\nfortune pod’s web-server container. Let’s say you want your Nginx server to compress\\nresponses it sends to the client. To enable compression, the config file for Nginx\\nneeds to look like the following listing.\\nserver {\\n  listen              80;\\n  server_name         www.kubia-example.com;\\n  gzip on;                                       \\n  gzip_types text/plain application/xml;         \\n  location / {\\n    root   /usr/share/nginx/html;\\n    index  index.html index.htm;\\n  }\\n}\\nNow delete your existing fortune-config ConfigMap with kubectl delete config-\\nmap fortune-config, so that you can replace it with a new one, which will include the\\nNginx config file. You’ll create the ConfigMap from files stored on your local disk. \\n Create a new directory called configmap-files and store the Nginx config from the\\nprevious listing into configmap-files/my-nginx-config.conf. To make the ConfigMap\\nalso contain the sleep-interval entry, add a plain text file called sleep-interval to the\\nsame directory and store the number 25 in it (see figure 7.8).\\nNow create a ConfigMap from all the files in the directory like this:\\n$ kubectl create configmap fortune-config --from-file=configmap-files\\nconfigmap \"fortune-config\" created\\nListing 7.12\\nAn Nginx config with enabled gzip compression: my-nginx-config.conf\\nThis enables gzip compression \\nfor plain text and XML files.\\nconﬁgmap-ﬁles/\\nmy-nginx-conﬁg.conf\\nserver {\\nlisten 80;\\nserver_name www.kubia...\\n...\\n}\\nsleep-interval\\n25\\nFigure 7.8\\nThe contents of the \\nconfigmap-files directory and its files\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMap',\n",
       "    'description': 'A resource used to store configuration data for an application.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool for interacting with a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMaps',\n",
       "    'description': 'A method of passing large config files to a container.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'fortuneloop.sh',\n",
       "    'description': 'A shell script used to configure the Nginx web server.',\n",
       "    'category': 'script'},\n",
       "   {'entity': 'Nginx',\n",
       "    'description': 'A web server software that can be configured using a config file.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'gzip',\n",
       "    'description': 'A compression algorithm used by Nginx to compress responses.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'text/plain',\n",
       "    'description': 'A MIME type indicating plain text content.',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'application/xml',\n",
       "    'description': 'A MIME type indicating XML content.',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'server_name',\n",
       "    'description': 'An Nginx configuration directive specifying the server name.',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'listen',\n",
       "    'description': 'An Nginx configuration directive specifying the port to listen on.',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'gzip_types',\n",
       "    'description': 'An Nginx configuration directive specifying which MIME types are compressible.',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'location',\n",
       "    'description': 'An Nginx configuration directive specifying a location within the server block.',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'index',\n",
       "    'description': 'An Nginx configuration directive specifying the default index file for a location.',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'root',\n",
       "    'description': 'An Nginx configuration directive specifying the root directory for a location.',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'html',\n",
       "    'description': 'A file extension indicating HTML content.',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'configmap-files',\n",
       "    'description': 'A directory containing config files used to create a ConfigMap.',\n",
       "    'category': 'file system'},\n",
       "   {'entity': 'my-nginx-config.conf',\n",
       "    'description': 'An Nginx configuration file enabling gzip compression.',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'sleep-interval',\n",
       "    'description': 'A plain text file containing the sleep interval value.',\n",
       "    'category': 'file system'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"my-nginx-config.conf\", \"description\": \"contains Nginx config to enable gzip compression for plain text and XML files\", \"destination_entity\": \"gzip\"},\\n  {\"source_entity\": \"Nginx\", \"description\": \"uses ConfigMap to configure web server settings\", \"destination_entity\": \"ConfigMaps\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"creates a new ConfigMap from local disk files\", \"destination_entity\": \"ConfigMap\"},\\n  {\"source_entity\": \"listen\", \"description\": \"is a directive in Nginx config file to specify port number\", \"destination_entity\": \"port number\"},\\n  {\"source_entity\": \"ConfigMaps\", \"description\": \"provides a way to pass large config files to containers\", \"destination_entity\": \"containers\"},\\n  {\"source_entity\": \"ConfigMap\", \"description\": \"is used to configure Nginx web server settings\", \"destination_entity\": \"Nginx\"},\\n  {\"source_entity\": \"root\", \"description\": \"is a directive in Nginx config file to specify document root directory\", \"destination_entity\": \"document root directory\"},\\n  {\"source_entity\": \"application/xml\", \"description\": \"is a MIME type that gzip compression is enabled for\", \"destination_entity\": \"gzip_types\"},\\n  {\"source_entity\": \"text/plain\", \"description\": \"is a MIME type that gzip compression is enabled for\", \"destination_entity\": \"gzip_types\"},\\n  {\"source_entity\": \"index\", \"description\": \"is a directive in Nginx config file to specify index files\", \"destination_entity\": \"index files\"},\\n  {\"source_entity\": \"sleep-interval\", \"description\": \"is used to specify the interval between HTTP requests\", \"destination_entity\": \"HTTP requests\"},\\n  {\"source_entity\": \"server_name\", \"description\": \"is a directive in Nginx config file to specify server name\", \"destination_entity\": \"server name\"},\\n  {\"source_entity\": \"gzip_types\", \"description\": \"specifies the MIME types that gzip compression is enabled for\", \"destination_entity\": \"MIME types\"},\\n  {\"source_entity\": \"location\", \"description\": \"is a directive in Nginx config file to specify location of files\", \"destination_entity\": \"files\"},\\n  {\"source_entity\": \"gzip\", \"description\": \"is used to enable gzip compression\", \"destination_entity\": \"compression\"},\\n  {\"source_entity\": \"html\", \"description\": \"is a MIME type that gzip compression is enabled for\", \"destination_entity\": \"MIME types\"},\\n  {\"source_entity\": \"configmap-files\", \"description\": \"is the directory where Nginx config file and sleep-interval file are stored\", \"destination_entity\": \"files\"},\\n  {\"source_entity\": \"fortuneloop.sh\", \"description\": \"is a script that uses ConfigMap to configure web server settings\", \"destination_entity\": \"ConfigMaps\"}\\n]\\n```'},\n",
       " {'page': 239,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"207\\nDecoupling configuration with a ConfigMap\\nThe following listing shows what the YAML of this ConfigMap looks like.\\n$ kubectl get configmap fortune-config -o yaml\\napiVersion: v1\\ndata:\\n  my-nginx-config.conf: |                            \\n    server {                                         \\n      listen              80;                        \\n      server_name         www.kubia-example.com;     \\n      gzip on;                                       \\n      gzip_types text/plain application/xml;         \\n      location / {                                   \\n        root   /usr/share/nginx/html;                \\n        index  index.html index.htm;                 \\n      }                                              \\n    }                                                \\n  sleep-interval: |         \\n    25                      \\nkind: ConfigMap\\n...\\nNOTE\\nThe pipeline character after the colon in the first line of both entries\\nsignals that a literal multi-line value follows.\\nThe ConfigMap contains two entries, with keys corresponding to the actual names\\nof the files they were created from. You’ll now use the ConfigMap in both of your\\npod’s containers.\\nUSING THE CONFIGMAP'S ENTRIES IN A VOLUME\\nCreating a volume populated with the contents of a ConfigMap is as easy as creating\\na volume that references the ConfigMap by name and mounting the volume in a\\ncontainer. You already learned how to create volumes and mount them, so the only\\nthing left to learn is how to initialize the volume with files created from a Config-\\nMap’s entries.\\n Nginx reads its config file from /etc/nginx/nginx.conf. The Nginx image\\nalready contains this file with default configuration options, which you don’t want\\nto override, so you don’t want to replace this file as a whole. Luckily, the default\\nconfig file automatically includes all .conf files in the /etc/nginx/conf.d/ subdirec-\\ntory as well, so you should add your config file in there. Figure 7.9 shows what you\\nwant to achieve.\\n The pod descriptor is shown in listing 7.14 (the irrelevant parts are omitted, but\\nyou’ll find the complete file in the code archive).\\n \\n \\nListing 7.13\\nYAML definition of a config map created from a file\\nThe entry holding the \\nNginx config file’s \\ncontents\\nThe sleep-interval entry\\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMap',\n",
       "    'description': 'A Kubernetes object that stores unstructured data as key-value pairs',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for interacting with a Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'The version of the API used to create or update an object in Kubernetes',\n",
       "    'category': 'parameter'},\n",
       "   {'entity': 'data',\n",
       "    'description': 'A section in a ConfigMap that stores key-value pairs of unstructured data',\n",
       "    'category': 'section'},\n",
       "   {'entity': 'my-nginx-config.conf',\n",
       "    'description': 'A file containing Nginx configuration options',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'server',\n",
       "    'description': 'A directive in an Nginx configuration file that starts a new server block',\n",
       "    'category': 'directive'},\n",
       "   {'entity': 'listen',\n",
       "    'description': 'A directive in an Nginx configuration file that specifies the port on which the server listens',\n",
       "    'category': 'directive'},\n",
       "   {'entity': 'sleep-interval',\n",
       "    'description': 'An entry in a ConfigMap that stores the interval at which a process sleeps',\n",
       "    'category': 'entry'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'The type of Kubernetes object being created or updated',\n",
       "    'category': 'parameter'},\n",
       "   {'entity': 'volume',\n",
       "    'description': 'A feature in Kubernetes that allows for shared storage between containers in a pod',\n",
       "    'category': 'feature'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A lightweight and standalone executable package that includes an application, libraries, and dependencies',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'The basic execution unit in a Kubernetes cluster, consisting of one or more containers',\n",
       "    'category': 'unit'},\n",
       "   {'entity': 'entry',\n",
       "    'description': 'A single key-value pair stored in a ConfigMap or other data structure',\n",
       "    'category': 'item'},\n",
       "   {'entity': '/etc/nginx/nginx.conf',\n",
       "    'description': 'The default configuration file for Nginx, which includes all .conf files in the /etc/nginx/conf.d/ subdirectory',\n",
       "    'category': 'file'},\n",
       "   {'entity': '/usr/share/nginx/html',\n",
       "    'description': 'A directory containing HTML files served by an Nginx container',\n",
       "    'category': 'directory'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"get configmap fortune-config\", \"destination_entity\": \"ConfigMap\"},\\n  {\"source_entity\": \"ConfigMap\", \"description\": \"contains two entries with keys and values\", \"destination_entity\": \"data\"},\\n  {\"source_entity\": \"data\", \"description\": \"holds the contents of a ConfigMap\\'s entries\", \"destination_entity\": \"entry\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"get configmap fortune-config -o yaml\", \"destination_entity\": \"yaml\"},\\n  {\"source_entity\": \"ConfigMap\", \"description\": \"created from a file\", \"destination_entity\": \"file\"},\\n  {\"source_entity\": \"Nginx\", \"description\": \"reads its config file from /etc/nginx/nginx.conf\", \"destination_entity\": \"/etc/nginx/nginx.conf\"},\\n  {\"source_entity\": \"Nginx\", \"description\": \"contains default configuration options\", \"destination_entity\": \"default config file\"},\\n  {\"source_entity\": \"ConfigMap\", \"description\": \"contains entries with keys and values\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"get configmap fortune-config -o yaml\", \"destination_entity\": \"/usr/share/nginx/html\"},\\n  {\"source_entity\": \"Nginx\", \"description\": \"already contains this file with default configuration options\", \"destination_entity\": \"config file in /etc/nginx/conf.d/\"},\\n  {\"source_entity\": \"ConfigMap\", \"description\": \"contains entries with keys and values\", \"destination_entity\": \"/etc/nginx/nginx.conf\"},\\n  {\"source_entity\": \"apiVersion\", \"description\": \"defines the API version of a resource\", \"destination_entity\": \"resource\"},\\n  {\"source_entity\": \"server\", \"description\": \"defines a server block in an Nginx config file\", \"destination_entity\": \"Nginx config file\"},\\n  {\"source_entity\": \"kind\", \"description\": \"defines the type of a Kubernetes resource\", \"destination_entity\": \"Kubernetes resource\"},\\n  {\"source_entity\": \"volume\", \"description\": \"is initialized with files created from a ConfigMap\\'s entries\", \"destination_entity\": \"ConfigMap\"}\\n]'},\n",
       " {'page': 240,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '208\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: fortune-configmap-volume\\nspec:\\n  containers:\\n  - image: nginx:alpine\\n    name: web-server\\n    volumeMounts:\\n    ...\\n    - name: config\\n      mountPath: /etc/nginx/conf.d      \\n      readOnly: true\\n    ...\\n  volumes:\\n  ...\\n  - name: config              \\n    configMap:                 \\n      name: fortune-config     \\n  ...\\nThis pod definition includes a volume, which references your fortune-config\\nConfigMap. You mount the volume into the /etc/nginx/conf.d directory to make\\nNginx use it. \\nVERIFYING NGINX IS USING THE MOUNTED CONFIG FILE\\nThe web server should now be configured to compress the responses it sends. You can\\nverify this by enabling port-forwarding from localhost:8080 to the pod’s port 80 and\\nchecking the server’s response with curl, as shown in the following listing.\\n \\nListing 7.14\\nA pod with ConfigMap entries mounted as files: fortune-pod-configmap-\\nvolume.yaml\\nPod\\nContainer: html-generator\\nContainer: web-server\\nFilesystem\\n/\\netc/\\nnginx/\\nconf.d/\\nConﬁgMap: fortune-conﬁg\\nmy-nginx-conﬁg.conf\\nserver {\\n…\\n}\\nVolume:\\nconﬁg\\nFigure 7.9\\nPassing ConfigMap entries to a pod as files in a volume\\nYou’re mounting the \\nconfigMap volume at \\nthis location.\\nThe volume refers to your \\nfortune-config ConfigMap.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [my-nginx-config.conf, server {\n",
       "   …\n",
       "   }]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'ConfigMaps',\n",
       "    'description': 'A way to decouple configuration from code',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'A way to securely store sensitive information',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'The version of the API being used',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'The type of resource being defined',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Information about the Pod, such as its name and labels',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'The specification for the Pod, including its containers and volumes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'The individual processes that make up a Pod',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'The container image being used',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'volumeMounts',\n",
       "    'description': 'The mount points for volumes within the Pod',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'volumes',\n",
       "    'description': 'Persistent storage for the Pod',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'configMap',\n",
       "    'description': 'A way to configure a Pod using external configuration data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMap entries',\n",
       "    'description': 'Individual pieces of configuration data within a ConfigMap',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Nginx',\n",
       "    'description': 'A web server software',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'volume',\n",
       "    'description': 'Persistent storage for the Pod',\n",
       "    'category': 'database'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ConfigMaps\", \"description\": \"configures applications\", \"destination_entity\": \"applications\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"includes a volume\", \"destination_entity\": \"volume\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"references ConfigMap\", \"destination_entity\": \"ConfigMap\"},\\n  {\"source_entity\": \"Nginx\", \"description\": \"uses mounted ConfigMap entries\", \"destination_entity\": \"ConfigMap entries\"},\\n  {\"source_entity\": \"volumes\", \"description\": \"mounts ConfigMap volume at /etc/nginx/conf.d\", \"destination_entity\": \"/etc/nginx/conf.d\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"verifies Nginx is using mounted Config file\", \"destination_entity\": \"Nginx\"},\\n  {\"source_entity\": \"curl\", \"description\": \"checks server\\'s response\", \"destination_entity\": \"server\\'s response\"},\\n  {\"source_entity\": \"volume\", \"description\": \"refers to fortune-config ConfigMap\", \"destination_entity\": \"fortune-config ConfigMap\"},\\n  {\"source_entity\": \"ConfigMap entries\", \"description\": \"mounted as files in a volume\", \"destination_entity\": \"files in a volume\"}\\n]\\n```'},\n",
       " {'page': 241,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '209\\nDecoupling configuration with a ConfigMap\\n$ kubectl port-forward fortune-configmap-volume 8080:80 &\\nForwarding from 127.0.0.1:8080 -> 80\\nForwarding from [::1]:8080 -> 80\\n$ curl -H \"Accept-Encoding: gzip\" -I localhost:8080\\nHTTP/1.1 200 OK\\nServer: nginx/1.11.1\\nDate: Thu, 18 Aug 2016 11:52:57 GMT\\nContent-Type: text/html\\nLast-Modified: Thu, 18 Aug 2016 11:52:55 GMT\\nConnection: keep-alive\\nETag: W/\"57b5a197-37\"\\nContent-Encoding: gzip           \\nEXAMINING THE MOUNTED CONFIGMAP VOLUME’S CONTENTS\\nThe response shows you achieved what you wanted, but let’s look at what’s in the\\n/etc/nginx/conf.d directory now:\\n$ kubectl exec fortune-configmap-volume -c web-server ls /etc/nginx/conf.d\\nmy-nginx-config.conf\\nsleep-interval\\nBoth entries from the ConfigMap have been added as files to the directory. The\\nsleep-interval entry is also included, although it has no business being there,\\nbecause it’s only meant to be used by the fortuneloop container. You could create\\ntwo different ConfigMaps and use one to configure the fortuneloop container and\\nthe other one to configure the web-server container. But somehow it feels wrong to\\nuse multiple ConfigMaps to configure containers of the same pod. After all, having\\ncontainers in the same pod implies that the containers are closely related and should\\nprobably also be configured as a unit. \\nEXPOSING CERTAIN CONFIGMAP ENTRIES IN THE VOLUME\\nLuckily, you can populate a configMap volume with only part of the ConfigMap’s\\nentries—in your case, only the my-nginx-config.conf entry. This won’t affect the\\nfortuneloop container, because you’re passing the sleep-interval entry to it through\\nan environment variable and not through the volume. \\n To define which entries should be exposed as files in a configMap volume, use the\\nvolume’s items attribute as shown in the following listing.\\n  volumes:\\n  - name: config              \\n    configMap:                                  \\n      name: fortune-config                      \\n      items:                       \\n      - key: my-nginx-config.conf        \\n        path: gzip.conf                  \\nListing 7.15\\nSeeing if nginx responses have compression enabled\\nListing 7.16\\nA pod with a specific ConfigMap entry mounted into a file directory: \\nfortune-pod-configmap-volume-with-items.yaml\\nThis shows the response \\nis compressed.\\nSelecting which entries to include \\nin the volume by listing them\\nYou want the entry \\nunder this key included.\\nThe entry’s value should \\nbe stored in this file.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMap',\n",
       "    'description': 'a configuration data object that can be used to populate environment variables, volumes, or other resources in a Kubernetes pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the command-line tool for interacting with Kubernetes clusters',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'port-forward',\n",
       "    'description': 'a command that forwards traffic from the local machine to a container running inside a pod',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ConfigMap volume',\n",
       "    'description': 'a persistent storage object that can be used to store configuration data for a Kubernetes pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'fortune-configmap-volume',\n",
       "    'description': 'the name of the ConfigMap volume',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'nginx',\n",
       "    'description': 'a popular web server software',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'fortune-pod-configmap-volume-with-items.yaml',\n",
       "    'description': 'a YAML file that defines a pod with a specific ConfigMap entry mounted into a file directory',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'items attribute',\n",
       "    'description': 'an attribute of the volume object used to specify which entries from a ConfigMap should be exposed as files in a configMap volume',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'volume',\n",
       "    'description': 'a persistent storage object that can be used to store data for a Kubernetes pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'configMap',\n",
       "    'description': 'the name of the ConfigMap being referenced',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'fortune-config',\n",
       "    'description': 'the name of the ConfigMap',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'key',\n",
       "    'description': 'a key in a ConfigMap used to specify which entry should be exposed as a file in a configMap volume',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'path',\n",
       "    'description': 'the path where an entry from a ConfigMap will be stored in a file directory',\n",
       "    'category': 'command'}],\n",
       "  'relationships': '[{\"source_entity\": \"kubectl\",\"description\": \"used to port-forward fortune-configmap-volume\",\"destination_entity\": \"port-forward\"},{\"source_entity\": \"kubectl\",\"description\": \"forwarded from 127.0.0.1:8080 to 80\",\"destination_entity\": \"localhost:8080\"},{\"source_entity\": \"kubectl\",\"description\": \"forwarded from ::1:8080 to 80\",\"destination_entity\": \"localhost:8080\"},{\"source_entity\": \"curl\",\"description\": \"performed HTTP request to localhost:8080 with Accept-Encoding header set to gzip\",\"destination_entity\": \"localhost:8080\"},{\"source_entity\": \"ConfigMap\",\"description\": \"used to decouple configuration of fortune-configmap-volume\",\"destination_entity\": \"fortune-configmap-volume\"},{\"source_entity\": \"kubectl\",\"description\": \"executed command ls /etc/nginx/conf.d in fortune-configmap-volume container\",\"destination_entity\": \"/etc/nginx/conf.d\"},{\"source_entity\": \"fortune-configmap-volume\",\"description\": \"had ConfigMap entries added as files to /etc/nginx/conf.d directory\",\"destination_entity\": \"/etc/nginx/conf.d\"},{\"source_entity\": \"kubectl\",\"description\": \"used to create ConfigMap volume with items attribute specified\",\"destination_entity\": \"configMap volume\"},{\"source_entity\": \"fortune-pod-configmap-volume-with-items.yaml\",\"description\": \"specified which entries of ConfigMap should be exposed as files in configMap volume\",\"destination_entity\": \"items attribute\"},{\"source_entity\": \"kubectl\",\"description\": \"used to create pod fortune-pod-configmap-volume-with-items.yaml with configMap volume specified\",\"destination_entity\": \"fortune-pod-configmap-volume-with-items.yaml\"}]\\n\\nNote that I\\'ve extracted all the relations between the entities in the provided document page and listed them as JSON objects in a list. Each relation has three keys: source_entity, description, and destination_entity.'},\n",
       " {'page': 242,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '210\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\nWhen specifying individual entries, you need to set the filename for each individual\\nentry, along with the entry’s key. If you run the pod from the previous listing, the\\n/etc/nginx/conf.d directory is kept nice and clean, because it only contains the\\ngzip.conf file and nothing else. \\nUNDERSTANDING THAT MOUNTING A DIRECTORY HIDES EXISTING FILES IN THAT DIRECTORY\\nThere’s one important thing to discuss at this point. In both this and in your previous\\nexample, you mounted the volume as a directory, which means you’ve hidden any files\\nthat are stored in the /etc/nginx/conf.d directory in the container image itself. \\n This is generally what happens in Linux when you mount a filesystem into a non-\\nempty directory. The directory then only contains the files from the mounted filesys-\\ntem, whereas the original files in that directory are inaccessible for as long as the\\nfilesystem is mounted. \\n In your case, this has no terrible side effects, but imagine mounting a volume to\\nthe /etc directory, which usually contains many important files. This would most likely\\nbreak the whole container, because all of the original files that should be in the /etc\\ndirectory would no longer be there. If you need to add a file to a directory like /etc,\\nyou can’t use this method at all.\\nMOUNTING INDIVIDUAL CONFIGMAP ENTRIES AS FILES WITHOUT HIDING OTHER FILES IN THE DIRECTORY\\nNaturally, you’re now wondering how to add individual files from a ConfigMap into\\nan existing directory without hiding existing files stored in it. An additional subPath\\nproperty on the volumeMount allows you to mount either a single file or a single direc-\\ntory from the volume instead of mounting the whole volume. Perhaps this is easier to\\nexplain visually (see figure 7.10).\\n Say you have a configMap volume containing a myconfig.conf file, which you want\\nto add to the /etc directory as someconfig.conf. You can use the subPath property to\\nmount it there without affecting any other files in that directory. The relevant part of\\nthe pod definition is shown in the following listing.\\nPod\\nContainer\\nFilesystem\\n/\\netc/\\nsomeconﬁg.conf\\nexistingﬁle1\\nexistingﬁle2\\nConﬁgMap: app-conﬁg\\nmyconﬁg.conf\\nContents\\nof the ﬁle\\nanother-ﬁle\\nContents\\nof the ﬁle\\nconﬁgMap\\nvolume\\nmyconﬁg.conf\\nanother-ﬁle\\nexistingﬁle1\\nand existingﬁle2\\naren’t hidden.\\nOnly myconﬁg.conf is mounted\\ninto the container (yet under a\\ndifferent ﬁlename).\\nanother-ﬁle isn’t\\nmounted into the\\ncontainer.\\nFigure 7.10\\nMounting a single file from a volume\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [myconfig.conf, Contents\n",
       "   of the file]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'ConfigMaps',\n",
       "    'description': 'a feature in Kubernetes that allows you to store and retrieve configuration data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'a feature in Kubernetes that allows you to store sensitive information such as passwords or OAuth tokens',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'the basic execution unit in a containerized application',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'a lightweight and portable executable package of an application and its dependencies',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Filesystem',\n",
       "    'description': 'a system that provides persistent storage for files on a computer',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': '/etc/nginx/conf.d directory',\n",
       "    'description': 'a directory in the filesystem where NGINX configuration files are stored',\n",
       "    'category': 'directory'},\n",
       "   {'entity': 'gzip.conf file',\n",
       "    'description': 'a configuration file that controls gzip compression for NGINX',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'subPath property',\n",
       "    'description': 'an option in Kubernetes that allows you to mount a single file or directory from a volume',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMap volume',\n",
       "    'description': 'a type of persistent storage in Kubernetes that stores configuration data',\n",
       "    'category': 'volume'},\n",
       "   {'entity': 'myconfig.conf file',\n",
       "    'description': 'a configuration file stored in the ConfigMap volume',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'existing files',\n",
       "    'description': 'files already present in a directory or filesystem',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'mounting point',\n",
       "    'description': 'the location where a volume is mounted into a container',\n",
       "    'category': 'directory'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ConfigMap volume\",\\n    \"description\": \"contains individual files to be added to a directory without hiding existing files\",\\n    \"destination_entity\": \"existing files\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"specifies individual entries with filename and key\",\\n    \"destination_entity\": \"entry\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMap volume\",\\n    \"description\": \"contains a myconfig.conf file to be added as someconfig.conf to /etc directory\",\\n    \"destination_entity\": \"/etc directory\"\\n  },\\n  {\\n    \"source_entity\": \"subPath property\",\\n    \"description\": \"allows mounting single files or directories from a volume without hiding existing files\",\\n    \"destination_entity\": \"existing files\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMap\",\\n    \"description\": \"used to add individual files to an existing directory\",\\n    \"destination_entity\": \"/etc directory\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMap volume\",\\n    \"description\": \"mounted as a directory, hiding existing files in the container image\",\\n    \"destination_entity\": \"container image\"\\n  },\\n  {\\n    \"source_entity\": \"mounting point\",\\n    \"description\": \"directory where individual files from ConfigMap are mounted\",\\n    \"destination_entity\": \"/etc/nginx/conf.d directory\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMaps\",\\n    \"description\": \"mechanism for adding individual files to a directory without hiding existing files\",\\n    \"destination_entity\": \"existing files\"\\n  },\\n  {\\n    \"source_entity\": \"Container\",\\n    \"description\": \"uses ConfigMap volume with subPath property to add single file from volume\",\\n    \"destination_entity\": \"/etc directory\"\\n  },\\n  {\\n    \"source_entity\": \"gzip.conf file\",\\n    \"description\": \"example of a single file from ConfigMap being mounted into /etc/nginx/conf.d directory\",\\n    \"destination_entity\": \"/etc/nginx/conf.d directory\"\\n  }\\n]'},\n",
       " {'page': 243,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '211\\nDecoupling configuration with a ConfigMap\\nspec:\\n  containers:\\n  - image: some/image\\n    volumeMounts:\\n    - name: myvolume\\n      mountPath: /etc/someconfig.conf     \\n      subPath: myconfig.conf            \\nThe subPath property can be used when mounting any kind of volume. Instead of\\nmounting the whole volume, you can mount part of it. But this method of mounting\\nindividual files has a relatively big deficiency related to updating files. You’ll learn\\nmore about this in the following section, but first, let’s finish talking about the initial\\nstate of a configMap volume by saying a few words about file permissions.\\nSETTING THE FILE PERMISSIONS FOR FILES IN A CONFIGMAP VOLUME\\nBy default, the permissions on all files in a configMap volume are set to 644 (-rw-r—r--).\\nYou can change this by setting the defaultMode property in the volume spec, as shown\\nin the following listing.\\n  volumes:\\n  - name: config\\n    configMap:\\n      name: fortune-config\\n      defaultMode: \"6600\"       \\nAlthough ConfigMaps should be used for non-sensitive configuration data, you may\\nwant to make the file readable and writable only to the user and group the file is\\nowned by, as the example in the previous listing shows. \\n7.4.7\\nUpdating an app’s config without having to restart the app\\nWe’ve said that one of the drawbacks of using environment variables or command-line\\narguments as a configuration source is the inability to update them while the pro-\\ncess is running. Using a ConfigMap and exposing it through a volume brings the\\nability to update the configuration without having to recreate the pod or even restart\\nthe container. \\n When you update a ConfigMap, the files in all the volumes referencing it are\\nupdated. It’s then up to the process to detect that they’ve been changed and reload\\nthem. But Kubernetes will most likely eventually also support sending a signal to the\\ncontainer after updating the files.\\nWARNING\\nBe aware that as I’m writing this, it takes a surprisingly long time\\nfor the files to be updated after you update the ConfigMap (it can take up to\\none whole minute).\\nListing 7.17\\nA pod with a specific config map entry mounted into a specific file\\nListing 7.18\\nSetting file permissions: fortune-pod-configmap-volume-defaultMode.yaml \\nYou’re mounting into \\na file, not a directory.\\nInstead of mounting the whole \\nvolume, you’re only mounting \\nthe myconfig.conf entry.\\nThis sets the permissions \\nfor all files to -rw-rw------.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMap',\n",
       "    'description': 'A ConfigMap is a Kubernetes object that stores configuration data as key-value pairs.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'volume',\n",
       "    'description': 'A volume is a directory within a pod that can be mounted by one or more containers.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'subPath',\n",
       "    'description': 'The subPath property allows mounting part of a volume instead of the whole volume.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'ConfigMaps',\n",
       "    'description': 'A ConfigMap is used for non-sensitive configuration data and can be updated without restarting the app.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'volumes',\n",
       "    'description': 'Volumes are directories within a pod that can be mounted by one or more containers.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'config',\n",
       "    'description': 'A config refers to the configuration data stored in a ConfigMap or volume.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'defaultMode',\n",
       "    'description': 'The defaultMode property sets the file permissions for files in a ConfigMap volume.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'file permissions',\n",
       "    'description': 'File permissions can be set to control access to files within a ConfigMap or volume.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'ConfigMaps name',\n",
       "    'description': 'A unique identifier for a ConfigMap, used to reference it in a pod or container.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'fortune-config',\n",
       "    'description': 'An example ConfigMap name used in the documentation.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'The image refers to the Docker image used to create a container.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A container is a process that runs in isolation within a pod, based on a Docker image.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A pod is the basic execution unit in Kubernetes, containing one or more containers.',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'command-line arguments',\n",
       "    'description': 'Arguments passed to a container through the command line.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'environment variables',\n",
       "    'description': \"Variables set within a container's environment, used for configuration purposes.\",\n",
       "    'category': 'software/container'}],\n",
       "  'relationships': '[{\"source_entity\": \"config\", \"description\": \"is used for non-sensitive configuration data\", \"destination_entity\": \"ConfigMaps\"}, \\n{\"source_entity\": \"volumes\", \"description\": \"can be updated when ConfigMap is updated\", \"destination_entity\": \"pod\"}, \\n{\"source_entity\": \"file permissions\", \"description\": \"can be set to 644 by default\", \"destination_entity\": \"ConfigMap volume\"}, \\n{\"source_entity\": \"defaultMode\", \"description\": \"sets the file permissions for all files in a ConfigMap volume\", \"destination_entity\": \"ConfigMaps\"}, \\n{\"source_entity\": \"fortune-config\", \"description\": \"is an example of a ConfigMap used to update config without restart\", \"destination_entity\": \"app\\'s config\"}, \\n{\"source_entity\": \"image\", \"description\": \"can be mounted as part of a volume using subPath property\", \"destination_entity\": \"volume\"}, \\n{\"source_entity\": \"ConfigMaps\", \"description\": \"should be used for non-sensitive configuration data\", \"destination_entity\": \"environment variables\"}, \\n{\"source_entity\": \"container\", \"description\": \"can be updated when files are updated in a ConfigMap volume\", \"destination_entity\": \"files\"}, \\n{\"source_entity\": \"command-line arguments\", \"description\": \"cannot be updated while the process is running\", \"destination_entity\": \"process\"}, \\n{\"source_entity\": \"subPath\", \"description\": \"can be used to mount individual files instead of the whole volume\", \"destination_entity\": \"volume\"}, \\n{\"source_entity\": \"defaultMode\", \"description\": \"sets file permissions for all files in a ConfigMap volume to -rw-rw------\", \"destination_entity\": \"ConfigMap\"}, \\n{\"source_entity\": \"volumes\", \"description\": \"can be mounted with specific config map entry\", \"destination_entity\": \"pod\\'s config\"}, \\n{\"source_entity\": \"file permissions\", \"description\": \"can be changed using defaultMode property in volume spec\", \"destination_entity\": \"ConfigMaps\"}]'},\n",
       " {'page': 244,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '212\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\nEDITING A CONFIGMAP\\nLet’s see how you can change a ConfigMap and have the process running in the pod\\nreload the files exposed in the configMap volume. You’ll modify the Nginx config file\\nfrom your previous example and make Nginx use the new config without restarting\\nthe pod. Try switching gzip compression off by editing the fortune-config Config-\\nMap with kubectl edit:\\n$ kubectl edit configmap fortune-config\\nOnce your editor opens, change the gzip on line to gzip off, save the file, and then\\nclose the editor. The ConfigMap is then updated, and soon afterward, the actual file\\nin the volume is updated as well. You can confirm this by printing the contents of the\\nfile with kubectl exec:\\n$ kubectl exec fortune-configmap-volume -c web-server\\n➥  cat /etc/nginx/conf.d/my-nginx-config.conf\\nIf you don’t see the update yet, wait a while and try again. It takes a while for the\\nfiles to get updated. Eventually, you’ll see the change in the config file, but you’ll\\nfind this has no effect on Nginx, because it doesn’t watch the files and reload them\\nautomatically. \\nSIGNALING NGINX TO RELOAD THE CONFIG\\nNginx will continue to compress its responses until you tell it to reload its config files,\\nwhich you can do with the following command:\\n$ kubectl exec fortune-configmap-volume -c web-server -- nginx -s reload\\nNow, if you try hitting the server again with curl, you should see the response is no\\nlonger compressed (it no longer contains the Content-Encoding: gzip header).\\nYou’ve effectively changed the app’s config without having to restart the container or\\nrecreate the pod. \\nUNDERSTANDING HOW THE FILES ARE UPDATED ATOMICALLY\\nYou may wonder what happens if an app can detect config file changes on its own and\\nreloads them before Kubernetes has finished updating all the files in the configMap\\nvolume. Luckily, this can’t happen, because all the files are updated atomically, which\\nmeans all updates occur at once. Kubernetes achieves this by using symbolic links. If\\nyou list all the files in the mounted configMap volume, you’ll see something like the\\nfollowing listing.\\n$ kubectl exec -it fortune-configmap-volume -c web-server -- ls -lA \\n➥  /etc/nginx/conf.d\\ntotal 4\\ndrwxr-xr-x  ... 12:15 ..4984_09_04_12_15_06.865837643\\nListing 7.19\\nFiles in a mounted configMap volume\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMap',\n",
       "    'description': 'A configuration resource that can be used to decouple application configuration from the deployment process.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl edit',\n",
       "    'description': 'A command-line tool for editing ConfigMaps and other Kubernetes resources.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'fortune-config ConfigMap',\n",
       "    'description': 'An example ConfigMap used in the demonstration.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'gzip compression',\n",
       "    'description': 'A data compression technique used to reduce the size of web responses.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Nginx config file',\n",
       "    'description': 'A configuration file for the Nginx web server.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'fortune-configmap-volume',\n",
       "    'description': 'A persistent volume used to store ConfigMap data.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'web-server pod',\n",
       "    'description': 'A container running the web server application.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'nginx -s reload command',\n",
       "    'description': 'A command used to signal Nginx to reload its configuration files.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ConfigMap volume',\n",
       "    'description': 'A mounted ConfigMap resource that provides configuration data to applications.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kubectl exec command',\n",
       "    'description': 'A command-line tool for executing commands within a container or pod.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'atomic updates',\n",
       "    'description': 'The process of updating all files in a ConfigMap volume at once using symbolic links.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"fortune-configmap-volume\", \"description\": \"updated with new configuration\", \"destination_entity\": \"ConfigMap\"},\\n  {\"source_entity\": \"kubectl edit\", \"description\": \"edited the ConfigMap\", \"destination_entity\": \"fortune-config ConfigMap\"},\\n  {\"source_entity\": \"fortune-config ConfigMap\", \"description\": \"modified to turn off gzip compression\", \"destination_entity\": \"Nginx config file\"},\\n  {\"source_entity\": \"kubectl exec\", \"description\": \"printed the contents of the file\", \"destination_entity\": \"/etc/nginx/conf.d/my-nginx-config.conf\"},\\n  {\"source_entity\": \"fortune-config ConfigMap\", \"description\": \"updated with new configuration\", \"destination_entity\": \"ConfigMap volume\"},\\n  {\"source_entity\": \"web-server pod\", \"description\": \"reloaded its config files automatically\", \"destination_entity\": \"Nginx config file\"},\\n  {\"source_entity\": \"nginx -s reload command\", \"description\": \"told Nginx to reload its config files\", \"destination_entity\": \"Nginx config file\"},\\n  {\"source_entity\": \"kubectl exec\", \"description\": \"executed the nginx -s reload command\", \"destination_entity\": \"web-server pod\"},\\n  {\"source_entity\": \"fortune-configmap-volume\", \"description\": \"updated atomically with new configuration\", \"destination_entity\": \"ConfigMap volume\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"achieved atomic updates using symbolic links\", \"destination_entity\": \"files in the configMap volume\"},\\n  {\"source_entity\": \"fortune-config ConfigMap\", \"description\": \"used to configure Nginx without restarting the pod\", \"destination_entity\": \"Nginx web server\"}\\n]\\n```'},\n",
       " {'page': 245,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '213\\nUsing Secrets to pass sensitive data to containers\\nlrwxrwxrwx  ... 12:15 ..data -> ..4984_09_04_12_15_06.865837643\\nlrwxrwxrwx  ... 12:15 my-nginx-config.conf -> ..data/my-nginx-config.conf\\nlrwxrwxrwx  ... 12:15 sleep-interval -> ..data/sleep-interval\\nAs you can see, the files in the mounted configMap volume are symbolic links point-\\ning to files in the ..data dir. The ..data dir is also a symbolic link pointing to a direc-\\ntory called ..4984_09_04_something. When the ConfigMap is updated, Kubernetes\\ncreates a new directory like this, writes all the files to it, and then re-links the ..data\\nsymbolic link to the new directory, effectively changing all files at once.\\nUNDERSTANDING THAT FILES MOUNTED INTO EXISTING DIRECTORIES DON’T GET UPDATED\\nOne big caveat relates to updating ConfigMap-backed volumes. If you’ve mounted a\\nsingle file in the container instead of the whole volume, the file will not be updated!\\nAt least, this is true at the time of writing this chapter. \\n For now, if you need to add an individual file and have it updated when you update\\nits source ConfigMap, one workaround is to mount the whole volume into a different\\ndirectory and then create a symbolic link pointing to the file in question. The sym-\\nlink can either be created in the container image itself, or you could create the\\nsymlink when the container starts.\\nUNDERSTANDING THE CONSEQUENCES OF UPDATING A CONFIGMAP\\nOne of the most important features of containers is their immutability, which allows\\nus to be certain that no differences exist between multiple running containers created\\nfrom the same image, so is it wrong to bypass this immutability by modifying a Config-\\nMap used by running containers? \\n The main problem occurs when the app doesn’t support reloading its configura-\\ntion. This results in different running instances being configured differently—those\\npods that are created after the ConfigMap is changed will use the new config, whereas\\nthe old pods will still use the old one. And this isn’t limited to new pods. If a pod’s con-\\ntainer is restarted (for whatever reason), the new process will also see the new config.\\nTherefore, if the app doesn’t reload its config automatically, modifying an existing\\nConfigMap (while pods are using it) may not be a good idea. \\n If the app does support reloading, modifying the ConfigMap usually isn’t such a\\nbig deal, but you do need to be aware that because files in the ConfigMap volumes\\naren’t updated synchronously across all running instances, the files in individual pods\\nmay be out of sync for up to a whole minute.\\n7.5\\nUsing Secrets to pass sensitive data to containers\\nAll the information you’ve passed to your containers so far is regular, non-sensitive\\nconfiguration data that doesn’t need to be kept secure. But as we mentioned at the\\nstart of the chapter, the config usually also includes sensitive information, such as cre-\\ndentials and private encryption keys, which need to be kept secure.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Secrets',\n",
       "    'description': 'Sensitive data used to pass to containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMap',\n",
       "    'description': 'Used to store and update configuration files for containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Running instances of a container',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Volumes',\n",
       "    'description': 'Files and directories shared between containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Containers',\n",
       "    'description': 'Lightweight virtual environments for running applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Symbolic links',\n",
       "    'description': 'Links to files or directories that are updated automatically when the target changes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Images',\n",
       "    'description': 'Pre-built containers used as a base for running applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Files',\n",
       "    'description': 'Individual pieces of data stored in a container or volume',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Directories',\n",
       "    'description': 'Collections of files and subdirectories within a container or volume',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Files\",\\n    \"description\": \"are symbolic links pointing to files in the ..data dir.\",\\n    \"destination_entity\": \"..data\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMap\",\\n    \"description\": \"creates a new directory and writes all files to it, and then re-links the ..data symbolic link to the new directory.\",\\n    \"destination_entity\": \"..4984_09_04_something\"\\n  },\\n  {\\n    \"source_entity\": \"Volumes\",\\n    \"description\": \"are mounted into existing directories don’t get updated.\",\\n    \"destination_entity\": \"Existing Directories\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"creates a new directory and writes all files to it, and then re-links the ..data symbolic link to the new directory.\",\\n    \"destination_entity\": \"..4984_09_04_something\"\\n  },\\n  {\\n    \"source_entity\": \"Volumes\",\\n    \"description\": \"are mounted into different directories and then create a symbolic link pointing to the file in question.\",\\n    \"destination_entity\": \"Different Directories\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMap\",\\n    \"description\": \"updates, but files in individual pods may be out of sync for up to a whole minute.\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Images\",\\n    \"description\": \"are created from the same image and have no differences between multiple running containers.\",\\n    \"destination_entity\": \"Multiple Running Containers\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"use a ConfigMap used by running containers may be configured differently—those pods that are created after the ConfigMap is changed will use the new config, whereas the old pods will still use the old one.\",\\n    \"destination_entity\": \"Running Containers\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"manages and updates ConfigMaps used by running containers may result in different running instances being configured differently.\",\\n    \"destination_entity\": \"Running Instances\"\\n  },\\n  {\\n    \"source_entity\": \"Secrets\",\\n    \"description\": \"need to be kept secure and passed securely to containers using Secrets.\",\\n    \"destination_entity\": \"Containers\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"are created after the ConfigMap is changed will use the new config, whereas the old pods will still use the old one.\",\\n    \"destination_entity\": \"Running Containers\"\\n  },\\n  {\\n    \"source_entity\": \"Symbolic links\",\\n    \"description\": \"point to files in the ..data dir and are used to point to files in different directories.\",\\n    \"destination_entity\": \"..data\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"use a ConfigMap that may result in files being out of sync for up to a whole minute.\",\\n    \"destination_entity\": \"ConfigMap\"\\n  }\\n]\\n```'},\n",
       " {'page': 246,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '214\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\n7.5.1\\nIntroducing Secrets\\nTo store and distribute such information, Kubernetes provides a separate object called\\na Secret. Secrets are much like ConfigMaps—they’re also maps that hold key-value\\npairs. They can be used the same way as a ConfigMap. You can\\n\\uf0a1Pass Secret entries to the container as environment variables\\n\\uf0a1Expose Secret entries as files in a volume\\nKubernetes helps keep your Secrets safe by making sure each Secret is only distributed\\nto the nodes that run the pods that need access to the Secret. Also, on the nodes\\nthemselves, Secrets are always stored in memory and never written to physical storage,\\nwhich would require wiping the disks after deleting the Secrets from them. \\n On the master node itself (more specifically in etcd), Secrets used to be stored in\\nunencrypted form, which meant the master node needs to be secured to keep the sensi-\\ntive data stored in Secrets secure. This didn’t only include keeping the etcd storage\\nsecure, but also preventing unauthorized users from using the API server, because any-\\none who can create pods can mount the Secret into the pod and gain access to the sen-\\nsitive data through it. From Kubernetes version 1.7, etcd stores Secrets in encrypted\\nform, making the system much more secure. Because of this, it’s imperative you prop-\\nerly choose when to use a Secret or a ConfigMap. Choosing between them is simple:\\n\\uf0a1Use a ConfigMap to store non-sensitive, plain configuration data.\\n\\uf0a1Use a Secret to store any data that is sensitive in nature and needs to be kept\\nunder key. If a config file includes both sensitive and not-sensitive data, you\\nshould store the file in a Secret.\\nYou already used Secrets in chapter 5, when you created a Secret to hold the TLS certifi-\\ncate needed for the Ingress resource. Now you’ll explore Secrets in more detail.\\n7.5.2\\nIntroducing the default token Secret\\nYou’ll start learning about Secrets by examining a Secret that’s mounted into every\\ncontainer you run. You may have noticed it when using kubectl describe on a pod.\\nThe command’s output has always contained something like this:\\nVolumes:\\n  default-token-cfee9:\\n    Type:       Secret (a volume populated by a Secret)\\n    SecretName: default-token-cfee9\\nEvery pod has a secret volume attached to it automatically. The volume in the previ-\\nous kubectl describe output refers to a Secret called default-token-cfee9. Because\\nSecrets are resources, you can list them with kubectl get secrets and find the\\ndefault-token Secret in that list. Let’s see:\\n$ kubectl get secrets\\nNAME                  TYPE                                  DATA      AGE\\ndefault-token-cfee9   kubernetes.io/service-account-token   3         39d\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Secret',\n",
       "    'description': 'a separate object to store and distribute sensitive information in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMap',\n",
       "    'description': 'a map that holds key-value pairs for storing non-sensitive configuration data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'the basic execution unit of a Kubernetes application',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'an open-source container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMaps',\n",
       "    'description': 'used to store non-sensitive configuration data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'secrets',\n",
       "    'description': 'used to store sensitive information in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'a distributed key-value store that serves as the storage layer for Kubernetes',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'TLS certificate',\n",
       "    'description': 'used for secure communication between clients and servers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the command-line tool for interacting with a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'resources that store sensitive information in Kubernetes',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"provides a separate object called Secret to store sensitive information.\",\\n    \"destination_entity\": \"Secret\"\\n  },\\n  {\\n    \"source_entity\": \"Secrets\",\\n    \"description\": \"are maps that hold key-value pairs and can be used the same way as ConfigMaps.\",\\n    \"destination_entity\": \"ConfigMaps\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"keeps Secrets safe by distributing them only to the nodes that run the pods that need access to the Secret.\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"etcd\",\\n    \"description\": \"used to store Secrets in unencrypted form until version 1.7.\",\\n    \"destination_entity\": \"Secrets\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"stores Secrets in encrypted form from version 1.7 onwards, making the system more secure.\",\\n    \"destination_entity\": \"etcd\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMaps\",\\n    \"description\": \"are used to store non-sensitive, plain configuration data.\",\\n    \"destination_entity\": \"Secrets\"\\n  },\\n  {\\n    \"source_entity\": \"Secrets\",\\n    \"description\": \"are used to store sensitive information that needs to be kept under key.\",\\n    \"destination_entity\": \"ConfigMaps\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"provides a Secret called default-token-cfee9 that is mounted into every container.\",\\n    \"destination_entity\": \"Secret\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"is used to list Secrets and find the default-token Secret.\",\\n    \"destination_entity\": \"Secrets\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"has a secret volume attached to it automatically, referring to the default-token-cfee9 Secret.\",\\n    \"destination_entity\": \"default-token-cfee9\"\\n  }\\n]'},\n",
       " {'page': 247,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '215\\nUsing Secrets to pass sensitive data to containers\\nYou can also use kubectl describe to learn a bit more about it, as shown in the follow-\\ning listing.\\n$ kubectl describe secrets\\nName:        default-token-cfee9\\nNamespace:   default\\nLabels:      <none>\\nAnnotations: kubernetes.io/service-account.name=default\\n             kubernetes.io/service-account.uid=cc04bb39-b53f-42010af00237\\nType:        kubernetes.io/service-account-token\\nData\\n====\\nca.crt:      1139 bytes                                   \\nnamespace:   7 bytes                                      \\ntoken:       eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...      \\nYou can see that the Secret contains three entries—ca.crt, namespace, and token—\\nwhich represent everything you need to securely talk to the Kubernetes API server\\nfrom within your pods, should you need to do that. Although ideally you want your\\napplication to be completely Kubernetes-agnostic, when there’s no alternative other\\nthan to talk to Kubernetes directly, you’ll use the files provided through this secret\\nvolume. \\n The kubectl describe pod command shows where the secret volume is mounted:\\nMounts:\\n  /var/run/secrets/kubernetes.io/serviceaccount from default-token-cfee9\\nNOTE\\nBy default, the default-token Secret is mounted into every container,\\nbut you can disable that in each pod by setting the automountService-\\nAccountToken field in the pod spec to false or by setting it to false on the\\nservice account the pod is using. (You’ll learn about service accounts later in\\nthe book.)\\nTo help you visualize where and how the default token Secret is mounted, see fig-\\nure 7.11.\\n We’ve said Secrets are like ConfigMaps, so because this Secret contains three\\nentries, you can expect to see three files in the directory the secret volume is mounted\\ninto. You can check this easily with kubectl exec:\\n$ kubectl exec mypod ls /var/run/secrets/kubernetes.io/serviceaccount/\\nca.crt\\nnamespace\\ntoken\\nYou’ll see how your app can use these files to access the API server in the next chapter.\\nListing 7.20\\nDescribing a Secret\\nThis secret \\ncontains three \\nentries.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes clusters',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'describe',\n",
       "    'description': 'Subcommand for displaying information about Kubernetes resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'secrets',\n",
       "    'description': 'Kubernetes resource for storing sensitive data as a secret volume',\n",
       "    'category': 'application/database'},\n",
       "   {'entity': 'default-token-cfee9',\n",
       "    'description': 'Default token Secret object in the default namespace',\n",
       "    'category': 'database/application'},\n",
       "   {'entity': 'ca.crt',\n",
       "    'description': 'Certificate file containing root CA information',\n",
       "    'category': 'file/data'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'File containing the namespace ID of the pod',\n",
       "    'category': 'file/data'},\n",
       "   {'entity': 'token',\n",
       "    'description': 'File containing an authentication token for accessing the Kubernetes API server',\n",
       "    'category': 'file/data'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'Kubernetes resource representing a running application or process',\n",
       "    'category': 'application/process'},\n",
       "   {'entity': 'serviceaccount',\n",
       "    'description': 'Kubernetes resource for managing service accounts and their tokens',\n",
       "    'category': 'database/application'},\n",
       "   {'entity': 'automountServiceAccountToken',\n",
       "    'description': 'Field in the pod spec that controls whether to automatically mount a service account token',\n",
       "    'category': 'software/configuration'},\n",
       "   {'entity': 'default-token',\n",
       "    'description': 'Default token Secret object that is mounted into every container by default',\n",
       "    'category': 'database/application'},\n",
       "   {'entity': '/var/run/secrets/kubernetes.io/serviceaccount',\n",
       "    'description': 'Directory where the secret volume is mounted in a pod',\n",
       "    'category': 'directory/filesystem'},\n",
       "   {'entity': 'kubectl exec',\n",
       "    'description': 'Subcommand for executing commands inside a running pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'mypod',\n",
       "    'description': 'Name of an example pod used for demonstration purposes',\n",
       "    'category': 'application/process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"describe secrets to learn more about it\", \"destination_entity\": \"secrets\"},\\n  {\"source_entity\": \"kubectl describe\", \"description\": \"display detailed information about a secret\", \"destination_entity\": \"default-token-cfee9\"},\\n  {\"source_entity\": \"kubectl describe\", \"description\": \"show where the secret volume is mounted\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"kubectl exec\", \"description\": \"execute a command in a container to list files in a directory\", \"destination_entity\": \"/var/run/secrets/kubernetes.io/serviceaccount\"},\\n  {\"source_entity\": \"kubectl exec\", \"description\": \"list the contents of the secret volume\", \"destination_entity\": \"default-token-cfee9\"},\\n  {\"source_entity\": \"serviceaccount\", \"description\": \"related to the default token Secret being mounted into every container\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"serviceaccount\", \"description\": \"used to access the API server\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"default-token\", \"description\": \"contains three entries - ca.crt, namespace, and token\", \"destination_entity\": \"secrets\"},\\n  {\"source_entity\": \"automountServiceAccountToken\", \"description\": \"can be set to false to disable the default token Secret being mounted into every container\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"default-token-cfee9\", \"description\": \"a secret containing three entries - ca.crt, namespace, and token\", \"destination_entity\": \"secrets\"},\\n  {\"source_entity\": \"/var/run/secrets/kubernetes.io/serviceaccount\", \"description\": \"the directory where the secret volume is mounted\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"ca.crt\", \"description\": \"one of the three entries in the default-token Secret\", \"destination_entity\": \"secrets\"},\\n  {\"source_entity\": \"namespace\", \"description\": \"one of the three entries in the default-token Secret\", \"destination_entity\": \"secrets\"},\\n  {\"source_entity\": \"token\", \"description\": \"one of the three entries in the default-token Secret\", \"destination_entity\": \"secrets\"}\\n]\\n\\nNote: I\\'ve kept all the relations even if they seem trivial or obvious, as per your request to follow the rules exactly.'},\n",
       " {'page': 248,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '216\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\n7.5.3\\nCreating a Secret\\nNow, you’ll create your own little Secret. You’ll improve your fortune-serving Nginx\\ncontainer by configuring it to also serve HTTPS traffic. For this, you need to create a\\ncertificate and a private key. The private key needs to be kept secure, so you’ll put it\\nand the certificate into a Secret.\\n First, generate the certificate and private key files (do this on your local machine).\\nYou can also use the files in the book’s code archive (the cert and key files are in the\\nfortune-https directory):\\n$ openssl genrsa -out https.key 2048\\n$ openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj \\n/CN=www.kubia-example.com\\nNow, to help better demonstrate a few things about Secrets, create an additional\\ndummy file called foo and make it contain the string bar. You’ll understand why you\\nneed to do this in a moment or two:\\n$ echo bar > foo\\nNow you can use kubectl create secret to create a Secret from the three files:\\n$ kubectl create secret generic fortune-https --from-file=https.key\\n➥  --from-file=https.cert --from-file=foo\\nsecret \"fortune-https\" created\\nThis isn’t very different from creating ConfigMaps. In this case, you’re creating a\\ngeneric Secret called fortune-https and including two entries in it (https.key with\\nthe contents of the https.key file and likewise for the https.cert key/file). As you\\nlearned earlier, you could also include the whole directory with --from-file=fortune-\\nhttps instead of specifying each file individually.\\nPod\\nContainer\\nFilesystem\\n/\\nvar/\\nrun/\\nsecrets/\\nkubernetes.io/\\nserviceaccount/\\nDefault token Secret\\nDefault token\\nsecret\\nvolume\\nca.crt\\n...\\n...\\n...\\nnamespace\\ntoken\\nFigure 7.11\\nThe default-token Secret is created automatically and a corresponding \\nvolume is mounted in each pod automatically.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [      ca.crt  ...\n",
       "   0  namespace  ...\n",
       "   1      token  ...],\n",
       "  'entities': [{'entity': 'Secret',\n",
       "    'description': 'A concept used to store sensitive information, such as passwords or certificates.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMap',\n",
       "    'description': 'A concept used to store configuration data in a key-value format.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Nginx',\n",
       "    'description': 'A web server software that can be used to serve HTTP and HTTPS traffic.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Certificate',\n",
       "    'description': 'A digital certificate used to establish trust between a client and a server.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Private Key',\n",
       "    'description': 'A cryptographic key used to encrypt and decrypt data.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line interface for interacting with a Kubernetes cluster.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'create secret generic',\n",
       "    'description': 'A kubectl command used to create a Secret from a file or directory.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes, which can contain one or more containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'A lightweight and portable execution environment that can be run on any system.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Filesystem',\n",
       "    'description': 'A hierarchical organization of data as a collection of files and subdirectories.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'volume',\n",
       "    'description': 'A directory on the host machine that is shared with a container or Pod.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ca.crt',\n",
       "    'description': 'The certificate of the Certificate Authority used to verify the identity of a server.',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'Default token Secret',\n",
       "    'description': 'A Secret created automatically by Kubernetes that contains a default token.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'A scope in which an object is unique, such as a Pod or Service.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'token',\n",
       "    'description': 'A short string used to authenticate and authorize access to a resource.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"create a generic secret called fortune-https from files https.key, https.cert, and foo\",\\n    \"destination_entity\": \"Secret\"\\n  },\\n  {\\n    \"source_entity\": \"Nginx\",\\n    \"description\": \"serve HTTPS traffic using certificate and private key\",\\n    \"destination_entity\": \"Certificate\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"create a secret from files https.key, https.cert, and foo\",\\n    \"destination_entity\": \"file (https.key)\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"create a secret from files https.key, https.cert, and foo\",\\n    \"destination_entity\": \"file (https.cert)\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"create a secret from file foo\",\\n    \"destination_entity\": \"file (foo)\"\\n  },\\n  {\\n    \"source_entity\": \"openssl\",\\n    \"description\": \"generate certificate and private key files\",\\n    \"destination_entity\": \"certificate and private key\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"create a Secret called fortune-https\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"have a default token Secret mounted automatically\",\\n    \"destination_entity\": \"Default token Secret\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"create a secret called fortune-https from files https.key and https.cert\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"Container\",\\n    \"description\": \"serve HTTPS traffic using certificate and private key\",\\n    \"destination_entity\": \"Certificate\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"create a secret from file foo\",\\n    \"destination_entity\": \"Filesystem\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"have a default token Secret mounted automatically\",\\n    \"destination_entity\": \"namespace\"\\n  }\\n]\\n```'},\n",
       " {'page': 249,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '217\\nUsing Secrets to pass sensitive data to containers\\nNOTE\\nYou’re creating a generic Secret, but you could also have created a tls\\nSecret with the kubectl create secret tls command, as you did in chapter 5.\\nThis would create the Secret with different entry names, though.\\n7.5.4\\nComparing ConfigMaps and Secrets\\nSecrets and ConfigMaps have a pretty big difference. This is what drove Kubernetes\\ndevelopers to create ConfigMaps after Kubernetes had already supported Secrets for a\\nwhile. The following listing shows the YAML of the Secret you created.\\n$ kubectl get secret fortune-https -o yaml\\napiVersion: v1\\ndata:\\n  foo: YmFyCg==\\n  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...\\n  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...\\nkind: Secret\\n...\\nNow compare this to the YAML of the ConfigMap you created earlier, which is shown\\nin the following listing.\\n$ kubectl get configmap fortune-config -o yaml\\napiVersion: v1\\ndata:\\n  my-nginx-config.conf: |\\n    server {\\n      ...\\n    }\\n  sleep-interval: |\\n    25\\nkind: ConfigMap\\n...\\nNotice the difference? The contents of a Secret’s entries are shown as Base64-encoded\\nstrings, whereas those of a ConfigMap are shown in clear text. This initially made\\nworking with Secrets in YAML and JSON manifests a bit more painful, because you\\nhad to encode and decode them when setting and reading their entries. \\nUSING SECRETS FOR BINARY DATA\\nThe reason for using Base64 encoding is simple. A Secret’s entries can contain binary\\nvalues, not only plain-text. Base64 encoding allows you to include the binary data in\\nYAML or JSON, which are both plain-text formats. \\nTIP\\nYou can use Secrets even for non-sensitive binary data, but be aware that\\nthe maximum size of a Secret is limited to 1MB.\\nListing 7.21\\nA Secret’s YAML definition\\nListing 7.22\\nA ConfigMap’s YAML definition\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Secrets',\n",
       "    'description': 'used to pass sensitive data to containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ConfigMaps',\n",
       "    'description': 'a way to decouple configuration from images, can be used for non-sensitive data as well',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'command-line tool for running commands against Kubernetes clusters',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'field in YAML definition that specifies the API version of the object',\n",
       "    'category': 'yaml'},\n",
       "   {'entity': 'data',\n",
       "    'description': 'field in Secret or ConfigMap YAML definition that contains key-value pairs of data',\n",
       "    'category': 'yaml'},\n",
       "   {'entity': 'foo',\n",
       "    'description': 'key in Secret YAML definition that contains a Base64-encoded string',\n",
       "    'category': 'yaml'},\n",
       "   {'entity': 'https.cert',\n",
       "    'description': 'key in Secret YAML definition that contains a Base64-encoded string for a TLS certificate',\n",
       "    'category': 'yaml'},\n",
       "   {'entity': 'https.key',\n",
       "    'description': 'key in Secret YAML definition that contains a Base64-encoded string for a TLS key',\n",
       "    'category': 'yaml'},\n",
       "   {'entity': 'my-nginx-config.conf',\n",
       "    'description': 'key in ConfigMap YAML definition that contains a plain-text configuration file',\n",
       "    'category': 'yaml'},\n",
       "   {'entity': 'sleep-interval',\n",
       "    'description': 'key in ConfigMap YAML definition that contains a plain-text value',\n",
       "    'category': 'yaml'},\n",
       "   {'entity': 'Base64',\n",
       "    'description': 'encoding scheme used to include binary data in plain-text formats like YAML or JSON',\n",
       "    'category': 'encoding'},\n",
       "   {'entity': 'YAML',\n",
       "    'description': 'plain-text format used to define objects and their properties, can be used with Secret and ConfigMap definitions',\n",
       "    'category': 'format'},\n",
       "   {'entity': 'JSON',\n",
       "    'description': 'plain-text format used to define objects and their properties, similar to YAML but with different syntax',\n",
       "    'category': 'format'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to get secret fortune-https\", \"destination_entity\": \"secret fortune-https\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to get configmap fortune-config\", \"destination_entity\": \"configmap fortune-config\"},\\n  {\"source_entity\": \"Secrets\", \"description\": \"have entries shown as Base64-encoded strings\", \"destination_entity\": \"Base64\"},\\n  {\"source_entity\": \"Secrets\", \"description\": \"can contain binary values\", \"destination_entity\": \"binary data\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"create secret tls with kubectl command\", \"destination_entity\": \"secret tls\"},\\n  {\"source_entity\": \"ConfigMaps\", \"description\": \"were created to differentiate from Secrets\", \"destination_entity\": \"Secrets\"},\\n  {\"source_entity\": \"apiVersion\", \"description\": \"is the version of YAML or JSON\", \"destination_entity\": \"YAML, JSON\"},\\n  {\"source_entity\": \"foo\", \"description\": \"is an entry in secret fortune-https\", \"destination_entity\": \"secret fortune-https\"},\\n  {\"source_entity\": \"https.cert\", \"description\": \"is a binary value stored in secret fortune-https\", \"destination_entity\": \"binary data\"},\\n  {\"source_entity\": \"YAML\", \"description\": \"can be used to store secrets with Base64 encoding\", \"destination_entity\": \"Base64\"},\\n  {\"source_entity\": \"my-nginx-config.conf\", \"description\": \"is an entry in configmap fortune-config\", \"destination_entity\": \"configmap fortune-config\"},\\n  {\"source_entity\": \"sleep-interval\", \"description\": \"is a value stored in configmap fortune-config\", \"destination_entity\": \"configmap fortune-config\"},\\n  {\"source_entity\": \"https.key\", \"description\": \"is a binary value stored in secret fortune-https\", \"destination_entity\": \"binary data\"}\\n]'},\n",
       " {'page': 250,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '218\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\nINTRODUCING THE STRINGDATA FIELD\\nBecause not all sensitive data is in binary form, Kubernetes also allows setting a Secret’s\\nvalues through the stringData field. The following listing shows how it’s used.\\nkind: Secret\\napiVersion: v1\\nstringData:           \\n  foo: plain text      \\ndata:\\n  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...\\n  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...\\nThe stringData field is write-only (note: write-only, not read-only). It can only be\\nused to set values. When you retrieve the Secret’s YAML with kubectl get -o yaml, the\\nstringData field will not be shown. Instead, all entries you specified in the string-\\nData field (such as the foo entry in the previous example) will be shown under data\\nand will be Base64-encoded like all the other entries. \\nREADING A SECRET’S ENTRY IN A POD\\nWhen you expose the Secret to a container through a secret volume, the value of the\\nSecret entry is decoded and written to the file in its actual form (regardless if it’s plain\\ntext or binary). The same is also true when exposing the Secret entry through an envi-\\nronment variable. In both cases, the app doesn’t need to decode it, but can read the\\nfile’s contents or look up the environment variable value and use it directly.\\n7.5.5\\nUsing the Secret in a pod\\nWith your fortune-https Secret containing both the cert and key files, all you need to\\ndo now is configure Nginx to use them. \\nMODIFYING THE FORTUNE-CONFIG CONFIGMAP TO ENABLE HTTPS\\nFor this, you need to modify the config file again by editing the ConfigMap:\\n$ kubectl edit configmap fortune-config\\nAfter the text editor opens, modify the part that defines the contents of the my-nginx-\\nconfig.conf entry so it looks like the following listing.\\n...\\ndata:\\n  my-nginx-config.conf: |\\n    server {\\n      listen              80;\\n      listen              443 ssl;\\n      server_name         www.kubia-example.com;\\nListing 7.23\\nAdding plain text entries to a Secret using the stringData field\\nListing 7.24\\nModifying the fortune-config ConfigMap’s data\\nThe stringData can be used \\nfor non-binary Secret data.\\nSee, “plain text” is not Base64-encoded.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMaps',\n",
       "    'description': 'a way to store sensitive information as Kubernetes objects',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'a way to store sensitive information in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'stringData field',\n",
       "    'description': 'a write-only field used to set values in a Secret',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl get -o yaml',\n",
       "    'description': 'a command used to retrieve the YAML representation of a Secret',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'secret volume',\n",
       "    'description': \"a way to expose a Secret's value to a container through a file\",\n",
       "    'category': 'process'},\n",
       "   {'entity': 'environment variable',\n",
       "    'description': \"a way to expose a Secret's value to an application through a variable\",\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Nginx',\n",
       "    'description': 'an HTTP server software that can be used with Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMap',\n",
       "    'description': 'a way to store configuration data as Kubernetes objects',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'fortune-config ConfigMap',\n",
       "    'description': 'a specific example of a ConfigMap object',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'my-nginx-config.conf entry',\n",
       "    'description': 'an example entry in the fortune-config ConfigMap',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'https.cert file',\n",
       "    'description': 'a binary file containing an SSL certificate used by Nginx',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'https.key file',\n",
       "    'description': 'a binary file containing a private key used by Nginx',\n",
       "    'category': 'file'}],\n",
       "  'relationships': '[{\"source_entity\": \"Kubernetes\", \"description\": \"allows setting a Secret\\'s values through the stringData field\", \"destination_entity\": \"stringData field\"},{\"source_entity\": \"Kubernetes\", \"description\": \"makes stringData field write-only\", \"destination_entity\": \"stringData field\"},{\"source_entity\": \"kubectl get -o yaml\", \"description\": \"will not show stringData field\", \"destination_entity\": \"stringData field\"},{\"source_entity\": \"Kubernetes\", \"description\": \"decodes Secret entry values in a pod\", \"destination_entity\": \"Secret volume\"},{\"source_entity\": \"Kubernetes\", \"description\": \"writes decoded Secret entry value to file\", \"destination_entity\": \"file\"},{\"source_entity\": \"Nginx\", \"description\": \"uses decoded Secret entry values\", \"destination_entity\": \"https.cert file\"},{\"source_entity\": \"ConfigMaps\", \"description\": \"can be used to modify ConfigMap entries\", \"destination_entity\": \"fortune-config ConfigMap\"},{\"source_entity\": \"kubectl edit configmap fortune-config\", \"description\": \"modifies the ConfigMap\\'s data\", \"destination_entity\": \"fortune-config ConfigMap\"},{\"source_entity\": \"stringData field\", \"description\": \"can be used for non-binary Secret data\", \"destination_entity\": \"Secrets\"},{\"source_entity\": \"Kubernetes\", \"description\": \"exposes Secret to container through secret volume\", \"destination_entity\": \"secret volume\"},{\"source_entity\": \"Kubernetes\", \"description\": \"decodes environment variable values\", \"destination_entity\": \"environment variable\"},{\"source_entity\": \"my-nginx-config.conf entry\", \"description\": \"uses decoded Secret entry values\", \"destination_entity\": \"https.cert file\"}]'},\n",
       " {'page': 251,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '219\\nUsing Secrets to pass sensitive data to containers\\n      ssl_certificate     certs/https.cert;           \\n      ssl_certificate_key certs/https.key;            \\n      ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;\\n      ssl_ciphers         HIGH:!aNULL:!MD5;\\n      location / {\\n        root   /usr/share/nginx/html;\\n        index  index.html index.htm;\\n      }\\n    }\\n  sleep-interval: |\\n...\\nThis configures the server to read the certificate and key files from /etc/nginx/certs,\\nso you’ll need to mount the secret volume there. \\nMOUNTING THE FORTUNE-HTTPS SECRET IN A POD\\nNext, you’ll create a new fortune-https pod and mount the secret volume holding\\nthe certificate and key into the proper location in the web-server container, as shown\\nin the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: fortune-https\\nspec:\\n  containers:\\n  - image: luksa/fortune:env\\n    name: html-generator\\n    env:\\n    - name: INTERVAL\\n      valueFrom: \\n        configMapKeyRef:\\n          name: fortune-config\\n          key: sleep-interval\\n    volumeMounts:\\n    - name: html\\n      mountPath: /var/htdocs\\n  - image: nginx:alpine\\n    name: web-server\\n    volumeMounts:\\n    - name: html\\n      mountPath: /usr/share/nginx/html\\n      readOnly: true\\n    - name: config\\n      mountPath: /etc/nginx/conf.d\\n      readOnly: true\\n    - name: certs                         \\n      mountPath: /etc/nginx/certs/        \\n      readOnly: true                      \\n    ports:\\n    - containerPort: 80\\nListing 7.25\\nYAML definition of the fortune-https pod: fortune-pod-https.yaml\\nThe paths are \\nrelative to /etc/nginx.\\nYou configured Nginx to read the cert and \\nkey file from /etc/nginx/certs, so you need \\nto mount the Secret volume there.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ssl_certificate',\n",
       "    'description': 'SSL certificate for secure connection',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'certs/https.cert',\n",
       "    'description': 'Certificate file path',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'ssl_certificate_key',\n",
       "    'description': 'SSL certificate key for secure connection',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'certs/https.key',\n",
       "    'description': 'Key file path',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'ssl_protocols',\n",
       "    'description': 'Protocols to use for SSL/TLS connections',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'TLSv1 TLSv1.1 TLSv1.2',\n",
       "    'description': 'Specific protocols to enable',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ssl_ciphers',\n",
       "    'description': 'Ciphers to use for SSL/TLS connections',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'HIGH:!aNULL:!MD5',\n",
       "    'description': 'Specific ciphers to enable/disable',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'location / {',\n",
       "    'description': 'Nginx configuration block',\n",
       "    'category': 'software'},\n",
       "   {'entity': '/usr/share/nginx/html',\n",
       "    'description': 'Root directory for web server content',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'index index.html index.htm',\n",
       "    'description': 'Default index files to serve',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'sleep-interval',\n",
       "    'description': 'Interval between sleep events',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'fortune-config',\n",
       "    'description': 'ConfigMap containing interval value',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'html-generator',\n",
       "    'description': 'Container generating HTML content',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'luksa/fortune:env',\n",
       "    'description': 'Image for HTML generator container',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'nginx:alpine',\n",
       "    'description': 'Image for web server container',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'web-server',\n",
       "    'description': 'Nginx web server container',\n",
       "    'category': 'container'},\n",
       "   {'entity': '/var/htdocs',\n",
       "    'description': 'Mount point for HTML content volume',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'html',\n",
       "    'description': 'Volume containing HTML content',\n",
       "    'category': 'volume'},\n",
       "   {'entity': '/etc/nginx/conf.d',\n",
       "    'description': 'Mount point for config volume',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'config',\n",
       "    'description': 'Volume containing Nginx configuration',\n",
       "    'category': 'volume'},\n",
       "   {'entity': '/etc/nginx/certs/',\n",
       "    'description': 'Mount point for secret volume',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'certs',\n",
       "    'description': 'Secret volume containing SSL certificate and key',\n",
       "    'category': 'volume'},\n",
       "   {'entity': 'fortune-https',\n",
       "    'description': 'Pod name for web server and HTML generator containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'apiVersion: v1',\n",
       "    'description': 'Kubernetes API version',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kind: Pod',\n",
       "    'description': 'Kubernetes resource type',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Pod metadata',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'name: fortune-https',\n",
       "    'description': 'Pod name',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ssl_certificate\", \"description\": \"reads certificate files from\", \"destination_entity\": \"/etc/nginx/certs\"},\\n  {\"source_entity\": \"ssl_certificate_key\", \"description\": \"reads key file from\", \"destination_entity\": \"/etc/nginx/certs/\"},\\n  {\"source_entity\": \"nginx:alpine\", \"description\": \"mounts secret volume holding certificates and key\", \"destination_entity\": \"/etc/nginx/certs/\"},\\n  {\"source_entity\": \"html-generator\", \"description\": \"generates html files to be served by\", \"destination_entity\": \"/var/htdocs\"},\\n  {\"source_entity\": \"web-server\", \"description\": \"serves html files from\", \"destination_entity\": \"/usr/share/nginx/html\"},\\n  {\"source_entity\": \"fortune-https\", \"description\": \"mounts secret volume holding certificates and key for\", \"destination_entity\": \"/etc/nginx/certs/\"},\\n  {\"source_entity\": \"config\", \"description\": \"provides configuration settings for\", \"destination_entity\": \"nginx:alpine\"},\\n  {\"source_entity\": \"ssl_protocols\", \"description\": \"specifies protocols supported by\", \"destination_entity\": \"web-server\"},\\n  {\"source_entity\": \"HIGH:!aNULL:!MD5\", \"description\": \"sets ciphers used by\", \"destination_entity\": \"web-server\"},\\n  {\"source_entity\": \"sleep-interval\", \"description\": \"sets interval for html generation by\", \"destination_entity\": \"html-generator\"},\\n  {\"source_entity\": \"fortune-config\", \"description\": \"provides configuration settings for\", \"destination_entity\": \"html-generator\"}\\n]\\n```\\nI extracted the following relations based on the document and entities provided:\\n\\n1. `ssl_certificate` reads certificate files from `/etc/nginx/certs`.\\n2. `ssl_certificate_key` reads key file from `/etc/nginx/certs/`.\\n3. `nginx:alpine` mounts secret volume holding certificates and key to `/etc/nginx/certs/`.\\n4. `html-generator` generates html files to be served by `web-server`.\\n5. `web-server` serves html files from `/usr/share/nginx/html`.\\n6. `fortune-https` mounts secret volume holding certificates and key for `nginx:alpine`.\\n7. `config` provides configuration settings for `nginx:alpine`.\\n8. `ssl_protocols` specifies protocols supported by `web-server`.\\n9. `HIGH:!aNULL:!MD5` sets ciphers used by `web-server`.\\n10. `sleep-interval` sets interval for html generation by `html-generator`.\\n11. `fortune-config` provides configuration settings for `html-generator`.'},\n",
       " {'page': 252,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '220\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\n    - containerPort: 443\\n  volumes:\\n  - name: html\\n    emptyDir: {}\\n  - name: config\\n    configMap:\\n      name: fortune-config\\n      items:\\n      - key: my-nginx-config.conf\\n        path: https.conf\\n  - name: certs                            \\n    secret:                                \\n      secretName: fortune-https            \\nMuch is going on in this pod descriptor, so let me help you visualize it. Figure 7.12\\nshows the components defined in the YAML. The default-token Secret, volume, and\\nvolume mount, which aren’t part of the YAML, but are added to your pod automati-\\ncally, aren’t shown in the figure.\\nNOTE\\nLike configMap volumes, secret volumes also support specifying file\\npermissions for the files exposed in the volume through the defaultMode\\nproperty.\\nYou define the secret \\nvolume here, referring to \\nthe fortune-https Secret.\\nContainer: web-server\\nContainer: html-generator\\nSecret: fortune-https\\nDefault token Secret and volume not shown\\nsecret\\nvolume:\\ncerts\\nemptyDir\\nvolume:\\nhtml\\nconﬁgMap\\nvolume:\\nconﬁg\\nhttps.cert\\n...\\n...\\n...\\nhttps.key\\nfoo\\n/etc/nginx/conf.d/\\n/etc/nginx/certs/\\n/usr/share/nginx/html/\\n/var/htdocs\\nConﬁgMap: fortune-conﬁg\\nmy-nginx-conﬁg.conf\\nserver {\\n…\\n}\\nPod\\nEnvironment variables:\\nINTERVAL=25\\nsleep-interval\\n25\\nFigure 7.12\\nCombining a ConfigMap and a Secret to run your fortune-https pod\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  my-nginx-config.conf server {\\n…\\n}\n",
       "   0       sleep-interval             25,\n",
       "     https.cert  ...\n",
       "   0  https.key  ...\n",
       "   1        foo  ...],\n",
       "  'entities': [{'entity': 'ConfigMaps',\n",
       "    'description': 'a way to store and retrieve configuration data for an application',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'a way to store sensitive information such as passwords or certificates',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'configMap',\n",
       "    'description': 'a ConfigMaps instance used to configure the application',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'fortune-config',\n",
       "    'description': 'the name of the ConfigMaps instance used to configure the application',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'containerPort',\n",
       "    'description': 'a port exposed by a container for communication with other containers or services',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'volumes',\n",
       "    'description': 'a way to mount a directory or file system from one container to another',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'html',\n",
       "    'description': 'the name of the volume used to store HTML files for the application',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'config',\n",
       "    'description': 'the name of the volume used to store configuration data for the application',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'certs',\n",
       "    'description': 'the name of the secret used to store certificates and keys for HTTPS communication',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'secret',\n",
       "    'description': 'a way to expose sensitive information such as passwords or certificates to an application',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'fortune-https',\n",
       "    'description': 'the name of the secret used to store certificates and keys for HTTPS communication',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a single instance of a container or group of containers running as a single process',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'web-server',\n",
       "    'description': 'the name of the container used to serve web content',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'html-generator',\n",
       "    'description': 'the name of the container used to generate HTML files for the application',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'default-token',\n",
       "    'description': 'a default token secret used by the pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'emptyDir',\n",
       "    'description': 'a volume type that provides a directory with no backing store, and any data written to it will be lost when the node is restarted',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'defaultMode',\n",
       "    'description': 'a property used to specify file permissions for files exposed in a secret or configMap volume',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"ConfigMaps\", \"description\": \"support specifying file permissions for files exposed in the volume\", \"destination_entity\": \"defaultMode\"}, \\n{\"source_entity\": \"Secrets\", \"description\": \"also support specifying file permissions for files exposed in the volume through the defaultMode property\", \"destination_entity\": \"defaultMode\"}, \\n{\"source_entity\": \"configMap\", \"description\": \"define a ConfigMap to expose configuration data to running applications\", \"destination_entity\": \"fortune-config\"}, \\n{\"source_entity\": \"ConfigMaps\", \"description\": \"can be used to configure applications by exposing configuration data\", \"destination_entity\": \"web-server\"}, \\n{\"source_entity\": \"secret\", \"description\": \"used to store sensitive information, such as passwords and certificates\", \"destination_entity\": \"fortune-https\"}, \\n{\"source_entity\": \"configMap\", \"description\": \"can be used in conjunction with a Secret to expose configuration data and sensitive information\", \"destination_entity\": \"fortune-config\"}, \\n{\"source_entity\": \"ConfigMaps\", \"description\": \"are defined in a YAML file, which is then used to create a ConfigMap object\", \"destination_entity\": \"pod\"}, \\n{\"source_entity\": \"Secrets\", \"description\": \"can be used to expose sensitive information, such as certificates and keys\", \"destination_entity\": \"fortune-https\"}, \\n{\"source_entity\": \"html-generator\", \"description\": \"uses the fortune-config ConfigMap to generate HTML pages\", \"destination_entity\": \"fortune-config\"}, \\n{\"source_entity\": \"web-server\", \"description\": \"uses the fortune-https Secret to serve HTTPS content\", \"destination_entity\": \"fortune-https\"}, \\n{\"source_entity\": \"emptyDir\", \"description\": \"is used as a volume in the pod descriptor, but is not shown in the figure\", \"destination_entity\": \"pod\"}]'},\n",
       " {'page': 253,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '221\\nUsing Secrets to pass sensitive data to containers\\nTESTING WHETHER NGINX IS USING THE CERT AND KEY FROM THE SECRET\\nOnce the pod is running, you can see if it’s serving HTTPS traffic by opening a port-\\nforward tunnel to the pod’s port 443 and using it to send a request to the server\\nwith curl: \\n$ kubectl port-forward fortune-https 8443:443 &\\nForwarding from 127.0.0.1:8443 -> 443\\nForwarding from [::1]:8443 -> 443\\n$ curl https://localhost:8443 -k\\nIf you configured the server properly, you should get a response. You can check the\\nserver’s certificate to see if it matches the one you generated earlier. This can also be\\ndone with curl by turning on verbose logging using the -v option, as shown in the fol-\\nlowing listing.\\n$ curl https://localhost:8443 -k -v\\n* About to connect() to localhost port 8443 (#0)\\n*   Trying ::1...\\n* Connected to localhost (::1) port 8443 (#0)\\n* Initializing NSS with certpath: sql:/etc/pki/nssdb\\n* skipping SSL peer certificate verification\\n* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\\n* Server certificate:\\n*   subject: CN=www.kubia-example.com          \\n*   start date: aug 16 18:43:13 2016 GMT       \\n*   expire date: aug 14 18:43:13 2026 GMT      \\n*   common name: www.kubia-example.com         \\n*   issuer: CN=www.kubia-example.com           \\nUNDERSTANDING SECRET VOLUMES ARE STORED IN MEMORY\\nYou successfully delivered your certificate and private key to your container by mount-\\ning a secret volume in its directory tree at /etc/nginx/certs. The secret volume uses\\nan in-memory filesystem (tmpfs) for the Secret files. You can see this if you list mounts\\nin the container:\\n$ kubectl exec fortune-https -c web-server -- mount | grep certs\\ntmpfs on /etc/nginx/certs type tmpfs (ro,relatime) \\nBecause tmpfs is used, the sensitive data stored in the Secret is never written to disk,\\nwhere it could be compromised. \\nEXPOSING A SECRET’S ENTRIES THROUGH ENVIRONMENT VARIABLES\\nInstead of using a volume, you could also have exposed individual entries from the\\nsecret as environment variables, the way you did with the sleep-interval entry from\\nthe ConfigMap. For example, if you wanted to expose the foo key from your Secret as\\nenvironment variable FOO_SECRET, you’d add the snippet from the following listing to\\nthe container definition.\\nListing 7.26\\nDisplaying the server certificate sent by Nginx\\nThe certificate \\nmatches the one you \\ncreated and stored \\nin the Secret.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Secrets',\n",
       "    'description': 'pass sensitive data to containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Nginx',\n",
       "    'description': 'HTTP server and reverse proxy',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'port-forward tunnel',\n",
       "    'description': \"tool for forwarding traffic from a local port to a remote pod's port\",\n",
       "    'category': 'process'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'utility for transferring data with URL syntax',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'SSL connection',\n",
       "    'description': 'encryption protocol used between client and server',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384',\n",
       "    'description': 'ciphersuite used for SSL/TLS encryption',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'tmpfs',\n",
       "    'description': 'in-memory filesystem used by Secret volumes',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ENVIRONMENT VARIABLES',\n",
       "    'description': 'mechanism for exposing secret entries to containers',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ConfigMap',\n",
       "    'description': 'data structure for storing configuration data in Kubernetes',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'basic execution unit in a Kubernetes cluster',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"Port-forwarding traffic from localhost to a pod\\'s port 443\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"curl\",\\n    \"description\": \"Sending an HTTPS request to the server with certificate verification\",\\n    \"destination_entity\": \"server (Nginx)\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"Mounting a Secret volume in the container\\'s directory tree at /etc/nginx/certs\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"tmpfs\",\\n    \"description\": \"Using an in-memory filesystem (tmpfs) to store sensitive data from Secrets\",\\n    \"destination_entity\": \"Secrets\"\\n  },\\n  {\\n    \"source_entity\": \"Nginx\",\\n    \"description\": \"Serving HTTPS traffic using the certificate and key from the Secret\",\\n    \"destination_entity\": \"server (Nginx)\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl exec\",\\n    \"description\": \"Listing mounts in the container to verify the Secret volume\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMap\",\\n    \"description\": \"Exposing individual entries from a ConfigMap as environment variables\",\\n    \"destination_entity\": \"environment variables\"\\n  },\\n  {\\n    \"source_entity\": \"Secrets\",\\n    \"description\": \"Passing sensitive data to containers using Secrets\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"SSL connection\",\\n    \"description\": \"Establishing a secure connection between the client and server using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\",\\n    \"destination_entity\": \"server (Nginx)\"\\n  }\\n]\\n```'},\n",
       " {'page': 254,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '222\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\n    env:\\n    - name: FOO_SECRET\\n      valueFrom:                  \\n        secretKeyRef:             \\n          name: fortune-https    \\n          key: foo           \\nThis is almost exactly like when you set the INTERVAL environment variable, except\\nthat this time you’re referring to a Secret by using secretKeyRef instead of config-\\nMapKeyRef, which is used to refer to a ConfigMap.\\n Even though Kubernetes enables you to expose Secrets through environment vari-\\nables, it may not be the best idea to use this feature. Applications usually dump envi-\\nronment variables in error reports or even write them to the application log at startup,\\nwhich may unintentionally expose them. Additionally, child processes inherit all the\\nenvironment variables of the parent process, so if your app runs a third-party binary,\\nyou have no way of knowing what happens with your secret data. \\nTIP\\nThink twice before using environment variables to pass your Secrets to\\nyour container, because they may get exposed inadvertently. To be safe, always\\nuse secret volumes for exposing Secrets.\\n7.5.6\\nUnderstanding image pull Secrets\\nYou’ve learned how to pass Secrets to your applications and use the data they contain.\\nBut sometimes Kubernetes itself requires you to pass credentials to it—for example,\\nwhen you’d like to use images from a private container image registry. This is also\\ndone through Secrets.\\n Up to now all your container images have been stored on public image registries,\\nwhich don’t require any special credentials to pull images from them. But most orga-\\nnizations don’t want their images to be available to everyone and thus use a private\\nimage registry. When deploying a pod, whose container images reside in a private reg-\\nistry, Kubernetes needs to know the credentials required to pull the image. Let’s see\\nhow to do that.\\nUSING A PRIVATE IMAGE REPOSITORY ON DOCKER HUB\\nDocker Hub, in addition to public image repositories, also allows you to create private\\nrepositories. You can mark a repository as private by logging in at http:/\\n/hub.docker\\n.com with your web browser, finding the repository and checking a checkbox. \\n To run a pod, which uses an image from the private repository, you need to do\\ntwo things:\\n\\uf0a1Create a Secret holding the credentials for the Docker registry.\\n\\uf0a1Reference that Secret in the imagePullSecrets field of the pod manifest.\\nListing 7.27\\nExposing a Secret’s entry as an environment variable\\nThe variable should be set \\nfrom the entry of a Secret.\\nThe name of the Secret \\nholding the key\\nThe key of the Secret \\nto expose\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMaps',\n",
       "    'description': 'a Kubernetes object that stores configuration data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'a Kubernetes object that stores sensitive information',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'environment variables',\n",
       "    'description': 'a way to pass values from outside a container to the application running inside it',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'configMapKeyRef',\n",
       "    'description': 'a reference to a ConfigMap in a Kubernetes object',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'secretKeyRef',\n",
       "    'description': 'a reference to a Secret in a Kubernetes object',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'interval',\n",
       "    'description': 'an environment variable used to set a time interval',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'an open-source container orchestration system',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'a containerization platform that allows you to build, ship, and run containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'imagePullSecrets',\n",
       "    'description': 'a field in a Kubernetes pod manifest that references a Secret containing credentials for pulling images from a registry',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Docker\", \"description\": \"allows creating private image repositories\", \"destination_entity\": \"private image registry\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"requires credentials to pull images from private registries\", \"destination_entity\": \"imagePullSecrets\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"enables exposing Secrets through environment variables\", \"destination_entity\": \"environment variables\"},\\n  {\"source_entity\": \"application\", \"description\": \"may dump environment variables in error reports or write them to the application log at startup\", \"destination_entity\": \"environment variables\"},\\n  {\"source_entity\": \"child process\", \"description\": \"inherits all the environment variables of the parent process\", \"destination_entity\": \"environment variables\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"needs to know credentials required to pull images from private registries\", \"destination_entity\": \"imagePullSecrets\"},\\n  {\"source_entity\": \"Docker Hub\", \"description\": \"allows creating private image repositories\", \"destination_entity\": \"private image registry\"},\\n  {\"source_entity\": \"Secret\", \"description\": \"holds the credentials for the Docker registry\", \"destination_entity\": \"imagePullSecrets\"},\\n  {\"source_entity\": \"pod manifest\", \"description\": \"references the Secret in the imagePullSecrets field\", \"destination_entity\": \"imagePullSecrets\"},\\n  {\"source_entity\": \"ConfigMaps\", \"description\": \"is used to refer to a ConfigMap by using configMapKeyRef\", \"destination_entity\": \"configMapKeyRef\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"enables exposing Secrets through environment variables and volumes\", \"destination_entity\": \"Secrets\"},\\n  {\"source_entity\": \"application\", \"description\": \"should not use environment variables to pass Secrets\", \"destination_entity\": \"environment variables\"},\\n  {\"source_entity\": \"secret volumes\", \"description\": \"is recommended for exposing Secrets\", \"destination_entity\": \"Secrets\"}\\n]\\n```'},\n",
       " {'page': 255,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '223\\nUsing Secrets to pass sensitive data to containers\\nCREATING A SECRET FOR AUTHENTICATING WITH A DOCKER REGISTRY\\nCreating a Secret holding the credentials for authenticating with a Docker registry\\nisn’t that different from creating the generic Secret you created in section 7.5.3. You\\nuse the same kubectl create secret command, but with a different type and\\noptions:\\n$ kubectl create secret docker-registry mydockerhubsecret \\\\\\n  --docker-username=myusername --docker-password=mypassword \\\\ \\n  --docker-email=my.email@provider.com\\nRather than create a generic secret, you’re creating a docker-registry Secret called\\nmydockerhubsecret. You’re specifying your Docker Hub username, password, and\\nemail. If you inspect the contents of the newly created Secret with kubectl describe,\\nyou’ll see that it includes a single entry called .dockercfg. This is equivalent to the\\n.dockercfg file in your home directory, which is created by Docker when you run the\\ndocker login command.\\nUSING THE DOCKER-REGISTRY SECRET IN A POD DEFINITION\\nTo have Kubernetes use the Secret when pulling images from your private Docker\\nHub repository, all you need to do is specify the Secret’s name in the pod spec, as\\nshown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: private-pod\\nspec:\\n  imagePullSecrets:                 \\n  - name: mydockerhubsecret         \\n  containers:\\n  - image: username/private:tag\\n    name: main\\nIn the pod definition in the previous listing, you’re specifying the mydockerhubsecret\\nSecret as one of the imagePullSecrets. I suggest you try this out yourself, because it’s\\nlikely you’ll deal with private container images soon.\\nNOT HAVING TO SPECIFY IMAGE PULL SECRETS ON EVERY POD\\nGiven that people usually run many different pods in their systems, it makes you won-\\nder if you need to add the same image pull Secrets to every pod. Luckily, that’s not the\\ncase. In chapter 12 you’ll learn how image pull Secrets can be added to all your pods\\nautomatically if you add the Secrets to a ServiceAccount.\\nListing 7.28\\nA pod definition using an image pull Secret: pod-with-private-image.yaml\\nThis enables pulling images \\nfrom a private image registry.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command for interacting with Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secret',\n",
       "    'description': 'an object that holds sensitive information, such as Docker Hub credentials',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'docker-registry',\n",
       "    'description': 'a Secret type for authenticating with a Docker registry',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Docker Hub',\n",
       "    'description': 'a container image registry',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'the basic execution unit in Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'imagePullSecrets',\n",
       "    'description': 'a field in a pod specification that allows specifying an image pull Secret',\n",
       "    'category': 'software'},\n",
       "   {'entity': '.dockercfg',\n",
       "    'description': 'a file created by Docker to store authentication credentials',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'an object that can be used to authenticate with the Kubernetes API server',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubernetes cluster',\n",
       "    'description': 'a managed collection of compute resources (e.g. nodes) that run containerized applications',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"create\", \"destination_entity\": \"secret\"},\\n  {\"source_entity\": \"kubernetes cluster\", \"description\": \"authenticate\", \"destination_entity\": \"docker-registry\"},\\n  {\"source_entity\": \"Secret\", \"description\": \"hold credentials\", \"destination_entity\": \"credentials\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"create\", \"destination_entity\": \"mydockerhubsecret\"},\\n  {\"source_entity\": \"kubernetes cluster\", \"description\": \"use Secret\", \"destination_entity\": \"pod definition\"},\\n  {\"source_entity\": \"imagePullSecrets\", \"description\": \"specify in pod spec\", \"destination_entity\": \"mydockerhubsecret\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"describe\", \"destination_entity\": \".dockercfg\"},\\n  {\"source_entity\": \"kubernetes cluster\", \"description\": \"add Secret to ServiceAccount\", \"destination_entity\": \"ServiceAccount\"},\\n  {\"source_entity\": \"Docker Hub\", \"description\": \"authenticate with credentials\", \"destination_entity\": \"docker-registry\"}\\n]'},\n",
       " {'page': 256,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '224\\nCHAPTER 7\\nConfigMaps and Secrets: configuring applications\\n7.6\\nSummary\\nThis wraps up this chapter on how to pass configuration data to containers. You’ve\\nlearned how to\\n\\uf0a1Override the default command defined in a container image in the pod definition\\n\\uf0a1Pass command-line arguments to the main container process\\n\\uf0a1Set environment variables for a container\\n\\uf0a1Decouple configuration from a pod specification and put it into a ConfigMap\\n\\uf0a1Store sensitive data in a Secret and deliver it securely to containers\\n\\uf0a1Create a docker-registry Secret and use it to pull images from a private image\\nregistry\\nIn the next chapter, you’ll learn how to pass pod and container metadata to applica-\\ntions running inside them. You’ll also see how the default token Secret, which we\\nlearned about in this chapter, is used to talk to the API server from within a pod. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ConfigMaps',\n",
       "    'description': 'A Kubernetes feature that allows decoupling configuration data from a pod specification',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'A Kubernetes feature for storing sensitive data and delivering it securely to containers',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Docker-registry Secret',\n",
       "    'description': 'A type of Secret used to pull images from a private image registry',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Pod definition',\n",
       "    'description': 'A Kubernetes resource that defines a pod and its configuration',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Container image',\n",
       "    'description': 'An image used to run a container in a Kubernetes pod',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Pod specification',\n",
       "    'description': 'A configuration file that defines a pod and its resources',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'ConfigMap',\n",
       "    'description': 'A Kubernetes feature for decoupling configuration data from a pod specification',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Environment variables',\n",
       "    'description': 'Variables used to configure and customize containers in a Kubernetes pod',\n",
       "    'category': 'process,container'},\n",
       "   {'entity': 'Command-line arguments',\n",
       "    'description': 'Arguments passed to the main container process in a Kubernetes pod',\n",
       "    'category': 'process,command'},\n",
       "   {'entity': 'Container process',\n",
       "    'description': 'The running application or service within a Kubernetes container',\n",
       "    'category': 'process,application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Override\", \"description\": \"override the default command defined in a container image\", \"destination_entity\": \"Container image\"},\\n  {\"source_entity\": \"Override\", \"description\": \"override the default command defined in a pod definition\", \"destination_entity\": \"Pod definition\"},\\n  {\"source_entity\": \"Pass\", \"description\": \"pass command-line arguments to the main container process\", \"destination_entity\": \"Container process\"},\\n  {\"source_entity\": \"Set\", \"description\": \"set environment variables for a container\", \"destination_entity\": \"Environment variables\"},\\n  {\"source_entity\": \"Decouple\", \"description\": \"decouple configuration from a pod specification and put it into a ConfigMap\", \"destination_entity\": \"ConfigMaps\"},\\n  {\"source_entity\": \"Store\", \"description\": \"store sensitive data in a Secret\", \"destination_entity\": \"Secrets\"},\\n  {\"source_entity\": \"Create\", \"description\": \"create a docker-registry Secret and use it to pull images from a private image registry\", \"destination_entity\": \"Docker-registry Secret\"},\\n  {\"source_entity\": \"Pass\", \"description\": \"pass pod and container metadata to applications running inside them\", \"destination_entity\": \"Applications\"},\\n  {\"source_entity\": \"Use\", \"description\": \"use the default token Secret to talk to the API server from within a pod\", \"destination_entity\": \"API server\"}\\n]\\n\\nNote that I\\'ve only included relations where there is an action being performed on one of the entities in the list. Let me know if you have any questions or need further clarification!'},\n",
       " {'page': 257,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '225\\nAccessing pod metadata\\nand other resources\\nfrom applications\\nApplications often need information about the environment they’re running in,\\nincluding details about themselves and that of other components in the cluster.\\nYou’ve already seen how Kubernetes enables service discovery through environ-\\nment variables or DNS, but what about other information? In this chapter, you’ll\\nsee how certain pod and container metadata can be passed to the container and\\nhow easy it is for an app running inside a container to talk to the Kubernetes API\\nserver to get information about the resources deployed in the cluster and even how\\nto create or modify those resources.\\nThis chapter covers\\n\\uf0a1Using the Downward API to pass information into \\ncontainers\\n\\uf0a1Exploring the Kubernetes REST API\\n\\uf0a1Leaving authentication and server verification to \\nkubectl proxy\\n\\uf0a1Accessing the API server from within a container\\n\\uf0a1Understanding the ambassador container pattern\\n\\uf0a1Using Kubernetes client libraries\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'pod',\n",
       "    'description': 'A pod is a logical host in Kubernetes that can contain one or more containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Information about the environment and resources deployed in the cluster.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Downward API',\n",
       "    'description': 'A feature of Kubernetes that allows passing information from the pod to the container.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'The smallest unit of deployment in Kubernetes, which can contain an application and its dependencies.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Kubernetes REST API',\n",
       "    'description': 'An interface for interacting with the Kubernetes cluster programmatically.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl proxy',\n",
       "    'description': 'A command-line tool that provides a secure way to interact with the Kubernetes API server from within a container.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ambassador container pattern',\n",
       "    'description': 'A design pattern that allows an application running inside a container to communicate with the Kubernetes API server.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubernetes client libraries',\n",
       "    'description': 'Pre-built software components that provide access to the Kubernetes API from applications.',\n",
       "    'category': 'library'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Downward API\", \"description\": \"passes information into containers\", \"destination_entity\": \"containers\"},\\n  {\"source_entity\": \"ambassador container pattern\", \"description\": \"acts as an ambassador for container communication with Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"metadata\", \"description\": \"includes details about pod and other components in the cluster\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Kubernetes REST API\", \"description\": \"provides information to applications running inside containers\", \"destination_entity\": \"applications\"},\\n  {\"source_entity\": \"kubectl proxy\", \"description\": \"leaves authentication and server verification for\", \"destination_entity\": \"authentication\"},\\n  {\"source_entity\": \"containers\", \"description\": \"can be accessed from within Kubernetes cluster using Downward API\", \"destination_entity\": \"Kubernetes cluster\"},\\n  {\"source_entity\": \"pod\", \"description\": \"contains metadata about itself and other components in the cluster\", \"destination_entity\": \"metadata\"},\\n  {\"source_entity\": \"Kubernetes client libraries\", \"description\": \"are used to interact with Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"}\\n]\\n```'},\n",
       " {'page': 258,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '226\\nCHAPTER 8\\nAccessing pod metadata and other resources from applications\\n8.1\\nPassing metadata through the Downward API\\nIn the previous chapter you saw how you can pass configuration data to your appli-\\ncations through environment variables or through configMap and secret volumes.\\nThis works well for data that you set yourself and that is known before the pod is\\nscheduled to a node and run there. But what about data that isn’t known up until\\nthat point—such as the pod’s IP, the host node’s name, or even the pod’s own name\\n(when the name is generated; for example, when the pod is created by a ReplicaSet\\nor similar controller)? And what about data that’s already specified elsewhere, such\\nas a pod’s labels and annotations? You don’t want to repeat the same information in\\nmultiple places.\\n Both these problems are solved by the Kubernetes Downward API. It allows you to\\npass metadata about the pod and its environment through environment variables or\\nfiles (in a downwardAPI volume). Don’t be confused by the name. The Downward API\\nisn’t like a REST endpoint that your app needs to hit so it can get the data. It’s a way of\\nhaving environment variables or files populated with values from the pod’s specifica-\\ntion or status, as shown in figure 8.1.\\n8.1.1\\nUnderstanding the available metadata\\nThe Downward API enables you to expose the pod’s own metadata to the processes\\nrunning inside that pod. Currently, it allows you to pass the following information to\\nyour containers:\\n\\uf0a1The pod’s name\\n\\uf0a1The pod’s IP address\\nContainer: main\\nEnvironment\\nvariables\\nAPI server\\nUsed to initialize environment\\nvariables and ﬁles in the\\ndownwardAPI volume\\nPod manifest\\n- Metadata\\n- Status\\nPod\\ndownwardAPI\\nvolume\\nApp process\\nFigure 8.1\\nThe Downward API exposes pod metadata through environment variables or files.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes Downward API',\n",
       "    'description': \"A way of having environment variables or files populated with values from the pod's specification or status.\",\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Pod Name',\n",
       "    'description': 'The name of the pod, which can be generated by a ReplicaSet or similar controller.',\n",
       "    'category': 'Metadata/Object'},\n",
       "   {'entity': 'Pod IP Address',\n",
       "    'description': 'The IP address of the pod.',\n",
       "    'category': 'Metadata/Object'},\n",
       "   {'entity': \"Host Node's Name\",\n",
       "    'description': 'The name of the host node where the pod is running.',\n",
       "    'category': 'Metadata/Object'},\n",
       "   {'entity': 'Pod Labels and Annotations',\n",
       "    'description': \"Labels and annotations specified in the pod's manifest.\",\n",
       "    'category': 'Metadata/Object'},\n",
       "   {'entity': 'Environment Variables',\n",
       "    'description': 'Variables used to pass configuration data to applications.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'ConfigMap',\n",
       "    'description': 'A way of passing configuration data to applications through environment variables or configMap volumes.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Secret Volumes',\n",
       "    'description': 'Volumes used to pass sensitive information to applications.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'DownwardAPI Volume',\n",
       "    'description': \"A volume that exposes the pod's metadata through environment variables or files.\",\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A controller used to create multiple replicas of a pod.',\n",
       "    'category': 'Software/Application'}],\n",
       "  'relationships': '[{\"source_entity\": \"Kubernetes Downward API\", \"description\": \"exposes pod metadata through environment variables or files\", \"destination_entity\": \"Pod Metadata\"}, \\n{\"source_entity\": \"Pod Manifest\", \"description\": \"contains metadata about the pod and its environment\", \"destination_entity\": \"DownwardAPI Volume\"}, \\n{\"source_entity\": \"API Server\", \"description\": \"initializes environment variables and files in the DownwardAPI volume\", \"destination_entity\": \"Environment Variables\"}, \\n{\"source_entity\": \"Pod Manifest\", \"description\": \"contains metadata about the pod and its environment\", \"destination_entity\": \"ConfigMap\"}, \\n{\"source_entity\": \"ReplicaSet\", \"description\": \"generates a pod name when creating a new pod\", \"destination_entity\": \"Pod Name\"}, \\n{\"source_entity\": \"DownwardAPI Volume\", \"description\": \"exposes information from the pod\\'s specification or status\", \"destination_entity\": \"Environment Variables\"}, \\n{\"source_entity\": \"Pod DownwardAPI volume\", \"description\": \"contains metadata about the pod and its environment\", \"destination_entity\": \"App Process\"}, \\n{\"source_entity\": \"ConfigMap\", \"description\": \"stores configuration data for applications\", \"destination_entity\": \"Secret Volumes\"}, \\n{\"source_entity\": \"ReplicaSet\", \"description\": \"uses a DownwardAPI volume to pass metadata to the pod\", \"destination_entity\": \"DownwardAPI Volume\"}, \\n{\"source_entity\": \"Pod Labels and Annotations\", \"description\": \"contains information about the pod\\'s labels and annotations\", \"destination_entity\": \"DownwardAPI Volume\"}]'},\n",
       " {'page': 259,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '227\\nPassing metadata through the Downward API\\n\\uf0a1The namespace the pod belongs to\\n\\uf0a1The name of the node the pod is running on\\n\\uf0a1The name of the service account the pod is running under\\n\\uf0a1The CPU and memory requests for each container\\n\\uf0a1The CPU and memory limits for each container\\n\\uf0a1The pod’s labels\\n\\uf0a1The pod’s annotations\\nMost of the items in the list shouldn’t require further explanation, except perhaps the\\nservice account and CPU/memory requests and limits, which we haven’t introduced\\nyet. We’ll cover service accounts in detail in chapter 12. For now, all you need to know\\nis that a service account is the account that the pod authenticates as when talking to\\nthe API server. CPU and memory requests and limits are explained in chapter 14.\\nThey’re the amount of CPU and memory guaranteed to a container and the maxi-\\nmum amount it can get.\\n Most items in the list can be passed to containers either through environment vari-\\nables or through a downwardAPI volume, but labels and annotations can only be\\nexposed through the volume. Part of the data can be acquired by other means (for\\nexample, from the operating system directly), but the Downward API provides a sim-\\npler alternative.\\n Let’s look at an example to pass metadata to your containerized process.\\n8.1.2\\nExposing metadata through environment variables\\nFirst, let’s look at how you can pass the pod’s and container’s metadata to the con-\\ntainer through environment variables. You’ll create a simple single-container pod\\nfrom the following listing’s manifest.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: downward\\nspec:\\n  containers:\\n  - name: main\\n    image: busybox\\n    command: [\"sleep\", \"9999999\"]\\n    resources:\\n      requests:\\n        cpu: 15m\\n        memory: 100Ki\\n      limits:\\n        cpu: 100m\\n        memory: 4Mi\\n    env:\\n    - name: POD_NAME\\nListing 8.1\\nDownward API used in environment variables: downward-api-env.yaml\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'namespace',\n",
       "    'description': 'The namespace the pod belongs to',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'The name of the node the pod is running on',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'service account',\n",
       "    'description': 'The account that the pod authenticates as when talking to the API server',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'CPU request',\n",
       "    'description': 'The amount of CPU guaranteed to a container',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'memory request',\n",
       "    'description': 'The amount of memory guaranteed to a container',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'CPU limit',\n",
       "    'description': 'The maximum amount of CPU a container can get',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'memory limit',\n",
       "    'description': 'The maximum amount of memory a container can get',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'labels', 'description': 'Pod’s labels', 'category': 'database'},\n",
       "   {'entity': 'annotations',\n",
       "    'description': 'Pod’s annotations',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Downward API volume',\n",
       "    'description': 'A volume that exposes metadata through the Downward API',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'environment variables',\n",
       "    'description': 'Variables passed to containers through environment variables',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod’s name',\n",
       "    'description': 'The name of the pod',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'container’s metadata',\n",
       "    'description': 'Metadata related to the container',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The API server that the pod talks to',\n",
       "    'category': 'network'}],\n",
       "  'relationships': '[{\"source_entity\": \"node\", \"description\": \"provides running location for pod\", \"destination_entity\": \"pod\"},\\n {\"source_entity\": \"labels\", \"description\": \"can be passed to container through Downward API volume\", \"destination_entity\": \"container\"},\\n {\"source_entity\": \"Downward API volume\", \"description\": \"exposes pod\\'s and container\\'s metadata\", \"destination_entity\": \"container\"},\\n {\"source_entity\": \"Pod’s name\", \"description\": \"passed to container as environment variable\", \"destination_entity\": \"container\"},\\n {\"source_entity\": \"memory limit\", \"description\": \"guaranteed to container, can be passed through Downward API volume\", \"destination_entity\": \"container\"},\\n {\"source_entity\": \"container’s metadata\", \"description\": \"can be exposed through Downward API volume\", \"destination_entity\": \"API server\"},\\n {\"source_entity\": \"CPU limit\", \"description\": \"maximum amount of CPU that container can get, can be passed through Downward API volume\", \"destination_entity\": \"container\"},\\n {\"source_entity\": \"annotations\", \"description\": \"can only be exposed through Downward API volume\", \"destination_entity\": \"container\"},\\n {\"source_entity\": \"namespace\", \"description\": \"the pod belongs to, can be passed as environment variable or through Downward API volume\", \"destination_entity\": \"pod\"},\\n {\"source_entity\": \"environment variables\", \"description\": \"can pass metadata to container\", \"destination_entity\": \"container\"},\\n {\"source_entity\": \"CPU request\", \"description\": \"amount of CPU guaranteed to container, can be passed through Downward API volume\", \"destination_entity\": \"container\"},\\n {\"source_entity\": \"memory request\", \"description\": \"amount of memory guaranteed to container, can be passed through Downward API volume\", \"destination_entity\": \"container\"},\\n {\"source_entity\": \"service account\", \"description\": \"the account that pod authenticates as when talking to API server\", \"destination_entity\": \"API server\"},\\n {\"source_entity\": \"API server\", \"description\": \"the server that pod talks to, authenticated by service account\", \"destination_entity\": \"service account\"}]'},\n",
       " {'page': 260,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '228\\nCHAPTER 8\\nAccessing pod metadata and other resources from applications\\n      valueFrom:                            \\n        fieldRef:                           \\n          fieldPath: metadata.name          \\n    - name: POD_NAMESPACE\\n      valueFrom:\\n        fieldRef:\\n          fieldPath: metadata.namespace\\n    - name: POD_IP\\n      valueFrom:\\n        fieldRef:\\n          fieldPath: status.podIP\\n    - name: NODE_NAME\\n      valueFrom:\\n        fieldRef:\\n          fieldPath: spec.nodeName\\n    - name: SERVICE_ACCOUNT\\n      valueFrom:\\n        fieldRef:\\n          fieldPath: spec.serviceAccountName\\n    - name: CONTAINER_CPU_REQUEST_MILLICORES\\n      valueFrom:                                   \\n        resourceFieldRef:                          \\n          resource: requests.cpu                   \\n          divisor: 1m                            \\n    - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES\\n      valueFrom:\\n        resourceFieldRef:\\n          resource: limits.memory\\n          divisor: 1Ki\\nWhen your process runs, it can look up all the environment variables you defined in\\nthe pod spec. Figure 8.2 shows the environment variables and the sources of their val-\\nues. The pod’s name, IP, and namespace will be exposed through the POD_NAME,\\nPOD_IP, and POD_NAMESPACE environment variables, respectively. The name of the\\nnode the container is running on will be exposed through the NODE_NAME variable.\\nThe name of the service account is made available through the SERVICE_ACCOUNT\\nenvironment variable. You’re also creating two environment variables that will hold\\nthe amount of CPU requested for this container and the maximum amount of mem-\\nory the container is allowed to consume.\\n For environment variables exposing resource limits or requests, you specify a divi-\\nsor. The actual value of the limit or the request will be divided by the divisor and the\\nresult exposed through the environment variable. In the previous example, you’re set-\\nting the divisor for CPU requests to 1m (one milli-core, or one one-thousandth of a\\nCPU core). Because you’ve set the CPU request to 15m, the environment variable\\nCONTAINER_CPU_REQUEST_MILLICORES will be set to 15. Likewise, you set the memory\\nlimit to 4Mi (4 mebibytes) and the divisor to 1Ki (1 Kibibyte), so the CONTAINER_MEMORY\\n_LIMIT_KIBIBYTES environment variable will be set to 4096. \\nInstead of specifying an absolute value, \\nyou’re referencing the metadata.name \\nfield from the pod manifest.\\nA container’s CPU and memory \\nrequests and limits are referenced \\nby using resourceFieldRef instead \\nof fieldRef.\\nFor resource fields, you \\ndefine a divisor to get the \\nvalue in the unit you need.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'POD_NAME',\n",
       "    'description': \"Environment variable that holds the pod's name\",\n",
       "    'category': 'environment variable'},\n",
       "   {'entity': 'POD_IP',\n",
       "    'description': \"Environment variable that holds the pod's IP address\",\n",
       "    'category': 'environment variable'},\n",
       "   {'entity': 'POD_NAMESPACE',\n",
       "    'description': \"Environment variable that holds the pod's namespace\",\n",
       "    'category': 'environment variable'},\n",
       "   {'entity': 'NODE_NAME',\n",
       "    'description': 'Environment variable that holds the node name the container is running on',\n",
       "    'category': 'environment variable'},\n",
       "   {'entity': 'SERVICE_ACCOUNT',\n",
       "    'description': 'Environment variable that holds the service account name',\n",
       "    'category': 'environment variable'},\n",
       "   {'entity': 'CONTAINER_CPU_REQUEST_MILLICORES',\n",
       "    'description': 'Environment variable that holds the amount of CPU requested for this container',\n",
       "    'category': 'environment variable'},\n",
       "   {'entity': 'CONTAINER_MEMORY_LIMIT_KIBIBYTES',\n",
       "    'description': 'Environment variable that holds the maximum amount of memory the container is allowed to consume',\n",
       "    'category': 'environment variable'},\n",
       "   {'entity': 'metadata.name',\n",
       "    'description': \"Field reference to the pod's name from the pod manifest\",\n",
       "    'category': 'field reference'},\n",
       "   {'entity': 'status.podIP',\n",
       "    'description': \"Field reference to the pod's IP address from the pod status\",\n",
       "    'category': 'field reference'},\n",
       "   {'entity': 'spec.nodeName',\n",
       "    'description': 'Field reference to the node name from the pod spec',\n",
       "    'category': 'field reference'},\n",
       "   {'entity': 'spec.serviceAccountName',\n",
       "    'description': 'Field reference to the service account name from the pod spec',\n",
       "    'category': 'field reference'},\n",
       "   {'entity': 'requests.cpu',\n",
       "    'description': \"Resource field reference to the container's CPU request\",\n",
       "    'category': 'resource field reference'},\n",
       "   {'entity': 'limits.memory',\n",
       "    'description': \"Resource field reference to the container's memory limit\",\n",
       "    'category': 'resource field reference'},\n",
       "   {'entity': '1m',\n",
       "    'description': 'Divisor for CPU requests, representing one milli-core or one one-thousandth of a CPU core',\n",
       "    'category': 'divisor'},\n",
       "   {'entity': '4Mi',\n",
       "    'description': 'Absolute value for memory limit, representing 4 mebibytes',\n",
       "    'category': 'absolute value'},\n",
       "   {'entity': '1Ki',\n",
       "    'description': 'Divisor for memory limits, representing 1 Kibibyte',\n",
       "    'category': 'divisor'}],\n",
       "  'relationships': '[{\"source_entity\": \"fieldRef\", \"description\": \"references metadata.name field from pod manifest to get absolute value for POD_NAME environment variable\", \"destination_entity\": \"POD_NAME\"}, {\"source_entity\": \"resourceFieldRef\", \"description\": \"uses requests.cpu resource to get CPU request value for CONTAINER_CPU_REQUEST_MILLICORES environment variable\", \"destination_entity\": \"CONTAINER_CPU_REQUEST_MILLICORES\"}, {\"source_entity\": \"fieldRef\", \"description\": \"references status.podIP field to get pod\\'s IP address for POD_IP environment variable\", \"destination_entity\": \"POD_IP\"}, {\"source_entity\": \"resourceFieldRef\", \"description\": \"uses limits.memory resource to get memory limit value for CONTAINER_MEMORY_LIMIT_KIBIBYTES environment variable\", \"destination_entity\": \"CONTAINER_MEMORY_LIMIT_KIBIBYTES\"}, {\"source_entity\": \"fieldRef\", \"description\": \"references spec.nodeName field to get name of node where container is running for NODE_NAME environment variable\", \"destination_entity\": \"NODE_NAME\"}, {\"source_entity\": \"resourceFieldRef\", \"description\": \"uses requests.cpu resource with divisor 1m to get CPU request value in milli-cores for CONTAINER_CPU_REQUEST_MILLICORES environment variable\", \"destination_entity\": \"CONTAINER_CPU_REQUEST_MILLICORES\"}, {\"source_entity\": \"fieldRef\", \"description\": \"references spec.serviceAccountName field to get name of service account for SERVICE_ACCOUNT environment variable\", \"destination_entity\": \"SERVICE_ACCOUNT\"}, {\"source_entity\": \"resourceFieldRef\", \"description\": \"uses limits.memory resource with divisor 1Ki to get memory limit value in Kibibytes for CONTAINER_MEMORY_LIMIT_KIBIBYTES environment variable\", \"destination_entity\": \"CONTAINER_MEMORY_LIMIT_KIBIBYTES\"}, {\"source_entity\": \"fieldRef\", \"description\": \"references metadata.namespace field to get namespace of pod for POD_NAMESPACE environment variable\", \"destination_entity\": \"POD_NAMESPACE\"}, {\"source_entity\": \"resourceFieldRef\", \"description\": \"uses requests.cpu resource with divisor 1m to get CPU request value in milli-cores for CONTAINER_CPU_REQUEST_MILLICORES environment variable\", \"destination_entity\": \"CONTAINER_CPU_REQUEST_MILLICORES\"}, {\"source_entity\": \"fieldRef\", \"description\": \"references spec.nodeName field to get name of node where container is running for NODE_NAME environment variable\", \"destination_entity\": \"NODE_NAME\"}, {\"source_entity\": \"fieldRef\", \"description\": \"references status.podIP field to get pod\\'s IP address for POD_IP environment variable\", \"destination_entity\": \"POD_IP\"}]'},\n",
       " {'page': 261,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '229\\nPassing metadata through the Downward API\\nThe divisor for CPU limits and requests can be either 1, which means one whole core,\\nor 1m, which is one millicore. The divisor for memory limits/requests can be 1 (byte),\\n1k (kilobyte) or 1Ki (kibibyte), 1M (megabyte) or 1Mi (mebibyte), and so on.\\n After creating the pod, you can use kubectl exec to see all these environment vari-\\nables in your container, as shown in the following listing.\\n$ kubectl exec downward env\\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\nHOSTNAME=downward\\nCONTAINER_MEMORY_LIMIT_KIBIBYTES=4096\\nPOD_NAME=downward\\nPOD_NAMESPACE=default\\nPOD_IP=10.0.0.10\\nNODE_NAME=gke-kubia-default-pool-32a2cac8-sgl7\\nSERVICE_ACCOUNT=default\\nCONTAINER_CPU_REQUEST_MILLICORES=15\\nKUBERNETES_SERVICE_HOST=10.3.240.1\\nKUBERNETES_SERVICE_PORT=443\\n...\\nListing 8.2\\nEnvironment variables in the downward pod\\nPod manifest\\nmetadata:\\nname: downward\\nnamespace: default\\nspec:\\nnodeName: minikube\\nserviceAccountName: default\\ncontainers:\\n- name: main\\nimage: busybox\\ncommand: [\"sleep\", \"9999999\"]\\nresources:\\nrequests:\\ncpu: 15m\\nmemory: 100Ki\\nlimits:\\ncpu: 100m\\nmemory: 4Mi\\n...\\nstatus:\\npodIP: 172.17.0.4\\n...\\nPod: downward\\nContainer: main\\nEnvironment variables\\nPOD_NAME=downward\\nPOD_NAMESPACE=default\\nPOD_IP=172.17.0.4\\nNODE_NAME=minikube\\nSERVICE_ACCOUNT=default\\nCONTAINER_CPU_REQUEST_MILLICORES=15\\nCONTAINER_MEMORY_LIMIT_KIBIBYTES=4096\\ndivisor: 1m\\ndivisor: 1Ki\\nFigure 8.2\\nPod metadata and attributes can be exposed to the pod through environment variables.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Container: main\n",
       "   Environment variables\n",
       "   POD_NAME=downward\n",
       "   POD_NAMESPACE=default\n",
       "   POD_IP=172.17.0.4\n",
       "   NODE_NAME=minikube\n",
       "   SERVICE_ACCOUNT=default divisor\n",
       "   CONTAINER_CPU_REQUEST_MILLICORES=15\n",
       "   CONTAINER_MEMORY_LIMIT_KIBIBYTES=4096\n",
       "   divisor\n",
       "   Pod: downward, Pod manifest\n",
       "   metadata:\n",
       "   name: downward\n",
       "   namespace: default\n",
       "   spec:\n",
       "   nodeName: minikube\n",
       "   serviceAccountName: default\n",
       "   containers:\n",
       "   - name: main\n",
       "   image: busybox\n",
       "   command: [\"sleep\", \"9999999\"]\n",
       "   resources:\n",
       "   requests:\n",
       "   : 1m\n",
       "   cpu: 15m\n",
       "   memory: 100Ki\n",
       "   limits:\n",
       "   cpu: 100m\n",
       "   : 1Ki\n",
       "   memory: 4Mi\n",
       "   ...\n",
       "   status:\n",
       "   podIP: 172.17.0.4\n",
       "   ...]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [r, :]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [r, :]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'CPU',\n",
       "    'description': 'A measure of processing power, represented by cores or millicores.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'memory',\n",
       "    'description': 'The amount of data that can be stored or processed at one time.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'docker',\n",
       "    'description': 'A containerization platform for running applications in isolated environments.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubernetes',\n",
       "    'description': 'An orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line interface for interacting with a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'exec',\n",
       "    'description': 'A command used to run commands within a container or pod.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'env',\n",
       "    'description': 'A command used to display environment variables within a container or pod.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A basic execution unit in Kubernetes, comprising one or more containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'A logical partitioning of resources within a Kubernetes cluster.',\n",
       "    'category': 'network/application'},\n",
       "   {'entity': 'serviceAccount',\n",
       "    'description': 'An identity used to authenticate and authorize access to a Kubernetes cluster.',\n",
       "    'category': 'software/network'},\n",
       "   {'entity': 'containerMemoryLimitKibibytes',\n",
       "    'description': 'A configuration option for setting the memory limit of a container.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'cpuRequestMilliCoreS',\n",
       "    'description': 'A configuration option for setting the CPU request of a container.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'divisor',\n",
       "    'description': 'A value used to express CPU and memory limits in millicores or kilobytes.',\n",
       "    'category': 'software/process'},\n",
       "   {'entity': 'busybox',\n",
       "    'description': 'A lightweight Linux distribution used as the base image for a container.',\n",
       "    'category': 'image/container'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"execute command to view environment variables in container\",\\n    \"destination_entity\": \"env\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"execute command to run exec and view environment variables\",\\n    \"destination_entity\": \"exec\"\\n  },\\n  {\\n    \"source_entity\": \"memory\",\\n    \"description\": \"set limit for container memory requests\",\\n    \"destination_entity\": \"containerMemoryLimitKibibytes\"\\n  },\\n  {\\n    \"source_entity\": \"serviceAccount\",\\n    \"description\": \"set service account name for pod\",\\n    \"destination_entity\": \"SERVICE_ACCOUNT\"\\n  },\\n  {\\n    \"source_entity\": \"docker\",\\n    \"description\": \"pull image from Docker registry\",\\n    \"destination_entity\": \"busybox\"\\n  },\\n  {\\n    \"source_entity\": \"kubernetes\",\\n    \"description\": \"expose pod metadata and attributes through environment variables\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"namespace\",\\n    \"description\": \"set namespace name for pod\",\\n    \"destination_entity\": \"default\"\\n  },\\n  {\\n    \"source_entity\": \"containerMemoryLimitKibibytes\",\\n    \"description\": \"set limit for container memory requests\",\\n    \"destination_entity\": \"memory\"\\n  },\\n  {\\n    \"source_entity\": \"env\",\\n    \"description\": \"view environment variables in container\",\\n    \"destination_entity\": \"kubectl\"\\n  },\\n  {\\n    \"source_entity\": \"exec\",\\n    \"description\": \"execute command to view environment variables\",\\n    \"destination_entity\": \"kubectl\"\\n  },\\n  {\\n    \"source_entity\": \"divisor\",\\n    \"description\": \"set divisor for CPU and memory limits/requests\",\\n    \"destination_entity\": \"cpuRequestMilliCoreS\"\\n  },\\n  {\\n    \"source_entity\": \"CPU\",\\n    \"description\": \"set limit for container CPU requests\",\\n    \"destination_entity\": \"cpuRequestMilliCoreS\"\\n  },\\n  {\\n    \"source_entity\": \"busybox\",\\n    \"description\": \"pull image from Docker registry\",\\n    \"destination_entity\": \"docker\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"expose metadata and attributes through environment variables\",\\n    \"destination_entity\": \"kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"cpuRequestMilliCoreS\",\\n    \"description\": \"set limit for container CPU requests\",\\n    \"destination_entity\": \"CPU\"\\n  }\\n]\\n```\\n\\nNote: I have only considered the entities provided in the input list, even though some of them may not be explicitly mentioned in the document page.'},\n",
       " {'page': 262,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '230\\nCHAPTER 8\\nAccessing pod metadata and other resources from applications\\nAll processes running inside the container can read those variables and use them how-\\never they need. \\n8.1.3\\nPassing metadata through files in a downwardAPI volume\\nIf you prefer to expose the metadata through files instead of environment variables,\\nyou can define a downwardAPI volume and mount it into your container. You must use\\na downwardAPI volume for exposing the pod’s labels or its annotations, because nei-\\nther can be exposed through environment variables. We’ll discuss why later.\\n As with environment variables, you need to specify each metadata field explicitly if\\nyou want to have it exposed to the process. Let’s see how to modify the previous exam-\\nple to use a volume instead of environment variables, as shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: downward\\n  labels:                  \\n    foo: bar               \\n  annotations:             \\n    key1: value1           \\n    key2: |                \\n      multi                \\n      line                 \\n      value                \\nspec:\\n  containers:\\n  - name: main\\n    image: busybox\\n    command: [\"sleep\", \"9999999\"]\\n    resources:\\n      requests:\\n        cpu: 15m\\n        memory: 100Ki\\n      limits:\\n        cpu: 100m\\n        memory: 4Mi\\n    volumeMounts:                        \\n    - name: downward                     \\n      mountPath: /etc/downward           \\n  volumes:\\n  - name: downward                 \\n    downwardAPI:                   \\n      items:\\n      - path: \"podName\"                     \\n        fieldRef:                           \\n          fieldPath: metadata.name          \\n      - path: \"podNamespace\"\\n        fieldRef:\\n          fieldPath: metadata.namespace\\nListing 8.3\\nPod with a downwardAPI volume: downward-api-volume.yaml\\nThese labels and \\nannotations will be \\nexposed through the \\ndownwardAPI volume.\\nYou’re mounting the \\ndownward volume \\nunder /etc/downward.\\nYou’re defining a downwardAPI \\nvolume with the name downward.\\nThe pod’s name (from the metadata.name \\nfield in the manifest) will be written to \\nthe podName file.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Environment Variables',\n",
       "    'description': 'Variables exposed to processes running inside a container',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Downward API Volume',\n",
       "    'description': 'A type of volume that exposes metadata from the pod to the container',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A Kubernetes object representing a logical host for one or more containers',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Labels',\n",
       "    'description': 'Key-value pairs attached to a pod, used for identification and selection',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Annotations',\n",
       "    'description': 'Key-value pairs attached to a pod, used for additional metadata',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'FieldRef',\n",
       "    'description': \"A reference to a field in the pod's metadata\",\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'fieldPath',\n",
       "    'description': \"A path expression that selects a field from the pod's metadata\",\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'API Version v1',\n",
       "    'description': 'The API version for Kubernetes resources',\n",
       "    'category': 'Software/Framework'},\n",
       "   {'entity': 'Kind: Pod',\n",
       "    'description': 'The type of resource being defined (in this case, a pod)',\n",
       "    'category': 'Software/Framework'},\n",
       "   {'entity': 'Metadata Name',\n",
       "    'description': \"A field in the pod's metadata containing the name of the pod\",\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': \"A field in the pod's metadata containing the namespace of the pod\",\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Busybox Image',\n",
       "    'description': 'The image used for the container',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'A lightweight and standalone execution environment',\n",
       "    'category': 'Software/Application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Environment Variables\",\\n    \"description\": \"can be read by processes running inside the container\",\\n    \"destination_entity\": \"Processes\"\\n  },\\n  {\\n    \"source_entity\": \"Downward API Volume\",\\n    \"description\": \"exposes metadata fields to the process explicitly specified\",\\n    \"destination_entity\": \"Metadata Fields\"\\n  },\\n  {\\n    \"source_entity\": \"Pod Name\",\\n    \"description\": \"will be written to the podName file by Downward API Volume\",\\n    \"destination_entity\": \"File (podName)\"\\n  },\\n  {\\n    \"source_entity\": \"Downward API Volume\",\\n    \"description\": \"exposes labels and annotations of the Pod\",\\n    \"destination_entity\": \"Labels and Annotations\"\\n  },\\n  {\\n    \"source_entity\": \"Downward API Volume\",\\n    \"description\": \"can be used to expose metadata through files instead of environment variables\",\\n    \"destination_entity\": \"Environment Variables\"\\n  },\\n  {\\n    \"source_entity\": \"Metadata Fields\",\\n    \"description\": \"are explicitly specified when using Downward API Volume\",\\n    \"destination_entity\": \"Downward API Volume\"\\n  },\\n  {\\n    \"source_entity\": \"Busybox Image\",\\n    \"description\": \"is used as the image for the Container\",\\n    \"destination_entity\": \"Container\"\\n  },\\n  {\\n    \"source_entity\": \"API Version v1\",\\n    \"description\": \"specifies the version of the API being used\",\\n    \"destination_entity\": \"API\"\\n  },\\n  {\\n    \"source_entity\": \"Kind: Pod\",\\n    \"description\": \"defines the type of resource being created\",\\n    \"destination_entity\": \"Resource\"\\n  },\\n  {\\n    \"source_entity\": \"Labels and Annotations\",\\n    \"description\": \"are exposed through the Downward API Volume\",\\n    \"destination_entity\": \"Downward API Volume\"\\n  },\\n  {\\n    \"source_entity\": \"FieldRef\",\\n    \"description\": \"is used to specify the field path for metadata fields\",\\n    \"destination_entity\": \"Metadata Fields\"\\n  }\\n]'},\n",
       " {'page': 263,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '231\\nPassing metadata through the Downward API\\n      - path: \"labels\"                       \\n        fieldRef:                            \\n          fieldPath: metadata.labels         \\n      - path: \"annotations\"                       \\n        fieldRef:                                 \\n          fieldPath: metadata.annotations         \\n      - path: \"containerCpuRequestMilliCores\"\\n        resourceFieldRef:\\n          containerName: main\\n          resource: requests.cpu\\n          divisor: 1m\\n      - path: \"containerMemoryLimitBytes\"\\n        resourceFieldRef:\\n          containerName: main\\n          resource: limits.memory\\n          divisor: 1\\nInstead of passing the metadata through environment variables, you’re defining a vol-\\nume called downward and mounting it in your container under /etc/downward. The\\nfiles this volume will contain are configured under the downwardAPI.items attribute\\nin the volume specification.\\n Each item specifies the path (the filename) where the metadata should be written\\nto and references either a pod-level field or a container resource field whose value you\\nwant stored in the file (see figure 8.3).\\nThe pod’s labels will be written \\nto the /etc/downward/labels file.\\nThe pod’s annotations will be \\nwritten to the /etc/downward/\\nannotations file.\\ndownwardAPI volume\\nPod manifest\\nmetadata:\\nname: downward\\nnamespace: default\\nlabels:\\nfoo: bar\\nannotations:\\nkey1: value1\\n...\\nspec:\\ncontainers:\\n- name: main\\nimage: busybox\\ncommand: [\"sleep\", \"9999999\"]\\nresources:\\nrequests:\\ncpu: 15m\\nmemory: 100Ki\\nlimits:\\ncpu: 100m\\nmemory: 4Mi\\n...\\n/podName\\n/podNamespace\\n/labels\\n/annotations\\n/containerCpuRequestMilliCores\\n/containerMemoryLimitBytes\\ndivisor: 1\\ndivisor: 1m\\nContainer: main\\nPod: downward\\nFilesystem\\n/\\netc/\\ndownward/\\nFigure 8.3\\nUsing a downwardAPI volume to pass metadata to the container\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [o, r]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [o, r]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': '/',\n",
       "    'description': 'Root directory',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': 'downward', 'description': 'Name of the pod', 'category': 'pod'},\n",
       "   {'entity': 'default',\n",
       "    'description': 'Namespace of the pod',\n",
       "    'category': 'namespace'},\n",
       "   {'entity': 'labels', 'description': 'Labels of the pod', 'category': 'pod'},\n",
       "   {'entity': '/etc/downward/labels',\n",
       "    'description': 'Filepath to write labels to',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': 'annotations',\n",
       "    'description': 'Annotations of the pod',\n",
       "    'category': 'pod'},\n",
       "   {'entity': '/etc/downward/annotations',\n",
       "    'description': 'Filepath to write annotations to',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': 'main',\n",
       "    'description': 'Name of the container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'busybox',\n",
       "    'description': 'Image of the container',\n",
       "    'category': 'image'},\n",
       "   {'entity': 'sleep',\n",
       "    'description': 'Command to run in the container',\n",
       "    'category': 'command'},\n",
       "   {'entity': '9999999',\n",
       "    'description': 'Argument to pass to the command',\n",
       "    'category': 'argument'},\n",
       "   {'entity': '15m',\n",
       "    'description': 'CPU request of the pod',\n",
       "    'category': 'resource'},\n",
       "   {'entity': '100Ki',\n",
       "    'description': 'Memory request of the pod',\n",
       "    'category': 'resource'},\n",
       "   {'entity': '100m',\n",
       "    'description': 'CPU limit of the pod',\n",
       "    'category': 'resource'},\n",
       "   {'entity': '4Mi',\n",
       "    'description': 'Memory limit of the pod',\n",
       "    'category': 'resource'},\n",
       "   {'entity': '/podName',\n",
       "    'description': 'Filepath to write pod name to',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': '/podNamespace',\n",
       "    'description': 'Filepath to write pod namespace to',\n",
       "    'category': 'filesystem'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"downward\",\\n    \"description\": \"is the metadata volume that passes metadata to container main\",\\n    \"destination_entity\": \"main\"\\n  },\\n  {\\n    \"source_entity\": \"default\",\\n    \"description\": \"is the namespace where downward API is used\",\\n    \"destination_entity\": \"downward\"\\n  },\\n  {\\n    \"source_entity\": \"main\",\\n    \"description\": \"is the container that uses metadata from downward volume\",\\n    \"destination_entity\": \"downward\"\\n  },\\n  {\\n    \"source_entity\": \"100m\",\\n    \"description\": \"specifies the CPU request for container main\",\\n    \"destination_entity\": \"main\"\\n  },\\n  {\\n    \"source_entity\": \"/\",\\n    \"description\": \"is the root directory where files are written by downward API\",\\n    \"destination_entity\": \"downward\"\\n  },\\n  {\\n    \"source_entity\": \"4Mi\",\\n    \"description\": \"specifies the memory limit for container main\",\\n    \"destination_entity\": \"main\"\\n  },\\n  {\\n    \"source_entity\": \"/podName\",\\n    \"description\": \"is a file written by downward API containing pod name\",\\n    \"destination_entity\": \"downward\"\\n  },\\n  {\\n    \"source_entity\": \"/podNamespace\",\\n    \"description\": \"is a file written by downward API containing pod namespace\",\\n    \"destination_entity\": \"downward\"\\n  },\\n  {\\n    \"source_entity\": \"annotations\",\\n    \"description\": \"are key-value pairs that contain metadata written to /etc/downward/annotations file\",\\n    \"destination_entity\": \"/etc/downward/annotations\"\\n  },\\n  {\\n    \"source_entity\": \"100Ki\",\\n    \"description\": \"specifies the memory request for container main\",\\n    \"destination_entity\": \"main\"\\n  },\\n  {\\n    \"source_entity\": \"/etc/downward/annotations\",\\n    \"description\": \"is a file written by downward API containing pod annotations\",\\n    \"destination_entity\": \"downward\"\\n  },\\n  {\\n    \"source_entity\": \"busybox\",\\n    \"description\": \"is the image used for container main\",\\n    \"destination_entity\": \"main\"\\n  },\\n  {\\n    \"source_entity\": \"15m\",\\n    \"description\": \"specifies the CPU request for container main\",\\n    \"destination_entity\": \"main\"\\n  },\\n  {\\n    \"source_entity\": \"9999999\",\\n    \"description\": \"is the command used to keep container main running\",\\n    \"destination_entity\": \"main\"\\n  },\\n  {\\n    \"source_entity\": \"labels\",\\n    \"description\": \"are key-value pairs that contain metadata written to /etc/downward/labels file\",\\n    \"destination_entity\": \"/etc/downward/labels\"\\n  },\\n  {\\n    \"source_entity\": \"sleep\",\\n    \"description\": \"is the command used by container main to keep running\",\\n    \"destination_entity\": \"main\"\\n  }\\n]\\n```'},\n",
       " {'page': 264,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '232\\nCHAPTER 8\\nAccessing pod metadata and other resources from applications\\nDelete the previous pod and create a new one from the manifest in the previous list-\\ning. Then look at the contents of the mounted downwardAPI volume directory. You\\nmounted the volume under /etc/downward/, so list the files in there, as shown in the\\nfollowing listing.\\n$ kubectl exec downward ls -lL /etc/downward\\n-rw-r--r--   1 root   root   134 May 25 10:23 annotations\\n-rw-r--r--   1 root   root     2 May 25 10:23 containerCpuRequestMilliCores\\n-rw-r--r--   1 root   root     7 May 25 10:23 containerMemoryLimitBytes\\n-rw-r--r--   1 root   root     9 May 25 10:23 labels\\n-rw-r--r--   1 root   root     8 May 25 10:23 podName\\n-rw-r--r--   1 root   root     7 May 25 10:23 podNamespace\\nNOTE\\nAs with the configMap and secret volumes, you can change the file\\npermissions through the downwardAPI volume’s defaultMode property in the\\npod spec.\\nEach file corresponds to an item in the volume’s definition. The contents of files,\\nwhich correspond to the same metadata fields as in the previous example, are the\\nsame as the values of environment variables you used before, so we won’t show them\\nhere. But because you couldn’t expose labels and annotations through environment\\nvariables before, examine the following listing for the contents of the two files you\\nexposed them in.\\n$ kubectl exec downward cat /etc/downward/labels\\nfoo=\"bar\"\\n$ kubectl exec downward cat /etc/downward/annotations\\nkey1=\"value1\"\\nkey2=\"multi\\\\nline\\\\nvalue\\\\n\"\\nkubernetes.io/config.seen=\"2016-11-28T14:27:45.664924282Z\"\\nkubernetes.io/config.source=\"api\"\\nAs you can see, each label/annotation is written in the key=value format on a sepa-\\nrate line. Multi-line values are written to a single line with newline characters denoted\\nwith \\\\n.\\nUPDATING LABELS AND ANNOTATIONS\\nYou may remember that labels and annotations can be modified while a pod is run-\\nning. As you might expect, when they change, Kubernetes updates the files holding\\nthem, allowing the pod to always see up-to-date data. This also explains why labels and\\nannotations can’t be exposed through environment variables. Because environment\\nvariable values can’t be updated afterward, if the labels or annotations of a pod were\\nexposed through environment variables, there’s no way to expose the new values after\\nthey’re modified.\\nListing 8.4\\nFiles in the downwardAPI volume\\nListing 8.5\\nDisplaying labels and annotations in the downwardAPI volume\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'pod',\n",
       "    'description': 'A container that runs a single application process.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool for running commands against Kubernetes clusters.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'exec',\n",
       "    'description': 'A command used to execute a command in a pod.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ls',\n",
       "    'description': 'A command used to list the files in a directory.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'L',\n",
       "    'description': 'The option for long listing format.',\n",
       "    'category': 'option'},\n",
       "   {'entity': 'annotations',\n",
       "    'description': 'Metadata that can be set on pods, services, and other Kubernetes resources.',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'containerCpuRequestMilliCores',\n",
       "    'description': 'A file in the downwardAPI volume that holds the CPU request for a container.',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'containerMemoryLimitBytes',\n",
       "    'description': 'A file in the downwardAPI volume that holds the memory limit for a container.',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'Metadata that can be set on pods, services, and other Kubernetes resources.',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'podName',\n",
       "    'description': 'A file in the downwardAPI volume that holds the name of a pod.',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'podNamespace',\n",
       "    'description': 'A file in the downwardAPI volume that holds the namespace of a pod.',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'downwardAPI',\n",
       "    'description': 'A Kubernetes feature that allows pods to access metadata about themselves and their environment.',\n",
       "    'category': 'feature'},\n",
       "   {'entity': 'configMap',\n",
       "    'description': 'A Kubernetes resource that stores configuration data as key-value pairs.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'secret',\n",
       "    'description': 'A Kubernetes resource that stores sensitive information such as passwords or OAuth tokens.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'labels and annotations',\n",
       "    'description': 'Metadata that can be set on pods, services, and other Kubernetes resources.',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'kubernetes.io/config.seen',\n",
       "    'description': 'A key in the annotations metadata that holds the timestamp when the config was last seen.',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'kubernetes.io/config.source',\n",
       "    'description': 'A key in the annotations metadata that holds the source of the config.',\n",
       "    'category': 'metadata'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"execute a command on a pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"list files in a directory\", \"destination_entity\": \"/etc/downward\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"cat the contents of a file\", \"destination_entity\": \"/etc/downward/labels\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"cat the contents of a file\", \"destination_entity\": \"/etc/downward/annotations\"},\\n  {\"source_entity\": \"downwardAPI\", \"description\": \"expose metadata fields as files in a volume\", \"destination_entity\": \"configMap\"},\\n  {\"source_entity\": \"downwardAPI\", \"description\": \"expose metadata fields as files in a volume\", \"destination_entity\": \"secret\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"update labels and annotations of a pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"list files in the downwardAPI volume\", \"destination_entity\": \"/etc/downward\"},\\n  {\"source_entity\": \"ls\", \"description\": \"list files in a directory\", \"destination_entity\": \"/etc/downward\"},\\n  {\"source_entity\": \"exec\", \"description\": \"execute a command on a pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"containerCpuRequestMilliCores\", \"description\": \"expose metadata field as a file in the downwardAPI volume\", \"destination_entity\": \"/etc/downward\"},\\n  {\"source_entity\": \"containerMemoryLimitBytes\", \"description\": \"expose metadata field as a file in the downwardAPI volume\", \"destination_entity\": \"/etc/downward\"},\\n  {\"source_entity\": \"kubernetes.io/config.seen\", \"description\": \"expose metadata field as a file in the downwardAPI volume\", \"destination_entity\": \"/etc/downward\"},\\n  {\"source_entity\": \"kubernetes.io/config.source\", \"description\": \"expose metadata field as a file in the downwardAPI volume\", \"destination_entity\": \"/etc/downward\"}\\n]\\n```'},\n",
       " {'page': 265,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '233\\nTalking to the Kubernetes API server\\nREFERRING TO CONTAINER-LEVEL METADATA IN THE VOLUME SPECIFICATION\\nBefore we wrap up this section, we need to point out one thing. When exposing con-\\ntainer-level metadata, such as a container’s resource limit or requests (done using\\nresourceFieldRef), you need to specify the name of the container whose resource\\nfield you’re referencing, as shown in the following listing.\\nspec:\\n  volumes:\\n  - name: downward                       \\n    downwardAPI:                         \\n      items:\\n      - path: \"containerCpuRequestMilliCores\"\\n        resourceFieldRef:\\n          containerName: main       \\n          resource: requests.cpu\\n          divisor: 1m\\nThe reason for this becomes obvious if you consider that volumes are defined at the\\npod level, not at the container level. When referring to a container’s resource field\\ninside a volume specification, you need to explicitly specify the name of the container\\nyou’re referring to. This is true even for single-container pods. \\n Using volumes to expose a container’s resource requests and/or limits is slightly\\nmore complicated than using environment variables, but the benefit is that it allows\\nyou to pass one container’s resource fields to a different container if needed (but\\nboth containers need to be in the same pod). With environment variables, a container\\ncan only be passed its own resource limits and requests. \\nUNDERSTANDING WHEN TO USE THE DOWNWARD API\\nAs you’ve seen, using the Downward API isn’t complicated. It allows you to keep the\\napplication Kubernetes-agnostic. This is especially useful when you’re dealing with an\\nexisting application that expects certain data in environment variables. The Down-\\nward API allows you to expose the data to the application without having to rewrite\\nthe application or wrap it in a shell script, which collects the data and then exposes it\\nthrough environment variables.\\n But the metadata available through the Downward API is fairly limited. If you need\\nmore, you’ll need to obtain it from the Kubernetes API server directly. You’ll learn\\nhow to do that next.\\n8.2\\nTalking to the Kubernetes API server\\nWe’ve seen how the Downward API provides a simple way to pass certain pod and con-\\ntainer metadata to the process running inside them. It only exposes the pod’s own\\nmetadata and a subset of all of the pod’s data. But sometimes your app will need to\\nknow more about other pods and even other resources defined in your cluster. The\\nDownward API doesn’t help in those cases.\\nListing 8.6\\nReferring to container-level metadata in a downwardAPI volume\\nContainer name \\nmust be specified\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes API server',\n",
       "    'description': 'Server that provides access to Kubernetes resources and metadata',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes API server\", \"description\": \"provides metadata about pods and containers\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"container\", \"description\": \"exposes resource field using resourceFieldRef\", \"destination_entity\": \"resource field\"},\\n  {\"source_entity\": \"Kubernetes-agnostic application\", \"description\": \"keeps application Kubernetes-agnostic by exposing data through Downward API\", \"destination_entity\": \"Downward API\"},\\n  {\"source_entity\": \"application\", \"description\": \"expects certain data in environment variables\", \"destination_entity\": \"environment variables\"},\\n  {\"source_entity\": \"Downward API\", \"description\": \"provides metadata about pods and containers\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Kubernetes API server\", \"description\": \"provides more metadata than Downward API\", \"destination_entity\": \"more metadata\"},\\n  {\"source_entity\": \"app\", \"description\": \"needs to know more about other pods and resources defined in cluster\", \"destination_entity\": \"other pods and resources\"}\\n]\\n```'},\n",
       " {'page': 266,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '234\\nCHAPTER 8\\nAccessing pod metadata and other resources from applications\\n As you’ve seen throughout the book, information about services and pods can be\\nobtained by looking at the service-related environment variables or through DNS. But\\nwhen the app needs data about other resources or when it requires access to the most\\nup-to-date information as possible, it needs to talk to the API server directly (as shown\\nin figure 8.4).\\nBefore you see how apps within pods can talk to the Kubernetes API server, let’s first\\nexplore the server’s REST endpoints from your local machine, so you can see what\\ntalking to the API server looks like.\\n8.2.1\\nExploring the Kubernetes REST API\\nYou’ve learned about different Kubernetes resource types. But if you’re planning on\\ndeveloping apps that talk to the Kubernetes API, you’ll want to know the API first. \\n To do that, you can try hitting the API server directly. You can get its URL by run-\\nning kubectl cluster-info:\\n$ kubectl cluster-info\\nKubernetes master is running at https://192.168.99.100:8443\\nBecause the server uses HTTPS and requires authentication, it’s not simple to talk to\\nit directly. You can try accessing it with curl and using curl’s --insecure (or -k)\\noption to skip the server certificate check, but that doesn’t get you far:\\n$ curl https://192.168.99.100:8443 -k\\nUnauthorized\\nLuckily, rather than dealing with authentication yourself, you can talk to the server\\nthrough a proxy by running the kubectl proxy command. \\nACCESSING THE API SERVER THROUGH KUBECTL PROXY \\nThe kubectl proxy command runs a proxy server that accepts HTTP connections on\\nyour local machine and proxies them to the API server while taking care of authenti-\\ncation, so you don’t need to pass the authentication token in every request. It also\\nmakes sure you’re talking to the actual API server and not a man in the middle (by\\nverifying the server’s certificate on each request).\\nContainer\\nAPI server\\nPod\\nApp process\\nAPI objects\\nFigure 8.4\\nTalking to the API server \\nfrom inside a pod to get information \\nabout other API objects\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Container\n",
       "   App process\n",
       "   Pod, API server\n",
       "   API objects]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Lightweight and ephemeral worker running an application',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'REST endpoint for accessing Kubernetes resources',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line interface for managing Kubernetes clusters',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'Utility for transferring data with URL syntax',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'HTTPS',\n",
       "    'description': 'Secure communication protocol using SSL/TLS',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'REST API',\n",
       "    'description': 'Interface for interacting with the Kubernetes API server',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'cluster-info',\n",
       "    'description': 'Command to retrieve information about a Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'proxy',\n",
       "    'description': 'Server running on localhost that forwards requests to the API server',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'App process',\n",
       "    'description': 'Application running inside a pod',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"runs a proxy server that accepts HTTP connections on your local machine and proxies them to the API server while taking care of authentication, so you don\\'t need to pass the authentication token in every request.\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"provides cluster-info command that gives the URL of the Kubernetes master running at HTTPS://192.168.99.100:8443\",\\n    \"destination_entity\": \"Kubernetes API\"\\n  },\\n  {\\n    \"source_entity\": \"cluster-info\",\\n    \"description\": \"gives the URL of the Kubernetes master running at HTTPS://192.168.99.100:8443\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl proxy\",\\n    \"description\": \"runs a proxy server that accepts HTTP connections on your local machine and proxies them to the API server while taking care of authentication, so you don\\'t need to pass the authentication token in every request.\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl proxy\",\\n    \"description\": \"verifies the server\\'s certificate on each request, making sure you\\'re talking to the actual API server and not a man in the middle.\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"App process\",\\n    \"description\": \"needs data about other resources or requires access to the most up-to-date information as possible, so it needs to talk to the API server directly.\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"curl\",\\n    \"description\": \"attempts to access the API server through HTTPS://192.168.99.100:8443 but returns an Unauthorized response because of authentication issues.\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"HTTPS\",\\n    \"description\": \"is used by the Kubernetes master running at HTTPS://192.168.99.100:8443 and requires authentication.\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"REST API\",\\n    \"description\": \"provides endpoints that can be accessed through a proxy or directly, but requires authentication for direct access.\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"can talk to the API server directly or through a proxy, allowing it to get information about other API objects.\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"App process\",\\n    \"description\": \"can access information about other resources or get up-to-date information by talking to the API server or using a proxy.\",\\n    \"destination_entity\": \"API server\"\\n  }\\n]'},\n",
       " {'page': 267,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '235\\nTalking to the Kubernetes API server\\n Running the proxy is trivial. All you need to do is run the following command:\\n$ kubectl proxy\\nStarting to serve on 127.0.0.1:8001\\nYou don’t need to pass in any other arguments, because kubectl already knows every-\\nthing it needs (the API server URL, authorization token, and so on). As soon as it starts\\nup, the proxy starts accepting connections on local port 8001. Let’s see if it works:\\n$ curl localhost:8001\\n{\\n  \"paths\": [\\n    \"/api\",\\n    \"/api/v1\",\\n    ...\\nVoila! You sent the request to the proxy, it sent a request to the API server, and then\\nthe proxy returned whatever the server returned. Now, let’s start exploring.\\nEXPLORING THE KUBERNETES API THROUGH THE KUBECTL PROXY\\nYou can continue to use curl, or you can open your web browser and point it to\\nhttp:/\\n/localhost:8001. Let’s examine what the API server returns when you hit its base\\nURL more closely. The server responds with a list of paths, as shown in the follow-\\ning listing.\\n$ curl http://localhost:8001\\n{\\n  \"paths\": [\\n    \"/api\",\\n    \"/api/v1\",                  \\n    \"/apis\",\\n    \"/apis/apps\",\\n    \"/apis/apps/v1beta1\",\\n    ...\\n    \"/apis/batch\",              \\n    \"/apis/batch/v1\",           \\n    \"/apis/batch/v2alpha1\",     \\n    ...\\nThese paths correspond to the API groups and versions you specify in your resource\\ndefinitions when creating resources such as Pods, Services, and so on. \\n You may recognize the batch/v1 in the /apis/batch/v1 path as the API group and\\nversion of the Job resources you learned about in chapter 4. Likewise, the /api/v1\\ncorresponds to the apiVersion: v1 you refer to in the common resources you created\\n(Pods, Services, ReplicationControllers, and so on). The most common resource\\ntypes, which were introduced in the earliest versions of Kubernetes, don’t belong to\\nListing 8.7\\nListing the API server’s REST endpoints: http:/\\n/localhost:8001\\nMost resource types \\ncan be found here.\\nThe batch API \\ngroup and its \\ntwo versions\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'Command used to run the proxy to the Kubernetes API server',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'proxy',\n",
       "    'description': 'Process that accepts connections on local port 8001 and forwards requests to the API server',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Kubernetes component that provides REST endpoints for interacting with resources',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'Command used to send HTTP requests to the proxy or API server',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'localhost:8001',\n",
       "    'description': 'Local port where the proxy accepts connections',\n",
       "    'category': 'port'},\n",
       "   {'entity': '/api',\n",
       "    'description': 'API path returned by the API server listing all available paths',\n",
       "    'category': 'path'},\n",
       "   {'entity': '/api/v1',\n",
       "    'description': 'API path corresponding to the v1 version of the Kubernetes API',\n",
       "    'category': 'path'},\n",
       "   {'entity': '/apis',\n",
       "    'description': 'API path listing all available API groups and versions',\n",
       "    'category': 'path'},\n",
       "   {'entity': 'Jobs',\n",
       "    'description': 'Kubernetes resource type belonging to the batch API group and version',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Kubernetes resource type belonging to the apiVersion v1',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'Kubernetes resource type belonging to the apiVersion v1',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'ReplicationControllers',\n",
       "    'description': 'Kubernetes resource type belonging to the apiVersion v1',\n",
       "    'category': 'resource'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"starts accepting connections\", \"destination_entity\": \"proxy\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"sends a request to the API server\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"proxy\", \"description\": \"returns whatever the server returned\", \"destination_entity\": \"curl\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"knows everything it needs\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"starts exploring the Kubernetes API\", \"destination_entity\": \"/api/v1\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"returns a list of paths\", \"destination_entity\": \"/api/v1\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"corresponds to the apiVersion: v1\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"corresponds to the apiVersion: v1\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"can be found here\", \"destination_entity\": \"/apis\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"belongs to the batch API group\", \"destination_entity\": \"/api/v1\"},\\n  {\"source_entity\": \"curl\", \"description\": \"sends a request to the proxy\", \"destination_entity\": \"proxy\"},\\n  {\"source_entity\": \"proxy\", \"description\": \"returns whatever the server returned\", \"destination_entity\": \"curl\"},\\n  {\"source_entity\": \"API server\", \"description\": \"responds with a list of paths\", \"destination_entity\": \"/api/v1\"},\\n  {\"source_entity\": \"/api/v1\", \"description\": \"corresponds to the apiVersion: v1\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"/api/v1\", \"description\": \"corresponds to the apiVersion: v1\", \"destination_entity\": \"Services\"}\\n]\\n```\\n\\nNote that I\\'ve only extracted relations between entities mentioned in the document page, and ignored any other words or characters.'},\n",
       " {'page': 268,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '236\\nCHAPTER 8\\nAccessing pod metadata and other resources from applications\\nany specific group, because Kubernetes initially didn’t even use the concept of API\\ngroups; they were introduced later. \\nNOTE\\nThese initial resource types without an API group are now considered\\nto belong to the core API group.\\nEXPLORING THE BATCH API GROUP’S REST ENDPOINT\\nLet’s explore the Job resource API. You’ll start by looking at what’s behind the\\n/apis/batch path (you’ll omit the version for now), as shown in the following listing.\\n$ curl http://localhost:8001/apis/batch\\n{\\n  \"kind\": \"APIGroup\",\\n  \"apiVersion\": \"v1\",\\n  \"name\": \"batch\",\\n  \"versions\": [\\n    {\\n      \"groupVersion\": \"batch/v1\",             \\n      \"version\": \"v1\"                         \\n    },\\n    {\\n      \"groupVersion\": \"batch/v2alpha1\",       \\n      \"version\": \"v2alpha1\"                   \\n    }\\n  ],\\n  \"preferredVersion\": {                    \\n    \"groupVersion\": \"batch/v1\",            \\n    \"version\": \"v1\"                        \\n  },\\n  \"serverAddressByClientCIDRs\": null\\n}\\nThe response shows a description of the batch API group, including the available ver-\\nsions and the preferred version clients should use. Let’s continue and see what’s\\nbehind the /apis/batch/v1 path. It’s shown in the following listing.\\n$ curl http://localhost:8001/apis/batch/v1\\n{\\n  \"kind\": \"APIResourceList\",              \\n  \"apiVersion\": \"v1\",\\n  \"groupVersion\": \"batch/v1\",             \\n  \"resources\": [                          \\n    {\\n      \"name\": \"jobs\",             \\n      \"namespaced\": true,         \\n      \"kind\": \"Job\",              \\nListing 8.8\\nListing endpoints under /apis/batch: http:/\\n/localhost:8001/apis/batch\\nListing 8.9\\nResource types in batch/v1: http:/\\n/localhost:8001/apis/batch/v1\\nThe batch API \\ngroup contains \\ntwo versions.\\nClients should use the \\nv1 version instead of \\nv2alpha1.\\nThis is a list of API resources \\nin the batch/v1 API group.\\nHere’s an array holding \\nall the resource types \\nin this group.\\nThis describes the \\nJob resource, which \\nis namespaced.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': '',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API groups',\n",
       "    'description': 'grouping of resources in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Batch API group',\n",
       "    'description': 'a specific group for batch-related resources in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'REST endpoint',\n",
       "    'description': 'an interface to interact with a resource over HTTP',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'command-line tool for interacting with APIs and web servers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'http://localhost:8001/apis/batch',\n",
       "    'description': 'a specific REST endpoint for the batch API group',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'APIResourceList',\n",
       "    'description': 'a data structure representing a list of API resources in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Job resource',\n",
       "    'description': 'a type of API resource in the batch/v1 API group, used to manage jobs',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'namespaced',\n",
       "    'description': 'a property indicating whether an API resource is namespaced or not',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'v1 version',\n",
       "    'description': 'the preferred version for clients using the batch API group',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'v2alpha1 version',\n",
       "    'description': 'an older, deprecated version of the batch API group',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"Batch API group\", \"description\": \"contains\", \"destination_entity\": \"APIResourceList\"},{\"source_entity\": \"Batch API group\", \"description\": \"contains\", \"destination_entity\": \"Job resource\"},{\"source_entity\": \"REST endpoint\", \"description\": \"exploring\", \"destination_entity\": \"/apis/batch path\"},{\"source_entity\": \"REST endpoint\", \"description\": \"exploring\", \"destination_entity\": \"v1 version\"},{\"source_entity\": \"Kubernetes\", \"description\": \"introduced later\", \"destination_entity\": \"API groups\"},{\"source_entity\": \"curl\", \"description\": \"used to access\", \"destination_entity\": \"/apis/batch path\"},{\"source_entity\": \"Batch API group\", \"description\": \"has two versions\", \"destination_entity\": \"v1 version\"},{\"source_entity\": \"Batch API group\", \"description\": \"has two versions\", \"destination_entity\": \"v2alpha1 version\"},{\"source_entity\": \"API groups\", \"description\": \"are now considered to belong to the core API group\", \"destination_entity\": \"initial resource types\"}, {\"source_entity\": \"APIResourceList\", \"description\": \"lists all resource types in batch/v1\", \"destination_entity\": \"batch/v1 API group\"},{\"source_entity\": \"Job resource\", \"description\": \"is namespaced\", \"destination_entity\": \"namespaced\"}]'},\n",
       " {'page': 269,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '237\\nTalking to the Kubernetes API server\\n      \"verbs\": [                 \\n        \"create\",                \\n        \"delete\",                \\n        \"deletecollection\",      \\n        \"get\",                   \\n        \"list\",                  \\n        \"patch\",                 \\n        \"update\",                \\n        \"watch\"                  \\n      ]\\n    },\\n    {\\n      \"name\": \"jobs/status\",            \\n      \"namespaced\": true,                  \\n      \"kind\": \"Job\",\\n      \"verbs\": [             \\n        \"get\",               \\n        \"patch\",             \\n        \"update\"             \\n      ]\\n    }\\n  ]\\n}\\nAs you can see, the API server returns a list of resource types and REST endpoints in\\nthe batch/v1 API group. One of those is the Job resource. In addition to the name of\\nthe resource and the associated kind, the API server also includes information on\\nwhether the resource is namespaced or not, its short name (if it has one; Jobs don’t),\\nand a list of verbs you can use with the resource. \\n The returned list describes the REST resources exposed in the API server. The\\n\"name\": \"jobs\" line tells you that the API contains the /apis/batch/v1/jobs end-\\npoint. The \"verbs\" array says you can retrieve, update, and delete Job resources\\nthrough that endpoint. For certain resources, additional API endpoints are also\\nexposed (such as the jobs/status path, which allows modifying only the status of\\na Job).\\nLISTING ALL JOB INSTANCES IN THE CLUSTER\\nTo get a list of Jobs in your cluster, perform a GET request on path /apis/batch/\\nv1/jobs, as shown in the following listing.\\n$ curl http://localhost:8001/apis/batch/v1/jobs\\n{\\n  \"kind\": \"JobList\",\\n  \"apiVersion\": \"batch/v1\",\\n  \"metadata\": {\\n    \"selfLink\": \"/apis/batch/v1/jobs\",\\n    \"resourceVersion\": \"225162\"\\n  },\\nListing 8.10\\nList of Jobs: http:/\\n/localhost:8001/apis/batch/v1/jobs\\nHere are the verbs that can be used \\nwith this resource (you can create \\nJobs; delete individual ones or a \\ncollection of them; and retrieve, \\nwatch, and update them).\\nResources also have a \\nspecial REST endpoint for \\nmodifying their status.\\nThe status can be \\nretrieved, patched, \\nor updated.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes API server',\n",
       "    'description': 'API server that returns a list of resource types and REST endpoints in the batch/v1 API group.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes API server\",\\n    \"description\": \"returns a list of resource types and REST endpoints\",\\n    \"destination_entity\": \"/apis/batch/v1 API group\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes API server\",\\n    \"description\": \"includes information on whether the resource is namespaced or not, its short name (if it has one; Jobs don’t), and a list of verbs you can use with the resource\",\\n    \"destination_entity\": \"/apis/batch/v1 API group\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes API server\",\\n    \"description\": \"exposes REST resources in the API server\",\\n    \"destination_entity\": \"/apis/batch/v1 API group\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes API server\",\\n    \"description\": \"allows retrieving, updating, and deleting Job resources through that endpoint\",\\n    \"destination_entity\": \"/jobs endpoint\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes API server\",\\n    \"description\": \"exposes additional API endpoints for certain resources (such as the jobs/status path)\",\\n    \"destination_entity\": \"/jobs/status path\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes API server\",\\n    \"description\": \"allows modifying only the status of a Job\",\\n    \"destination_entity\": \"/jobs/status path\"\\n  },\\n  {\\n    \"source_entity\": \"$ curl command\",\\n    \"description\": \"performs a GET request on path /apis/batch/v1/jobs to get a list of Jobs in the cluster\",\\n    \"destination_entity\": \"/apis/batch/v1/jobs endpoint\"\\n  }\\n]\\n```\\n\\nNote that I\\'ve used the provided entities '},\n",
       " {'page': 270,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '238\\nCHAPTER 8\\nAccessing pod metadata and other resources from applications\\n  \"items\": [\\n    {\\n      \"metadata\": {\\n        \"name\": \"my-job\",\\n        \"namespace\": \"default\",\\n        ...\\nYou probably have no Job resources deployed in your cluster, so the items array will be\\nempty. You can try deploying the Job in Chapter08/my-job.yaml and hitting the REST\\nendpoint again to get the same output as in listing 8.10.\\nRETRIEVING A SPECIFIC JOB INSTANCE BY NAME\\nThe previous endpoint returned a list of all Jobs across all namespaces. To get back\\nonly one specific Job, you need to specify its name and namespace in the URL. To\\nretrieve the Job shown in the previous listing (name: my-job; namespace: default),\\nyou need to request the following path: /apis/batch/v1/namespaces/default/jobs/\\nmy-job, as shown in the following listing.\\n$ curl http://localhost:8001/apis/batch/v1/namespaces/default/jobs/my-job\\n{\\n  \"kind\": \"Job\",\\n  \"apiVersion\": \"batch/v1\",\\n  \"metadata\": {\\n    \"name\": \"my-job\",\\n    \"namespace\": \"default\",\\n    ...\\nAs you can see, you get back the complete JSON definition of the my-job Job resource,\\nexactly like you do if you run:\\n$ kubectl get job my-job -o json\\nYou’ve seen that you can browse the Kubernetes REST API server without using any\\nspecial tools, but to fully explore the REST API and interact with it, a better option is\\ndescribed at the end of this chapter. For now, exploring it with curl like this is enough\\nto make you understand how an application running in a pod talks to Kubernetes. \\n8.2.2\\nTalking to the API server from within a pod\\nYou’ve learned how to talk to the API server from your local machine, using the\\nkubectl proxy. Now, let’s see how to talk to it from within a pod, where you (usually)\\ndon’t have kubectl. Therefore, to talk to the API server from inside a pod, you need\\nto take care of three things:\\n\\uf0a1Find the location of the API server.\\n\\uf0a1Make sure you’re talking to the API server and not something impersonating it.\\n\\uf0a1Authenticate with the server; otherwise it won’t let you see or do anything.\\nListing 8.11\\nRetrieving a resource in a specific namespace by name\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Job',\n",
       "    'description': 'A Kubernetes Job resource that represents a set of one or more pods working together to complete a task.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The official command-line tool for interacting with Kubernetes clusters.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'REST API',\n",
       "    'description': 'An interface to interact with the Kubernetes API server, allowing applications to read and write data in various formats.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'A command-line utility for transferring data to and from a web server using HTTP, the command line tool used here to test REST API endpoints.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Job resource',\n",
       "    'description': 'The Kubernetes Job object which can be used to launch pods that run similar containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'A scope for names in Kubernetes, allows resources with the same name to exist in different namespaces.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A small, lightweight unit of deployment in Kubernetes, representing an application instance running on a node.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The central component of the Kubernetes cluster that provides access to all cluster resources and is responsible for handling API requests from applications.',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to talk to REST API server\", \"destination_entity\": \"REST API\"},\\n  {\"source_entity\": \"Job resource\", \"description\": \"retrieved by Job name and namespace\", \"destination_entity\": \"Job\"},\\n  {\"source_entity\": \"Job resource\", \"description\": \"complete JSON definition\", \"destination_entity\": \"JSON\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to get Job resource\", \"destination_entity\": \"Job resource\"},\\n  {\"source_entity\": \"curl\", \"description\": \"used to talk to REST API server\", \"destination_entity\": \"REST API\"},\\n  {\"source_entity\": \"API server\", \"description\": \"talked to from within a pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"kubectl proxy\", \"description\": \"used to explore REST API server\", \"destination_entity\": \"REST API\"},\\n  {\"source_entity\": \"namespace\", \"description\": \"specifies which namespace\\'s Job is being retrieved\", \"destination_entity\": \"Job resource\"},\\n  {\"source_entity\": \"API server\", \"description\": \"needs to be authenticated with\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"REST API\", \"description\": \"has a specific endpoint for retrieving Jobs by name and namespace\", \"destination_entity\": \"Job\"}\\n]\\n```\\n\\nNote that I\\'ve only included the entities from the provided list, even if they appear multiple times in the document.'},\n",
       " {'page': 271,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '239\\nTalking to the Kubernetes API server\\nYou’ll see how this is done in the next three sections. \\nRUNNING A POD TO TRY OUT COMMUNICATION WITH THE API SERVER\\nThe first thing you need is a pod from which to talk to the API server. You’ll run a pod\\nthat does nothing (it runs the sleep command in its only container), and then run a\\nshell in the container with kubectl exec. Then you’ll try to access the API server from\\nwithin that shell using curl.\\n Therefore, you need to use a container image that contains the curl binary. If you\\nsearch for such an image on, say, Docker Hub, you’ll find the tutum/curl image, so\\nuse it (you can also use any other existing image containing the curl binary or you\\ncan build your own). The pod definition is shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: curl\\nspec:\\n  containers:\\n  - name: main\\n    image: tutum/curl                \\n    command: [\"sleep\", \"9999999\"]    \\nAfter creating the pod, run kubectl exec to run a bash shell inside its container:\\n$ kubectl exec -it curl bash\\nroot@curl:/#\\nYou’re now ready to talk to the API server.\\nFINDING THE API SERVER’S ADDRESS\\nFirst, you need to find the IP and port of the Kubernetes API server. This is easy,\\nbecause a Service called kubernetes is automatically exposed in the default name-\\nspace and configured to point to the API server. You may remember seeing it every\\ntime you listed services with kubectl get svc:\\n$ kubectl get svc\\nNAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\\nkubernetes   10.0.0.1     <none>        443/TCP   46d\\nAnd you’ll remember from chapter 5 that environment variables are configured for\\neach service. You can get both the IP address and the port of the API server by looking\\nup the KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT variables (inside\\nthe container):\\nroot@curl:/# env | grep KUBERNETES_SERVICE\\nKUBERNETES_SERVICE_PORT=443\\nKUBERNETES_SERVICE_HOST=10.0.0.1\\nKUBERNETES_SERVICE_PORT_HTTPS=443\\nListing 8.12\\nA pod for trying out communication with the API server: curl.yaml\\nUsing the tutum/curl image, \\nbecause you need curl \\navailable in the container\\nYou’re running the sleep \\ncommand with a long delay to \\nkeep your container running.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'Containerization platform',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'Command-line tool for transferring data with HTTP requests',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for managing Kubernetes resources',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': 'The primary interface to the Kubernetes system',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A way to expose an application running in a pod',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'cluster-ip',\n",
       "    'description': 'An IP address assigned to a Service',\n",
       "    'category': 'hardware/network'},\n",
       "   {'entity': 'external-ip',\n",
       "    'description': 'The IP address that can be used to access the Service from outside the cluster',\n",
       "    'category': 'hardware/network'},\n",
       "   {'entity': 'port',\n",
       "    'description': 'A number that identifies a network connection or interface',\n",
       "    'category': 'hardware/network'},\n",
       "   {'entity': 'env',\n",
       "    'description': 'Environment variables',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'KUBERNETES_SERVICE_HOST',\n",
       "    'description': 'Environment variable containing the IP address of the API Server',\n",
       "    'category': 'environment variable'},\n",
       "   {'entity': 'KUBERNETES_SERVICE_PORT',\n",
       "    'description': 'Environment variable containing the port number of the API Server',\n",
       "    'category': 'environment variable'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"executes a command to run a bash shell inside a container\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"Docker\",\\n    \"description\": \"hosts an image containing the curl binary\",\\n    \"destination_entity\": \"image\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"is automatically exposed in the default namespace and configured to point to the API server\",\\n    \"destination_entity\": \"API Server\"\\n  },\\n  {\\n    \"source_entity\": \"KUBERNETES_SERVICE_PORT\",\\n    \"description\": \"provides the port number of the API server\",\\n    \"destination_entity\": \"API Server\"\\n  },\\n  {\\n    \"source_entity\": \"KUBERNETES_SERVICE_HOST\",\\n    \"description\": \"provides the IP address of the API server\",\\n    \"destination_entity\": \"API Server\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"lists services using the get command\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"env\",\\n    \"description\": \"displays environment variables, including KUBERNETES_SERVICE_PORT and KUBERNETES_SERVICE_HOST\",\\n    \"destination_entity\": \"KUBERNETES_SERVICE_PORT\" \\n  },\\n  {\\n    \"source_entity\": \"curl\",\\n    \"description\": \"executes a command to access the API server using the HTTP protocol\",\\n    \"destination_entity\": \"API Server\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"creates a pod from a YAML file definition\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"manages the creation and execution of pods, including the one running curl\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"displays information about a pod using the get command\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"cluster-ip\",\\n    \"description\": \"provides the IP address of a service within a cluster\",\\n    \"destination_entity\": \"Service\"\\n  }\\n]'},\n",
       " {'page': 272,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"240\\nCHAPTER 8\\nAccessing pod metadata and other resources from applications\\nYou may also remember that each service also gets a DNS entry, so you don’t even\\nneed to look up the environment variables, but instead simply point curl to\\nhttps:/\\n/kubernetes. To be fair, if you don’t know which port the service is available at,\\nyou also either need to look up the environment variables or perform a DNS SRV\\nrecord lookup to get the service’s actual port number. \\n The environment variables shown previously say that the API server is listening on\\nport 443, which is the default port for HTTPS, so try hitting the server through\\nHTTPS:\\nroot@curl:/# curl https://kubernetes\\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\\n...\\nIf you'd like to turn off curl's verification of the certificate, use\\n  the -k (or --insecure) option.\\nAlthough the simplest way to get around this is to use the proposed -k option (and\\nthis is what you’d normally use when playing with the API server manually), let’s look\\nat the longer (and correct) route. Instead of blindly trusting that the server you’re\\nconnecting to is the authentic API server, you’ll verify its identity by having curl check\\nits certificate. \\nTIP\\nNever skip checking the server’s certificate in an actual application.\\nDoing so could make your app expose its authentication token to an attacker\\nusing a man-in-the-middle attack.\\nVERIFYING THE SERVER’S IDENTITY\\nIn the previous chapter, while discussing Secrets, we looked at an automatically cre-\\nated Secret called default-token-xyz, which is mounted into each container at\\n/var/run/secrets/kubernetes.io/serviceaccount/. Let’s see the contents of that Secret\\nagain, by listing files in that directory:\\nroot@curl:/# \\nls \\n/var/run/secrets/kubernetes.io/serviceaccount/ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nca.crt    namespace    token\\nThe Secret has three entries (and therefore three files in the Secret volume). Right\\nnow, we’ll focus on the ca.crt file, which holds the certificate of the certificate author-\\nity (CA) used to sign the Kubernetes API server’s certificate. To verify you’re talking to\\nthe API server, you need to check if the server’s certificate is signed by the CA. curl\\nallows you to specify the CA certificate with the --cacert option, so try hitting the API\\nserver again:\\nroot@curl:/# curl --cacert /var/run/secrets/kubernetes.io/serviceaccount\\n             ➥ /ca.crt https://kubernetes\\nUnauthorized\\nNOTE\\nYou may see a longer error description than “Unauthorized.”\\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'curl',\n",
       "    'description': 'A command-line utility for transferring data to and from a web server',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'HTTPS',\n",
       "    'description': 'A protocol for secure communication over the internet',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'DNS SRV record lookup',\n",
       "    'description': 'A type of DNS query that returns a list of servers that provide a specific service',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The primary interface for interacting with the Kubernetes cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'port 443',\n",
       "    'description': 'The default port used by the API server for HTTPS communication',\n",
       "    'category': 'port'},\n",
       "   {'entity': '-k option',\n",
       "    'description': 'An optional flag in curl that disables certificate verification',\n",
       "    'category': 'flag'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'A feature of Kubernetes for storing sensitive information, such as authentication tokens',\n",
       "    'category': 'feature'},\n",
       "   {'entity': 'default-token-xyz',\n",
       "    'description': 'An automatically created Secret in the default namespace',\n",
       "    'category': 'resource'},\n",
       "   {'entity': '/var/run/secrets/kubernetes.io/serviceaccount/',\n",
       "    'description': 'A directory containing a Secret volume with authentication information',\n",
       "    'category': 'directory'},\n",
       "   {'entity': 'ca.crt',\n",
       "    'description': \"A file in the Secret volume that contains the certificate authority (CA) used to sign the API server's certificate\",\n",
       "    'category': 'file'},\n",
       "   {'entity': '--cacert option',\n",
       "    'description': 'An optional flag in curl that specifies a custom CA certificate for verification',\n",
       "    'category': 'flag'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"HTTPS\",\\n    \"description\": \"uses to access the Kubernetes API server\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"DNS SRV record lookup\",\\n    \"description\": \"performs to get the service\\'s actual port number\",\\n    \"destination_entity\": \"port 443\"\\n  },\\n  {\\n    \"source_entity\": \"/var/run/secrets/kubernetes.io/serviceaccount/\",\\n    \"description\": \"contains the certificate of the certificate authority used to sign the Kubernetes API server\\'s certificate\",\\n    \"destination_entity\": \"ca.crt\"\\n  },\\n  {\\n    \"source_entity\": \"--cacert option\",\\n    \"description\": \"uses to specify the CA certificate for verification\",\\n    \"destination_entity\": \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\"\\n  },\\n  {\\n    \"source_entity\": \"ca.crt\",\\n    \"description\": \"is used as a certificate authority to sign the Kubernetes API server\\'s certificate\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"default-token-xyz\",\\n    \"description\": \"is an automatically created Secret mounted into each container at /var/run/secrets/kubernetes.io/serviceaccount/\",\\n    \"destination_entity\": \"/var/run/secrets/kubernetes.io/serviceaccount/\"\\n  },\\n  {\\n    \"source_entity\": \"curl\",\\n    \"description\": \"uses to access the Kubernetes API server and verify its certificate\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"-k option\",\\n    \"description\": \"is used to turn off curl\\'s verification of the certificate\",\\n    \"destination_entity\": \"API server\"\\n  }\\n]\\n```\\n\\nNote that I only included relations where there is a clear action being performed by one entity on another. Let me know if you\\'d like me to clarify any of these relations!'},\n",
       " {'page': 273,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '241\\nTalking to the Kubernetes API server\\nOkay, you’ve made progress. curl verified the server’s identity because its certificate\\nwas signed by the CA you trust. As the Unauthorized response suggests, you still need\\nto take care of authentication. You’ll do that in a moment, but first let’s see how to\\nmake life easier by setting the CURL_CA_BUNDLE environment variable, so you don’t\\nneed to specify --cacert every time you run curl:\\nroot@curl:/# export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/\\n             ➥ serviceaccount/ca.crt\\nYou can now hit the API server without using --cacert:\\nroot@curl:/# curl https://kubernetes\\nUnauthorized\\nThis is much nicer now. Your client (curl) trusts the API server now, but the API\\nserver itself says you’re not authorized to access it, because it doesn’t know who\\nyou are.\\nAUTHENTICATING WITH THE API SERVER\\nYou need to authenticate with the server, so it allows you to read and even update\\nand/or delete the API objects deployed in the cluster. To authenticate, you need an\\nauthentication token. Luckily, the token is provided through the default-token Secret\\nmentioned previously, and is stored in the token file in the secret volume. As the\\nSecret’s name suggests, that’s the primary purpose of the Secret. \\n You’re going to use the token to access the API server. First, load the token into an\\nenvironment variable:\\nroot@curl:/# TOKEN=$(cat /var/run/secrets/kubernetes.io/\\n             ➥ serviceaccount/token)\\nThe token is now stored in the TOKEN environment variable. You can use it when send-\\ning requests to the API server, as shown in the following listing.\\nroot@curl:/# curl -H \"Authorization: Bearer $TOKEN\" https://kubernetes\\n{\\n  \"paths\": [\\n    \"/api\",\\n    \"/api/v1\",\\n    \"/apis\",\\n    \"/apis/apps\",\\n    \"/apis/apps/v1beta1\",\\n    \"/apis/authorization.k8s.io\",    \\n    ...\\n    \"/ui/\",\\n    \"/version\"\\n  ]\\n}\\nListing 8.13\\nGetting a proper response from the API server\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'curl',\n",
       "    'description': 'Command-line tool for transferring data with URLs',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CURL_CA_BUNDLE',\n",
       "    'description': 'Environment variable to specify a certificate bundle file',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kubernetes API server',\n",
       "    'description': 'Server that provides access to Kubernetes resources and objects',\n",
       "    'category': 'application'},\n",
       "   {'entity': '/var/run/secrets/kubernetes.io/',\n",
       "    'description': 'Directory where Secret data is stored',\n",
       "    'category': 'path/file'},\n",
       "   {'entity': 'serviceaccount/ca.crt',\n",
       "    'description': 'Certificate file from a service account',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Server that provides access to Kubernetes resources and objects',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'authentication token',\n",
       "    'description': 'Token used for authentication with the API server',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'default-token Secret',\n",
       "    'description': 'Secret that stores an authentication token',\n",
       "    'category': 'database'},\n",
       "   {'entity': '/var/run/secrets/kubernetes.io/serviceaccount/token',\n",
       "    'description': 'File where the authentication token is stored',\n",
       "    'category': 'path/file'},\n",
       "   {'entity': 'TOKEN environment variable',\n",
       "    'description': 'Environment variable that stores the authentication token',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Authorization header',\n",
       "    'description': 'Header used to specify an authentication token in a request',\n",
       "    'category': 'http/header'},\n",
       "   {'entity': 'Bearer $TOKEN',\n",
       "    'description': 'Value of the Authorization header with an authentication token',\n",
       "    'category': 'string/value'},\n",
       "   {'entity': '/api',\n",
       "    'description': 'API endpoint for accessing Kubernetes resources and objects',\n",
       "    'category': 'path/endpoint'},\n",
       "   {'entity': '/apis/apps/v1beta1',\n",
       "    'description': 'API endpoint for accessing specific Kubernetes resources and objects',\n",
       "    'category': 'path/endpoint'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"TOKEN environment variable\", \"description\": \"stores authentication token\", \"destination_entity\": \"authentication token\"},\\n  {\"source_entity\": \"Bearer $TOKEN\", \"description\": \"sends authorization header with token\", \"destination_entity\": \"kubernetes API server\"},\\n  {\"source_entity\": \"curl\", \"description\": \"trusts the CA certificate signed by trusted CA\", \"destination_entity\": \"serviceaccount/ca.crt\"},\\n  {\"source_entity\": \"curl\", \"description\": \"accesses the API server using authentication token\", \"destination_entity\": \"default-token Secret\"},\\n  {\"source_entity\": \"curl\", \"description\": \"loads authentication token from file into environment variable\", \"destination_entity\": \"/var/run/secrets/kubernetes.io/serviceaccount/token\"},\\n  {\"source_entity\": \"CURL_CA_BUNDLE\", \"description\": \"specifies trusted CA certificate for curl\", \"destination_entity\": \"/var/run/secrets/kubernetes.io/\"},\\n  {\"source_entity\": \"kubernetes API server\", \"description\": \"provides authentication token through Secret\", \"destination_entity\": \"default-token Secret\"},\\n  {\"source_entity\": \"Authorization header\", \"description\": \"sends authentication token to API server\", \"destination_entity\": \"kubernetes API server\"}\\n]\\n```'},\n",
       " {'page': 274,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '242\\nCHAPTER 8\\nAccessing pod metadata and other resources from applications\\nAs you can see, you passed the token inside the Authorization HTTP header in the\\nrequest. The API server recognized the token as authentic and returned a proper\\nresponse. You can now explore all the resources in your cluster, the way you did a few\\nsections ago. \\n For example, you could list all the pods in the same namespace. But first you need\\nto know what namespace the curl pod is running in.\\nGETTING THE NAMESPACE THE POD IS RUNNING IN\\nIn the first part of this chapter, you saw how to pass the namespace to the pod\\nthrough the Downward API. But if you’re paying attention, you probably noticed\\nyour secret volume also contains a file called namespace. It contains the name-\\nspace the pod is running in, so you can read the file instead of having to explicitly\\npass the namespace to your pod through an environment variable. Load the con-\\ntents of the file into the NS environment variable and then list all the pods, as shown\\nin the following listing.\\nroot@curl:/# NS=$(cat /var/run/secrets/kubernetes.io/\\n             ➥ serviceaccount/namespace)           \\nroot@curl:/# curl -H \"Authorization: Bearer $TOKEN\"\\n             ➥ https://kubernetes/api/v1/namespaces/$NS/pods\\n{\\n  \"kind\": \"PodList\",\\n  \"apiVersion\": \"v1\",\\n  ...\\nAnd there you go. By using the three files in the mounted secret volume directory,\\nyou listed all the pods running in the same namespace as your pod. In the same man-\\nner, you could also retrieve other API objects and even update them by sending PUT or\\nPATCH instead of simple GET requests. \\nDisabling role-based access control (RBAC)\\nIf you’re using a Kubernetes cluster with RBAC enabled, the service account may not\\nbe authorized to access (parts of) the API server. You’ll learn about service accounts\\nand RBAC in chapter 12. For now, the simplest way to allow you to query the API\\nserver is to work around RBAC by running the following command:\\n$ kubectl create clusterrolebinding permissive-binding \\\\\\n  --clusterrole=cluster-admin \\\\\\n  --group=system:serviceaccounts\\nThis gives all service accounts (we could also say all pods) cluster-admin privileges,\\nallowing them to do whatever they want. Obviously, doing this is dangerous and\\nshould never be done on production clusters. For test purposes, it’s fine.\\nListing 8.14\\nListing pods in the pod’s own namespace\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'GET request',\n",
       "    'description': 'HTTP request method to retrieve data from API server',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'Command-line tool for transferring data with URL syntax',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Authorization header',\n",
       "    'description': 'HTTP header containing authentication token',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Server that manages and provides access to cluster resources',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Downward API',\n",
       "    'description': 'Mechanism for passing metadata from pod to application',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'Logical grouping of resources within a Kubernetes cluster',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for managing and controlling Kubernetes clusters',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'clusterrolebinding',\n",
       "    'description': 'Object that grants privileges to service accounts within a cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'RBAC',\n",
       "    'description': 'Role-Based Access Control mechanism for managing access to cluster resources',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'service account',\n",
       "    'description': 'Account used by pods to authenticate with API server',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PodList',\n",
       "    'description': 'API object representing a list of pod resources',\n",
       "    'category': 'application'},\n",
       "   {'entity': '$TOKEN',\n",
       "    'description': 'Environment variable containing authentication token',\n",
       "    'category': 'environmental'},\n",
       "   {'entity': 'PUT/PATCH requests',\n",
       "    'description': 'HTTP request methods for updating or patching API objects',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Downward API\",\\n    \"description\": \"provides namespace to pod through API\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"creates clusterrolebinding for permissive access\",\\n    \"destination_entity\": \"clusterrolebinding\"\\n  },\\n  {\\n    \"source_entity\": \"GET request\",\\n    \"description\": \"lists all pods in the same namespace as the pod\",\\n    \"destination_entity\": \"PodList\"\\n  },\\n  {\\n    \"source_entity\": \"service account\",\\n    \"description\": \"may not be authorized to access API server due to RBAC\",\\n    \"destination_entity\": \"RBAC\"\\n  },\\n  {\\n    \"source_entity\": \"Authorization header\",\\n    \"description\": \"contains token passed by user for authentication\",\\n    \"destination_entity\": \"$TOKEN\"\\n  },\\n  {\\n    \"source_entity\": \"namespace\",\\n    \"description\": \"is contained in a file that can be read to list all pods\",\\n    \"destination_entity\": \"PodList\"\\n  },\\n  {\\n    \"source_entity\": \"clusterrolebinding\",\\n    \"description\": \"grants cluster-admin privileges to service accounts for access\",\\n    \"destination_entity\": \"service account\"\\n  },\\n  {\\n    \"source_entity\": \"curl\",\\n    \"description\": \"makes request to API server with Authorization header and namespace\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"PUT/PATCH requests\",\\n    \"description\": \"can be used to update or patch API objects\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"$TOKEN\",\\n    \"description\": \"is passed as Authorization header for authentication\",\\n    \"destination_entity\": \"Authorization header\"\\n  }\\n]'},\n",
       " {'page': 275,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '243\\nTalking to the Kubernetes API server\\nRECAPPING HOW PODS TALK TO KUBERNETES\\nLet’s recap how an app running inside a pod can access the Kubernetes API properly:\\n\\uf0a1The app should verify whether the API server’s certificate is signed by the certif-\\nicate authority, whose certificate is in the ca.crt file. \\n\\uf0a1The app should authenticate itself by sending the Authorization header with\\nthe bearer token from the token file. \\n\\uf0a1The namespace file should be used to pass the namespace to the API server when\\nperforming CRUD operations on API objects inside the pod’s namespace.\\nDEFINITION\\nCRUD stands for Create, Read, Update, and Delete. The corre-\\nsponding HTTP methods are POST, GET, PATCH/PUT, and DELETE, respectively.\\nAll three aspects of pod to API server communication are displayed in figure 8.5.\\n8.2.3\\nSimplifying API server communication with ambassador \\ncontainers\\nDealing with HTTPS, certificates, and authentication tokens sometimes seems too\\ncomplicated to developers. I’ve seen developers disable validation of server certifi-\\ncates on way too many occasions (and I’ll admit to doing it myself a few times). Luck-\\nily, you can make the communication much simpler while keeping it secure. \\nAPI server\\nGET /api/v1/namespaces/<namespace>/pods\\nAuthorization: Bearer <token>\\nPod\\nContainer\\nFilesystem\\nApp\\n/\\nvar/\\nrun/\\nsecrets/\\nkubernetes.io/\\nserviceaccount/\\nDefault token secret volume\\nca.crt\\ntoken\\nnamespace\\nServer\\ncertiﬁcate\\nValidate\\ncertiﬁcate\\nFigure 8.5\\nUsing the files from the default-token Secret to talk to the API server\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes API server',\n",
       "    'description': 'A service that manages and provides access to Kubernetes resources.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A lightweight container runtime provided by Docker, used in Kubernetes for running application containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': \"API server's certificate\",\n",
       "    'description': 'A digital certificate that authenticates the identity of a server and verifies its integrity.',\n",
       "    'category': 'certificate'},\n",
       "   {'entity': 'ca.crt file',\n",
       "    'description': \"A file containing the root Certificate Authority (CA) certificate used to verify the authenticity of other certificates, including the API server's certificate.\",\n",
       "    'category': 'file'},\n",
       "   {'entity': 'Authorization header',\n",
       "    'description': 'An HTTP header that contains authentication information, such as a bearer token, to access protected resources.',\n",
       "    'category': 'header'},\n",
       "   {'entity': 'bearer token',\n",
       "    'description': 'A type of JSON Web Token (JWT) used for authentication and authorization in Kubernetes.',\n",
       "    'category': 'token'},\n",
       "   {'entity': 'namespace file',\n",
       "    'description': \"A file containing the namespace information that is passed to the API server when performing CRUD operations on API objects inside a pod's namespace.\",\n",
       "    'category': 'file'},\n",
       "   {'entity': 'CRUD (Create, Read, Update, Delete)',\n",
       "    'description': 'A set of HTTP methods used for interacting with resources in Kubernetes, including POST, GET, PATCH/PUT, and DELETE.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ambassador containers',\n",
       "    'description': 'Containers that simplify API server communication by handling HTTPS, certificates, and authentication tokens.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'default-token Secret',\n",
       "    'description': 'A Kubernetes Secret object containing default token information used for authenticating with the API server.',\n",
       "    'category': 'secret'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"The app\",\\n    \"description\": \"verifies whether the API server\\'s certificate is signed by the certificate authority\",\\n    \"destination_entity\": \"ca.crt file\"\\n  },\\n  {\\n    \"source_entity\": \"The app\",\\n    \"description\": \"sends the Authorization header with the bearer token from the token file\",\\n    \"destination_entity\": \"Authorization header\"\\n  },\\n  {\\n    \"source_entity\": \"The app\",\\n    \"description\": \"performs CRUD operations on API objects inside the pod\\'s namespace\",\\n    \"destination_entity\": \"namespace file\"\\n  },\\n  {\\n    \"source_entity\": \"The app\",\\n    \"description\": \"uses files from the default-token Secret to talk to the API server\",\\n    \"destination_entity\": \"default-token Secret\"\\n  },\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"returns a list of pods in the specified namespace\",\\n    \"destination_entity\": \"/api/v1/namespaces/<namespace>/pods\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"communicates with the Kubernetes API server using ambassador containers\",\\n    \"destination_entity\": \"ambassador containers\"\\n  },\\n  {\\n    \"source_entity\": \"API server\\'s certificate\",\\n    \"description\": \"is validated by the app to ensure it is signed by the certificate authority\",\\n    \"destination_entity\": \"ca.crt file\"\\n  },\\n  {\\n    \"source_entity\": \"The app\",\\n    \"description\": \"performs CRUD operations on API objects using HTTP methods\",\\n    \"destination_entity\": \"CRUD (Create, Read, Update, Delete)\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes API server\",\\n    \"description\": \"responds to requests from the app with bearer token authentication\",\\n    \"destination_entity\": \"bearer token\"\\n  }\\n]\\n\\nNote that some of these relations may be implicit or inferred from the context.'},\n",
       " {'page': 276,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '244\\nCHAPTER 8\\nAccessing pod metadata and other resources from applications\\n Remember the kubectl proxy command we mentioned in section 8.2.1? You ran\\nthe command on your local machine to make it easier to access the API server. Instead\\nof sending requests to the API server directly, you sent them to the proxy and let it\\ntake care of authentication, encryption, and server verification. The same method can\\nbe used inside your pods, as well.\\nINTRODUCING THE AMBASSADOR CONTAINER PATTERN\\nImagine having an application that (among other things) needs to query the API\\nserver. Instead of it talking to the API server directly, as you did in the previous sec-\\ntion, you can run kubectl proxy in an ambassador container alongside the main con-\\ntainer and communicate with the API server through it. \\n Instead of talking to the API server directly, the app in the main container can con-\\nnect to the ambassador through HTTP (instead of HTTPS) and let the ambassador\\nproxy handle the HTTPS connection to the API server, taking care of security trans-\\nparently (see figure 8.6). It does this by using the files from the default token’s secret\\nvolume.\\nBecause all containers in a pod share the same loopback network interface, your app\\ncan access the proxy through a port on localhost.\\nRUNNING THE CURL POD WITH AN ADDITIONAL AMBASSADOR CONTAINER\\nTo see the ambassador container pattern in action, you’ll create a new pod like the\\ncurl pod you created earlier, but this time, instead of running a single container in\\nthe pod, you’ll run an additional ambassador container based on a general-purpose\\nkubectl-proxy container image I’ve created and pushed to Docker Hub. You’ll find\\nthe Dockerfile for the image in the code archive (in /Chapter08/kubectl-proxy/) if\\nyou want to build it yourself.\\n The pod’s manifest is shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: curl-with-ambassador\\nspec:\\n  containers:\\n  - name: main\\nListing 8.15\\nA pod with an ambassador container: curl-with-ambassador.yaml\\nContainer:\\nmain\\nContainer:\\nambassador\\nHTTP\\nHTTPS\\nAPI server\\nPod\\nFigure 8.6\\nUsing an ambassador to connect to the API server\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command used for accessing pod metadata and other resources from applications',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'server that provides access to pod metadata and other resources',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl proxy',\n",
       "    'description': 'command used to make it easier to access the API server',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ambassador container',\n",
       "    'description': 'container pattern used for accessing the API server through an ambassador',\n",
       "    'category': 'pattern'},\n",
       "   {'entity': 'Docker Hub',\n",
       "    'description': 'registry service where Docker images are stored',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'Dockerfile',\n",
       "    'description': 'file that contains instructions for building a Docker image',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'kubectl-proxy container image',\n",
       "    'description': 'image used for creating an ambassador container based on kubectl-proxy',\n",
       "    'category': 'image'},\n",
       "   {'entity': 'HTTP',\n",
       "    'description': 'protocol used for communication between the app and the ambassador',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'HTTPS',\n",
       "    'description': 'protocol used for secure communication between the ambassador and the API server',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'localhost',\n",
       "    'description': 'loopback network interface shared by all containers in a pod',\n",
       "    'category': 'interface'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'basic execution unit in Kubernetes that contains one or more containers',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'execution environment for an application or service within a pod',\n",
       "    'category': 'resource'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"run kubectl proxy command to access API server\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"kubectl-proxy container image\", \"description\": \"provide a general-purpose image for ambassador container\", \"destination_entity\": \"ambassador container\"},\\n  {\"source_entity\": \"HTTPS\", \"description\": \"connect to API server through HTTPS protocol\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"localhost\", \"description\": \"access proxy on local machine\", \"destination_entity\": \"proxy\"},\\n  {\"source_entity\": \"HTTP\", \"description\": \"communicate with ambassador container through HTTP\", \"destination_entity\": \"ambassador container\"},\\n  {\"source_entity\": \"ambassador container\", \"description\": \"act as a proxy to API server for main container\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"pod\", \"description\": \"share same loopback network interface with ambassador container\", \"destination_entity\": \"ambassador container\"},\\n  {\"source_entity\": \"Dockerfile\", \"description\": \"provide image for ambassador container\", \"destination_entity\": \"kubectl-proxy container image\"},\\n  {\"source_entity\": \"API server\", \"description\": \"provide access to API through proxy\", \"destination_entity\": \"proxy\"},\\n  {\"source_entity\": \"container\", \"description\": \"run main container alongside ambassador container\", \"destination_entity\": \"ambassador container\"},\\n  {\"source_entity\": \"kubectl proxy\", \"description\": \"use as a method for accessing API server\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"Docker Hub\", \"description\": \"store image for ambassador container\", \"destination_entity\": \"kubectl-proxy container image\"}\\n]'},\n",
       " {'page': 277,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '245\\nTalking to the Kubernetes API server\\n    image: tutum/curl\\n    command: [\"sleep\", \"9999999\"]\\n  - name: ambassador                         \\n    image: luksa/kubectl-proxy:1.6.2         \\nThe pod spec is almost the same as before, but with a different pod name and an addi-\\ntional container. Run the pod and then enter the main container with\\n$ kubectl exec -it curl-with-ambassador -c main bash\\nroot@curl-with-ambassador:/#\\nYour pod now has two containers, and you want to run bash in the main container,\\nhence the -c main option. You don’t need to specify the container explicitly if you\\nwant to run the command in the pod’s first container. But if you want to run a com-\\nmand inside any other container, you do need to specify the container’s name using\\nthe -c option.\\nTALKING TO THE API SERVER THROUGH THE AMBASSADOR\\nNext you’ll try connecting to the API server through the ambassador container. By\\ndefault, kubectl proxy binds to port 8001, and because both containers in the pod\\nshare the same network interfaces, including loopback, you can point curl to local-\\nhost:8001, as shown in the following listing.\\nroot@curl-with-ambassador:/# curl localhost:8001\\n{\\n  \"paths\": [\\n    \"/api\",\\n    ...\\n  ]\\n}\\nSuccess! The output printed by curl is the same response you saw earlier, but this time\\nyou didn’t need to deal with authentication tokens and server certificates. \\n To get a clear picture of what exactly happened, refer to figure 8.7. curl sent the\\nplain HTTP request (without any authentication headers) to the proxy running inside\\nthe ambassador container, and then the proxy sent an HTTPS request to the API\\nserver, handling the client authentication by sending the token and checking the\\nserver’s identity by validating its certificate.\\n This is a great example of how an ambassador container can be used to hide the\\ncomplexities of connecting to an external service and simplify the app running in\\nthe main container. The ambassador container is reusable across many different apps,\\nregardless of what language the main app is written in. The downside is that an addi-\\ntional process is running and consuming additional resources.\\nListing 8.16\\nAccessing the API server through the ambassador container\\nThe ambassador container, \\nrunning the kubectl-proxy image\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'Container runtime',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'Command-line tool for transferring data',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Lightweight and portable container execution environment',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'Process that is running inside a pod',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Kubernetes component that exposes the API',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'ambassador container',\n",
       "    'description': 'Container that acts as a proxy to an external service',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl-proxy image',\n",
       "    'description': 'Image for running kubectl-proxy',\n",
       "    'category': 'image'},\n",
       "   {'entity': 'Tutum',\n",
       "    'description': 'Cloud platform provider',\n",
       "    'category': 'company'},\n",
       "   {'entity': 'Luksa',\n",
       "    'description': 'Developer of the kubectl-proxy image',\n",
       "    'category': 'company'}],\n",
       "  'relationships': '[{\"source_entity\": \"kubectl\", \"description\": \"executes a command to connect to API server through ambassador container\", \"destination_entity\": \"ambassador container\"}, {\"source_entity\": \"kubectl\", \"description\": \"runs a pod with an additional container for ambassador\", \"destination_entity\": \"pod\"}, {\"source_entity\": \"curl\", \"description\": \"sends a plain HTTP request to proxy running in ambassador container\", \"destination_entity\": \"ambassador container\"}, {\"source_entity\": \"ambassador container\", \"description\": \"acts as a proxy and sends an HTTPS request to API server\", \"destination_entity\": \"API server\"}, {\"source_entity\": \"kubectl-proxy image\", \"description\": \"runs inside the ambassador container\", \"destination_entity\": \"ambassador container\"}, {\"source_entity\": \"Docker\", \"description\": \"provides a runtime environment for the kubectl-proxy image\", \"destination_entity\": \"kubectl-proxy image\"}]\\n\\nNote: The relation between Luksa and the entities is not explicitly mentioned in the document, so I didn\\'t include it. Also, Tutum and curl are related as they are both container images, but the specific relation is not clear from the text, so I only included a general relation between them.\\n\\nAlso, here\\'s an explanation of each relation:\\n\\n1. **kubectl executes a command to connect to API server through ambassador container**: This relation describes how kubectl is used to execute a command that connects to the API server through the ambassador container.\\n2. **kubectl runs a pod with an additional container for ambassador**: This relation explains how kubectl is used to run a pod with an additional container (ambassador) added to it.\\n3. **curl sends a plain HTTP request to proxy running in ambassador container**: This relation describes how curl sends a plain HTTP request to the proxy running inside the ambassador container.\\n4. **ambassador container acts as a proxy and sends an HTTPS request to API server**: This relation explains how the ambassador container acts as a proxy, sending an HTTPS request to the API server.\\n5. **kubectl-proxy image runs inside the ambassador container**: This relation describes how the kubectl-proxy image is run inside the ambassador container.\\n6. **Docker provides a runtime environment for the kubectl-proxy image**: This relation explains how Docker provides a runtime environment for the kubectl-proxy image.\\n\\nLet me know if you\\'d like me to clarify anything!'},\n",
       " {'page': 278,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '246\\nCHAPTER 8\\nAccessing pod metadata and other resources from applications\\n8.2.4\\nUsing client libraries to talk to the API server\\nIf your app only needs to perform a few simple operations on the API server, you can\\noften use a regular HTTP client library and perform simple HTTP requests, especially\\nif you take advantage of the kubectl-proxy ambassador container the way you did in\\nthe previous example. But if you plan on doing more than simple API requests, it’s\\nbetter to use one of the existing Kubernetes API client libraries.\\nUSING EXISTING CLIENT LIBRARIES\\nCurrently, two Kubernetes API client libraries exist that are supported by the API\\nMachinery special interest group (SIG):\\n\\uf0a1Golang client—https:/\\n/github.com/kubernetes/client-go\\n\\uf0a1Python—https:/\\n/github.com/kubernetes-incubator/client-python\\nNOTE\\nThe Kubernetes community has a number of Special Interest Groups\\n(SIGs) and Working Groups that focus on specific parts of the Kubernetes\\necosystem. You’ll find a list of them at https:/\\n/github.com/kubernetes/com-\\nmunity/blob/master/sig-list.md.\\nIn addition to the two officially supported libraries, here’s a list of user-contributed cli-\\nent libraries for many other languages:\\n\\uf0a1Java client by Fabric8—https:/\\n/github.com/fabric8io/kubernetes-client\\n\\uf0a1Java client by Amdatu—https:/\\n/bitbucket.org/amdatulabs/amdatu-kubernetes\\n\\uf0a1Node.js client by tenxcloud—https:/\\n/github.com/tenxcloud/node-kubernetes-client\\n\\uf0a1Node.js client by GoDaddy—https:/\\n/github.com/godaddy/kubernetes-client\\n\\uf0a1PHP—https:/\\n/github.com/devstub/kubernetes-api-php-client\\n\\uf0a1Another PHP client—https:/\\n/github.com/maclof/kubernetes-client\\nContainer: main\\nAPI server\\nsleep\\ncurl\\nContainer: ambassador\\nkubectl proxy\\nPort 8001\\nGET http://localhost:8001\\nGET https://kubernetes:443\\nAuthorization: Bearer <token>\\nPod\\nFigure 8.7\\nOffloading encryption, authentication, and server verification to kubectl proxy in an \\nambassador container \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes API client libraries',\n",
       "    'description': '',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Golang client',\n",
       "    'description': 'https://github.com/kubernetes/client-go',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Python client',\n",
       "    'description': 'https://github.com/kubernetes-incubator/client-python',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl-proxy ambassador container',\n",
       "    'description': '',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'HTTP client library',\n",
       "    'description': '',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server', 'description': '', 'category': 'application'},\n",
       "   {'entity': 'Pod', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'Kubectl proxy', 'description': '', 'category': 'command'},\n",
       "   {'entity': 'curl', 'description': '', 'category': 'command'},\n",
       "   {'entity': 'sleep', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'GET',\n",
       "    'description': 'HTTP request method',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'http://localhost:8001', 'description': '', 'category': 'url'},\n",
       "   {'entity': 'https://kubernetes:443', 'description': '', 'category': 'url'},\n",
       "   {'entity': 'Authorization', 'description': '', 'category': 'header'},\n",
       "   {'entity': 'Bearer token', 'description': '', 'category': 'authorization'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Golang client\",\\n    \"description\": \"uses to perform simple API requests on Kubernetes API server\",\\n    \"destination_entity\": \"Kubernetes API server\"\\n  },\\n  {\\n    \"source_entity\": \"Python client\",\\n    \"description\": \"is a client library for performing operations on Kubernetes API server\",\\n    \"destination_entity\": \"Kubernetes API server\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl-proxy ambassador container\",\\n    \"description\": \"helps to perform simple HTTP requests using kubectl proxy\",\\n    \"destination_entity\": \"Kubectl proxy\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl-proxy ambassador container\",\\n    \"description\": \"uses port 8001 for making GET requests\",\\n    \"destination_entity\": \"http://localhost:8001\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl-proxy ambassador container\",\\n    \"description\": \"uses https protocol and port 443 for making GET requests\",\\n    \"destination_entity\": \"https://kubernetes:443\"\\n  },\\n  {\\n    \"source_entity\": \"curl\",\\n    \"description\": \"is used to make HTTP request on API server\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"curl\",\\n    \"description\": \"uses GET method to make requests\",\\n    \"destination_entity\": \"GET\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes community\",\\n    \"description\": \"has Special Interest Groups (SIGs) for focusing on specific parts of Kubernetes ecosystem\",\\n    \"destination_entity\": \"Special Interest Groups (SIGs)\"\\n  },\\n  {\\n    \"source_entity\": \"HTTP client library\",\\n    \"description\": \"can be used to make simple HTTP requests\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"Kubectl proxy\",\\n    \"description\": \"helps to offload encryption, authentication and server verification to kubectl proxy in an ambassador container\",\\n    \"destination_entity\": \"ambassador container\"\\n  },\\n  {\\n    \"source_entity\": \"Bearer token\",\\n    \"description\": \"is required for authorization\",\\n    \"destination_entity\": \"Authorization\"\\n  }\\n]\\n```'},\n",
       " {'page': 279,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '247\\nTalking to the Kubernetes API server\\n\\uf0a1Ruby—https:/\\n/github.com/Ch00k/kubr\\n\\uf0a1Another Ruby client—https:/\\n/github.com/abonas/kubeclient\\n\\uf0a1Clojure—https:/\\n/github.com/yanatan16/clj-kubernetes-api\\n\\uf0a1Scala—https:/\\n/github.com/doriordan/skuber\\n\\uf0a1Perl—https:/\\n/metacpan.org/pod/Net::Kubernetes\\nThese libraries usually support HTTPS and take care of authentication, so you won’t\\nneed to use the ambassador container. \\nAN EXAMPLE OF INTERACTING WITH KUBERNETES WITH THE FABRIC8 JAVA CLIENT\\nTo give you a sense of how client libraries enable you to talk to the API server, the fol-\\nlowing listing shows an example of how to list services in a Java app using the Fabric8\\nKubernetes client.\\nimport java.util.Arrays;\\nimport io.fabric8.kubernetes.api.model.Pod;\\nimport io.fabric8.kubernetes.api.model.PodList;\\nimport io.fabric8.kubernetes.client.DefaultKubernetesClient;\\nimport io.fabric8.kubernetes.client.KubernetesClient;\\npublic class Test {\\n  public static void main(String[] args) throws Exception {\\n    KubernetesClient client = new DefaultKubernetesClient();\\n    // list pods in the default namespace\\n    PodList pods = client.pods().inNamespace(\"default\").list();\\n    pods.getItems().stream()\\n      .forEach(s -> System.out.println(\"Found pod: \" +\\n               s.getMetadata().getName()));\\n    // create a pod\\n    System.out.println(\"Creating a pod\");\\n    Pod pod = client.pods().inNamespace(\"default\")\\n      .createNew()\\n      .withNewMetadata()\\n        .withName(\"programmatically-created-pod\")\\n      .endMetadata()\\n      .withNewSpec()\\n        .addNewContainer()\\n          .withName(\"main\")\\n          .withImage(\"busybox\")\\n          .withCommand(Arrays.asList(\"sleep\", \"99999\"))\\n        .endContainer()\\n      .endSpec()\\n      .done();\\n    System.out.println(\"Created pod: \" + pod);\\n    // edit the pod (add a label to it)\\n    client.pods().inNamespace(\"default\")\\n      .withName(\"programmatically-created-pod\")\\n      .edit()\\n      .editMetadata()\\nListing 8.17\\nListing, creating, updating, and deleting pods with the Fabric8 Java client\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Ruby',\n",
       "    'description': 'Programming language',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'https://github.com/Ch00k/kubr',\n",
       "    'description': 'Ruby client library for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Clojure',\n",
       "    'description': 'Programming language',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'https://github.com/yanatan16/clj-kubernetes-api',\n",
       "    'description': 'Clojure client library for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Scala',\n",
       "    'description': 'Programming language',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'https://github.com/doriordan/skuber',\n",
       "    'description': 'Scala client library for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Perl',\n",
       "    'description': 'Programming language',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'https://metacpan.org/pod/Net::Kubernetes',\n",
       "    'description': 'Perl client library for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'HTTPS',\n",
       "    'description': 'Protocol for secure communication',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Fabric8 Java Client',\n",
       "    'description': 'Java client library for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes API server',\n",
       "    'description': 'Server providing Kubernetes API',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Resource in Kubernetes representing a running container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'PodList',\n",
       "    'description': 'Collection of pods in Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'DefaultKubernetesClient',\n",
       "    'description': 'Java client library for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'KubernetesClient',\n",
       "    'description': 'Interface for interacting with Kubernetes API',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PodList pods',\n",
       "    'description': 'Listing pods in a namespace using Fabric8 Java Client',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'client.pods().inNamespace(\"default\")',\n",
       "    'description': 'Method for listing pods in a specific namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pods.getItems().stream()',\n",
       "    'description': 'Streaming API for iterating over pod list',\n",
       "    'category': 'software'},\n",
       "   {'entity': 's.getMetadata().getName()',\n",
       "    'description': \"Method for getting the name of a pod's metadata\",\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"DefaultKubernetesClient\", \"description\": \"uses to interact with Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"https://github.com/doriordan/skuber\", \"description\": \"provides support for Scala programming language to talk to Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"client.pods().inNamespace(\\\\\"default\\\\\")\", \"description\": \"used to list pods in default namespace\", \"destination_entity\": \"PodList pods\"},\\n  {\"source_entity\": \"https://github.com/yanatan16/clj-kubernetes-api\", \"description\": \"supports Clojure programming language to interact with Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"https://metacpan.org/pod/Net::Kubernetes\", \"description\": \"provides support for Perl programming language to talk to Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"s.getMetadata().getName()\", \"description\": \"used to get the name of a pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"HTTPS\", \"description\": \"used for secure communication between client and Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"pods.getItems().stream()\", \"description\": \"used to stream items from pods list\", \"destination_entity\": \"PodList pods\"},\\n  {\"source_entity\": \"Ruby\", \"description\": \"has a client library (kubr) that supports HTTPS and authentication\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"https://github.com/Ch00k/kubr\", \"description\": \"provides support for Ruby programming language to talk to Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"Fabric8 Java Client\", \"description\": \"has a client library that enables interaction with Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"PodList pods\", \"description\": \"used to store the list of pods\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"Perl\", \"description\": \"has a client library (Net::Kubernetes) that supports HTTPS and authentication\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"KubernetesClient\", \"description\": \"used to interact with Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"Scala\", \"description\": \"has a client library (skuber) that supports HTTPS and authentication\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"used to store the metadata of a pod\", \"destination_entity\": \"Kubernetes API server\"}\\n]'},\n",
       " {'page': 280,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '248\\nCHAPTER 8\\nAccessing pod metadata and other resources from applications\\n        .addToLabels(\"foo\", \"bar\")\\n      .endMetadata()\\n      .done();\\n    System.out.println(\"Added label foo=bar to pod\");\\n    System.out.println(\"Waiting 1 minute before deleting pod...\");\\n    Thread.sleep(60000);\\n    // delete the pod\\n    client.pods().inNamespace(\"default\")\\n      .withName(\"programmatically-created-pod\")\\n      .delete();\\n    System.out.println(\"Deleted the pod\");\\n  }\\n}\\nThe code should be self-explanatory, especially because the Fabric8 client exposes\\na nice, fluent Domain-Specific-Language (DSL) API, which is easy to read and\\nunderstand.\\nBUILDING YOUR OWN LIBRARY WITH SWAGGER AND OPENAPI\\nIf no client is available for your programming language of choice, you can use the\\nSwagger API framework to generate the client library and documentation. The Kuber-\\nnetes API server exposes Swagger API definitions at /swaggerapi and OpenAPI spec at\\n/swagger.json. \\n To find out more about the Swagger framework, visit the website at http:/\\n/swagger.io.\\nEXPLORING THE API WITH SWAGGER UI\\nEarlier in the chapter I said I’d point you to a better way of exploring the REST API\\ninstead of hitting the REST endpoints with curl. Swagger, which I mentioned in the\\nprevious section, is not just a tool for specifying an API, but also provides a web UI for\\nexploring REST APIs if they expose the Swagger API definitions. The better way of\\nexploring REST APIs is through this UI.\\n Kubernetes not only exposes the Swagger API, but it also has Swagger UI inte-\\ngrated into the API server, though it’s not enabled by default. You can enable it by\\nrunning the API server with the --enable-swagger-ui=true option.\\nTIP\\nIf you’re using Minikube, you can enable Swagger UI when starting the\\ncluster: minikube start --extra-config=apiserver.Features.Enable-\\nSwaggerUI=true\\nAfter you enable the UI, you can open it in your browser by pointing it to:\\nhttp(s)://<api server>:<port>/swagger-ui\\nI urge you to give Swagger UI a try. It not only allows you to browse the Kubernetes\\nAPI, but also interact with it (you can POST JSON resource manifests, PATCH resources,\\nor DELETE them, for example). \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Fabric8 client',\n",
       "    'description': 'Library for interacting with the Kubernetes API',\n",
       "    'category': 'library'},\n",
       "   {'entity': 'Domain-Specific-Language (DSL) API',\n",
       "    'description': 'Fluent API for interacting with the Kubernetes API',\n",
       "    'category': 'API'},\n",
       "   {'entity': 'Swagger API framework',\n",
       "    'description': 'Tool for specifying and documenting APIs',\n",
       "    'category': 'framework'},\n",
       "   {'entity': 'OpenAPI spec',\n",
       "    'description': 'Standard for describing RESTful APIs',\n",
       "    'category': 'standard'},\n",
       "   {'entity': 'REST endpoints',\n",
       "    'description': 'Endpoints for accessing resources via HTTP requests',\n",
       "    'category': 'endpoint'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'Command-line tool for sending HTTP requests',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Swagger UI',\n",
       "    'description': 'Web interface for exploring and interacting with APIs',\n",
       "    'category': 'UI'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Server that exposes the Kubernetes API',\n",
       "    'category': 'server'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'Tool for running a local Kubernetes cluster',\n",
       "    'category': 'tool'},\n",
       "   {'entity': 'cluster',\n",
       "    'description': 'Collection of nodes in a Kubernetes deployment',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Lightweight and ephemeral container in a Kubernetes cluster',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'client',\n",
       "    'description': 'Library or command-line tool for interacting with the Kubernetes API',\n",
       "    'category': 'library'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'Plural form of pod, referring to multiple containers running on the same node',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'Key-value pairs attached to resources in a Kubernetes cluster',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'Thread.sleep',\n",
       "    'description': 'Method for pausing program execution for a specified duration',\n",
       "    'category': 'method'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Fabric8 client\", \"description\": \"exposes a nice, fluent DSL API\", \"destination_entity\": \"Domain-Specific-Language (DSL) API\"},\\n  {\"source_entity\": \"Swagger UI\", \"description\": \"provides a web UI for exploring REST APIs\", \"destination_entity\": \"REST endpoints\"},\\n  {\"source_entity\": \"cluster\", \"description\": \"can be started with Swagger UI enabled\", \"destination_entity\": \"Swagger UI\"},\\n  {\"source_entity\": \"Fabric8 client\", \"description\": \"exposes a nice, fluent DSL API\", \"destination_entity\": \"client\"},\\n  {\"source_entity\": \"Kubernetes API server\", \"description\": \"exposes Swagger API definitions at /swaggerapi and OpenAPI spec at /swagger.json\", \"destination_entity\": \"Swagger API framework\"},\\n  {\"source_entity\": \"Fabric8 client\", \"description\": \"allows you to delete a pod\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"Minikube\", \"description\": \"can enable Swagger UI when starting the cluster\", \"destination_entity\": \"cluster\"},\\n  {\"source_entity\": \"Thread.sleep\", \"description\": \"is used to wait for 1 minute before deleting a pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Kubernetes API server\", \"description\": \"exposes OpenAPI spec at /swagger.json\", \"destination_entity\": \"OpenAPI spec\"},\\n  {\"source_entity\": \"Swagger UI\", \"description\": \"allows you to browse the Kubernetes API and interact with it\", \"destination_entity\": \"REST endpoints\"},\\n  {\"source_entity\": \"client\", \"description\": \"can be used to delete a pod\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"Fabric8 client\", \"description\": \"exposes a method to add labels to a pod\", \"destination_entity\": \"labels\"},\\n  {\"source_entity\": \"Kubernetes API server\", \"description\": \"is enabled by default but can be disabled\", \"destination_entity\": \"Swagger UI\"}\\n]\\n```'},\n",
       " {'page': 281,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '249\\nSummary\\n8.3\\nSummary\\nAfter reading this chapter, you now know how your app, running inside a pod, can get\\ndata about itself, other pods, and other components deployed in the cluster. You’ve\\nlearned\\n\\uf0a1How a pod’s name, namespace, and other metadata can be exposed to the pro-\\ncess either through environment variables or files in a downwardAPI volume\\n\\uf0a1How CPU and memory requests and limits are passed to your app in any unit\\nthe app requires\\n\\uf0a1How a pod can use downwardAPI volumes to get up-to-date metadata, which\\nmay change during the lifetime of the pod (such as labels and annotations) \\n\\uf0a1How you can browse the Kubernetes REST API through kubectl proxy\\n\\uf0a1How pods can find the API server’s location through environment variables or\\nDNS, similar to any other Service defined in Kubernetes\\n\\uf0a1How an application running in a pod can verify that it’s talking to the API\\nserver and how it can authenticate itself\\n\\uf0a1How using an ambassador container can make talking to the API server from\\nwithin an app much simpler\\n\\uf0a1How client libraries can get you interacting with Kubernetes in minutes\\nIn this chapter, you learned how to talk to the API server, so the next step is learning\\nmore about how it works. You’ll do that in chapter 11, but before we dive into such\\ndetails, you still need to learn about two other Kubernetes resources—Deployments\\nand StatefulSets. They’re explained in the next two chapters.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A pod is a logical host for an application in Kubernetes.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': 'The API server is the central component of the Kubernetes control plane, responsible for managing cluster resources.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'REST API',\n",
       "    'description': 'The REST API is an interface to interact with the Kubernetes API server, allowing users to perform various operations such as creating and deleting resources.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'kubectl proxy',\n",
       "    'description': 'kubectl proxy is a command-line tool that enables browsing the Kubernetes REST API through a proxy server, making it easier to interact with the cluster.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'DownwardAPI volume',\n",
       "    'description': 'A DownwardAPI volume allows a pod to get up-to-date metadata, such as labels and annotations, which can change during the lifetime of the pod.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': 'A namespace is a logical partitioning of resources within a Kubernetes cluster, allowing multiple applications to run on the same cluster without conflicts.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Labels and Annotations',\n",
       "    'description': 'Labels and annotations are metadata tags that can be attached to Kubernetes resources such as pods, services, and deployments, providing additional information about their purpose or characteristics.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Environment Variables',\n",
       "    'description': 'Environment variables are a way for a pod to expose its name, namespace, and other metadata to the process running within it, allowing the process to interact with the cluster.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'CPU and Memory Requests/Limits',\n",
       "    'description': 'CPU and memory requests/limits refer to the amount of resources that can be allocated to a pod or container, ensuring that applications have sufficient resources to run efficiently.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"can expose metadata to process through environment variables or files in a downwardAPI volume\",\\n    \"destination_entity\": \"Environment Variables\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"can use downwardAPI volumes to get up-to-date metadata, such as labels and annotations\",\\n    \"destination_entity\": \"Labels and Annotations\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"can find API server\\'s location through environment variables or DNS\",\\n    \"destination_entity\": \"REST API\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"can get CPU and memory requests and limits in any unit the app requires\",\\n    \"destination_entity\": \"CPU and Memory Requests/Limits\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl proxy\",\\n    \"description\": \"allows browsing Kubernetes REST API\",\\n    \"destination_entity\": \"REST API\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"can use downwardAPI volumes to get up-to-date metadata, such as namespace\",\\n    \"destination_entity\": \"Namespace\"\\n  },\\n  {\\n    \"source_entity\": \"DownwardAPI volume\",\\n    \"description\": \"provides up-to-date metadata to pod, such as labels and annotations\",\\n    \"destination_entity\": \"Labels and Annotations\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"can authenticate itself with API server\",\\n    \"destination_entity\": \"API Server\"\\n  },\\n  {\\n    \"source_entity\": \"Application running in a pod\",\\n    \"description\": \"can verify it\\'s talking to API server using an ambassador container\",\\n    \"destination_entity\": \"REST API\"\\n  },\\n  {\\n    \"source_entity\": \"Client libraries\",\\n    \"description\": \"can get user interacting with Kubernetes in minutes\",\\n    \"destination_entity\": \"Kubernetes REST API\"\\n  }\\n]\\n```\\n\\nNote: The above JSON list contains all the possible relations extracted from the document page based on the entities provided.'},\n",
       " {'page': 282,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '250\\nDeployments: updating\\napplications declaratively\\nYou now know how to package your app components into containers, group them\\ninto pods, provide them with temporary or permanent storage, pass both secret\\nand non-secret config data to them, and allow pods to find and talk to each other.\\nYou know how to run a full-fledged system composed of independently running\\nsmaller components—microservices, if you will. Is there anything else? \\n Eventually, you’re going to want to update your app. This chapter covers how to\\nupdate apps running in a Kubernetes cluster and how Kubernetes helps you move\\ntoward a true zero-downtime update process. Although this can be achieved using\\nonly ReplicationControllers or ReplicaSets, Kubernetes also provides a Deployment\\nThis chapter covers\\n\\uf0a1Replacing pods with newer versions\\n\\uf0a1Updating managed pods\\n\\uf0a1Updating pods declaratively using Deployment \\nresources\\n\\uf0a1Performing rolling updates\\n\\uf0a1Automatically blocking rollouts of bad versions\\n\\uf0a1Controlling the rate of the rollout\\n\\uf0a1Reverting pods to a previous version\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Deployments',\n",
       "    'description': 'update applications declaratively',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'applications',\n",
       "    'description': 'package your app components into containers, group them into pods, provide them with temporary or permanent storage, pass both secret and non-secret config data to them, and allow pods to find and talk to each other',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'grouped components of an application',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'storage',\n",
       "    'description': 'temporary or permanent storage for pods',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'config data',\n",
       "    'description': 'secret and non-secret configuration data passed to pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicationControllers',\n",
       "    'description': 'used for updating apps running in a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': ' ReplicaSets ',\n",
       "    'description': 'used for updating apps running in a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'orchestration system used to manage containerized applications',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment resources',\n",
       "    'description': 'used for performing rolling updates and controlling the rate of rollout',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'rolling updates',\n",
       "    'description': 'process of updating pods without interrupting the service',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'rollout',\n",
       "    'description': 'process of deploying a new version of an application',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"You\", \"description\": \"package app components into containers\", \"destination_entity\": \"containers\"},\\n  {\"source_entity\": \"You\", \"description\": \"group them into pods\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"You\", \"description\": \"provide them with temporary or permanent storage\", \"destination_entity\": \"storage\"},\\n  {\"source_entity\": \"You\", \"description\": \"pass secret and non-secret config data to them\", \"destination_entity\": \"config data\"},\\n  {\"source_entity\": \"pods\", \"description\": \"find and talk to each other\", \"destination_entity\": \"each other\"},\\n  {\"source_entity\": \"You\", \"description\": \"run a full-fledged system composed of independently running smaller components\", \"destination_entity\": \"microservices\"},\\n  {\"source_entity\": \"Deployments\", \"description\": \"update apps running in a Kubernetes cluster\", \"destination_entity\": \"Kubernetes cluster\"},\\n  {\"source_entity\": \"ReplicationControllers\", \"description\": \"achieve zero-downtime update process\", \"destination_entity\": \"zero-downtime update process\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"provide Deployment resources for updating apps\", \"destination_entity\": \"Deployment resources\"},\\n  {\"source_entity\": \"Deployments\", \"description\": \"update managed pods declaratively\", \"destination_entity\": \"managed pods\"},\\n  {\"source_entity\": \"You\", \"description\": \"replace older pods with newer versions\", \"destination_entity\": \"older pods\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"automatically block rollouts of bad versions\", \"destination_entity\": \"bad versions\"},\\n  {\"source_entity\": \"You\", \"description\": \"control the rate of rollout for updating apps\", \"destination_entity\": \"rollout\"},\\n  {\"source_entity\": \"Deployments\", \"description\": \"perform rolling updates for apps\", \"destination_entity\": \"rolling updates\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"revert pods to a previous version if needed\", \"destination_entity\": \"previous version\"}\\n]'},\n",
       " {'page': 283,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '251\\nUpdating applications running in pods\\nresource that sits on top of ReplicaSets and enables declarative application updates. If\\nyou’re not completely sure what that means, keep reading—it’s not as complicated as\\nit sounds.\\n9.1\\nUpdating applications running in pods\\nLet’s start off with a simple example. Imagine having a set of pod instances providing a\\nservice to other pods and/or external clients. After reading this book up to this point,\\nyou likely recognize that these pods are backed by a ReplicationController or a\\nReplicaSet. A Service also exists through which clients (apps running in other pods or\\nexternal clients) access the pods. This is how a basic application looks in Kubernetes\\n(shown in figure 9.1).\\nInitially, the pods run the first version of your application—let’s suppose its image is\\ntagged as v1. You then develop a newer version of the app and push it to an image\\nrepository as a new image, tagged as v2. You’d next like to replace all the pods with\\nthis new version. Because you can’t change an existing pod’s image after the pod is\\ncreated, you need to remove the old pods and replace them with new ones running\\nthe new image. \\n You have two ways of updating all those pods. You can do one of the following:\\n\\uf0a1Delete all existing pods first and then start the new ones.\\n\\uf0a1Start new ones and, once they’re up, delete the old ones. You can do this either\\nby adding all the new pods and then deleting all the old ones at once, or\\nsequentially, by adding new pods and removing old ones gradually.\\nBoth these strategies have their benefits and drawbacks. The first option would lead to\\na short period of time when your application is unavailable. The second option\\nrequires your app to handle running two versions of the app at the same time. If your\\napp stores data in a data store, the new version shouldn’t modify the data schema or\\nthe data in such a way that breaks the previous version.\\nReplicationController\\nor ReplicaSet\\nClients\\nService\\nPod\\nPod\\nPod\\nFigure 9.1\\nThe basic outline of an \\napplication running in Kubernetes\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A pod is a container which can contain one or more application containers.',\n",
       "    'category': 'Container'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A resource that sits on top of ReplicaSets and enables declarative application updates.',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'Enables access to the pods from other pods or external clients.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Image',\n",
       "    'description': 'The version of an application, e.g. v1 or v2.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An orchestration system for containerized applications.',\n",
       "    'category': 'Container Orchestration System'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A resource that enables declarative application updates, similar to ReplicaSet.',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'Pod instances',\n",
       "    'description': 'Multiple pod instances providing a service to other pods or external clients.',\n",
       "    'category': 'Container'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"Backs a set of pod instances providing a service to other pods and/or external clients\",\\n    \"destination_entity\": \"Pod instances\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"Allows clients (apps running in other pods or external clients) to access the pods\",\\n    \"destination_entity\": \"Clients\"\\n  },\\n  {\\n    \"source_entity\": \"Image\",\\n    \"description\": \"Represents a version of an application, e.g. v1 and v2\",\\n    \"destination_entity\": \"Application\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"Runs a specific image (e.g. v1 or v2) after being created\",\\n    \"destination_entity\": \"Image\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSet\",\\n    \"description\": \"Enables declarative application updates by managing the desired number of pod instances\",\\n    \"destination_entity\": \"Pod instances\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"Provides a platform for deploying and managing containerized applications, including services and pods\",\\n    \"destination_entity\": \"Application\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"Can be deleted or replaced with new ones running a different image\",\\n    \"destination_entity\": \"Image\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"Is deprecated in favor of ReplicaSet, which provides similar functionality\",\\n    \"destination_entity\": \"ReplicaSet\"\\n  }\\n]\\n```\\n\\nNote that I\\'ve used the provided entities as the list of possible relations. If you\\'d like to extract additional entities or relations, please let me know!'},\n",
       " {'page': 284,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '252\\nCHAPTER 9\\nDeployments: updating applications declaratively\\n How do you perform these two update methods in Kubernetes? First, let’s look at\\nhow to do this manually; then, once you know what’s involved in the process, you’ll\\nlearn how to have Kubernetes perform the update automatically.\\n9.1.1\\nDeleting old pods and replacing them with new ones\\nYou already know how to get a ReplicationController to replace all its pod instances\\nwith pods running a new version. You probably remember the pod template of a\\nReplicationController can be updated at any time. When the ReplicationController\\ncreates new instances, it uses the updated pod template to create them.\\n If you have a ReplicationController managing a set of v1 pods, you can easily\\nreplace them by modifying the pod template so it refers to version v2 of the image and\\nthen deleting the old pod instances. The ReplicationController will notice that no\\npods match its label selector and it will spin up new instances. The whole process is\\nshown in figure 9.2.\\nThis is the simplest way to update a set of pods, if you can accept the short downtime\\nbetween the time the old pods are deleted and new ones are started.\\n9.1.2\\nSpinning up new pods and then deleting the old ones\\nIf you don’t want to see any downtime and your app supports running multiple ver-\\nsions at once, you can turn the process around and first spin up all the new pods and\\nPod template\\nchanged\\nv pods deleted\\n1\\nmanually\\nReplicationController\\nService\\nPod: v1\\nPod: v1\\nPod\\ntemplate: v2\\nReplicationController\\nPod\\ntemplate: v2\\nPod: v1\\nService\\nPod: v2\\nPod: v2\\nPod: v2\\nReplicationController\\nService\\nPod: v1\\nPod: v1\\nPod\\ntemplate: v1\\nPod: v1\\nReplicationController\\nService\\nPod: v1\\nPod: v1\\nPod: v1\\nPod\\ntemplate: v2\\nShort period of\\ndowntime here\\nv2 pods created by\\nReplicationController\\nFigure 9.2\\nUpdating pods by changing a ReplicationController’s pod template and deleting old Pods\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'Kubernetes component for managing pod replicas',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Lightweight and ephemeral containers in Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Image',\n",
       "    'description': 'Container image with specific version (e.g. v1, v2)',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Label selector',\n",
       "    'description': 'Mechanism for selecting pods based on labels',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'Kubernetes component for exposing applications to network traffic',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'Kubernetes concept for managing rollouts and rollbacks of applications',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod template',\n",
       "    'description': 'Template used by ReplicationController to create new pods',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[{\"source_entity\": \"ReplicationController\", \"description\": \"updates pod instances with new version\", \"destination_entity\": \"pods\"}, \\n {\"source_entity\": \"ReplicationController\", \"description\": \"uses updated pod template to create new instances\", \"destination_entity\": \"new pods\"}, \\n {\"source_entity\": \"user\", \"description\": \"modifies pod template to refer to new image version\", \"destination_entity\": \"pod template\"}, \\n {\"source_entity\": \"ReplicationController\", \"description\": \"notices old pods do not match label selector and spins up new instances\", \"destination_entity\": \"old pods\"}, \\n {\"source_entity\": \"user\", \"description\": \"deletes old pod instances manually\", \"destination_entity\": \"old pods\"}, \\n {\"source_entity\": \"ReplicationController\", \"description\": \"spins up new pods first, then deletes old ones\", \"destination_entity\": \"old pods\"}, \\n {\"source_entity\": \"Service\", \"description\": \"provides connectivity to pods\", \"destination_entity\": \"pods\"}, \\n {\"source_entity\": \"Kubernetes\", \"description\": \"manages deployments and updates automatically\", \"destination_entity\": \"deployments\"}, \\n {\"source_entity\": \"Deployment\", \"description\": \"orchestrates deployment and scaling of applications\", \"destination_entity\": \"applications\"}, \\n {\"source_entity\": \"ReplicationController\", \"description\": \"replaces old pods with new ones based on label selector\", \"destination_entity\": \"pods\"}, \\n {\"source_entity\": \"Image\", \"description\": \"provides versioned application code\", \"destination_entity\": \"application code\"}, \\n {\"source_entity\": \"Pods\", \"description\": \"are managed by ReplicationController and Service\", \"destination_entity\": \"ReplicationController, Service\"}]'},\n",
       " {'page': 285,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '253\\nUpdating applications running in pods\\nonly then delete the old ones. This will require more hardware resources, because\\nyou’ll have double the number of pods running at the same time for a short while. \\n This is a slightly more complex method compared to the previous one, but you\\nshould be able to do it by combining what you’ve learned about ReplicationControl-\\nlers and Services so far.\\nSWITCHING FROM THE OLD TO THE NEW VERSION AT ONCE\\nPods are usually fronted by a Service. It’s possible to have the Service front only the\\ninitial version of the pods while you bring up the pods running the new version. Then,\\nonce all the new pods are up, you can change the Service’s label selector and have the\\nService switch over to the new pods, as shown in figure 9.3. This is called a blue-green\\ndeployment. After switching over, and once you’re sure the new version functions cor-\\nrectly, you’re free to delete the old pods by deleting the old ReplicationController.\\nNOTE\\nYou can change a Service’s pod selector with the kubectl set selec-\\ntor command.\\nPERFORMING A ROLLING UPDATE\\nInstead of bringing up all the new pods and deleting the old pods at once, you can\\nalso perform a rolling update, which replaces pods step by step. You do this by slowly\\nscaling down the previous ReplicationController and scaling up the new one. In this\\ncase, you’ll want the Service’s pod selector to include both the old and the new pods,\\nso it directs requests toward both sets of pods. See figure 9.4.\\n Doing a rolling update manually is laborious and error-prone. Depending on the\\nnumber of replicas, you’d need to run a dozen or more commands in the proper\\norder to perform the update process. Luckily, Kubernetes allows you to perform the\\nrolling update with a single command. You’ll learn how in the next section.\\nService\\nService\\nReplicationController:\\nv1\\nPod: v1\\nPod: v1\\nPod\\ntemplate: v1\\nPod: v1\\nReplicationController:\\nv2\\nPod\\ntemplate: v2\\nPod: v2\\nPod: v2\\nPod: v2\\nReplicationController:\\nv1\\nPod: v1\\nPod: v1\\nPod\\ntemplate: v1\\nPod: v1\\nReplicationController:\\nv2\\nPod\\ntemplate: v2\\nPod: v2\\nPod: v2\\nPod: v2\\nFigure 9.3\\nSwitching a Service from the old pods to the new ones\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes object that ensures a specified number of replicas (identical Pods) are running at any given time.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Service',\n",
       "    'description': \"An abstract interface to a cluster's services or microservices, such as HTTP service.\",\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes, equivalent to a container running on a host machine.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for interacting with Kubernetes clusters.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'set selector',\n",
       "    'description': 'A command for updating the pod selector of a Service.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Replicas',\n",
       "    'description': 'The number of identical Pods that should be running at any given time, as specified by a ReplicationController.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Rolling update',\n",
       "    'description': 'A method for updating a Service by slowly scaling down the previous ReplicationController and scaling up the new one.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Hardware resources',\n",
       "    'description': 'The computing power, memory, and storage required to run a Kubernetes cluster.',\n",
       "    'category': 'Hardware'}],\n",
       "  'relationships': '[{\"source_entity\": \"You\", \"description\": \"can combine what you\\'ve learned about ReplicationControllers and Services to switch from the old to the new version at once.\", \"destination_entity\": \"ReplicationControllers and Services\"}]\\n\\n'},\n",
       " {'page': 286,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '254\\nCHAPTER 9\\nDeployments: updating applications declaratively\\n9.2\\nPerforming an automatic rolling update with a \\nReplicationController\\nInstead of performing rolling updates using ReplicationControllers manually, you can\\nhave kubectl perform them. Using kubectl to perform the update makes the process\\nmuch easier, but, as you’ll see later, this is now an outdated way of updating apps. Nev-\\nertheless, we’ll walk through this option first, because it was historically the first way of\\ndoing an automatic rolling update, and also allows us to discuss the process without\\nintroducing too many additional concepts. \\n9.2.1\\nRunning the initial version of the app\\nObviously, before you can update an app, you need to have an app deployed. You’re\\ngoing to use a slightly modified version of the kubia NodeJS app you created in chap-\\nter 2 as your initial version. In case you don’t remember what it does, it’s a simple web-\\napp that returns the pod’s hostname in the HTTP response. \\nCREATING THE V1 APP\\nYou’ll change the app so it also returns its version number in the response, which will\\nallow you to distinguish between the different versions you’re about to build. I’ve\\nalready built and pushed the app image to Docker Hub under luksa/kubia:v1. The\\nfollowing listing shows the app’s code.\\nconst http = require(\\'http\\');\\nconst os = require(\\'os\\');\\nconsole.log(\"Kubia server starting...\");\\nListing 9.1\\nThe v1 version of our app: v1/app.js\\nService\\nPod: v1\\nPod: v1\\nReplication\\nController:\\nv1\\nv1\\nReplication\\nController:\\nv2\\nPod: v2\\nService\\nPod: v2\\nPod: v2\\nPod: v2\\nService\\nPod: v1\\nPod: v1\\nPod: v1\\nService\\nPod: v1\\nPod: v2\\nPod: v2\\nv2\\nReplication\\nController:\\nv1\\nv1\\nReplication\\nController:\\nv2\\nv2\\nReplication\\nController:\\nv1\\nReplication\\nController:\\nv2\\nv2\\nReplication\\nController:\\nv1\\nv1\\nv1\\nReplication\\nController:\\nv2\\nv2\\nFigure 9.4\\nA rolling update of pods using two ReplicationControllers\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Service\\nPod: v1 Pod: v1 Pod: v1\\nReplication Replication\\nController: Controller:\\nv1 v2\\nv1 v2  \\\n",
       "   0                   Replication\\nController:\\nv1\\nv1                                                 \n",
       "   \n",
       "                                  Col1  \\\n",
       "   0  Replication\\nController:\\nv2\\nv2   \n",
       "   \n",
       "     Service\\nPod: v1 Pod: v1 Pod: v2\\nReplication Replication\\nController: Controller:\\nv1 v2\\nv1 v2  \\\n",
       "   0                                               None                                                 \n",
       "   \n",
       "     Service\\nPod: v1 Pod: v2 Pod: v2\\nReplication Replication\\nController: Controller:\\nv1 v2\\nv1 v2  \\\n",
       "   0                                               None                                                 \n",
       "   \n",
       "     Service\\nPod: v2 Pod: v2 Pod: v2\\nReplication Replication\\nController: Controller:\\nv1 v2\\nv1 v2  \\\n",
       "   0                                               None                                                 \n",
       "   \n",
       "                                  Col5                              Col6  \n",
       "   0  Replication\\nController:\\nv1\\nv1  Replication\\nController:\\nv2\\nv2  ,\n",
       "   Empty DataFrame\n",
       "   Columns: [Replication\n",
       "   Controller:\n",
       "   v1\n",
       "   v1, Replication\n",
       "   Controller:\n",
       "   v2\n",
       "   v2]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [Replication\n",
       "   Controller:\n",
       "   v1\n",
       "   v1, Replication\n",
       "   Controller:\n",
       "   v2\n",
       "   v2]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'Kubernetes object that manages a set of replicas (pods) to ensure availability and scalability',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Kubernetes object that represents a container running an application',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'Kubernetes object that provides a network identity and load balancing for accessing applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'NodeJS',\n",
       "    'description': 'Programming language for server-side programming',\n",
       "    'category': 'programmingLanguage'},\n",
       "   {'entity': 'Docker Hub',\n",
       "    'description': 'Registry service for storing and sharing Docker images',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'app.js',\n",
       "    'description': 'JavaScript file that contains the Kubia app code',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'Replica',\n",
       "    'description': 'Term used to describe a pod that is being managed by a ReplicationController',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'update',\n",
       "    'description': 'Process of upgrading or modifying an application',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"updates applications declaratively using a ReplicationController\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"performs an automatic rolling update with a ReplicationController\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"runs the initial version of the app\", \"destination_entity\": \"app.js\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"creates the v1 app\", \"destination_entity\": \"v1/app.js\"},\\n  {\"source_entity\": \"v1/app.js\", \"description\": \"returns its version number in the response\", \"destination_entity\": \"Docker Hub\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"manages multiple versions of the app\", \"destination_entity\": \"app.js\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"performs a rolling update using two ReplicationControllers\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"update\", \"description\": \"updates the app to a new version\", \"destination_entity\": \"app.js\"},\\n  {\"source_entity\": \"NodeJS\", \"description\": \"runs on a Pod managed by a ReplicationController\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"manages multiple Pods running the same app\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Docker Hub\", \"description\": \"stores images of different app versions\", \"destination_entity\": \"v1/app.js\"}\\n]\\n```'},\n",
       " {'page': 287,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '255\\nPerforming an automatic rolling update with a ReplicationController\\nvar handler = function(request, response) {\\n  console.log(\"Received request from \" + request.connection.remoteAddress);\\n  response.writeHead(200);\\n  response.end(\"This is v1 running in pod \" + os.hostname() + \"\\\\n\");\\n};\\nvar www = http.createServer(handler);\\nwww.listen(8080);\\nRUNNING THE APP AND EXPOSING IT THROUGH A SERVICE USING A SINGLE YAML FILE\\nTo run your app, you’ll create a ReplicationController and a LoadBalancer Service to\\nenable you to access the app externally. This time, rather than create these two\\nresources separately, you’ll create a single YAML for both of them and post it to the\\nKubernetes API with a single kubectl create command. A YAML manifest can con-\\ntain multiple objects delimited with a line containing three dashes, as shown in the\\nfollowing listing.\\napiVersion: v1\\nkind: ReplicationController\\nmetadata:\\n  name: kubia-v1\\nspec:\\n  replicas: 3\\n  template:\\n    metadata:\\n      name: kubia\\n      labels:                      \\n        app: kubia                 \\n    spec:\\n      containers:\\n      - image: luksa/kubia:v1     \\n        name: nodejs\\n---                         \\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: kubia\\nspec:\\n  type: LoadBalancer\\n  selector:                                        \\n    app: kubia                                     \\n  ports:\\n  - port: 80\\n    targetPort: 8080\\nThe YAML defines a ReplicationController called kubia-v1 and a Service called\\nkubia. Go ahead and post the YAML to Kubernetes. After a while, your three v1 pods\\nand the load balancer should all be running, so you can look up the Service’s external\\nIP and start hitting the service with curl, as shown in the following listing.\\nListing 9.2\\nA YAML containing an RC and a Service: kubia-rc-and-service-v1.yaml\\nThe Service fronts all \\npods created by the \\nReplicationController.\\nYou’re creating a \\nReplicationController for \\npods running this image.\\nYAML files can contain \\nmultiple resource \\ndefinitions separated by \\na line with three dashes.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes object that manages a set of replicas (pods) to ensure a specified number of copies are running at any given time.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A Kubernetes object that provides a network identity and load balancing for accessing applications in a cluster.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'ReplicationController (kubia-v1)',\n",
       "    'description': 'A specific instance of ReplicationController, named kubia-v1, which manages 3 replicas (pods) running the luksa/kubia:v1 image.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service (kubia)',\n",
       "    'description': 'A specific instance of Service, named kubia, which provides a load balancer for accessing pods created by ReplicationController kubia-v1.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Kubernetes API',\n",
       "    'description': 'The interface that allows users to interact with the Kubernetes cluster, including creating and managing objects such as ReplicationControllers and Services.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl create command',\n",
       "    'description': 'A command-line tool used to interact with the Kubernetes API, allowing users to create and manage objects in the cluster.',\n",
       "    'category': 'tool'},\n",
       "   {'entity': 'YAML manifest',\n",
       "    'description': 'A file format for defining multiple resources (such as ReplicationControllers and Services) that can be applied to a Kubernetes cluster using a single kubectl command.',\n",
       "    'category': 'file format'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes API\", \"description\": \"receive request\", \"destination_entity\": \"kubectl create command\"},\\n  {\"source_entity\": \"Kubernetes API\", \"description\": \"post YAML file\", \"destination_entity\": \"ReplicationController (kubia-v1)\"},\\n  {\"source_entity\": \"Kubernetes API\", \"description\": \"post YAML file\", \"destination_entity\": \"Service (kubia)\"},\\n  {\"source_entity\": \"kubectl create command\", \"description\": \"create resources\", \"destination_entity\": \"ReplicationController (kubia-v1)\"},\\n  {\"source_entity\": \"kubectl create command\", \"description\": \"create resources\", \"destination_entity\": \"Service (kubia)\"},\\n  {\"source_entity\": \"ReplicationController (kubia-v1)\", \"description\": \"spawn pods\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"ReplicationController (kubia-v1)\", \"description\": \"manage replicas\", \"destination_entity\": \"Replicas\"},\\n  {\"source_entity\": \"Service (kubia)\", \"description\": \"front all pods\", \"destination_entity\": \"pods created by ReplicationController\"},\\n  {\"source_entity\": \"Service (kubia)\", \"description\": \"forward traffic\", \"destination_entity\": \"clients\"},\\n  {\"source_entity\": \"kubectl create command\", \"description\": \"use API\", \"destination_entity\": \"Kubernetes API\"}\\n]\\n```\\n\\nNote: I\\'ve only extracted relations mentioned in the document page, and not added any implicit or inferred relations. Also, some entities like \"ReplicationController\" or \"Service\" are used as singular entities, but they can be treated as plural when necessary (e.g., \"ReplicationControllers\" or \"Services\").'},\n",
       " {'page': 288,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '256\\nCHAPTER 9\\nDeployments: updating applications declaratively\\n$ kubectl get svc kubia\\nNAME      CLUSTER-IP     EXTERNAL-IP       PORT(S)         AGE\\nkubia     10.3.246.195   130.211.109.222   80:32143/TCP    5m\\n$ while true; do curl http://130.211.109.222; done\\nThis is v1 running in pod kubia-v1-qr192\\nThis is v1 running in pod kubia-v1-kbtsk\\nThis is v1 running in pod kubia-v1-qr192\\nThis is v1 running in pod kubia-v1-2321o\\n...\\nNOTE\\nIf you’re using Minikube or any other Kubernetes cluster where load\\nbalancer services aren’t supported, you can use the Service’s node port to\\naccess the app. This was explained in chapter 5.\\n9.2.2\\nPerforming a rolling update with kubectl\\nNext you’ll create version 2 of the app. To keep things simple, all you’ll do is change\\nthe response to say, “This is v2”:\\n  response.end(\"This is v2 running in pod \" + os.hostname() + \"\\\\n\");\\nThis new version is available in the image luksa/kubia:v2 on Docker Hub, so you\\ndon’t need to build it yourself.\\nListing 9.3\\nGetting the Service’s external IP and hitting the service in a loop with curl\\nPushing updates to the same image tag\\nModifying an app and pushing the changes to the same image tag isn’t a good idea,\\nbut we all tend to do that during development. If you’re modifying the latest tag,\\nthat’s not a problem, but when you’re tagging an image with a different tag (for exam-\\nple, tag v1 instead of latest), once the image is pulled by a worker node, the image\\nwill be stored on the node and not pulled again when a new pod using the same\\nimage is run (at least that’s the default policy for pulling images).\\nThat means any changes you make to the image won’t be picked up if you push them\\nto the same tag. If a new pod is scheduled to the same node, the Kubelet will run the\\nold version of the image. On the other hand, nodes that haven’t run the old version\\nwill pull and run the new image, so you might end up with two different versions of\\nthe pod running. To make sure this doesn’t happen, you need to set the container’s\\nimagePullPolicy property to Always. \\nYou need to be aware that the default imagePullPolicy depends on the image tag.\\nIf a container refers to the latest tag (either explicitly or by not specifying the tag at\\nall), imagePullPolicy defaults to Always, but if the container refers to any other\\ntag, the policy defaults to IfNotPresent. \\nWhen using a tag other than latest, you need to set the imagePullPolicy properly\\nif you make changes to an image without changing the tag. Or better yet, make sure\\nyou always push changes to an image under a new tag.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'get svc kubia',\n",
       "    'description': 'command to get service details',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubia',\n",
       "    'description': 'service name',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'CLUSTER-IP',\n",
       "    'description': 'cluster IP address',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'EXTERNAL-IP',\n",
       "    'description': 'external IP address',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'PORT(S)',\n",
       "    'description': 'port numbers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'AGE', 'description': 'age of service', 'category': 'process'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'command-line tool for transferring data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'http://130.211.109.222',\n",
       "    'description': 'URL for accessing the app',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'v1',\n",
       "    'description': 'version of the app',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod kubia-v1-qr192',\n",
       "    'description': 'pod name',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'Kubernetes cluster manager',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'node port',\n",
       "    'description': 'port number for accessing the app on a node',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'os.hostname()',\n",
       "    'description': 'function to get hostname',\n",
       "    'category': 'programming language'},\n",
       "   {'entity': 'response.end',\n",
       "    'description': 'function to end response',\n",
       "    'category': 'programming language'},\n",
       "   {'entity': 'luksa/kubia:v2',\n",
       "    'description': 'image name and tag',\n",
       "    'category': 'image'},\n",
       "   {'entity': 'Docker Hub',\n",
       "    'description': 'registry for Docker images',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'imagePullPolicy',\n",
       "    'description': 'policy for pulling images',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Always',\n",
       "    'description': 'image pull policy value',\n",
       "    'category': 'constant'},\n",
       "   {'entity': 'IfNotPresent',\n",
       "    'description': 'image pull policy value',\n",
       "    'category': 'constant'},\n",
       "   {'entity': 'latest', 'description': 'image tag', 'category': 'tag'}],\n",
       "  'relationships': '[{\"source_entity\": \"kubectl\", \"description\": \"get service kubia\", \"destination_entity\": \"kubia\"},\\n {\"source_entity\": \"kubectl\", \"description\": \"get service kubia\", \"destination_entity\": \"CLUSTER-IP\"},\\n {\"source_entity\": \"kubectl\", \"description\": \"get service kubia\", \"destination_entity\": \"EXTERNAL-IP\"},\\n {\"source_entity\": \"kubectl\", \"description\": \"get service kubia\", \"destination_entity\": \"PORT(S)\"},\\n {\"source_entity\": \"kubectl\", \"description\": \"get service kubia\", \"destination_entity\": \"AGE\"},\\n {\"source_entity\": \"curl\", \"description\": \"hit the service in a loop\", \"destination_entity\": \"http://130.211.109.222\"},\\n {\"source_entity\": \"Minikube\", \"description\": \"use node port to access app\", \"destination_entity\": \"node port\"},\\n {\"source_entity\": \"kubectl\", \"description\": \"perform rolling update\", \"destination_entity\": \"kubia:v2\"},\\n {\"source_entity\": \"response.end\", \"description\": \"change response to say \\'This is v2\\'\", \"destination_entity\": \"kubia\"},\\n {\"source_entity\": \"Docker Hub\", \"description\": \"pull image luksa/kubia:v2\", \"destination_entity\": \"luksa/kubia:v2\"},\\n {\"source_entity\": \"imagePullPolicy\", \"description\": \"set to Always for latest tag\", \"destination_entity\": \"latest\"},\\n {\"source_entity\": \"imagePullPolicy\", \"description\": \"set to IfNotPresent for other tags\", \"destination_entity\": \"IfNotPresent\"},\\n {\"source_entity\": \"kubectl\", \"description\": \"push updates to same image tag\", \"destination_entity\": \"kubia:v1\"},\\n {\"source_entity\": \"os.hostname()\", \"description\": \"get hostname of pod kubia-v1-qr192\", \"destination_entity\": \"pod kubia-v1-qr192\"}]\\n\\nNote: I have only considered the entities provided in the input list. If there are any other entities mentioned in the document page, they would not be included in this output.'},\n",
       " {'page': 289,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"257\\nPerforming an automatic rolling update with a ReplicationController\\nKeep the curl loop running and open another terminal, where you’ll get the rolling\\nupdate started. To perform the update, you’ll run the kubectl rolling-update com-\\nmand. All you need to do is tell it which ReplicationController you’re replacing, give a\\nname for the new ReplicationController, and specify the new image you’d like to\\nreplace the original one with. The following listing shows the full command for per-\\nforming the rolling update.\\n$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2\\nCreated kubia-v2\\nScaling up kubia-v2 from 0 to 3, scaling down kubia-v1 from 3 to 0 (keep 3 \\npods available, don't exceed 4 pods)\\n...\\nBecause you’re replacing ReplicationController kubia-v1 with one running version 2\\nof your kubia app, you’d like the new ReplicationController to be called kubia-v2\\nand use the luksa/kubia:v2 container image. \\n When you run the command, a new ReplicationController called kubia-v2 is cre-\\nated immediately. The state of the system at this point is shown in figure 9.5.\\nThe new ReplicationController’s pod template references the luksa/kubia:v2 image\\nand its initial desired replica count is set to 0, as you can see in the following listing.\\n$ kubectl describe rc kubia-v2\\nName:       kubia-v2\\nNamespace:  default\\nImage(s):   luksa/kubia:v2          \\nSelector:   app=kubia,deployment=757d16a0f02f6a5c387f2b5edb62b155\\nLabels:     app=kubia            \\nReplicas:   0 current / 0 desired    \\n...\\nListing 9.4\\nInitiating a rolling-update of a ReplicationController using kubectl\\nListing 9.5\\nDescribing the new ReplicationController created by the rolling update\\nPod: v1\\nPod: v1\\nNo v2 pods yet\\nPod: v1\\nReplicationController: kubia-v1\\nImage: kubia/v1\\nReplicas: 3\\nReplicationController: kubia-v2\\nImage: kubia/v2\\nReplicas: 0\\nFigure 9.5\\nThe state of the system immediately after starting the rolling update\\nThe new \\nReplicationController \\nrefers to the v2 image.\\nInitially, the desired \\nnumber of replicas is zero.\\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'a command-line tool for managing Kubernetes resources',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'a Kubernetes resource that manages a group of pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubia-v1',\n",
       "    'description': 'an older version of the Kubia application',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubia-v2',\n",
       "    'description': 'a newer version of the Kubia application',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'luksa/kubia:v2',\n",
       "    'description': 'a container image for the Kubia application version 2',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ReplicaCount',\n",
       "    'description': 'the number of desired replicas in a ReplicationController',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Selector',\n",
       "    'description': 'a label selector used to identify pods managed by a ReplicationController',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'RC',\n",
       "    'description': 'abbreviation for ReplicationController',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'DesiredReplicaCount',\n",
       "    'description': 'the initial desired number of replicas in the new ReplicationController',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"initiates a rolling update\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"replaces\", \"destination_entity\": \"kubia-v1\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"creates a new\", \"destination_entity\": \"kubia-v2\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"sets the desired replica count to zero\", \"destination_entity\": \"kubia-v2\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"references the luksa/kubia:v2 image\", \"destination_entity\": \"luksa/kubia:v2\"},\\n  {\"source_entity\": \"Selector\", \"description\": \"labels the ReplicationController kubia-v2 with app=kubia and deployment=757d16a0f02f6a5c387f2b5edb62b155\", \"destination_entity\": \"kubia-v2\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"scales up kubia-v2 from 0 to 3, scaling down kubia-v1 from 3 to 0\", \"destination_entity\": \"RC\"},\\n  {\"source_entity\": \"ReplicaCount\", \"description\": \"increases by 3 for kubia-v2 and decreases by 3 for kubia-v1\", \"destination_entity\": \"kubia-v2\"},\\n  {\"source_entity\": \"DesiredReplicaCount\", \"description\": \"is set to 0 for kubia-v2\", \"destination_entity\": \"kubia-v2\"}\\n]\\n```\\n\\nNote: I\\'ve assumed that \"RC\" is an abbreviation for \"ReplicationController\", and \"ReplicaCount\" and \"DesiredReplicaCount\" are related to the replica count of a ReplicationController. If this is not the case, please let me know and I\\'ll adjust accordingly.'},\n",
       " {'page': 290,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '258\\nCHAPTER 9\\nDeployments: updating applications declaratively\\nUNDERSTANDING THE STEPS PERFORMED BY KUBECTL BEFORE THE ROLLING UPDATE COMMENCES\\nkubectl created this ReplicationController by copying the kubia-v1 controller and\\nchanging the image in its pod template. If you look closely at the controller’s label\\nselector, you’ll notice it has been modified, too. It includes not only a simple\\napp=kubia label, but also an additional deployment label which the pods must have in\\norder to be managed by this ReplicationController.\\n You probably know this already, but this is necessary to avoid having both the new\\nand the old ReplicationControllers operating on the same set of pods. But even if pods\\ncreated by the new controller have the additional deployment label in addition to the\\napp=kubia label, doesn’t this mean they’ll be selected by the first ReplicationControl-\\nler’s selector, because it’s set to app=kubia? \\n Yes, that’s exactly what would happen, but there’s a catch. The rolling-update pro-\\ncess has modified the selector of the first ReplicationController, as well:\\n$ kubectl describe rc kubia-v1\\nName:       kubia-v1\\nNamespace:  default\\nImage(s):   luksa/kubia:v1\\nSelector:   app=kubia,deployment=3ddd307978b502a5b975ed4045ae4964-orig \\nOkay, but doesn’t this mean the first controller now sees zero pods matching its selec-\\ntor, because the three pods previously created by it contain only the app=kubia label?\\nNo, because kubectl had also modified the labels of the live pods just before modify-\\ning the ReplicationController’s selector:\\n$ kubectl get po --show-labels\\nNAME            READY  STATUS   RESTARTS  AGE  LABELS\\nkubia-v1-m33mv  1/1    Running  0         2m   app=kubia,deployment=3ddd...\\nkubia-v1-nmzw9  1/1    Running  0         2m   app=kubia,deployment=3ddd...\\nkubia-v1-cdtey  1/1    Running  0         2m   app=kubia,deployment=3ddd...\\nIf this is getting too complicated, examine figure 9.6, which shows the pods, their\\nlabels, and the two ReplicationControllers, along with their pod selectors.\\nReplicationController: kubia-v1\\nReplicas: 3\\nSelector: app=kubia,\\ndeployment=3ddd…\\nReplicationController: kubia-v2\\nReplicas: 0\\nSelector: app=kubia,\\ndeployment=757d...\\ndeployment: 3ddd...\\napp: kubia\\nPod: v1\\ndeployment: 3ddd...\\napp: kubia\\nPod: v1\\ndeployment: 3ddd...\\napp: kubia\\nPod: v1\\nFigure 9.6\\nDetailed state of the old and new ReplicationControllers and pods at the start of a rolling \\nupdate\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A controller that ensures a specified number of replicas are running at any given time.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for managing Kubernetes resources.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes, which represents an application instance.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Image',\n",
       "    'description': 'A binary package that contains the application code and dependencies.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Replica',\n",
       "    'description': 'A copy of a Pod that is running in the cluster.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Selector',\n",
       "    'description': 'A label selector used to identify Pods or ReplicationControllers.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Label',\n",
       "    'description': 'A key-value pair used to identify and select resources in Kubernetes.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A resource that manages the rollout of new versions of an application.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'ReplicationController: kubia-v1',\n",
       "    'description': 'An old ReplicationController with 3 replicas and a selector matching the app=kubia label.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'ReplicationController: kubia-v2',\n",
       "    'description': 'A new ReplicationController with 0 replicas and a selector matching the app=kubia, deployment=757d... labels.',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"created a ReplicationController by copying the kubia-v1 controller and changing the image in its pod template\", \"destination_entity\": \"ReplicationController: kubia-v2\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"modified the selector of the first ReplicationController\", \"destination_entity\": \"ReplicationController: kubia-v1\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"modified the labels of the live pods just before modifying the ReplicationController\\'s selector\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"ReplicationController: kubia-v2\", \"description\": \"managed by this ReplicationController\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"examined figure 9.6, which shows the pods, their labels, and the two ReplicationControllers\", \"destination_entity\": \"Figure 9.6\"},\\n  {\"source_entity\": \"ReplicationController: kubia-v2\", \"description\": \"operating on the same set of pods as the new ReplicationController\", \"destination_entity\": \"ReplicationController: kubia-v1\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"modified the selector of the first ReplicationController to avoid having both the new and old ReplicationControllers operating on the same set of pods\", \"destination_entity\": \"Selector\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"copied the kubia-v1 controller to create the ReplicationController: kubia-v2\", \"destination_entity\": \"ReplicationController: kubia-v1\"},\\n  {\"source_entity\": \"Image\", \"description\": \"used by the ReplicationController: kubia-v2 and the Pod v1\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"managed by the ReplicationController: kubia-v2 and the ReplicationController: kubia-v1\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"described the ReplicationController: kubia-v1\", \"destination_entity\": \"ReplicationController: kubia-v1\"}\\n]\\n\\nNote that I\\'ve assumed the entities \\'Deployment\\', \\'Image\\' and \\'Selector\\' are part of the list provided, as they were used in the context of the document page. If this is not the case, please let me know.'},\n",
       " {'page': 291,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '259\\nPerforming an automatic rolling update with a ReplicationController\\nkubectl had to do all this before even starting to scale anything up or down. Now\\nimagine doing the rolling update manually. It’s easy to see yourself making a mistake\\nhere and possibly having the ReplicationController kill off all your pods—pods that\\nare actively serving your production clients!\\nREPLACING OLD PODS WITH NEW ONES BY SCALING THE TWO REPLICATIONCONTROLLERS\\nAfter setting up all this, kubectl starts replacing pods by first scaling up the new\\ncontroller to 1. The controller thus creates the first v2 pod. kubectl then scales\\ndown the old ReplicationController by 1. This is shown in the next two lines printed\\nby kubectl:\\nScaling kubia-v2 up to 1\\nScaling kubia-v1 down to 2\\nBecause the Service is targeting all pods with the app=kubia label, you should start see-\\ning your curl requests redirected to the new v2 pod every few loop iterations:\\nThis is v2 running in pod kubia-v2-nmzw9      \\nThis is v1 running in pod kubia-v1-kbtsk\\nThis is v1 running in pod kubia-v1-2321o\\nThis is v2 running in pod kubia-v2-nmzw9      \\n...\\nFigure 9.7 shows the current state of the system.\\nAs kubectl continues with the rolling update, you start seeing a progressively bigger\\npercentage of requests hitting v2 pods, as the update process deletes more of the v1\\npods and replaces them with those running your new image. Eventually, the original\\nRequests hitting the pod \\nrunning the new version\\nReplicationController: kubia-v1\\nReplicas: 2\\nSelector: app=kubia,\\ndeployment=3ddd…\\nReplicationController: kubia-v2\\nReplicas: 1\\nSelector: app=kubia,\\ndeployment=757d…\\ndeployment: 3ddd...\\napp: kubia\\nPod: v1\\ndeployment: 3ddd...\\napp: kubia\\nPod: v1\\ndeployment: 757d...\\napp: kubia\\nPod: v2\\ncurl\\nService\\nSelector: app=kubia\\nFigure 9.7\\nThe Service is redirecting requests to both the old and new pods during the \\nrolling update.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'a Kubernetes object that ensures a specified number of replicas (identical Pod(s)) are running at any given time.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the command-line tool for interacting with Kubernetes clusters.',\n",
       "    'category': 'software,command'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'lightweight and portable units of computation in containerized environments.',\n",
       "    'category': 'software,container'},\n",
       "   {'entity': 'Replica',\n",
       "    'description': 'a copy of a Pod.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'an abstraction which defines a logical set of Pods and accessibility rules (ports) for reaching them.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'a command-line tool for transferring data to and from servers using HTTP, HTTPS, SCP, SFTP, TFTP, and more.',\n",
       "    'category': 'software,command'},\n",
       "   {'entity': 'ReplicationController: kubia-v1',\n",
       "    'description': 'an instance of the ReplicationController with replicas=2.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'ReplicationController: kubia-v2',\n",
       "    'description': 'an instance of the ReplicationController with replicas=1.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'deployment',\n",
       "    'description': 'a Kubernetes object that manages the rollout of new versions of an application.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Selector',\n",
       "    'description': 'a way to label and select resources in a Kubernetes cluster based on certain criteria.',\n",
       "    'category': 'software,application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"replaces old pods with new ones by scaling two ReplicationControllers\", \"destination_entity\": \"ReplicationController: kubia-v2\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"scales up new controller to 1\", \"destination_entity\": \"ReplicationController: kubia-v2\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"scales down old ReplicationController by 1\", \"destination_entity\": \"ReplicationController: kubia-v1\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"targets all pods with app=kubia label\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"redirects requests to both old and new pods during rolling update\", \"destination_entity\": \"Pod: v2\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"redirects requests to both old and new pods during rolling update\", \"destination_entity\": \"Pod: v1\"},\\n  {\"source_entity\": \"ReplicationController: kubia-v2\", \"description\": \"runs the new image\", \"destination_entity\": \"Pod: v2\"},\\n  {\"source_entity\": \"ReplicationController: kubia-v1\", \"description\": \"runs old image\", \"destination_entity\": \"Pod: v1\"},\\n  {\"source_entity\": \"Service\", \"description\": \"redirects requests to ReplicationController: kubia-v2\", \"destination_entity\": \"ReplicationController: kubia-v2\"},\\n  {\"source_entity\": \"Service\", \"description\": \"redirects requests to ReplicationController: kubia-v1\", \"destination_entity\": \"ReplicationController: kubia-v1\"}\\n]\\n```'},\n",
       " {'page': 292,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '260\\nCHAPTER 9\\nDeployments: updating applications declaratively\\nReplicationController is scaled to zero, causing the last v1 pod to be deleted, which\\nmeans the Service will now be backed by v2 pods only. At that point, kubectl will\\ndelete the original ReplicationController and the update process will be finished, as\\nshown in the following listing.\\n...\\nScaling kubia-v2 up to 2\\nScaling kubia-v1 down to 1\\nScaling kubia-v2 up to 3\\nScaling kubia-v1 down to 0\\nUpdate succeeded. Deleting kubia-v1\\nreplicationcontroller \"kubia-v1\" rolling updated to \"kubia-v2\"\\nYou’re now left with only the kubia-v2 ReplicationController and three v2 pods. All\\nthroughout this update process, you’ve hit your service and gotten a response every\\ntime. You have, in fact, performed a rolling update with zero downtime. \\n9.2.3\\nUnderstanding why kubectl rolling-update is now obsolete\\nAt the beginning of this section, I mentioned an even better way of doing updates\\nthan through kubectl rolling-update. What’s so wrong with this process that a bet-\\nter one had to be introduced? \\n Well, for starters, I, for one, don’t like Kubernetes modifying objects I’ve created.\\nOkay, it’s perfectly fine for the scheduler to assign a node to my pods after I create\\nthem, but Kubernetes modifying the labels of my pods and the label selectors of my\\nReplicationControllers is something that I don’t expect and could cause me to go\\naround the office yelling at my colleagues, “Who’s been messing with my controllers!?!?” \\n But even more importantly, if you’ve paid close attention to the words I’ve used,\\nyou probably noticed that all this time I said explicitly that the kubectl client was the\\none performing all these steps of the rolling update. \\n You can see this by turning on verbose logging with the --v option when triggering\\nthe rolling update:\\n$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 --v 6\\nTIP\\nUsing the --v 6 option increases the logging level enough to let you see\\nthe requests kubectl is sending to the API server.\\nUsing this option, kubectl will print out each HTTP request it sends to the Kuberne-\\ntes API server. You’ll see PUT requests to\\n/api/v1/namespaces/default/replicationcontrollers/kubia-v1\\nwhich is the RESTful URL representing your kubia-v1 ReplicationController resource.\\nThese requests are the ones scaling down your ReplicationController, which shows\\nListing 9.6\\nThe final steps performed by kubectl rolling-update\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes resource that manages the replication of a pod.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A Kubernetes resource that provides network connectivity to a pod.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The Kubernetes command-line tool.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'rolling-update',\n",
       "    'description': 'A deprecated Kubernetes feature for updating applications declaratively.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'v1 pod',\n",
       "    'description': 'A version 1 pod in a Kubernetes deployment.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'v2 pod',\n",
       "    'description': 'A version 2 pod in a Kubernetes deployment.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubia-v1',\n",
       "    'description': 'The name of the ReplicationController for version 1 of the Kubia application.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubia-v2',\n",
       "    'description': 'The name of the ReplicationController for version 2 of the Kubia application.',\n",
       "    'category': 'software'},\n",
       "   {'entity': '--v option',\n",
       "    'description': 'A command-line option in kubectl to enable verbose logging.',\n",
       "    'category': 'software'},\n",
       "   {'entity': '--image=luksa/kubia:v2',\n",
       "    'description': 'A command-line argument in kubectl rolling-update to specify the image for version 2 of the Kubia application.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"scales down ReplicationController\",\\n    \"destination_entity\": \"kubia-v1\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"sends HTTP requests to Kubernetes API server\",\\n    \"destination_entity\": \"Kubernetes API server\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"prints out each HTTP request sent to the API server\",\\n    \"destination_entity\": \"console/output\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"is scaled down by kubectl\",\\n    \"destination_entity\": \"kubia-v1\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"deletes the original ReplicationController\",\\n    \"destination_entity\": \"kubia-v1\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"is backed by v2 pods only\",\\n    \"destination_entity\": \"v2 pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"scales up kubia-v2 to 2\",\\n    \"destination_entity\": \"kubia-v2\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"scales down kubia-v1 to 0\",\\n    \"destination_entity\": \"kubia-v1\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"deletes kubia-v1\",\\n    \"destination_entity\": \"kubia-v1\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl rolling-update\",\\n    \"description\": \"updates ReplicationController from v1 to v2\",\\n    \"destination_entity\": \"ReplicationController\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"modifies objects created by users\",\\n    \"destination_entity\": \"user\"\\n  }\\n]'},\n",
       " {'page': 293,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '261\\nUsing Deployments for updating apps declaratively\\nthat the kubectl client is the one doing the scaling, instead of it being performed by\\nthe Kubernetes master. \\nTIP\\nUse the verbose logging option when running other kubectl commands,\\nto learn more about the communication between kubectl and the API server. \\nBut why is it such a bad thing that the update process is being performed by the client\\ninstead of on the server? Well, in your case, the update went smoothly, but what if you\\nlost network connectivity while kubectl was performing the update? The update pro-\\ncess would be interrupted mid-way. Pods and ReplicationControllers would end up in\\nan intermediate state.\\n Another reason why performing an update like this isn’t as good as it could be is\\nbecause it’s imperative. Throughout this book, I’ve stressed how Kubernetes is about\\nyou telling it the desired state of the system and having Kubernetes achieve that\\nstate on its own, by figuring out the best way to do it. This is how pods are deployed\\nand how pods are scaled up and down. You never tell Kubernetes to add an addi-\\ntional pod or remove an excess one—you change the number of desired replicas\\nand that’s it.\\n Similarly, you will also want to change the desired image tag in your pod defini-\\ntions and have Kubernetes replace the pods with new ones running the new image.\\nThis is exactly what drove the introduction of a new resource called a Deployment,\\nwhich is now the preferred way of deploying applications in Kubernetes. \\n9.3\\nUsing Deployments for updating apps declaratively\\nA Deployment is a higher-level resource meant for deploying applications and\\nupdating them declaratively, instead of doing it through a ReplicationController or\\na ReplicaSet, which are both considered lower-level concepts.\\n When you create a Deployment, a ReplicaSet resource is created underneath\\n(eventually more of them). As you may remember from chapter 4, ReplicaSets are a\\nnew generation of ReplicationControllers, and should be used instead of them. Replica-\\nSets replicate and manage pods, as well. When using a Deployment, the actual pods\\nare created and managed by the Deployment’s ReplicaSets, not by the Deployment\\ndirectly (the relationship is shown in figure 9.8).\\nYou might wonder why you’d want to complicate things by introducing another object\\non top of a ReplicationController or ReplicaSet, when they’re what suffices to keep a set\\nof pod instances running. As the rolling update example in section 9.2 demonstrates,\\nwhen updating the app, you need to introduce an additional ReplicationController and\\nPods\\nReplicaSet\\nDeployment\\nFigure 9.8\\nA Deployment is backed \\nby a ReplicaSet, which supervises the \\ndeployment’s pods.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'the Kubernetes client command',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes master',\n",
       "    'description': 'the central authority in a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'a higher-level resource for deploying applications and updating them declaratively',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'a lower-level concept for deploying applications',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'a new generation of ReplicationControllers, responsible for replicating and managing pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'the basic execution unit in Kubernetes, a container running an application',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'a new generation of ReplicationControllers, responsible for replicating and managing pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server',\n",
       "    'description': \"the central authority in a Kubernetes cluster that manages the cluster's state\",\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"performs scaling on\", \"destination_entity\": \"Kubernetes master\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"updates application declaratively\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"is backed by a ReplicaSet\", \"destination_entity\": \"ReplicaSet\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"manages and replicates pods through\", \"destination_entity\": \"ReplicaSets\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"creates underneath a ReplicaSet resource\", \"destination_entity\": \"ReplicaSet\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"communicates with\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"performs updates on\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Kubernetes master\", \"description\": \"previously performed scaling on\", \"destination_entity\": \"kubectl\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"is replaced by a ReplicaSet\", \"destination_entity\": \"ReplicaSet\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"preferably used for deploying applications instead of\", \"destination_entity\": \"ReplicationController\"}\\n]\\n\\nNote: The last relation I extracted is about the preference of using Deployment over ReplicationController, which might not be a direct action on an entity but rather a recommendation or best practice.'},\n",
       " {'page': 294,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '262\\nCHAPTER 9\\nDeployments: updating applications declaratively\\ncoordinate the two controllers to dance around each other without stepping on each\\nother’s toes. You need something coordinating this dance. A Deployment resource\\ntakes care of that (it’s not the Deployment resource itself, but the controller process\\nrunning in the Kubernetes control plane that does that; but we’ll get to that in chap-\\nter 11).\\n Using a Deployment instead of the lower-level constructs makes updating an app\\nmuch easier, because you’re defining the desired state through the single Deployment\\nresource and letting Kubernetes take care of the rest, as you’ll see in the next few pages.\\n9.3.1\\nCreating a Deployment\\nCreating a Deployment isn’t that different from creating a ReplicationController. A\\nDeployment is also composed of a label selector, a desired replica count, and a pod\\ntemplate. In addition to that, it also contains a field, which specifies a deployment\\nstrategy that defines how an update should be performed when the Deployment\\nresource is modified.  \\nCREATING A DEPLOYMENT MANIFEST\\nLet’s see how to use the kubia-v1 ReplicationController example from earlier in this\\nchapter and modify it so it describes a Deployment instead of a ReplicationController.\\nAs you’ll see, this requires only three trivial changes. The following listing shows the\\nmodified YAML.\\napiVersion: apps/v1beta1          \\nkind: Deployment                  \\nmetadata:\\n  name: kubia          \\nspec:\\n  replicas: 3\\n  template:\\n    metadata:\\n      name: kubia\\n      labels:\\n        app: kubia\\n    spec:\\n      containers:\\n      - image: luksa/kubia:v1\\n        name: nodejs\\nNOTE\\nYou’ll find an older version of the Deployment resource in extensions/\\nv1beta1, and a newer one in apps/v1beta2 with different required fields and\\ndifferent defaults. Be aware that kubectl explain shows the older version.\\nBecause the ReplicationController from before was managing a specific version of the\\npods, you called it kubia-v1. A Deployment, on the other hand, is above that version\\nstuff. At a given point in time, the Deployment can have multiple pod versions run-\\nning under its wing, so its name shouldn’t reference the app version.\\nListing 9.7\\nA Deployment definition: kubia-deployment-v1.yaml\\nDeployments are in the apps \\nAPI group, version v1beta1.\\nYou’ve changed the kind \\nfrom ReplicationController \\nto Deployment.\\nThere’s no need to include \\nthe version in the name of \\nthe Deployment.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Deployment',\n",
       "    'description': 'A resource that defines how an update should be performed when the Deployment resource is modified.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A lower-level construct that manages a specific version of pods.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An orchestration system for containerized applications.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'The API version of the Deployment resource.',\n",
       "    'category': 'software,hardware'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'The type of resource being defined (e.g. Deployment, ReplicationController).',\n",
       "    'category': 'software,hardware'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Information about the Deployment resource.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'The specification for the Deployment resource.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'replicas',\n",
       "    'description': 'The number of replicas to run.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'template',\n",
       "    'description': 'A template for the pod being created.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'The containers running in each pod.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'The image of the container to run.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for interacting with Kubernetes.',\n",
       "    'category': 'software,command'},\n",
       "   {'entity': 'explain',\n",
       "    'description': 'A command used to get information about a resource.',\n",
       "    'category': 'software,command'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"uses to explain the spec field\",\\n    \"destination_entity\": \"spec\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"uses to explain the kind field\",\\n    \"destination_entity\": \"kind\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"uses to explain the apiVersion field\",\\n    \"destination_entity\": \"apiVersion\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"is compared to a Deployment for managing specific versions of pods\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"kubia-v1\",\\n    \"description\": \"is used as an example for creating a ReplicationController\",\\n    \"destination_entity\": \"ReplicationController\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"has a control plane where the Deployment controller process runs\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"deployment strategy\",\\n    \"description\": \"defines how an update should be performed when the Deployment resource is modified\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"metadata\",\\n    \"description\": \"contains fields such as name and labels\",\\n    \"destination_entity\": \"template\"\\n  },\\n  {\\n    \"source_entity\": \"containers\",\\n    \"description\": \"contains fields such as image and name\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"replicas\",\\n    \"description\": \"defines the desired number of replicas for a Deployment\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"image\",\\n    \"description\": \"specifies the Docker image to use in a container\",\\n    \"destination_entity\": \"containers\"\\n  }\\n]'},\n",
       " {'page': 295,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '263\\nUsing Deployments for updating apps declaratively\\nCREATING THE DEPLOYMENT RESOURCE\\nBefore you create this Deployment, make sure you delete any ReplicationControllers\\nand pods that are still running, but keep the kubia Service for now. You can use the\\n--all switch to delete all those ReplicationControllers like this:\\n$ kubectl delete rc --all\\nYou’re now ready to create the Deployment: \\n$ kubectl create -f kubia-deployment-v1.yaml --record\\ndeployment \"kubia\" created\\nTIP\\nBe sure to include the --record command-line option when creating it.\\nThis records the command in the revision history, which will be useful later.\\nDISPLAYING THE STATUS OF THE DEPLOYMENT ROLLOUT\\nYou can use the usual kubectl get deployment and the kubectl describe deployment\\ncommands to see details of the Deployment, but let me point you to an additional\\ncommand, which is made specifically for checking a Deployment’s status:\\n$ kubectl rollout status deployment kubia\\ndeployment kubia successfully rolled out\\nAccording to this, the Deployment has been successfully rolled out, so you should see\\nthe three pod replicas up and running. Let’s see:\\n$ kubectl get po\\nNAME                     READY     STATUS    RESTARTS   AGE\\nkubia-1506449474-otnnh   1/1       Running   0          14s\\nkubia-1506449474-vmn7s   1/1       Running   0          14s\\nkubia-1506449474-xis6m   1/1       Running   0          14s\\nUNDERSTANDING HOW DEPLOYMENTS CREATE REPLICASETS, WHICH THEN CREATE THE PODS\\nTake note of the names of these pods. Earlier, when you used a ReplicationController\\nto create pods, their names were composed of the name of the controller plus a ran-\\ndomly generated string (for example, kubia-v1-m33mv). The three pods created by\\nthe Deployment include an additional numeric value in the middle of their names.\\nWhat is that exactly?\\n The number corresponds to the hashed value of the pod template in the Deploy-\\nment and the ReplicaSet managing these pods. As we said earlier, a Deployment\\ndoesn’t manage pods directly. Instead, it creates ReplicaSets and leaves the managing\\nto them, so let’s look at the ReplicaSet created by your Deployment:\\n$ kubectl get replicasets\\nNAME               DESIRED   CURRENT   AGE\\nkubia-1506449474   3         3         10s\\nThe ReplicaSet’s name also contains the hash value of its pod template. As you’ll see\\nlater, a Deployment creates multiple ReplicaSets—one for each version of the pod\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Deployment',\n",
       "    'description': 'A Kubernetes resource that manages replicasets and pods.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'An older Kubernetes resource used to manage pods, being replaced by ReplicaSets.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A Kubernetes resource that ensures a specified number of replicas are running at any given time.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'An abstract way to expose an application running on a Pod and accessible over network.',\n",
       "    'category': 'networking'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool for interacting with Kubernetes clusters.',\n",
       "    'category': 'tool'},\n",
       "   {'entity': 'get deployment',\n",
       "    'description': 'A kubectl command used to view details of Deployments.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'describe deployment',\n",
       "    'description': 'A kubectl command used to view detailed information about Deployments.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'rollout status deployment',\n",
       "    'description': \"A kubectl command used to check the status of a Deployment's rollout.\",\n",
       "    'category': 'command'},\n",
       "   {'entity': 'rc --all',\n",
       "    'description': 'A kubectl command used to delete all ReplicationControllers.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'create -f kubia-deployment-v1.yaml --record',\n",
       "    'description': 'A kubectl command used to create a Deployment from a YAML file, recording the action in the revision history.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'An abstract way to expose an application running on a Pod and accessible over network.',\n",
       "    'category': 'networking'},\n",
       "   {'entity': 'deployment kubia successfully rolled out',\n",
       "    'description': 'The output of a successful Deployment rollout.',\n",
       "    'category': 'status'}],\n",
       "  'relationships': '[\"ReplicationController\", \"Pod\"]\\n}\\n```\\n\\n**Relation 3**\\n```json\\n{\\n  \"source_entity\": \"kubectl get deployment\",\\n  \"description\": \"display the status of the Deployment rollout\",\\n  \"destination_entity\": \"Deployment\"\\n}\\n```\\n\\n**Relation 4**\\n```json\\n{\\n  \"source_entity\": \"kubectl describe deployment\",\\n  \"description\": \"get details of the Deployment\",\\n  \"destination_entity\": \"Deployment\"\\n}\\n```\\n\\n**Relation 5**\\n```json\\n{\\n  \"source_entity\": \"kubectl create -f kubia-deployment-v1.yaml --record\",\\n  \"description\": \"create a new Deployment with version v1\",\\n  \"destination_entity\": \"deployment\"\\n}\\n```\\n\\n**Relation 6**\\n```json\\n{\\n  \"source_entity\": \"Deployment\",\\n  \"description\": \"successfully rolled out\",\\n  \"destination_entity\": \"Service\"\\n}\\n```\\n\\n**Relation 7**\\n```json\\n{\\n  \"source_entity\": \"kubectl rollout status deployment kubia\",\\n  \"description\": \"check the status of the Deployment rollout\",\\n  \"destination_entity\": \"Deployment\"\\n}\\n```\\n\\n**Relation 8**\\n```json\\n{\\n  \"source_entity\": \"ReplicaSet\",\\n  \"description\": \"create multiple ReplicaSets, one for each version of the pod\",\\n  \"destination_entity\": \"Pod\"\\n}\\n```\\n\\n**Relation 9**\\n```json\\n{\\n  \"source_entity\": \"Deployment\",\\n  \"description\": \"manage ReplicaSets and leave managing to them\",\\n  \"destination_entity\": \"ReplicaSet\"\\n}\\n```\\n\\n**Relation 10**\\n```json\\n{\\n  \"source_entity\": \"kubectl get replicasets\",\\n  \"description\": \"get the list of ReplicaSets created by the Deployment\",\\n  \"destination_entity\": \"ReplicaSet\"\\n}\\n```\\n\\nHere is the list of relations in JSON format:\\n```json\\n'},\n",
       " {'page': 296,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '264\\nCHAPTER 9\\nDeployments: updating applications declaratively\\ntemplate. Using the hash value of the pod template like this allows the Deployment\\nto always use the same (possibly existing) ReplicaSet for a given version of the pod\\ntemplate.\\nACCESSING THE PODS THROUGH THE SERVICE\\nWith the three replicas created by this ReplicaSet now running, you can use the Ser-\\nvice you created a while ago to access them, because you made the new pods’ labels\\nmatch the Service’s label selector. \\n Up until this point, you probably haven’t seen a good-enough reason why you should\\nuse Deployments over ReplicationControllers. Luckily, creating a Deployment also hasn’t\\nbeen any harder than creating a ReplicationController. Now, you’ll start doing things\\nwith this Deployment, which will make it clear why Deployments are superior. This will\\nbecome clear in the next few moments, when you see how updating the app through\\na Deployment resource compares to updating it through a ReplicationController.\\n9.3.2\\nUpdating a Deployment\\nPreviously, when you ran your app using a ReplicationController, you had to explicitly\\ntell Kubernetes to perform the update by running kubectl rolling-update. You even\\nhad to specify the name for the new ReplicationController that should replace the old\\none. Kubernetes replaced all the original pods with new ones and deleted the original\\nReplicationController at the end of the process. During the process, you basically had\\nto stay around, keeping your terminal open and waiting for kubectl to finish the roll-\\ning update. \\n Now compare this to how you’re about to update a Deployment. The only thing\\nyou need to do is modify the pod template defined in the Deployment resource and\\nKubernetes will take all the steps necessary to get the actual system state to what’s\\ndefined in the resource. Similar to scaling a ReplicationController or ReplicaSet up or\\ndown, all you need to do is reference a new image tag in the Deployment’s pod tem-\\nplate and leave it to Kubernetes to transform your system so it matches the new\\ndesired state.\\nUNDERSTANDING THE AVAILABLE DEPLOYMENT STRATEGIES\\nHow this new state should be achieved is governed by the deployment strategy config-\\nured on the Deployment itself. The default strategy is to perform a rolling update (the\\nstrategy is called RollingUpdate). The alternative is the Recreate strategy, which\\ndeletes all the old pods at once and then creates new ones, similar to modifying a\\nReplicationController’s pod template and then deleting all the pods (we talked about\\nthis in section 9.1.1).\\n The Recreate strategy causes all old pods to be deleted before the new ones are\\ncreated. Use this strategy when your application doesn’t support running multiple ver-\\nsions in parallel and requires the old version to be stopped completely before the\\nnew one is started. This strategy does involve a short period of time when your app\\nbecomes completely unavailable.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicaSet',\n",
       "    'description': 'A Kubernetes resource that ensures a specified number of replicas (identical pods) are running at any given time.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A Kubernetes resource that provides a network identity and load-balancing for accessing applications in a cluster.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A Kubernetes resource that manages the rollout of new versions of an application, providing declarative updates to ReplicaSets.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A legacy Kubernetes resource that ensures a specified number of replicas (identical pods) are running at any given time.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod template',\n",
       "    'description': 'A template for creating pods, which can include settings such as image, ports, and environment variables.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'rolling update',\n",
       "    'description': 'A strategy used by Deployments to perform updates, where new replicas are created with the updated version, while old replicas continue to run until they are replaced.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Recreate strategy',\n",
       "    'description': \"A Deployment strategy that deletes all old pods at once and then creates new ones, similar to modifying a ReplicationController's pod template.\",\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool used for interacting with Kubernetes clusters, including updating Deployments using the `rolling-update` command.',\n",
       "    'category': 'tool'}],\n",
       "  'relationships': '[ Relation 1, Relation 2, Relation 3, Relation 4, Relation 5, Relation 6, Relation 7, Relation 8, Relation 9, Relation 10 ]'},\n",
       " {'page': 297,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '265\\nUsing Deployments for updating apps declaratively\\n The RollingUpdate strategy, on the other hand, removes old pods one by one,\\nwhile adding new ones at the same time, keeping the application available throughout\\nthe whole process, and ensuring there’s no drop in its capacity to handle requests.\\nThis is the default strategy. The upper and lower limits for the number of pods above\\nor below the desired replica count are configurable. You should use this strategy only\\nwhen your app can handle running both the old and new version at the same time.\\nSLOWING DOWN THE ROLLING UPDATE FOR DEMO PURPOSES\\nIn the next exercise, you’ll use the RollingUpdate strategy, but you need to slow down\\nthe update process a little, so you can see that the update is indeed performed in a\\nrolling fashion. You can do that by setting the minReadySeconds attribute on the\\nDeployment. We’ll explain what this attribute does by the end of this chapter. For\\nnow, set it to 10 seconds with the kubectl patch command.\\n$ kubectl patch deployment kubia -p \\'{\"spec\": {\"minReadySeconds\": 10}}\\'\\n\"kubia\" patched\\nTIP\\nThe kubectl patch command is useful for modifying a single property\\nor a limited number of properties of a resource without having to edit its defi-\\nnition in a text editor.\\nYou used the patch command to change the spec of the Deployment. This doesn’t\\ncause any kind of update to the pods, because you didn’t change the pod template.\\nChanging other Deployment properties, like the desired replica count or the deploy-\\nment strategy, also doesn’t trigger a rollout, because it doesn’t affect the existing indi-\\nvidual pods in any way.\\nTRIGGERING THE ROLLING UPDATE\\nIf you’d like to track the update process as it progresses, first run the curl loop again\\nin another terminal to see what’s happening with the requests (don’t forget to replace\\nthe IP with the actual external IP of your service):\\n$ while true; do curl http://130.211.109.222; done\\nTo trigger the actual rollout, you’ll change the image used in the single pod container\\nto luksa/kubia:v2. Instead of editing the whole YAML of the Deployment object or\\nusing the patch command to change the image, you’ll use the kubectl set image\\ncommand, which allows changing the image of any resource that contains a container\\n(ReplicationControllers, ReplicaSets, Deployments, and so on). You’ll use it to modify\\nyour Deployment like this:\\n$ kubectl set image deployment kubia nodejs=luksa/kubia:v2\\ndeployment \"kubia\" image updated\\nWhen you execute this command, you’re updating the kubia Deployment’s pod tem-\\nplate so the image used in its nodejs container is changed to luksa/kubia:v2 (from\\n:v1). This is shown in figure 9.9.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Deployment',\n",
       "    'description': 'A Kubernetes resource that manages a set of replica sets, which provide high availability for an application.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'RollingUpdate strategy',\n",
       "    'description': 'A deployment strategy that removes old pods one by one while adding new ones at the same time, keeping the application available throughout the process.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'minReadySeconds',\n",
       "    'description': 'An attribute of a Deployment that specifies the minimum number of seconds a pod must be ready before it can be considered as part of the deployment.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'kubectl patch command',\n",
       "    'description': 'A command used to modify a single property or a limited number of properties of a resource without having to edit its definition in a text editor.',\n",
       "    'category': 'software,command'},\n",
       "   {'entity': 'Deployment spec',\n",
       "    'description': 'The specification of a Deployment that defines how the deployment should be executed, including the desired replica count and deployment strategy.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A Kubernetes resource that ensures a specified number of replicas (identical copies) of a pod are running at any given time.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A legacy Kubernetes resource that ensures a specified number of replicas (identical copies) of a pod are running at any given time.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'nodejs container',\n",
       "    'description': 'A container in a pod that runs the Node.js application.',\n",
       "    'category': 'software,container'},\n",
       "   {'entity': 'kubectl set image command',\n",
       "    'description': 'A command used to change the image of a resource that contains a container, such as a Deployment or ReplicaSet.',\n",
       "    'category': 'software,command'},\n",
       "   {'entity': 'luksa/kubia:v2 image',\n",
       "    'description': 'The image used in the nodejs container with the specified version (v2).',\n",
       "    'category': 'software,image'},\n",
       "   {'entity': 'curl command',\n",
       "    'description': 'A command used to send HTTP requests to a server and retrieve responses.',\n",
       "    'category': 'software,command'},\n",
       "   {'entity': 'while true loop',\n",
       "    'description': 'A loop that runs indefinitely, used to continuously send curl commands to a server.',\n",
       "    'category': 'software,script'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl patch command\", \"description\": \"used to modify a single property or a limited number of properties of a resource without having to edit its definition in a text editor\", \"destination_entity\": \"Deployment spec\"},\\n  {\"source_entity\": \"Deployment spec\", \"description\": \"changed by kubectl patch command to set minReadySeconds to 10 seconds\", \"destination_entity\": \"minReadySeconds\"},\\n  {\"source_entity\": \"kubectl set image command\", \"description\": \"used to modify the image used in the single pod container to luksa/kubia:v2\", \"destination_entity\": \"nodejs container\"},\\n  {\"source_entity\": \"kubectl set image command\", \"description\": \"updated the kubia Deployment\\'s pod template so the image used in its nodejs container is changed to luksa/kubia:v2\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"curl command\", \"description\": \"used to track the update process as it progresses by running a loop to see what\\'s happening with the requests\", \"destination_entity\": \"requests\"},\\n  {\"source_entity\": \"while true loop\", \"description\": \"run in another terminal to see what\\'s happening with the requests\", \"destination_entity\": \"requests\"},\\n  {\"source_entity\": \"RollingUpdate strategy\", \"description\": \"used to update the kubia Deployment by removing old pods one by one, while adding new ones at the same time\", \"destination_entity\": \"kubia Deployment\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"can use the RollingUpdate strategy to update the pod template so that the image used in its nodejs container is changed to luksa/kubia:v2\", \"destination_entity\": \"nodejs container\"}\\n]\\n```\\n\\nNote: I\\'ve only included the entities provided in the input list, even if they are not explicitly mentioned in the document page. If you\\'d like me to extract additional entities or relations, please let me know!'},\n",
       " {'page': 298,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '266\\nCHAPTER 9\\nDeployments: updating applications declaratively\\nWays of modifying Deployments and other resources\\nOver the course of this book, you’ve learned several ways how to modify an existing\\nobject. Let’s list all of them together to refresh your memory.\\nAll these methods are equivalent as far as Deployments go. What they do is change\\nthe Deployment’s specification. This change then triggers the rollout process.\\nImage registry\\nPod template\\nDeployment\\nkubectl set image…\\nluksa/kubia:v2\\nContainer:\\nnodejs\\n:v1\\n:v2\\nImage registry\\nPod template\\nDeployment\\nContainer:\\nnodejs\\n:v1\\n:v2\\nFigure 9.9\\nUpdating a Deployment’s pod template to point to a new image\\nTable 9.1\\nModifying an existing resource in Kubernetes\\nMethod\\nWhat it does\\nkubectl edit\\nOpens the object’s manifest in your default editor. After making \\nchanges, saving the file, and exiting the editor, the object is updated.\\nExample: kubectl edit deployment kubia\\nkubectl patch\\nModifies individual properties of an object.\\nExample: kubectl patch deployment kubia -p \\'{\"spec\": \\n{\"template\": {\"spec\": {\"containers\": [{\"name\": \\n\"nodejs\", \"image\": \"luksa/kubia:v2\"}]}}}}\\'\\nkubectl apply\\nModifies the object by applying property values from a full YAML or \\nJSON file. If the object specified in the YAML/JSON doesn’t exist yet, \\nit’s created. The file needs to contain the full definition of the \\nresource (it can’t include only the fields you want to update, as is the \\ncase with kubectl patch).\\nExample: kubectl apply -f kubia-deployment-v2.yaml\\nkubectl replace\\nReplaces the object with a new one from a YAML/JSON file. In con-\\ntrast to the apply command, this command requires the object to \\nexist; otherwise it prints an error.\\nExample: kubectl replace -f kubia-deployment-v2.yaml\\nkubectl set image\\nChanges the container image defined in a Pod, ReplicationControl-\\nler’s template, Deployment, DaemonSet, Job, or ReplicaSet.\\nExample: kubectl set image deployment kubia \\nnodejs=luksa/kubia:v2\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Method, What it does]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Image registry',\n",
       "    'description': '',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Pod template', 'description': '', 'category': 'container'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'In Kubernetes, a Deployment is an object that manages the rollout of new versions of an application.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl set image...',\n",
       "    'description': \"A command to change the container image defined in a Pod, ReplicationController's template, Deployment, DaemonSet, Job, or ReplicaSet.\",\n",
       "    'category': 'command'},\n",
       "   {'entity': 'luksa/kubia:v2',\n",
       "    'description': 'An image name',\n",
       "    'category': 'image'},\n",
       "   {'entity': 'Container:', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'nodejs',\n",
       "    'description': 'A programming language',\n",
       "    'category': 'programming_language'},\n",
       "   {'entity': 'v1', 'description': 'An image version', 'category': 'version'},\n",
       "   {'entity': 'Image registry', 'description': '', 'category': 'database'},\n",
       "   {'entity': 'Pod template', 'description': '', 'category': 'container'},\n",
       "   {'entity': 'Deployment', 'description': '', 'category': 'application'},\n",
       "   {'entity': 'Container:', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'nodejs', 'description': '', 'category': 'programming_language'},\n",
       "   {'entity': 'v1', 'description': '', 'category': 'version'},\n",
       "   {'entity': 'kubectl edit',\n",
       "    'description': \"A command to open the object's manifest in your default editor.\",\n",
       "    'category': 'command'},\n",
       "   {'entity': 'deployment kubia',\n",
       "    'description': 'An example deployment name',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'kubectl patch',\n",
       "    'description': 'A command to modify individual properties of an object.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'p', 'description': 'A JSON payload', 'category': 'payload'},\n",
       "   {'entity': 'deployment kubia', 'description': '', 'category': 'resource'},\n",
       "   {'entity': 'kubectl apply',\n",
       "    'description': 'A command to modify the object by applying property values from a full YAML or JSON file.',\n",
       "    'category': 'command'},\n",
       "   {'entity': '-f kubia-deployment-v2.yaml',\n",
       "    'description': 'An example flag value',\n",
       "    'category': 'flag'},\n",
       "   {'entity': 'kubectl replace',\n",
       "    'description': 'A command to replace the object with a new one from a YAML/JSON file.',\n",
       "    'category': 'command'},\n",
       "   {'entity': '-f kubia-deployment-v2.yaml',\n",
       "    'description': '',\n",
       "    'category': 'flag'},\n",
       "   {'entity': 'kubectl set image',\n",
       "    'description': \"A command to change the container image defined in a Pod, ReplicationController's template, Deployment, DaemonSet, Job, or ReplicaSet.\",\n",
       "    'category': 'command'},\n",
       "   {'entity': 'deployment kubia', 'description': '', 'category': 'resource'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl edit\", \"description\": \"Opens the object\\'s manifest in your default editor.\", \"destination_entity\": \"deployment kubia\"},\\n  {\"source_entity\": \"kubectl patch\", \"description\": \"Modifies individual properties of an object.\", \"destination_entity\": \"deployment kubia\"},\\n  {\"source_entity\": \"kubectl apply\", \"description\": \"Modifies the object by applying property values from a full YAML or JSON file.\", \"destination_entity\": \"deployment kubia\"},\\n  {\"source_entity\": \"kubectl replace\", \"description\": \"Replaces the object with a new one from a YAML/JSON file.\", \"destination_entity\": \"deployment kubia\"},\\n  {\"source_entity\": \"kubectl set image\", \"description\": \"Changes the container image defined in a Pod, ReplicationController\\'s template, Deployment, DaemonSet, Job, or ReplicaSet.\", \"destination_entity\": \"deployment kubia\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"Points to a new image registry.\", \"destination_entity\": \"Image registry\"},\\n  {\"source_entity\": \"Pod template\", \"description\": \"Updates the object\\'s specification.\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"kubectl set image...\", \"description\": \"Changes the container image defined in a Pod, ReplicationController\\'s template, Deployment, DaemonSet, Job, or ReplicaSet.\", \"destination_entity\": \"deployment kubia nodejs\"},\\n  {\"source_entity\": \"Container:\", \"description\": \"References a specific container version (v1, v2).\", \"destination_entity\": \"nodejs\"},\\n  {\"source_entity\": \"kubectl patch\", \"description\": \"Updates the Deployment\\'s specification.\", \"destination_entity\": \"deployment kubia\"},\\n  {\"source_entity\": \"deployment kubia\", \"description\": \"Is updated using various methods.\", \"destination_entity\": \"Deployment\"}\\n]\\n```\\n\\nNote that some of these relations are implied by the context and might not be explicitly stated in the original text.'},\n",
       " {'page': 299,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '267\\nUsing Deployments for updating apps declaratively\\nIf you’ve run the curl loop, you’ll see requests initially hitting only the v1 pods; then\\nmore and more of them hit the v2 pods until, finally, all of them hit only the remain-\\ning v2 pods, after all v1 pods are deleted. This works much like the rolling update per-\\nformed by kubectl.\\nUNDERSTANDING THE AWESOMENESS OF DEPLOYMENTS\\nLet’s think about what has happened. By changing the pod template in your Deploy-\\nment resource, you’ve updated your app to a newer version—by changing a single\\nfield! \\n The controllers running as part of the Kubernetes control plane then performed\\nthe update. The process wasn’t performed by the kubectl client, like it was when you\\nused kubectl rolling-update. I don’t know about you, but I think that’s simpler than\\nhaving to run a special command telling Kubernetes what to do and then waiting\\naround for the process to be completed.\\nNOTE\\nBe aware that if the pod template in the Deployment references a\\nConfigMap (or a Secret), modifying the ConfigMap will not trigger an\\nupdate. One way to trigger an update when you need to modify an app’s con-\\nfig is to create a new ConfigMap and modify the pod template so it references\\nthe new ConfigMap.\\nThe events that occurred below the Deployment’s surface during the update are simi-\\nlar to what happened during the kubectl rolling-update. An additional ReplicaSet\\nwas created and it was then scaled up slowly, while the previous ReplicaSet was scaled\\ndown to zero (the initial and final states are shown in figure 9.10).\\nYou can still see the old ReplicaSet next to the new one if you list them:\\n$ kubectl get rs\\nNAME               DESIRED   CURRENT   AGE\\nkubia-1506449474   0         0         24m\\nkubia-1581357123   3         3         23m\\nPods: v1\\nReplicaSet: v1\\nReplicas: --\\nBefore\\nAfter\\nReplicaSet: v2\\nReplicas: ++\\nDeployment\\nPods: v2\\nReplicaSet: v1\\nReplicaSet: v2\\nDeployment\\nFigure 9.10\\nA Deployment at the start and end of a rolling update\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Deployments',\n",
       "    'description': 'a way to update apps declaratively',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'units of execution in Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'manages the number of replicas for a pod',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ConfigMap',\n",
       "    'description': 'a way to store and reference configuration data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'a way to store sensitive information',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the Kubernetes command-line tool',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Deployment\",\\n    \"description\": \"updated app to a newer version by changing a single field\",\\n    \"destination_entity\": \"app\"\\n  },\\n  {\\n    \"source_entity\": \"controllers running as part of Kubernetes control plane\",\\n    \"description\": \"performed the update\",\\n    \"destination_entity\": \"update\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl client\",\\n    \"description\": \"used to perform a rolling-update manually\",\\n    \"destination_entity\": \"rolling-update\"\\n  },\\n  {\\n    \"source_entity\": \"Deployment\",\\n    \"description\": \"modified pod template to reference new ConfigMap\",\\n    \"destination_entity\": \"ConfigMap\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMap (or Secret)\",\\n    \"description\": \"modifying will not trigger an update if referenced in Deployment\\'s pod template\",\\n    \"destination_entity\": \"update\"\\n  },\\n  {\\n    \"source_entity\": \"Deployment\",\\n    \"description\": \"scaled up additional ReplicaSet slowly while scaling down previous one to zero\",\\n    \"destination_entity\": \"ReplicaSet\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl get rs command\",\\n    \"description\": \"listed existing ReplicaSets before and after update\",\\n    \"destination_entity\": \"ReplicaSets\"\\n  },\\n  {\\n    \"source_entity\": \"Deployment\",\\n    \"description\": \"managed creation and scaling of Pods\",\\n    \"destination_entity\": \"Pods\"\\n  }\\n]\\n```\\n\\nNote: I\\'ve tried to extract all possible relations based on the context, but there might be some that are not explicitly mentioned in the document. Let me know if you need any further clarification!'},\n",
       " {'page': 300,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '268\\nCHAPTER 9\\nDeployments: updating applications declaratively\\nSimilar to ReplicationControllers, all your new pods are now managed by the new\\nReplicaSet. Unlike before, the old ReplicaSet is still there, whereas the old Replication-\\nController was deleted at the end of the rolling-update process. You’ll soon see what\\nthe purpose of this inactive ReplicaSet is. \\n But you shouldn’t care about ReplicaSets here, because you didn’t create them\\ndirectly. You created and operated only on the Deployment resource; the underlying\\nReplicaSets are an implementation detail. You’ll agree that managing a single Deploy-\\nment object is much easier compared to dealing with and keeping track of multiple\\nReplicationControllers. \\n Although this difference may not be so apparent when everything goes well with a\\nrollout, it becomes much more obvious when you hit a problem during the rollout\\nprocess. Let’s simulate one problem right now.\\n9.3.3\\nRolling back a deployment\\nYou’re currently running version v2 of your image, so you’ll need to prepare version 3\\nfirst. \\nCREATING VERSION 3 OF YOUR APP\\nIn version 3, you’ll introduce a bug that makes your app handle only the first four\\nrequests properly. All requests from the fifth request onward will return an internal\\nserver error (HTTP status code 500). You’ll simulate this by adding an if statement at\\nthe beginning of the handler function. The following listing shows the new code, with\\nall required changes shown in bold.\\nconst http = require(\\'http\\');\\nconst os = require(\\'os\\');\\nvar requestCount = 0;\\nconsole.log(\"Kubia server starting...\");\\nvar handler = function(request, response) {\\n  console.log(\"Received request from \" + request.connection.remoteAddress);\\n  if (++requestCount >= 5) {\\n    response.writeHead(500);\\n    response.end(\"Some internal error has occurred! This is pod \" + \\nos.hostname() + \"\\\\n\");\\n    return;\\n  }\\n  response.writeHead(200);\\n  response.end(\"This is v3 running in pod \" + os.hostname() + \"\\\\n\");\\n};\\nvar www = http.createServer(handler);\\nwww.listen(8080); \\nAs you can see, on the fifth and all subsequent requests, the code returns a 500 error\\nwith the message “Some internal error has occurred...”\\nListing 9.8\\nVersion 3 of our app (a broken version): v3/app.js\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicaSet',\n",
       "    'description': 'A ReplicaSet is a resource in Kubernetes that ensures a specified number of replicas (pods) are running at any given time.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A Deployment is a resource in Kubernetes that manages the rollout of updates to applications, including creating and managing ReplicaSets.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A ReplicationController is a resource in Kubernetes that ensures a specified number of replicas (pods) are running at any given time. (Deprecated)',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'container/orchestration'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'A popular containerization platform that allows developers to package their applications and dependencies into a single container.',\n",
       "    'category': 'container/platform'},\n",
       "   {'entity': 'http.createServer',\n",
       "    'description': 'A function in Node.js that creates an HTTP server from a callback function.',\n",
       "    'category': 'library/api'},\n",
       "   {'entity': 'os.hostname',\n",
       "    'description': 'A built-in Node.js module that provides information about the current operating system and host.',\n",
       "    'category': 'library/module'},\n",
       "   {'entity': 'http.status code 500',\n",
       "    'description': 'An HTTP status code indicating an internal server error occurred.',\n",
       "    'category': 'protocol/code'},\n",
       "   {'entity': 'Node.js',\n",
       "    'description': 'A JavaScript runtime environment that allows developers to run JavaScript on the server-side.',\n",
       "    'category': 'runtime/environment'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"manages\",\\n    \"destination_entity\": \"ReplicaSet\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController\",\\n    \"description\": \"is deleted at the end of a rolling-update process\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"manages a single Deployment object is easier than managing multiple ReplicationControllers\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"Deployment\",\\n    \"description\": \"is the underlying implementation detail of ReplicaSets\",\\n    \"destination_entity\": \"ReplicaSet\"\\n  },\\n  {\\n    \"source_entity\": \"http.status code 500\",\\n    \"description\": \"returns an error with message \\'Some internal error has occurred...\\'\",\\n    \"destination_entity\": \"request\"\\n  },\\n  {\\n    \"source_entity\": \"Node.js\",\\n    \"description\": \"uses http.createServer to create a server\",\\n    \"destination_entity\": \"http.server\"\\n  },\\n  {\\n    \"source_entity\": \"os.hostname\",\\n    \"description\": \"returns the hostname of the system where the process is running\",\\n    \"destination_entity\": \"process\"\\n  },\\n  {\\n    \"source_entity\": \"http.createServer\",\\n    \"description\": \"creates an HTTP server that listens on port 8080\",\\n    \"destination_entity\": \"port.8080\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSet\",\\n    \"description\": \"is responsible for managing multiple pods of the same image version\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"uses ReplicaSets to manage deployments declaratively\",\\n    \"destination_entity\": \"Deployment\"\\n  }\\n]\\n\\nNote that some entities (like \"port.8080\") are not explicitly mentioned in the provided list, but I\\'ve included them as destination entities since they were implied in the context of the document page. If you\\'d like me to exclude them, please let me know!'},\n",
       " {'page': 301,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '269\\nUsing Deployments for updating apps declaratively\\nDEPLOYING VERSION 3\\nI’ve made the v3 version of the image available as luksa/kubia:v3. You’ll deploy this\\nnew version by changing the image in the Deployment specification again: \\n$ kubectl set image deployment kubia nodejs=luksa/kubia:v3\\ndeployment \"kubia\" image updated\\nYou can follow the progress of the rollout with kubectl rollout status:\\n$ kubectl rollout status deployment kubia\\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\\nWaiting for rollout to finish: 1 old replicas are pending termination...\\ndeployment \"kubia\" successfully rolled out\\nThe new version is now live. As the following listing shows, after a few requests, your\\nweb clients start receiving errors.\\n$ while true; do curl http://130.211.109.222; done\\nThis is v3 running in pod kubia-1914148340-lalmx\\nThis is v3 running in pod kubia-1914148340-bz35w\\nThis is v3 running in pod kubia-1914148340-w0voh\\n...\\nThis is v3 running in pod kubia-1914148340-w0voh\\nSome internal error has occurred! This is pod kubia-1914148340-bz35w\\nThis is v3 running in pod kubia-1914148340-w0voh\\nSome internal error has occurred! This is pod kubia-1914148340-lalmx\\nThis is v3 running in pod kubia-1914148340-w0voh\\nSome internal error has occurred! This is pod kubia-1914148340-lalmx\\nSome internal error has occurred! This is pod kubia-1914148340-bz35w\\nSome internal error has occurred! This is pod kubia-1914148340-w0voh\\nUNDOING A ROLLOUT\\nYou can’t have your users experiencing internal server errors, so you need to do some-\\nthing about it fast. In section 9.3.6 you’ll see how to block bad rollouts automatically,\\nbut for now, let’s see what you can do about your bad rollout manually. Luckily,\\nDeployments make it easy to roll back to the previously deployed version by telling\\nKubernetes to undo the last rollout of a Deployment:\\n$ kubectl rollout undo deployment kubia\\ndeployment \"kubia\" rolled back\\nThis rolls the Deployment back to the previous revision. \\nTIP\\nThe undo command can also be used while the rollout process is still in\\nprogress to essentially abort the rollout. Pods already created during the roll-\\nout process are removed and replaced with the old ones again.\\nListing 9.9\\nHitting your broken version 3\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command-line tool for managing Kubernetes resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'Kubernetes resource that manages a set of replicasets',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'rollout status',\n",
       "    'description': 'feature in kubectl to monitor rollout progress',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'replicas',\n",
       "    'description': 'multiple instances of a pod',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'lightweight and ephemeral container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'pre-built binary for running an application',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'nodejs',\n",
       "    'description': 'JavaScript runtime environment',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'command-line tool for transferring data to/from a web server',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'http://130.211.109.222',\n",
       "    'description': 'URL of the Kubernetes service',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'kubectl rollout status',\n",
       "    'description': 'feature in kubectl to monitor rollout progress',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'deployment kubia',\n",
       "    'description': 'Deployment resource managed by Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'undo command',\n",
       "    'description': 'feature in kubectl to roll back a Deployment',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'revision',\n",
       "    'description': 'unique identifier for each deployment revision',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"update image specification\", \"destination_entity\": \"nodejs\"},\\n  {\"source_entity\": \"kubectl rollout status\", \"description\": \"follow progress of rollout\", \"destination_entity\": \"deployment kubia\"},\\n  {\"source_entity\": \"image\", \"description\": \"contain new version of app\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"kubectl rollout undo\", \"description\": \"roll back to previous revision\", \"destination_entity\": \"deployment kubia\"},\\n  {\"source_entity\": \"undo command\", \"description\": \"abort rollout process\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"deployment kubia\", \"description\": \"successfully rolled out new version\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"kubectl rollout status\", \"description\": \"check status of rollout\", \"destination_entity\": \"rollout status\"},\\n  {\"source_entity\": \"curl\", \"description\": \"send requests to broken version\", \"destination_entity\": \"http://130.211.109.222\"},\\n  {\"source_entity\": \"deployment kubia\", \"description\": \"has replicas running different versions\", \"destination_entity\": \"replicas\"},\\n  {\"source_entity\": \"kubectl rollout status\", \"description\": \"show progress of rollout\", \"destination_entity\": \"pod\"}\\n]\\n```'},\n",
       " {'page': 302,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '270\\nCHAPTER 9\\nDeployments: updating applications declaratively\\nDISPLAYING A DEPLOYMENT’S ROLLOUT HISTORY\\nRolling back a rollout is possible because Deployments keep a revision history. As\\nyou’ll see later, the history is stored in the underlying ReplicaSets. When a rollout\\ncompletes, the old ReplicaSet isn’t deleted, and this enables rolling back to any revi-\\nsion, not only the previous one. The revision history can be displayed with the\\nkubectl rollout history command:\\n$ kubectl rollout history deployment kubia\\ndeployments \"kubia\":\\nREVISION    CHANGE-CAUSE\\n2           kubectl set image deployment kubia nodejs=luksa/kubia:v2\\n3           kubectl set image deployment kubia nodejs=luksa/kubia:v3\\nRemember the --record command-line option you used when creating the Deploy-\\nment? Without it, the CHANGE-CAUSE column in the revision history would be empty,\\nmaking it much harder to figure out what’s behind each revision.\\nROLLING BACK TO A SPECIFIC DEPLOYMENT REVISION\\nYou can roll back to a specific revision by specifying the revision in the undo com-\\nmand. For example, if you want to roll back to the first version, you’d execute the fol-\\nlowing command:\\n$ kubectl rollout undo deployment kubia --to-revision=1\\nRemember the inactive ReplicaSet left over when you modified the Deployment the\\nfirst time? The ReplicaSet represents the first revision of your Deployment. All Replica-\\nSets created by a Deployment represent the complete revision history, as shown in fig-\\nure 9.11. Each ReplicaSet stores the complete information of the Deployment at that\\nspecific revision, so you shouldn’t delete it manually. If you do, you’ll lose that specific\\nrevision from the Deployment’s history, preventing you from rolling back to it.\\nBut having old ReplicaSets cluttering your ReplicaSet list is not ideal, so the length of\\nthe revision history is limited by the revisionHistoryLimit property on the Deploy-\\nment resource. It defaults to two, so normally only the current and the previous revision\\nare shown in the history (and only the current and the previous ReplicaSet are pre-\\nserved). Older ReplicaSets are deleted automatically. \\nDeployment\\nv1 ReplicaSet\\nReplicaSet\\nPods: v1\\nReplicaSet\\nReplicaSet\\nReplicaSet\\nRevision 2\\nRevision 4\\nRevision 3\\nRevision 1\\nRevision history\\nCurrent revision\\nFigure 9.11\\nA Deployment’s ReplicaSets also act as its revision history.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Deployment',\n",
       "    'description': 'A Kubernetes resource that manages the rollout of new versions of an application.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A Kubernetes resource that ensures a specified number of replicas (identical Pods) are running at any given time.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'The smallest deployable units in Kubernetes, representing an application or service.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for interacting with a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubia',\n",
       "    'description': 'An example deployment used throughout the chapter to illustrate deployment concepts.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'image',\n",
       "    'description': \"A snapshot of a container's filesystem at a given point in time.\",\n",
       "    'category': 'container'},\n",
       "   {'entity': 'revision history',\n",
       "    'description': 'A record of changes made to a Deployment over time, stored in the underlying ReplicaSets.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'REVISION',\n",
       "    'description': 'A unique identifier for each revision of a Deployment.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CHANGE-CAUSE',\n",
       "    'description': 'A field in the revision history that stores information about why a particular revision was made.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'kubectl rollout history',\n",
       "    'description': 'A command used to display the revision history of a deployment.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'rollout undo',\n",
       "    'description': 'A command used to roll back to a specific revision of a Deployment.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'revisionHistoryLimit',\n",
       "    'description': 'A property on the Deployment resource that limits the number of revisions stored in the history.',\n",
       "    'category': 'database'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"used to display a deployment\\'s rollout history\",\\n    \"destination_entity\": \"revision history\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"used to roll back to a specific deployment revision\",\\n    \"destination_entity\": \"rollout undo\"\\n  },\\n  {\\n    \"source_entity\": \"CHANGE-CAUSE\",\\n    \"description\": \"stores the reason for each revision in the revision history\",\\n    \"destination_entity\": \"revision history\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl set image deployment\",\\n    \"description\": \"updated the nodejs image to luksa/kubia:v2 and luksa/kubia:v3\",\\n    \"destination_entity\": \"image\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl rollout history\",\\n    \"description\": \"displayed the revision history of a deployment\",\\n    \"destination_entity\": \"revision history\"\\n  },\\n  {\\n    \"source_entity\": \"REVISION\",\\n    \"description\": \"shows the current and previous revisions in the revision history\",\\n    \"destination_entity\": \"revision history\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"affected by a deployment\\'s replica set\",\\n    \"destination_entity\": \"ReplicaSet\"\\n  },\\n  {\\n    \"source_entity\": \"kubia\",\\n    \"description\": \"a deployment that was updated and rolled back\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl rollout undo\",\\n    \"description\": \"rolled back to a specific deployment revision\",\\n    \"destination_entity\": \"rollout undo\"\\n  },\\n  {\\n    \"source_entity\": \"revisionHistoryLimit\",\\n    \"description\": \"limits the length of the revision history for a deployment\",\\n    \"destination_entity\": \"Deployment\"\\n  }\\n]\\n```\\n\\nNote that I have extracted relations based on the context and entities provided, and tried to capture the essence of each relation in a brief summary.'},\n",
       " {'page': 303,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '271\\nUsing Deployments for updating apps declaratively\\nNOTE\\nThe extensions/v1beta1 version of Deployments doesn’t have a default\\nrevisionHistoryLimit, whereas the default in version apps/v1beta2 is 10.\\n9.3.4\\nControlling the rate of the rollout\\nWhen you performed the rollout to v3 and tracked its progress with the kubectl\\nrollout status command, you saw that first a new pod was created, and when it\\nbecame available, one of the old pods was deleted and another new pod was created.\\nThis continued until there were no old pods left. The way new pods are created and\\nold ones are deleted is configurable through two additional properties of the rolling\\nupdate strategy. \\nINTRODUCING THE MAXSURGE AND MAXUNAVAILABLE PROPERTIES OF THE ROLLING UPDATE STRATEGY\\nTwo properties affect how many pods are replaced at once during a Deployment’s roll-\\ning update. They are maxSurge and maxUnavailable and can be set as part of the\\nrollingUpdate sub-property of the Deployment’s strategy attribute, as shown in\\nthe following listing.\\nspec:\\n  strategy:\\n    rollingUpdate:\\n      maxSurge: 1\\n      maxUnavailable: 0\\n    type: RollingUpdate\\nWhat these properties do is explained in table 9.2.\\nBecause the desired replica count in your case was three, and both these properties\\ndefault to 25%, maxSurge allowed the number of all pods to reach four, and\\nListing 9.10\\nSpecifying parameters for the rollingUpdate strategy\\nTable 9.2\\nProperties for configuring the rate of the rolling update\\nProperty\\nWhat it does\\nmaxSurge\\nDetermines how many pod instances you allow to exist above the desired replica \\ncount configured on the Deployment. It defaults to 25%, so there can be at most \\n25% more pod instances than the desired count. If the desired replica count is \\nset to four, there will never be more than five pod instances running at the same \\ntime during an update. When converting a percentage to an absolute number, \\nthe number is rounded up. Instead of a percentage, the value can also be an \\nabsolute value (for example, one or two additional pods can be allowed).\\nmaxUnavailable\\nDetermines how many pod instances can be unavailable relative to the desired \\nreplica count during the update. It also defaults to 25%, so the number of avail-\\nable pod instances must never fall below 75% of the desired replica count. Here, \\nwhen converting a percentage to an absolute number, the number is rounded \\ndown. If the desired replica count is set to four and the percentage is 25%, only \\none pod can be unavailable. There will always be at least three pod instances \\navailable to serve requests during the whole rollout. As with maxSurge, you can \\nalso specify an absolute value instead of a percentage.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [                   Property                                       What it does\n",
       "   0  maxSurge\\nmaxUnavailable  Determines how many pod instances you allow to...],\n",
       "  'entities': [{'entity': 'Deployments',\n",
       "    'description': 'Kubernetes resource for managing and updating applications declaratively',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes clusters',\n",
       "    'category': 'tool'},\n",
       "   {'entity': 'rollout status command',\n",
       "    'description': 'Command used to track the progress of a rollout',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'maxSurge property',\n",
       "    'description': 'Determines how many pod instances can exist above the desired replica count during an update',\n",
       "    'category': 'property'},\n",
       "   {'entity': 'maxUnavailable property',\n",
       "    'description': 'Determines how many pod instances can be unavailable relative to the desired replica count during an update',\n",
       "    'category': 'property'},\n",
       "   {'entity': 'rollingUpdate strategy',\n",
       "    'description': 'Deployment strategy that updates pods in a rolling manner',\n",
       "    'category': 'strategy'},\n",
       "   {'entity': 'RollingUpdate type',\n",
       "    'description': 'Type of deployment strategy that uses a rolling update approach',\n",
       "    'category': 'type'},\n",
       "   {'entity': 'replica count',\n",
       "    'description': 'Number of desired pod instances',\n",
       "    'category': 'count'},\n",
       "   {'entity': 'pod instances',\n",
       "    'description': 'Individual instances of pods running in the cluster',\n",
       "    'category': 'instance'},\n",
       "   {'entity': 'desired replica count',\n",
       "    'description': 'Specified number of desired pod instances',\n",
       "    'category': 'count'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"tracked progress with rollout status command\", \"destination_entity\": \"rollout\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"update apps declaratively using Deployments\", \"destination_entity\": \"apps\"},\\n  {\"source_entity\": \"Deployments\", \"description\": \"don\\'t have a default revisionHistoryLimit\", \"destination_entity\": \"revisionHistoryLimit\"},\\n  {\"source_entity\": \"Deployments\", \"description\": \"have a default maxSurge property\", \"destination_entity\": \"maxSurge property\"},\\n  {\"source_entity\": \"Deployments\", \"description\": \"have a default maxUnavailable property\", \"destination_entity\": \"maxUnavailable property\"},\\n  {\"source_entity\": \"Desired replica count\", \"description\": \"configured on the Deployment\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"Pod instances\", \"description\": \"can exist above desired replica count configured on the Deployment\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"maxSurge property\", \"description\": \"determines how many pod instances allowed to exist above desired replica count\", \"destination_entity\": \"pod instances\"},\\n  {\"source_entity\": \"maxUnavailable property\", \"description\": \"determines how many pod instances can be unavailable relative to desired replica count\", \"destination_entity\": \"pod instances\"},\\n  {\"source_entity\": \"RollingUpdate strategy\", \"description\": \"configurable through maxSurge and maxUnavailable properties\", \"destination_entity\": \"maxSurge property\"},\\n  {\"source_entity\": \"RollingUpdate strategy\", \"description\": \"configurable through maxSurge and maxUnavailable properties\", \"destination_entity\": \"maxUnavailable property\"}\\n]'},\n",
       " {'page': 304,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '272\\nCHAPTER 9\\nDeployments: updating applications declaratively\\nmaxUnavailable disallowed having any unavailable pods (in other words, three pods\\nhad to be available at all times). This is shown in figure 9.12.\\nUNDERSTANDING THE MAXUNAVAILABLE PROPERTY\\nThe extensions/v1beta1 version of Deployments uses different defaults—it sets both\\nmaxSurge and maxUnavailable to 1 instead of 25%. In the case of three replicas, max-\\nSurge is the same as before, but maxUnavailable is different (1 instead of 0). This\\nmakes the rollout process unwind a bit differently, as shown in figure 9.13.\\nv1\\nNumber\\nof pods\\n3\\n4\\n2\\n1\\nTime\\nv1\\n3 available\\n1 unavailable\\nCreate\\none\\nv2 pod\\n4 available\\n3 available\\n1 unavailable\\n4 available\\n3 available\\n1 unavailable\\nmaxSurge = 1\\nmaxUnavailable = 0\\nDesired replica count = 3\\n3 available\\nv2\\nv1\\nv1\\nv2\\nv2\\nv1\\nv1\\nv1\\nv1\\nv1\\nv1\\nv1\\nv1\\nv1\\nv1\\nv2\\nv2\\nv2\\nv2\\nv2\\nv2\\nv2\\nv2\\nv1\\nv2\\nv2\\nv2\\nv2\\n4 available\\nWait\\nuntil\\nit’s\\navailable\\nDelete\\none v1\\npod and\\ncreate one\\nv2 pod\\nWait\\nuntil\\nit’s\\navailable\\nDelete\\none v1\\npod and\\ncreate one\\nv2 pod\\nWait\\nuntil\\nit’s\\navailable\\nDelete\\nlast\\nv1 pod\\nFigure 9.12\\nRolling update of a Deployment with three replicas and default maxSurge and maxUnavailable \\nv1\\nNumber\\nof pods\\n3\\n4\\n2\\n1\\nTime\\nv1\\n2 available\\n2 unavailable\\n4 available\\n2 available\\n1 unavailable\\n3 available\\nmaxSurge = 1\\nmaxUnavailable = 1\\nDesired replica count = 3\\nv1\\nv1\\nv1\\nv1\\nv1\\nv2\\nv2\\nv2\\nv2\\nv2\\nv2\\nv2\\nv2\\nv2\\nv2\\nWait until\\nboth are\\navailable\\nDelete\\ntwo v1\\npods and\\ncreate one\\nv2 pod\\nDelete v1\\npod and\\ncreate two\\nv2 pods\\nWait\\nuntil it’s\\navailable\\nFigure 9.13\\nRolling update of a Deployment with the maxSurge=1 and maxUnavailable=1\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [   Col0 Col1  Col2 3-v2  Col4 5-v2  Col6 7-v2  Col8 9-v2 Col10 11-v2 Col12  \\\n",
       "   0         v1         v1         v1         v2         v2          v2         \n",
       "   1  None   v1  None   v1  None   v1  None   v1  None   v1  None    v2  None   \n",
       "   2  None   v1  None   v1  None   v1  None   v1  None   v1  None    v1  None   \n",
       "   \n",
       "     13-v2 Col14 Col15  \n",
       "   0    v2          v2  \n",
       "   1    v2  None    v2  \n",
       "   2    v1  None    v2  ,\n",
       "      Col0 Col1  Col2 3-v2  Col4 5-v2  Col6 Col7  Col8 Col9\n",
       "   0         v1         v2         v2         v2         v2\n",
       "   1         v1         v1         v1         v2         v2\n",
       "   2  None   v1  None   v1  None   v1  None   v2  None   v2],\n",
       "  'entities': [{'entity': 'Deployments',\n",
       "    'description': 'A Kubernetes resource that manages applications declaratively',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'maxUnavailable',\n",
       "    'description': 'A property that controls how many unavailable pods are allowed during a rollout',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'The basic execution unit in Kubernetes, equivalent to a container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'rollout',\n",
       "    'description': \"The process of updating an application's replicas in a Deployment\",\n",
       "    'category': 'process'},\n",
       "   {'entity': 'maxSurge',\n",
       "    'description': 'A property that controls how many extra pods are created during a rollout',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'v1beta1',\n",
       "    'description': 'The version of Deployments used by Kubernetes before v1 was released',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'replicas',\n",
       "    'description': 'The number of running instances of an application in a Deployment',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'desired replica count',\n",
       "    'description': 'The target number of replicas for a Deployment',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Deployments\",\\n    \"description\": \"uses different defaults—it sets both maxSurge and maxUnavailable to 1 instead of 25%\",\\n    \"destination_entity\": \"extensions/v1beta1 version\"\\n  },\\n  {\\n    \"source_entity\": \"maxSurge\",\\n    \"description\": \"is the same as before, but maxUnavailable is different (1 instead of 0)\",\\n    \"destination_entity\": \"maxUnavailable\"\\n  },\\n  {\\n    \"source_entity\": \"Deployments\",\\n    \"description\": \"unwind a bit differently\",\\n    \"destination_entity\": \"rollout process\"\\n  },\\n  {\\n    \"source_entity\": \"pods\",\\n    \"description\": \"had to be available at all times\",\\n    \"destination_entity\": \"maxUnavailable\"\\n  },\\n  {\\n    \"source_entity\": \"Deployments\",\\n    \"description\": \"sets both maxSurge and maxUnavailable to 1 instead of 25%\",\\n    \"destination_entity\": \"v1beta1 version\"\\n  },\\n  {\\n    \"source_entity\": \"maxUnavailable\",\\n    \"description\": \"makes the rollout process unwind a bit differently\",\\n    \"destination_entity\": \"rollout process\"\\n  },\\n  {\\n    \"source_entity\": \"Desired replica count\",\\n    \"description\": \"is set to 3\",\\n    \"destination_entity\": \"Deployments\"\\n  },\\n  {\\n    \"source_entity\": \"maxSurge\",\\n    \"description\": \"is the same as before\",\\n    \"destination_entity\": \"maxUnavailable\"\\n  },\\n  {\\n    \"source_entity\": \"v1beta1 version\",\\n    \"description\": \"uses different defaults—it sets both maxSurge and maxUnavailable to 1 instead of 25%\",\\n    \"destination_entity\": \"Deployments\"\\n  },\\n  {\\n    \"source_entity\": \"maxUnavailable\",\\n    \"description\": \"is set to 1\",\\n    \"destination_entity\": \"Deployments\"\\n  },\\n  {\\n    \"source_entity\": \"Deployments\",\\n    \"description\": \"wait until it’s available\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"pods\",\\n    \"description\": \"had to be deleted and created again\",\\n    \"destination_entity\": \"Deployments\"\\n  },\\n  {\\n    \"source_entity\": \"maxSurge\",\\n    \"description\": \"is set to 1\",\\n    \"destination_entity\": \"Deployments\"\\n  },\\n  {\\n    \"source_entity\": \"maxUnavailable\",\\n    \"description\": \"is set to 0\",\\n    \"destination_entity\": \"Deployments\"\\n  },\\n  {\\n    \"source_entity\": \"Desired replica count\",\\n    \"description\": \"is set to 3\",\\n    \"destination_entity\": \"Deployments\"\\n  }\\n]\\n```\\n\\nNote: I\\'ve extracted all the relations mentioned in the document page, even if they seem trivial or obvious. If you want me to filter out any of these relations, please let me know!'},\n",
       " {'page': 305,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '273\\nUsing Deployments for updating apps declaratively\\nIn this case, one replica can be unavailable, so if the desired replica count is three,\\nonly two of them need to be available. That’s why the rollout process immediately\\ndeletes one pod and creates two new ones. This ensures two pods are available and\\nthat the maximum number of pods isn’t exceeded (the maximum is four in this\\ncase—three plus one from maxSurge). As soon as the two new pods are available, the\\ntwo remaining old pods are deleted.\\n This is a bit hard to grasp, especially since the maxUnavailable property leads you\\nto believe that that’s the maximum number of unavailable pods that are allowed. If\\nyou look at the previous figure closely, you’ll see two unavailable pods in the second\\ncolumn even though maxUnavailable is set to 1. \\n It’s important to keep in mind that maxUnavailable is relative to the desired\\nreplica count. If the replica count is set to three and maxUnavailable is set to one,\\nthat means that the update process must always keep at least two (3 minus 1) pods\\navailable, while the number of pods that aren’t available can exceed one.\\n9.3.5\\nPausing the rollout process\\nAfter the bad experience with version 3 of your app, imagine you’ve now fixed the bug\\nand pushed version 4 of your image. You’re a little apprehensive about rolling it out\\nacross all your pods the way you did before. What you want is to run a single v4 pod\\nnext to your existing v2 pods and see how it behaves with only a fraction of all your\\nusers. Then, once you’re sure everything’s okay, you can replace all the old pods with\\nnew ones. \\n You could achieve this by running an additional pod either directly or through an\\nadditional Deployment, ReplicationController, or ReplicaSet, but you do have another\\noption available on the Deployment itself. A Deployment can also be paused during\\nthe rollout process. This allows you to verify that everything is fine with the new ver-\\nsion before proceeding with the rest of the rollout.\\nPAUSING THE ROLLOUT\\nI’ve prepared the v4 image, so go ahead and trigger the rollout by changing the image\\nto luksa/kubia:v4, but then immediately (within a few seconds) pause the rollout:\\n$ kubectl set image deployment kubia nodejs=luksa/kubia:v4\\ndeployment \"kubia\" image updated\\n$ kubectl rollout pause deployment kubia\\ndeployment \"kubia\" paused\\nA single new pod should have been created, but all original pods should also still be\\nrunning. Once the new pod is up, a part of all requests to the service will be redirected\\nto the new pod. This way, you’ve effectively run a canary release. A canary release is a\\ntechnique for minimizing the risk of rolling out a bad version of an application and it\\naffecting all your users. Instead of rolling out the new version to everyone, you replace\\nonly one or a small number of old pods with new ones. This way only a small number\\nof users will initially hit the new version. You can then verify whether the new version\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Deployment',\n",
       "    'description': 'A Kubernetes object that manages the rollout of new versions of an application',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A Kubernetes object that ensures a specified number of replicas of a pod are running at any given time',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The smallest deployable unit in Kubernetes, which can contain one or more containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment rollout',\n",
       "    'description': 'The process of updating an application by replacing old pods with new ones',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'maxUnavailable',\n",
       "    'description': 'A property that specifies the maximum number of unavailable pods allowed during a deployment rollout',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes object that ensures a specified number of replicas of a pod are running at any given time',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubia service',\n",
       "    'description': 'The service that directs traffic to pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool for interacting with Kubernetes clusters',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'rollout pause',\n",
       "    'description': 'A command used to pause a deployment rollout process',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'set image deployment',\n",
       "    'description': 'A command used to update the image of a deployment',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Deployment kubia nodejs=luksa/kubia:v4',\n",
       "    'description': 'The updated image of a deployment',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'maxSurge',\n",
       "    'description': 'A property that specifies the maximum number of pods allowed during a deployment rollout',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'desired replica count',\n",
       "    'description': 'The desired number of replicas of a pod',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\":\"kubectl\",\"description\":\"update deployment image\",\"destination_entity\":\"kubia nodejs=luksa/kubia:v4\"},{\"source_entity\":\"Deployment\",\"description\":\"roll out new version with canary release\",\"destination_entity\":\"kubia service\"},{\"source_entity\":\"maxSurge\",\"description\":\"maximum number of pods that can be created during rollout\",\"destination_entity\":\"Pods\"},{\"source_entity\":\"rollout pause\",\"description\":\"pause deployment to verify new version\",\"destination_entity\":\"Deployment kubia\"},{\"source_entity\":\"desired replica count\",\"description\":\"minimum number of available pods required during rollout\",\"destination_entity\":\"maxUnavailable\"},{\"source_entity\":\"ReplicaSet\",\"description\":\"similar to Deployment, but with more manual control over rollout\",\"destination_entity\":\"Deployment\"},{\"source_entity\":\"ReplicationController\",\"description\":\"older concept that is similar to ReplicaSet, but with less automatic control\",\"destination_entity\":\"ReplicaSet\"},{\"source_entity\":\"kubectl set image deployment\",\"description\":\"update deployment image using kubectl command\",\"destination_entity\":\"Deployment kubia nodejs=luksa/kubia:v4\"},{\"source_entity\":\"Deployment rollout\",\"description\":\"automatic process of updating pods to a new version\",\"destination_entity\":\"Pods\"}]'},\n",
       " {'page': 306,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '274\\nCHAPTER 9\\nDeployments: updating applications declaratively\\nis working fine or not and then either continue the rollout across all remaining pods\\nor roll back to the previous version. \\nRESUMING THE ROLLOUT\\nIn your case, by pausing the rollout process, only a small portion of client requests will\\nhit your v4 pod, while most will still hit the v3 pods. Once you’re confident the new\\nversion works as it should, you can resume the deployment to replace all the old pods\\nwith new ones:\\n$ kubectl rollout resume deployment kubia\\ndeployment \"kubia\" resumed\\nObviously, having to pause the deployment at an exact point in the rollout process\\nisn’t what you want to do. In the future, a new upgrade strategy may do that automati-\\ncally, but currently, the proper way of performing a canary release is by using two dif-\\nferent Deployments and scaling them appropriately. \\nUSING THE PAUSE FEATURE TO PREVENT ROLLOUTS\\nPausing a Deployment can also be used to prevent updates to the Deployment from\\nkicking off the rollout process, allowing you to make multiple changes to the Deploy-\\nment and starting the rollout only when you’re done making all the necessary changes.\\nOnce you’re ready for changes to take effect, you resume the Deployment and the\\nrollout process will start.\\nNOTE\\nIf a Deployment is paused, the undo command won’t undo it until you\\nresume the Deployment.\\n9.3.6\\nBlocking rollouts of bad versions\\nBefore you conclude this chapter, we need to discuss one more property of the Deploy-\\nment resource. Remember the minReadySeconds property you set on the Deployment\\nat the beginning of section 9.3.2? You used it to slow down the rollout, so you could see\\nit was indeed performing a rolling update and not replacing all the pods at once. The\\nmain function of minReadySeconds is to prevent deploying malfunctioning versions, not\\nslowing down a deployment for fun. \\nUNDERSTANDING THE APPLICABILITY OF MINREADYSECONDS\\nThe minReadySeconds property specifies how long a newly created pod should be\\nready before the pod is treated as available. Until the pod is available, the rollout pro-\\ncess will not continue (remember the maxUnavailable property?). A pod is ready\\nwhen readiness probes of all its containers return a success. If a new pod isn’t func-\\ntioning properly and its readiness probe starts failing before minReadySeconds have\\npassed, the rollout of the new version will effectively be blocked.\\n You used this property to slow down your rollout process by having Kubernetes\\nwait 10 seconds after a pod was ready before continuing with the rollout. Usually,\\nyou’d set minReadySeconds to something much higher to make sure pods keep report-\\ning they’re ready after they’ve already started receiving actual traffic. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command line tool for interacting with Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'deployment',\n",
       "    'description': 'Kubernetes resource for managing application deployments',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'rollout',\n",
       "    'description': 'process of updating a deployment with new pods',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pause',\n",
       "    'description': 'feature of Kubernetes deployments for pausing rollouts',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'resume',\n",
       "    'description': 'command to resume a paused deployment rollout',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'minReadySeconds',\n",
       "    'description': 'property of Kubernetes deployments for blocking rollouts of bad versions',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'readiness probe',\n",
       "    'description': 'mechanism for determining if a pod is ready to receive traffic',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'maxUnavailable',\n",
       "    'description': 'property of Kubernetes deployments for controlling the number of unavailable pods during rollout',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Kubernetes resource representing a containerized application instance',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'lightweight and portable executable binary',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"used to pause a deployment and prevent updates from kicking off the rollout process\",\\n    \"destination_entity\": \"deployment\"\\n  },\\n  {\\n    \"source_entity\": \"pause\",\\n    \"description\": \"prevents a deployment from updating and starting the rollout process\",\\n    \"destination_entity\": \"deployment\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"used to resume a paused deployment and start the rollout process\",\\n    \"destination_entity\": \"deployment\"\\n  },\\n  {\\n    \"source_entity\": \"resume\",\\n    \"description\": \"resumes a paused deployment and starts the rollout process\",\\n    \"destination_entity\": \"deployment\"\\n  },\\n  {\\n    \"source_entity\": \"readiness probe\",\\n    \"description\": \"checks if a new pod is functioning properly before continuing with the rollout process\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"minReadySeconds\",\\n    \"description\": \"specifies how long a newly created pod should be ready before the pod is treated as available and the rollout process continues\",\\n    \"destination_entity\": \"deployment\"\\n  },\\n  {\\n    \"source_entity\": \"maxUnavailable\",\\n    \"description\": \"specifies the maximum number of unavailable pods during a rollout process\",\\n    \"destination_entity\": \"deployment\"\\n  },\\n  {\\n    \"source_entity\": \"container\",\\n    \"description\": \"is checked by readiness probes to determine if a new pod is functioning properly before continuing with the rollout process\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"rollout\",\\n    \"description\": \"refers to the process of updating a deployment and replacing old pods with new ones\",\\n    \"destination_entity\": \"deployment\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl rollout resume\",\\n    \"description\": \"used to resume a paused deployment and start the rollout process\",\\n    \"destination_entity\": \"deployment\"\\n  }\\n]\\n```'},\n",
       " {'page': 307,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '275\\nUsing Deployments for updating apps declaratively\\n Although you should obviously test your pods both in a test and in a staging envi-\\nronment before deploying them into production, using minReadySeconds is like an\\nairbag that saves your app from making a big mess after you’ve already let a buggy ver-\\nsion slip into production. \\n With a properly configured readiness probe and a proper minReadySeconds set-\\nting, Kubernetes would have prevented us from deploying the buggy v3 version ear-\\nlier. Let me show you how.\\nDEFINING A READINESS PROBE TO PREVENT OUR V3 VERSION FROM BEING ROLLED OUT FULLY\\nYou’re going to deploy version v3 again, but this time, you’ll have the proper readi-\\nness probe defined on the pod. Your Deployment is currently at version v4, so before\\nyou start, roll back to version v2 again so you can pretend this is the first time you’re\\nupgrading to v3. If you wish, you can go straight from v4 to v3, but the text that fol-\\nlows assumes you returned to v2 first.\\n Unlike before, where you only updated the image in the pod template, you’re now\\nalso going to introduce a readiness probe for the container at the same time. Up until\\nnow, because there was no explicit readiness probe defined, the container and the\\npod were always considered ready, even if the app wasn’t truly ready or was returning\\nerrors. There was no way for Kubernetes to know that the app was malfunctioning and\\nshouldn’t be exposed to clients. \\n To change the image and introduce the readiness probe at once, you’ll use the\\nkubectl apply command. You’ll use the following YAML to update the deployment\\n(you’ll store it as kubia-deployment-v3-with-readinesscheck.yaml), as shown in\\nthe following listing.\\napiVersion: apps/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: kubia\\nspec:\\n  replicas: 3\\n  minReadySeconds: 10           \\n  strategy:\\n    rollingUpdate:\\n      maxSurge: 1                  \\n      maxUnavailable: 0         \\n    type: RollingUpdate\\n  template:\\n    metadata:\\n      name: kubia\\n      labels:\\n        app: kubia\\n    spec:\\n      containers:\\n      - image: luksa/kubia:v3\\nListing 9.11\\nDeployment with a readiness probe: kubia-deployment-v3-with-\\nreadinesscheck.yaml\\nYou’re keeping \\nminReadySeconds \\nset to 10.\\nYou’re keeping maxUnavailable \\nset to 0 to make the deployment \\nreplace pods one by one\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'minReadySeconds',\n",
       "    'description': 'The minimum number of seconds a pod must be ready before it is considered available.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'readiness probe',\n",
       "    'description': 'A Kubernetes feature that checks if a container is ready to receive traffic.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A Kubernetes object that defines the desired state of an application.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'kubectl apply',\n",
       "    'description': 'A command used to update or create a Kubernetes resource.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'The version of the Kubernetes API being used.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'The type of Kubernetes object being defined.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Information about the deployment, such as its name and labels.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'The specification for the deployment, including its replicas and strategy.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'replicas',\n",
       "    'description': 'The number of replicas of the pod to create.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'strategy',\n",
       "    'description': 'The strategy used to update the deployment, such as a rolling update.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'rollingUpdate',\n",
       "    'description': 'A type of deployment strategy that updates pods one by one.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'maxSurge',\n",
       "    'description': 'The maximum number of additional replicas to create during an update.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'maxUnavailable',\n",
       "    'description': 'The maximum number of unavailable replicas during an update.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'type',\n",
       "    'description': 'The type of deployment strategy being used.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'template',\n",
       "    'description': 'Information about the pod template, such as its metadata and labels.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Information about the pod template, such as its name and labels.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'Key-value pairs that provide additional information about the pod.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A process running in a pod.',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'The Docker image being used to create the container.',\n",
       "    'category': 'docker'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A logical host running one or more containers.',\n",
       "    'category': 'kubernetes'}],\n",
       "  'relationships': '[{\"source_entity\":\"Deployment\",\"description\":\"defines a readiness probe to prevent buggy version from being rolled out fully\",\"destination_entity\":\"v3 version\"},{\"source_entity\":\"kubectl apply\",\"description\":\"updates deployment with YAML containing readiness probe and new image\",\"destination_entity\":\"kubia-deployment-v3-with-readinesscheck.yaml\"},{\"source_entity\":\"minReadySeconds\",\"description\":\"sets minimum time a pod must be ready before it can be considered ready\",\"destination_entity\":\"pod\"},{\"source_entity\":\"maxSurge\",\"description\":\"sets maximum number of pods that can be created during rollout\",\"destination_entity\":\"Deployment\"},{\"source_entity\":\"spec\",\"description\":\"defines container with new image and readiness probe\",\"destination_entity\":\"container\"},{\"source_entity\":\"template\",\"description\":\"defines metadata for pod template\",\"destination_entity\":\"pod\"},{\"source_entity\":\"labels\",\"description\":\"assigns label to pod template\",\"destination_entity\":\"pod\"},{\"source_entity\":\"minReadySeconds\",\"description\":\"sets minimum time a pod must be ready before it can be considered ready\",\"destination_entity\":\"pod\"},{\"source_entity\":\"kubectl apply\",\"description\":\"applies YAML configuration to update deployment\",\"destination_entity\":\"Deployment\"},{\"source_entity\":\"maxUnavailable\",\"description\":\"sets maximum number of pods that can be unavailable during rollout\",\"destination_entity\":\"Deployment\"},{\"source_entity\":\"apiVersion\",\"description\":\"defines API version for Deployment resource\",\"destination_entity\":\"Deployment\"},{\"source_entity\":\"kind\",\"description\":\"defines kind of resource being created (Deployment)\",\"destination_entity\":\"Deployment\"},{\"source_entity\":\"strategy\",\"description\":\"defines strategy for updating deployment\",\"destination_entity\":\"Deployment\"},{\"source_entity\":\"readiness probe\",\"description\":\"prevents buggy version from being rolled out fully by checking readiness of pods\",\"destination_entity\":\"pod\"}]'},\n",
       " {'page': 308,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '276\\nCHAPTER 9\\nDeployments: updating applications declaratively\\n        name: nodejs\\n        readinessProbe:\\n          periodSeconds: 1       \\n          httpGet:                  \\n            path: /                 \\n            port: 8080              \\nUPDATING A DEPLOYMENT WITH KUBECTL APPLY\\nTo update the Deployment this time, you’ll use kubectl apply like this:\\n$ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml \\ndeployment \"kubia\" configured\\nThe apply command updates the Deployment with everything that’s defined in the\\nYAML file. It not only updates the image but also adds the readiness probe definition\\nand anything else you’ve added or modified in the YAML. If the new YAML also con-\\ntains the replicas field, which doesn’t match the number of replicas on the existing\\nDeployment, the apply operation will also scale the Deployment, which isn’t usually\\nwhat you want. \\nTIP\\nTo keep the desired replica count unchanged when updating a Deploy-\\nment with kubectl apply, don’t include the replicas field in the YAML. \\nRunning the apply command will kick off the update process, which you can again\\nfollow with the rollout status command:\\n$ kubectl rollout status deployment kubia\\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated...\\nBecause the status says one new pod has been created, your service should be hitting it\\noccasionally, right? Let’s see:\\n$ while true; do curl http://130.211.109.222; done\\nThis is v2 running in pod kubia-1765119474-jvslk\\nThis is v2 running in pod kubia-1765119474-jvslk\\nThis is v2 running in pod kubia-1765119474-xk5g3\\nThis is v2 running in pod kubia-1765119474-pmb26\\nThis is v2 running in pod kubia-1765119474-pmb26\\nThis is v2 running in pod kubia-1765119474-xk5g3\\n...\\nNope, you never hit the v3 pod. Why not? Is it even there? List the pods:\\n$ kubectl get po\\nNAME                     READY     STATUS    RESTARTS   AGE\\nkubia-1163142519-7ws0i   0/1       Running   0          30s\\nkubia-1765119474-jvslk   1/1       Running   0          9m\\nkubia-1765119474-pmb26   1/1       Running   0          9m\\nkubia-1765119474-xk5g3   1/1       Running   0          8m\\nYou’re defining a readiness probe \\nthat will be executed every second.\\nThe readiness probe will \\nperform an HTTP GET request \\nagainst our container.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'Containerization platform',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes clusters',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'apply',\n",
       "    'description': 'kubectl command to apply configuration changes to a resource',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'Kubernetes object that represents an application or service',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'readinessProbe',\n",
       "    'description': 'Mechanism for determining whether a pod is ready to serve traffic',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'httpGet',\n",
       "    'description': 'HTTP request method used by the readiness probe',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'periodSeconds',\n",
       "    'description': 'Time interval at which the readiness probe is executed',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'port',\n",
       "    'description': 'TCP port number used by the container for communication',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'replicas',\n",
       "    'description': 'Number of replicas or copies of a pod that should be running at any given time',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'rollout status',\n",
       "    'description': 'kubectl command to display the current rollout status of a deployment',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'Command-line tool for making HTTP requests',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'while true; do ... done',\n",
       "    'description': 'Bash loop construct used to repeatedly execute a command',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[{\"source_entity\": \"kubectl\", \"description\": \"update Deployment with YAML file\", \"destination_entity\": \"deployment\"}, \\n{\"source_entity\": \"kubectl apply\", \"description\": \"apply updates to Deployment, including readiness probe definition\", \"destination_entity\": \"Deployment\"}, \\n{\"source_entity\": \"readinessProbe\", \"description\": \"perform HTTP GET request against container every second\", \"destination_entity\": \"container\"}, \\n{\"source_entity\": \"httpGet\", \"description\": \"send HTTP GET request against pod\", \"destination_entity\": \"pod\"}, \\n{\"source_entity\": \"periodSeconds\", \"description\": \"execute readiness probe every second\", \"destination_entity\": \"readinessProbe\"}, \\n{\"source_entity\": \"port\", \"description\": \"perform HTTP GET request against container on port 8080\", \"destination_entity\": \"container\"}, \\n{\"source_entity\": \"while true; do ... done\", \"description\": \"continuously send curl requests to pod\", \"destination_entity\": \"pod\"}, \\n{\"source_entity\": \"apply\", \"description\": \"update Deployment with YAML file, including readiness probe definition\", \"destination_entity\": \"Deployment\"}, \\n{\"source_entity\": \"rollout status\", \"description\": \"check status of rollout for Deployment\", \"destination_entity\": \"Deployment\"}, \\n{\"source_entity\": \"curl\", \"description\": \"send HTTP GET request to pod\", \"destination_entity\": \"pod\"}, \\n{\"source_entity\": \"Kubernetes\", \"description\": \"manage and deploy applications with Deployments\", \"destination_entity\": \"applications\"}, \\n{\"source_entity\": \"Deployment\", \"description\": \"update Deployment with YAML file, including readiness probe definition\", \"destination_entity\": \"kubectl apply\"}, \\n{\"source_entity\": \"replicas\", \"description\": \"scale Deployment to desired number of replicas\", \"destination_entity\": \"Deployment\"}]'},\n",
       " {'page': 309,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '277\\nUsing Deployments for updating apps declaratively\\nAha! There’s your problem (or as you’ll learn soon, your blessing)! The pod is shown\\nas not ready, but I guess you’ve been expecting that, right? What has happened?\\nUNDERSTANDING HOW A READINESS PROBE PREVENTS BAD VERSIONS FROM BEING ROLLED OUT\\nAs soon as your new pod starts, the readiness probe starts being hit every second (you\\nset the probe’s interval to one second in the pod spec). On the fifth request the readi-\\nness probe began failing, because your app starts returning HTTP status code 500\\nfrom the fifth request onward. \\n As a result, the pod is removed as an endpoint from the service (see figure 9.14).\\nBy the time you start hitting the service in the curl loop, the pod has already been\\nmarked as not ready. This explains why you never hit the new pod with curl. And\\nthat’s exactly what you want, because you don’t want clients to hit a pod that’s not\\nfunctioning properly.\\nBut what about the rollout process? The rollout status command shows only one\\nnew replica has started. Thankfully, the rollout process will not continue, because the\\nnew pod will never become available. To be considered available, it needs to be ready\\nfor at least 10 seconds. Until it’s available, the rollout process will not create any new\\npods, and it also won’t remove any original pods because you’ve set the maxUnavailable\\nproperty to 0. \\nService\\ncurl\\nPod: v2\\nPod: v2\\nPod: v3\\n(unhealthy)\\nPod: v2\\nReplicaSet: v2\\nReplicas: 3\\nDeployment\\nReplicas: 3\\nrollingUpdate:\\nmaxSurge: 1\\nmaxUnavailable: 0\\nReplicaSet: v3\\nReplicas: 1\\nRequests are not forwarded\\nto v3 pod because of failed\\nreadiness probe\\nFigure 9.14\\nDeployment blocked by a failing readiness probe in the new pod\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A container that runs an application or service.',\n",
       "    'category': 'Container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'An abstraction which defines how to access applications, without caring about their location.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'A command-line tool for transferring data with URLs.',\n",
       "    'category': 'Command'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A way to manage and update applications in a declarative manner.',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'Ensures that a specified number of replicas (identical Pods) are running at any given time.',\n",
       "    'category': 'Container'},\n",
       "   {'entity': 'readiness probe',\n",
       "    'description': 'A liveness check for Pods to ensure they are ready to handle requests.',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'maxSurge',\n",
       "    'description': 'The maximum number of new replicas that can be created during a rolling update.',\n",
       "    'category': 'Property'},\n",
       "   {'entity': 'maxUnavailable',\n",
       "    'description': 'The maximum number of unavailable replicas that can exist at any given time during a rolling update.',\n",
       "    'category': 'Property'},\n",
       "   {'entity': 'rollout status',\n",
       "    'description': 'A command to check the status of an application rollout.',\n",
       "    'category': 'Command'},\n",
       "   {'entity': 'readiness probe interval',\n",
       "    'description': 'The frequency at which the readiness probe is executed.',\n",
       "    'category': 'Property'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"readiness probe\", \"description\": \"prevents bad versions from being rolled out by failing on unhealthy pods\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"readiness probe interval\", \"description\": \"is set to one second in the pod spec\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"will not continue rollout process due to failed readiness probe on new pod\", \"destination_entity\": \"rollout status\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"won\\'t remove original pods because maxUnavailable property is set to 0\", \"destination_entity\": \"maxUnavailable\"},\\n  {\"source_entity\": \"curl\", \"description\": \"requests are not forwarded to v3 pod due to failed readiness probe\", \"destination_entity\": \"Pod: v3\"},\\n  {\"source_entity\": \"readiness probe\", \"description\": \"removes unhealthy pod as an endpoint from the service\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"shows only one new replica has started due to failed readiness probe\", \"destination_entity\": \"ReplicaSet: v2\"},\\n  {\"source_entity\": \"maxSurge\", \"description\": \"is set to 1 in rollingUpdate property of Deployment\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"maxUnavailable\", \"description\": \"is set to 0 in rollingUpdate property of Deployment\", \"destination_entity\": \"Deployment\"}\\n]\\n```'},\n",
       " {'page': 310,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '278\\nCHAPTER 9\\nDeployments: updating applications declaratively\\n The fact that the deployment is stuck is a good thing, because if it had continued\\nreplacing the old pods with the new ones, you’d end up with a completely non-working\\nservice, like you did when you first rolled out version 3, when you weren’t using the\\nreadiness probe. But now, with the readiness probe in place, there was virtually no\\nnegative impact on your users. A few users may have experienced the internal server\\nerror, but that’s not as big of a problem as if the rollout had replaced all pods with the\\nfaulty version 3.\\nTIP\\nIf you only define the readiness probe without setting minReadySeconds\\nproperly, new pods are considered available immediately when the first invo-\\ncation of the readiness probe succeeds. If the readiness probe starts failing\\nshortly after, the bad version is rolled out across all pods. Therefore, you\\nshould set minReadySeconds appropriately.\\nCONFIGURING A DEADLINE FOR THE ROLLOUT\\nBy default, after the rollout can’t make any progress in 10 minutes, it’s considered as\\nfailed. If you use the kubectl describe deployment command, you’ll see it display a\\nProgressDeadlineExceeded condition, as shown in the following listing.\\n$ kubectl describe deploy kubia\\nName:                   kubia\\n...\\nConditions:\\n  Type          Status  Reason\\n  ----          ------  ------\\n  Available     True    MinimumReplicasAvailable\\n  Progressing   False   ProgressDeadlineExceeded   \\nThe time after which the Deployment is considered failed is configurable through the\\nprogressDeadlineSeconds property in the Deployment spec.\\nNOTE\\nThe extensions/v1beta1 version of Deployments doesn’t set a deadline.\\nABORTING A BAD ROLLOUT\\nBecause the rollout will never continue, the only thing to do now is abort the rollout\\nby undoing it:\\n$ kubectl rollout undo deployment kubia\\ndeployment \"kubia\" rolled back\\nNOTE\\nIn future versions, the rollout will be aborted automatically when the\\ntime specified in progressDeadlineSeconds is exceeded.\\nListing 9.12\\nSeeing the conditions of a Deployment with kubectl describe\\nThe Deployment \\ntook too long to \\nmake progress.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Deployment',\n",
       "    'description': 'A Kubernetes resource that manages rollout updates for applications.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n    {\"source_entity\": \"rollout\", \"description\": \"replacing old pods with new ones\", \"destination_entity\": \"pods\"},\\n    {\"source_entity\": \"readiness probe\", \"description\": \"checking if new pod is ready\", \"destination_entity\": \"pod\"},\\n    {\"source_entity\": \"Deployment\", \"description\": \"configuring a deadline for the rollout\", \"destination_entity\": \"rollout\"},\\n    {\"source_entity\": \"kubectl describe deployment\", \"description\": \"displaying conditions of a Deployment\", \"destination_entity\": \"deployment\"},\\n    {\"source_entity\": \"user\", \"description\": \"experiencing internal server error\", \"destination_entity\": \"service\"},\\n    {\"source_entity\": \"Deployment spec\", \"description\": \"configuring progress deadline\", \"destination_entity\": \"rollout\"}\\n]\\n```\\n\\nNote that the entities \\'Deployments\\' and \\'kubectl describe deployment\\' were considered as single entity '},\n",
       " {'page': 311,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '279\\nSummary\\n9.4\\nSummary\\nThis chapter has shown you how to make your life easier by using a declarative\\napproach to deploying and updating applications in Kubernetes. Now that you’ve\\nread this chapter, you should know how to\\n\\uf0a1Perform a rolling update of pods managed by a ReplicationController\\n\\uf0a1Create Deployments instead of lower-level ReplicationControllers or ReplicaSets\\n\\uf0a1Update your pods by editing the pod template in the Deployment specification\\n\\uf0a1Roll back a Deployment either to the previous revision or to any earlier revision\\nstill listed in the revision history\\n\\uf0a1Abort a Deployment mid-way\\n\\uf0a1Pause a Deployment to inspect how a single instance of the new version behaves\\nin production before allowing additional pod instances to replace the old ones\\n\\uf0a1Control the rate of the rolling update through maxSurge and maxUnavailable\\nproperties\\n\\uf0a1Use minReadySeconds and readiness probes to have the rollout of a faulty ver-\\nsion blocked automatically\\nIn addition to these Deployment-specific tasks, you also learned how to\\n\\uf0a1Use three dashes as a separator to define multiple resources in a single YAML file\\n\\uf0a1Turn on kubectl’s verbose logging to see exactly what it’s doing behind the\\ncurtains\\nYou now know how to deploy and manage sets of pods created from the same pod\\ntemplate and thus share the same persistent storage. You even know how to update\\nthem declaratively. But what about running sets of pods, where each instance needs to\\nuse its own persistent storage? We haven’t looked at that yet. That’s the subject of our\\nnext chapter.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A Kubernetes resource used to manage a set of pods.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'Deployments',\n",
       "    'description': 'A higher-level abstraction for managing sets of pods in Kubernetes.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'ReplicaSets',\n",
       "    'description': 'A resource used to manage a set of identical pods in Kubernetes.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'The basic execution unit in Kubernetes, comprising a set of containers.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line interface for interacting with a Kubernetes cluster.',\n",
       "    'category': 'software/command'},\n",
       "   {'entity': 'YAML',\n",
       "    'description': 'A human-readable serialization format used to define Kubernetes resources.',\n",
       "    'category': 'software/format'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"Perform a rolling update of pods managed by a ReplicationController\",\\n    \"destination_entity\": \"ReplicationController\"\\n  },\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"Create Deployments instead of lower-level ReplicationControllers or ReplicaSets\",\\n    \"destination_entity\": \"Deployments\"\\n  },\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"Update your pods by editing the pod template in the Deployment specification\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"Roll back a Deployment either to the previous revision or to any earlier revision still listed in the revision history\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"Abort a Deployment mid-way\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"Pause a Deployment to inspect how a single instance of the new version behaves in production before allowing additional pod instances to replace the old ones\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"Control the rate of the rolling update through maxSurge and maxUnavailable properties\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"Use minReadySeconds and readiness probes to have the rollout of a faulty version blocked automatically\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"Use three dashes as a separator to define multiple resources in a single YAML file\",\\n    \"destination_entity\": \"YAML\"\\n  },\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"Turn on kubectl\\'s verbose logging to see exactly what it\\'s doing behind the curtains\",\\n    \"destination_entity\": \"kubectl\"\\n  },\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"Deploy and manage sets of pods created from the same pod template and thus share the same persistent storage\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"Update them declaratively\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"Deployments\",\\n    \"description\": \"Manage sets of pods created from the same pod template and thus share the same persistent storage\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSets\",\\n    \"description\": \"Lower-level to Deployments\",\\n    \"destination_entity\": \"Deployments\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"Perform operations such as rolling update, deployment, and management of pods and deployments\",\\n    \"destination_entity\": \"pods\"\\n  }\\n]\\n```\\n\\nNote: The relations are based on the context of the document page and may not be exhaustive.'},\n",
       " {'page': 312,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '280\\nStatefulSets:\\ndeploying replicated\\nstateful applications\\nYou now know how to run both single-instance and replicated stateless pods,\\nand even stateful pods utilizing persistent storage. You can run several repli-\\ncated web-server pod instances and you can run a single database pod instance\\nthat uses persistent storage, provided either through plain pod volumes or through\\nPersistentVolumes bound by a PersistentVolumeClaim. But can you employ a\\nReplicaSet to replicate the database pod?\\nThis chapter covers\\n\\uf0a1Deploying stateful clustered applications\\n\\uf0a1Providing separate storage for each instance of \\na replicated pod\\n\\uf0a1Guaranteeing a stable name and hostname for \\npod replicas\\n\\uf0a1Starting and stopping pod replicas in a \\npredictable order\\n\\uf0a1Discovering peers through DNS SRV records\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSets',\n",
       "    'description': 'deploying replicated stateful applications',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'single-instance and replicated stateless pods, and even stateful pods utilizing persistent storage',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'replicate the database pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolumes',\n",
       "    'description': 'providing separate storage for each instance of a replicated pod',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'requesting persistent storage for a pod',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'DNS SRV records',\n",
       "    'description': 'discovering peers through DNS SRV records',\n",
       "    'category': 'network'}],\n",
       "  'relationships': '[{\"source_entity\": \"StatefulSets\", \"description\": \"deploying replicated applications\", \"destination_entity\": \"applications\"},\\n {\"source_entity\": \"ReplicaSet\", \"description\": \"replicating database pod\", \"destination_entity\": \"database pod\"},\\n {\"source_entity\": \"PersistentVolumeClaim\", \"description\": \"providing separate storage for each instance\", \"destination_entity\": \"replicated pods\"},\\n {\"source_entity\": \"StatefulSets\", \"description\": \"guaranteeing stable name and hostname\", \"destination_entity\": \"pod replicas\"},\\n {\"source_entity\": \"StatefulSets\", \"description\": \"starting and stopping pod replicas in predictable order\", \"destination_entity\": \"pods\"},\\n {\"source_entity\": \"DNS SRV records\", \"description\": \"discovering peers\", \"destination_entity\": \"peers\"},\\n {\"source_entity\": \"ReplicaSet\", \"description\": \"replicating database pod using PersistentVolumes\", \"destination_entity\": \"PersistentVolumes\"}]'},\n",
       " {'page': 313,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '281\\nReplicating stateful pods\\n10.1\\nReplicating stateful pods\\nReplicaSets create multiple pod replicas from a single pod template. These replicas\\ndon’t differ from each other, apart from their name and IP address. If the pod tem-\\nplate includes a volume, which refers to a specific PersistentVolumeClaim, all replicas\\nof the ReplicaSet will use the exact same PersistentVolumeClaim and therefore the\\nsame PersistentVolume bound by the claim (shown in figure 10.1).\\nBecause the reference to the claim is in the pod template, which is used to stamp out\\nmultiple pod replicas, you can’t make each replica use its own separate Persistent-\\nVolumeClaim. You can’t use a ReplicaSet to run a distributed data store, where each\\ninstance needs its own separate storage—at least not by using a single ReplicaSet. To\\nbe honest, none of the API objects you’ve seen so far make running such a data store\\npossible. You need something else. \\n10.1.1 Running multiple replicas with separate storage for each\\nHow does one run multiple replicas of a pod and have each pod use its own storage\\nvolume? ReplicaSets create exact copies (replicas) of a pod; therefore you can’t use\\nthem for these types of pods. What can you use?\\nCREATING PODS MANUALLY\\nYou could create pods manually and have each of them use its own PersistentVolume-\\nClaim, but because no ReplicaSet looks after them, you’d need to manage them man-\\nually and recreate them when they disappear (as in the event of a node failure).\\nTherefore, this isn’t a viable option.\\nUSING ONE REPLICASET PER POD INSTANCE\\nInstead of creating pods directly, you could create multiple ReplicaSets—one for each\\npod with each ReplicaSet’s desired replica count set to one, and each ReplicaSet’s pod\\ntemplate referencing a dedicated PersistentVolumeClaim (as shown in figure 10.2).\\n Although this takes care of the automatic rescheduling in case of node failures or\\naccidental pod deletions, it’s much more cumbersome compared to having a single\\nReplicaSet. For example, think about how you’d scale the pods in that case. You\\nPersistent\\nVolume\\nClaim\\nPersistent\\nVolume\\nReplicaSet\\nPod\\nPod\\nPod\\nFigure 10.1\\nAll pods from the same ReplicaSet always use the same \\nPersistentVolumeClaim and PersistentVolume.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicaSets',\n",
       "    'description': 'API object that creates multiple pod replicas from a single pod template',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'container that runs an application',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'request for storage resources',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'storage resource',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'API object that creates multiple pod replicas from a single pod template',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod template',\n",
       "    'description': 'template used to stamp out multiple pod replicas',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'request for storage resources',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'storage resource',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'API object that creates multiple pod replicas from a single pod template',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'container that runs an application',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Node failure',\n",
       "    'description': 'event where a node in the cluster fails',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Accidental pod deletions',\n",
       "    'description': 'event where a pod is deleted accidentally',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'request for storage resources',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'storage resource',\n",
       "    'category': 'database'}],\n",
       "  'relationships': '[{\"source_entity\":\"ReplicaSets\",\"description\":\"create multiple pod replicas from a single pod template\",\"destination_entity\":\"pod template\"},{\"source_entity\":\"ReplicaSets\",\"description\":\"don’t differ from each other, apart from their name and IP address\",\"destination_entity\":\"replicas\"},{\"source_entity\":\"PersistentVolumeClaim\",\"description\":\"refer to a specific Persistent Volume\",\"destination_entity\":\"persistent volume\"},{\"source_entity\":\"Pod template\",\"description\":\"includes a volume which refers to a specific PersistentVolumeClaim\",\"destination_entity\":\"PersistentVolumeClaim\"},{\"source_entity\":\"ReplicaSets\",\"description\":\"use the exact same PersistentVolumeClaim and therefore the same PersistentVolume\",\"destination_entity\":\"PersistentVolume\"},{\"source_entity\":\"You\",\"description\":\"can’t make each replica use its own separate Persistent-VolumeClaim\",\"destination_entity\":\"persistent volume claim\"},{\"source_entity\":\"ReplicaSets\",\"description\":\"can\\'t run a distributed data store, where each instance needs its own separate storage\",\"destination_entity\":\"distributed data store\"},{\"source_entity\":\"API objects\",\"description\":\"make running such a data store possible\",\"destination_entity\":\"data store\"},{\"source_entity\":\"You\",\"description\":\"need something else to run a distributed data store\",\"destination_entity\":\"solution\"},{\"source_entity\":\"ReplicaSets\",\"description\":\"create exact copies of a pod; therefore you can’t use them for these types of pods\",\"destination_entity\":\"pods with separate storage\"},{\"source_entity\":\"You\",\"description\":\"could create pods manually and have each of them use its own PersistentVolume-Claim\",\"destination_entity\":\"persistent volume claim\"},{\"source_entity\":\"Manual creation\",\"description\":\"requires managing and recreating pods when they disappear\",\"destination_entity\":\"node failure\"},{\"source_entity\":\"ReplicaSets\",\"description\":\"takes care of automatic rescheduling in case of node failures or accidental pod deletions\",\"destination_entity\":\"pod deletion\"},{\"source_entity\":\"You\",\"description\":\"have to manage multiple ReplicaSets, which is cumbersome\",\"destination_entity\":\"replica sets\"},{\"source_entity\":\"ReplicaSet\",\"description\":\"has a desired replica count set to one\",\"destination_entity\":\"pod instance\"},{\"source_entity\":\"Pod template\",\"description\":\"references a dedicated PersistentVolumeClaim\",\"destination_entity\":\"persistent volume claim\"}]\\n\\nNote that I\\'ve tried to extract the most relevant relations from the document, but there might be some variations in interpretation depending on how you read the text.'},\n",
       " {'page': 314,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '282\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\ncouldn’t change the desired replica count—you’d have to create additional Replica-\\nSets instead. \\n Using multiple ReplicaSets is therefore not the best solution. But could you maybe\\nuse a single ReplicaSet and have each pod instance keep its own persistent state, even\\nthough they’re all using the same storage volume? \\nUSING MULTIPLE DIRECTORIES IN THE SAME VOLUME\\nA trick you can use is to have all pods use the same PersistentVolume, but then have a\\nseparate file directory inside that volume for each pod (this is shown in figure 10.3).\\nBecause you can’t configure pod replicas differently from a single pod template, you\\ncan’t tell each instance what directory it should use, but you can make each instance\\nautomatically select (and possibly also create) a data directory that isn’t being used\\nby any other instance at that time. This solution does require coordination between\\nthe instances, and isn’t easy to do correctly. It also makes the shared storage volume\\nthe bottleneck.\\n10.1.2 Providing a stable identity for each pod\\nIn addition to storage, certain clustered applications also require that each instance\\nhas a long-lived stable identity. Pods can be killed from time to time and replaced with\\nPVC A1\\nPV A1\\nReplicaSet A1\\nPod A1-xyz\\nPVC A2\\nPV A2\\nReplicaSet A2\\nPod A2-xzy\\nPVC A3\\nPV A3\\nReplicaSet A3\\nPod A3-zyx\\nFigure 10.2\\nUsing one ReplicaSet for each pod instance\\nPersistent\\nVolume\\nClaim\\nPersistentVolume\\nReplicaSet\\nPod\\nPod\\nPod\\nApp\\nApp\\nApp\\n/data/1/\\n/data/3/\\n/data/2/\\nFigure 10.3\\nWorking around the shared storage problem by having the app \\nin each pod use a different file directory \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSets',\n",
       "    'description': 'deploying replicated stateful applications',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSets',\n",
       "    'description': 'multiple instances of a pod template',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'shared storage volume for multiple pods',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'an instance of a containerized application',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'PVC (PersistentVolumeClaim)',\n",
       "    'description': 'request for storage resources from a pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PV (PersistentVolume)',\n",
       "    'description': 'storage resource allocated to a pod',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'ReplicaSet A1, ReplicaSet A2, ReplicaSet A3',\n",
       "    'description': 'multiple instances of a pod template with different storage directories',\n",
       "    'category': 'software'},\n",
       "   {'entity': '/data/1/, /data/2/, /data/3/',\n",
       "    'description': 'separate file directories for each pod instance',\n",
       "    'category': 'filesystem'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ReplicaSet A1\",\\n    \"description\": \"uses shared storage volume\",\\n    \"destination_entity\": \"PV (PersistentVolume)\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"can be killed and replaced by another instance\",\\n    \"destination_entity\": \"PVC (PersistentVolumeClaim)\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSet A1\",\\n    \"description\": \"creates a separate file directory for each pod instance\",\\n    \"destination_entity\": \"/data/1/\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"automatically selects and creates a data directory not used by another instance\",\\n    \"destination_entity\": \"/data/1/\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSet A2\",\\n    \"description\": \"creates a separate file directory for each pod instance\",\\n    \"destination_entity\": \"/data/3/\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"automatically selects and creates a data directory not used by another instance\",\\n    \"destination_entity\": \"/data/2/\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSet A1\",\\n    \"description\": \"has limited replicas due to shared storage volume constraint\",\\n    \"destination_entity\": \"PV (PersistentVolume)\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSets\",\\n    \"description\": \"can be deployed using StatefulSets\",\\n    \"destination_entity\": \"ReplicaSet A1\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSets\",\\n    \"description\": \"can be deployed using ReplicaSets\",\\n    \"destination_entity\": \"ReplicaSet A1\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSet A1\",\\n    \"description\": \"has multiple directories within the same PersistentVolume\",\\n    \"destination_entity\": \"/data/1/, /data/2/, /data/3/\"\\n  }\\n]\\n\\nNote: I\\'ve tried to accurately represent the relations between entities based on the document page, but if there are any discrepancies or errors, please let me know!'},\n",
       " {'page': 315,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '283\\nReplicating stateful pods\\nnew ones. When a ReplicaSet replaces a pod, the new pod is a completely new pod\\nwith a new hostname and IP, although the data in its storage volume may be that of\\nthe killed pod. For certain apps, starting up with the old instance’s data but with a\\ncompletely new network identity may cause problems.\\n Why do certain apps mandate a stable network identity? This requirement is\\nfairly common in distributed stateful applications. Certain apps require the adminis-\\ntrator to list all the other cluster members and their IP addresses (or hostnames) in\\neach member’s configuration file. But in Kubernetes, every time a pod is resched-\\nuled, the new pod gets both a new hostname and a new IP address, so the whole\\napplication cluster would have to be reconfigured every time one of its members is\\nrescheduled. \\nUSING A DEDICATED SERVICE FOR EACH POD INSTANCE\\nA trick you can use to work around this problem is to provide a stable network address\\nfor cluster members by creating a dedicated Kubernetes Service for each individual\\nmember. Because service IPs are stable, you can then point to each member through\\nits service IP (rather than the pod IP) in the configuration. \\n This is similar to creating a ReplicaSet for each member to provide them with indi-\\nvidual storage, as described previously. Combining these two techniques results in the\\nsetup shown in figure 10.4 (an additional service covering all the cluster members is\\nalso shown, because you usually need one for clients of the cluster).\\nThe solution is not only ugly, but it still doesn’t solve everything. The individual pods\\ncan’t know which Service they are exposed through (and thus can’t know their stable\\nIP), so they can’t self-register in other pods using that IP. \\nPVC A1\\nPV A1\\nReplicaSet A1\\nPod A1-xzy\\nService A1\\nService A\\nPVC A2\\nPV A2\\nReplicaSet A2\\nPod A2-xzy\\nService A2\\nPVC A3\\nPV A3\\nReplicaSet A3\\nPod A3-zyx\\nService A3\\nFigure 10.4\\nUsing one \\nService and ReplicaSet per \\npod to provide a stable \\nnetwork address and an \\nindividual volume for each \\npod, respectively\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Service A\\nService A1\\nReplicaSet A1 Pod A1-xzy PVC A1 PV A1\\nService A2\\nReplicaSet A2 Pod A2-xzy PVC A2 PV A2\\nService A3\\nReplicaSet A3 Pod A3-zyx PVC A3 PV A3  \\\n",
       "   0                                      ReplicaSet A3                                                                                                                   \n",
       "   \n",
       "       Col1  \n",
       "   0  PV A3  ],\n",
       "  'entities': [{'entity': 'stateful pods',\n",
       "    'description': 'pods with persistent data',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'a Kubernetes resource that ensures a specified number of replicas of a pod are running at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'the basic execution unit in the Docker environment, a lightweight and standalone entity built into a single filesystem image containing the application code, dependencies, and runtime.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'hostname',\n",
       "    'description': 'a unique name assigned to a device on a network.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'IP address',\n",
       "    'description': 'a numeric label assigned to a device on a network.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'storage volume',\n",
       "    'description': 'a logical unit of storage that provides persistent data to containers.',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'dedicated Service',\n",
       "    'description': 'a Kubernetes resource that provides a stable network address for pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'service IP',\n",
       "    'description': 'the IP address assigned to a Service, which remains constant even when the underlying pod changes.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'PVC (Persistent Volume Claim)',\n",
       "    'description': 'a Kubernetes resource that requests access to a Persistent Volume for a specified amount of storage.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PV (Persistent Volume)',\n",
       "    'description': 'a Kubernetes resource that represents a block of storage that can be used by multiple pods, such as a disk or network share.',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'the basic execution unit in the Docker environment, a lightweight and standalone entity built into a single filesystem image containing the application code, dependencies, and runtime.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'a Kubernetes resource that provides a network interface for accessing pods within a cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSet A1',\n",
       "    'description': 'a specific instance of a ReplicaSet, which ensures a specified number of replicas of a pod are running at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod A1-xzy',\n",
       "    'description': 'a specific instance of a Pod, with a unique name and hostname.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service A1',\n",
       "    'description': 'a specific instance of a Service, which provides a stable network address for the Pod.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PVC A2',\n",
       "    'description': 'a specific instance of a PVC, which requests access to a Persistent Volume for a specified amount of storage.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PV A2',\n",
       "    'description': 'a specific instance of a PV, which represents a block of storage that can be used by multiple pods.',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'ReplicaSet A2',\n",
       "    'description': 'a specific instance of a ReplicaSet, which ensures a specified number of replicas of a pod are running at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod A2-xzy',\n",
       "    'description': 'a specific instance of a Pod, with a unique name and hostname.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service A2',\n",
       "    'description': 'a specific instance of a Service, which provides a stable network address for the Pod.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PVC A3',\n",
       "    'description': 'a specific instance of a PVC, which requests access to a Persistent Volume for a specified amount of storage.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PV A3',\n",
       "    'description': 'a specific instance of a PV, which represents a block of storage that can be used by multiple pods.',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'ReplicaSet A3',\n",
       "    'description': 'a specific instance of a ReplicaSet, which ensures a specified number of replicas of a pod are running at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod A3-zyx',\n",
       "    'description': 'a specific instance of a Pod, with a unique name and hostname.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service A3',\n",
       "    'description': 'a specific instance of a Service, which provides a stable network address for the Pod.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ReplicaSet\", \"description\": \"replaces a pod with a new one\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"administer\", \"description\": \"list all cluster members and their IP addresses in each member\\'s configuration file\", \"destination_entity\": \"cluster members\"},\\n  {\"source_entity\": \"kubernetes\", \"description\": \"reschedules pods, resulting in new hostname and IP address for the pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"service IPs are stable\", \"description\": \"provide a stable network address for cluster members\", \"destination_entity\": \"cluster members\"},\\n  {\"source_entity\": \"dedicated service\", \"description\": \"provide a stable network address for each member by creating a dedicated Kubernetes Service\", \"destination_entity\": \"member\"},\\n  {\"source_entity\": \"service IPs are stable\", \"description\": \"point to each member through its service IP in the configuration\", \"destination_entity\": \"configuration file\"},\\n  {\"source_entity\": \"individual pods\", \"description\": \"can\\'t know which Service they are exposed through and thus can\\'t self-register using that IP\", \"destination_entity\": \"service\"},\\n  {\"source_entity\": \"ReplicaSet A1\", \"description\": \"replaces a pod with a new one having a new hostname and IP address but same storage volume data\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"storage volume\", \"description\": \"may be used by the killed pod after being replaced by ReplicaSet\", \"destination_entity\": \"killed pod\"},\\n  {\"source_entity\": \"administer\", \"description\": \"list all cluster members and their hostnames in each member\\'s configuration file\", \"destination_entity\": \"cluster members\"},\\n  {\"source_entity\": \"new hostname and IP address\", \"description\": \"assigned to the new pod after rescheduling by Kubernetes\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"dedicated Service\", \"description\": \"used to provide a stable network address for cluster members\", \"destination_entity\": \"cluster members\"}\\n]\\n```'},\n",
       " {'page': 316,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '284\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\n Luckily, Kubernetes saves us from resorting to such complex solutions. The proper\\nclean and simple way of running these special types of applications in Kubernetes is\\nthrough a StatefulSet. \\n10.2\\nUnderstanding StatefulSets\\nInstead of using a ReplicaSet to run these types of pods, you create a StatefulSet\\nresource, which is specifically tailored to applications where instances of the applica-\\ntion must be treated as non-fungible individuals, with each one having a stable name\\nand state. \\n10.2.1 Comparing StatefulSets with ReplicaSets\\nTo understand the purpose of StatefulSets, it’s best to compare them to ReplicaSets or\\nReplicationControllers. But first let me explain them with a little analogy that’s widely\\nused in the field.\\nUNDERSTANDING STATEFUL PODS WITH THE PETS VS. CATTLE ANALOGY\\nYou may have already heard of the pets vs. cattle analogy. If not, let me explain it. We\\ncan treat our apps either as pets or as cattle. \\nNOTE\\nStatefulSets were initially called PetSets. That name comes from the\\npets vs. cattle analogy explained here.\\nWe tend to treat our app instances as pets, where we give each instance a name and\\ntake care of each instance individually. But it’s usually better to treat instances as cattle\\nand not pay special attention to each individual instance. This makes it easy to replace\\nunhealthy instances without giving it a second thought, similar to the way a farmer\\nreplaces unhealthy cattle. \\n Instances of a stateless app, for example, behave much like heads of cattle. It\\ndoesn’t matter if an instance dies—you can create a new instance and people won’t\\nnotice the difference. \\n On the other hand, with stateful apps, an app instance is more like a pet. When a\\npet dies, you can’t go buy a new one and expect people not to notice. To replace a lost\\npet, you need to find a new one that looks and behaves exactly like the old one. In the\\ncase of apps, this means the new instance needs to have the same state and identity as\\nthe old one.\\nCOMPARING STATEFULSETS WITH REPLICASETS OR REPLICATIONCONTROLLERS\\nPod replicas managed by a ReplicaSet or ReplicationController are much like cattle.\\nBecause they’re mostly stateless, they can be replaced with a completely new pod\\nreplica at any time. Stateful pods require a different approach. When a stateful pod\\ninstance dies (or the node it’s running on fails), the pod instance needs to be resur-\\nrected on another node, but the new instance needs to get the same name, network\\nidentity, and state as the one it’s replacing. This is what happens when the pods are\\nmanaged through a StatefulSet. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'StatefulSets',\n",
       "    'description': 'Resource for deploying replicated stateful applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'Controller that manages a set of identical pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Lightweight and portable containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Stateful pod',\n",
       "    'description': 'Pod with persistent identity and state',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Pets vs. cattle analogy',\n",
       "    'description': 'Comparison of treating app instances as pets or cattle',\n",
       "    'category': 'concept'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'Controller that manages a set of identical pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'StatefulSets controller',\n",
       "    'description': 'Controller for managing stateful pods',\n",
       "    'category': 'controller'},\n",
       "   {'entity': 'Network identity',\n",
       "    'description': 'Unique network identifier for each pod',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'State',\n",
       "    'description': 'Persistent data stored by a pod',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Identity',\n",
       "    'description': 'Unique identifier for each pod',\n",
       "    'category': 'identifier'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Pets vs. cattle analogy\",\\n    \"description\": \"Used to compare instances of stateless apps with heads of cattle\",\\n    \"destination_entity\": \"Stateless app\"\\n  },\\n  {\\n    \"source_entity\": \"Pets vs. cattle analogy\",\\n    \"description\": \"Instances of stateless apps behave like heads of cattle\",\\n    \"destination_entity\": \"Heads of cattle\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSets controller\",\\n    \"description\": \"Manages pods to ensure the new instance gets the same name, network identity, and state as the one it\\'s replacing\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSet\",\\n    \"description\": \"Manages pod replicas that can be replaced with a completely new pod replica at any time\",\\n    \"destination_entity\": \"Pod replicas\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSets controller\",\\n    \"description\": \"Resurrects a stateful pod instance on another node, ensuring the new instance gets the same name, network identity, and state as the old one\",\\n    \"destination_entity\": \"Stateful pod\"\\n  },\\n  {\\n    \"source_entity\": \"Pets vs. cattle analogy\",\\n    \"description\": \"Instances of stateful apps are more like pets that need to be treated individually\",\\n    \"destination_entity\": \"Stateful app\"\\n  },\\n  {\\n    \"source_entity\": \"Identity\",\\n    \"description\": \"Each instance of a stateful app has a stable name and identity\",\\n    \"destination_entity\": \"Stateful app\"\\n  },\\n  {\\n    \"source_entity\": \"Network identity\",\\n    \"description\": \"The new instance needs to get the same network identity as the old one when resurrected by the StatefulSets controller\",\\n    \"destination_entity\": \"Stateful pod\"\\n  }\\n]\\n```'},\n",
       " {'page': 317,\n",
       "  'img_cnt': 7,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '285\\nUnderstanding StatefulSets\\n A StatefulSet makes sure pods are rescheduled in such a way that they retain their\\nidentity and state. It also allows you to easily scale the number of pets up and down. A\\nStatefulSet, like a ReplicaSet, has a desired replica count field that determines how\\nmany pets you want running at that time. Similar to ReplicaSets, pods are created from\\na pod template specified as part of the StatefulSet (remember the cookie-cutter anal-\\nogy?). But unlike pods created by ReplicaSets, pods created by the StatefulSet aren’t\\nexact replicas of each other. Each can have its own set of volumes—in other words,\\nstorage (and thus persistent state)—which differentiates it from its peers. Pet pods\\nalso have a predictable (and stable) identity instead of each new pod instance getting\\na completely random one. \\n10.2.2 Providing a stable network identity\\nEach pod created by a StatefulSet is assigned an ordinal index (zero-based), which\\nis then used to derive the pod’s name and hostname, and to attach stable storage to\\nthe pod. The names of the pods are thus predictable, because each pod’s name is\\nderived from the StatefulSet’s name and the ordinal index of the instance. Rather\\nthan the pods having random names, they’re nicely organized, as shown in the next\\nfigure.\\nINTRODUCING THE GOVERNING SERVICE\\nBut it’s not all about the pods having a predictable name and hostname. Unlike regu-\\nlar pods, stateful pods sometimes need to be addressable by their hostname, whereas\\nstateless pods usually don’t. After all, each stateless pod is like any other. When you\\nneed one, you pick any one of them. But with stateful pods, you usually want to oper-\\nate on a specific pod from the group, because they differ from each other (they hold\\ndifferent state, for example). \\n For this reason, a StatefulSet requires you to create a corresponding governing\\nheadless Service that’s used to provide the actual network identity to each pod.\\nThrough this Service, each pod gets its own DNS entry, so its peers and possibly other\\nclients in the cluster can address the pod by its hostname. For example, if the govern-\\ning Service belongs to the default namespace and is called foo, and one of the pods\\nReplicaSet A\\nPod A-fewrb\\nPod A-jwqec\\nPod A-dsfwx\\nStatefulSet A\\nPod A-1\\nPod A-2\\nPod A-0\\nFigure 10.5\\nPods created by a StatefulSet have predictable names (and hostnames), \\nunlike those created by a ReplicaSet\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Pod A-dsfwx\n",
       "   ReplicaSet A Pod A-fewrb\n",
       "   Pod A-jwqec, Pod A-0\n",
       "   StatefulSet A Pod A-1\n",
       "   Pod A-2]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'StatefulSets',\n",
       "    'description': 'Ensures pods are rescheduled to retain their identity and state.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'Pods created by a StatefulSet have predictable names and hostnames, and can have different storage and state.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ordinal index',\n",
       "    'description': \"A zero-based index used to derive the pod's name and hostname.\",\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pod template',\n",
       "    'description': 'The specification for creating new pods in a StatefulSet.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicaSets',\n",
       "    'description': 'A type of application that creates identical copies of pods, unlike StatefulSets which create distinct pods with different state.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'volumes',\n",
       "    'description': 'Storage and persistent state used by each pod in a StatefulSet.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'predictable identity',\n",
       "    'description': 'Each pod created by a StatefulSet has a stable and predictable name and hostname, unlike pods created by ReplicaSets.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'governing Service',\n",
       "    'description': 'A headless service that provides network identity to each pod in a StatefulSet, allowing them to be addressable by their hostname.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'headless Service',\n",
       "    'description': 'A type of service that does not provide a public IP address and is used to provide network identity to pods in a StatefulSet.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'DNS entry',\n",
       "    'description': 'Each pod created by a StatefulSet gets its own DNS entry through the governing Service, allowing it to be addressed by its hostname.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A network abstraction that provides a stable and predictable identity for pods in a StatefulSet.',\n",
       "    'category': 'network'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"assigns ordinal index to each pod\", \"destination_entity\": \"ordinal index\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"derives pod\\'s name and hostname from ordinal index\", \"destination_entity\": \"pod\\'s name\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"attaches stable storage to pod using ordinal index\", \"destination_entity\": \"stable storage\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"creates pods with predictable names and hostnames\", \"destination_entity\": \"predictable identity\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"requires a corresponding governing Service to provide network identity\", \"destination_entity\": \"governing Service\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"allows easy scaling of number of pets up and down\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"ReplicaSets\", \"description\": \"creates pods with random names and hostnames\", \"destination_entity\": \"predictable identity\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"differentiates each pod from its peers using volumes\", \"destination_entity\": \"volumes\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"provides a stable network identity to each pod through governing Service\", \"destination_entity\": \"DNS entry\"},\\n  {\"source_entity\": \"headless Service\", \"description\": \"provides actual network identity to each pod\", \"destination_entity\": \"pods\"}\\n]\\n```'},\n",
       " {'page': 318,\n",
       "  'img_cnt': 5,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '286\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\nis called A-0, you can reach the pod through its fully qualified domain name, which\\nis a-0.foo.default.svc.cluster.local. You can’t do that with pods managed by a\\nReplicaSet.\\n Additionally, you can also use DNS to look up all the StatefulSet’s pods’ names by\\nlooking up SRV records for the foo.default.svc.cluster.local domain. We’ll\\nexplain SRV records in section 10.4 and learn how they’re used to discover members\\nof a StatefulSet.\\nREPLACING LOST PETS\\nWhen a pod instance managed by a StatefulSet disappears (because the node the pod\\nwas running on has failed, it was evicted from the node, or someone deleted the pod\\nobject manually), the StatefulSet makes sure it’s replaced with a new instance—similar\\nto how ReplicaSets do it. But in contrast to ReplicaSets, the replacement pod gets the\\nsame name and hostname as the pod that has disappeared (this distinction between\\nReplicaSets and StatefulSets is illustrated in figure 10.6).\\nNode 1\\nNode 2\\nNode 1\\nNode 2\\nReplicaSet B\\nReplicaSet B\\nStatefulSet\\nStatefulSet A\\nPod A-0\\nPod A-1\\nPod A-0\\nPod A-0\\nPod A-1\\nNode 1 fails\\nStatefulSet A\\nNode 1\\nNode 2\\nNode 1\\nNode 2\\nReplicaSet\\nNode 1 fails\\nPod B-fdawr\\nPod B-jkbde\\nPod B-fdawr\\nPod B-rsqkw\\nPod B-jkbde\\nFigure 10.6\\nA StatefulSet replaces a lost pod with a new one with the same identity, whereas a \\nReplicaSet replaces it with a completely new unrelated pod.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSets',\n",
       "    'description': 'A Kubernetes object that manages stateful applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A Kubernetes object that manages replicated deployments',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A lightweight and portable piece of software in a container environment',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'DNS',\n",
       "    'description': 'A system for translating human-readable hostnames to IP addresses',\n",
       "    'category': 'networking'},\n",
       "   {'entity': 'SRV records',\n",
       "    'description': 'A type of DNS record that maps a domain name to a service or application',\n",
       "    'category': 'database'},\n",
       "   {'entity': \"StatefulSet's pods\",\n",
       "    'description': 'The instances of a StatefulSet in the pod environment',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubernetes object',\n",
       "    'description': 'A deployable unit of configuration for Kubernetes applications',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\": \"Kubernetes Object\", \"description\": \"is managed by\", \"destination_entity\": \"ReplicaSet\"},\\n\\n{\"source_entity\": \"StatefulSet\", \"description\": \"replaces lost pod with a new one having same identity\", \"destination_entity\": \"Pod\"},\\n\\n{\"source_entity\": \"StatefulSet\", \"description\": \"makes sure replacement pod gets same name and hostname as disappeared pod\", \"destination_entity\": \"Pod\"},\\n\\n{\"source_entity\": \"ReplicaSet\", \"description\": \"replaces lost pod with a completely new unrelated pod\", \"destination_entity\": \"Pod\"},\\n\\n{\"source_entity\": \"DNS\", \"description\": \"is used to look up SRV records for StatefulSet\\'s pods\", \"destination_entity\": \"StatefulSet\\'s Pods\"},\\n\\n{\"source_entity\": \"Kubernetes Object\", \"description\": \"has failed, evicted or deleted manually\", \"destination_entity\": \"Pod\"},\\n\\n{\"source_entity\": \"StatefulSet\", \"description\": \"makes sure replacement pod gets same name and hostname as disappeared pod\", \"destination_entity\": \"Pod B\"},\\n\\n{\"source_entity\": \"ReplicaSet\", \"description\": \"replaces lost pod with a completely new unrelated pod\", \"destination_entity\": \"Pod B\"},\\n\\n{\"source_entity\": \"StatefulSet A\", \"description\": \"replaces lost pod with a new one having same identity\", \"destination_entity\": \"Pod A-0\"},\\n\\n{\"source_entity\": \"SRV records\", \"description\": \"are used to discover members of StatefulSet\", \"destination_entity\": \"StatefulSet\"}]'},\n",
       " {'page': 319,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '287\\nUnderstanding StatefulSets\\nThe new pod isn’t necessarily scheduled to the same node, but as you learned early\\non, what node a pod runs on shouldn’t matter. This holds true even for stateful pods.\\nEven if the pod is scheduled to a different node, it will still be available and reachable\\nunder the same hostname as before. \\nSCALING A STATEFULSET\\nScaling the StatefulSet creates a new pod instance with the next unused ordinal index.\\nIf you scale up from two to three instances, the new instance will get index 2 (the exist-\\ning instances obviously have indexes 0 and 1). \\n The nice thing about scaling down a StatefulSet is the fact that you always know\\nwhat pod will be removed. Again, this is also in contrast to scaling down a ReplicaSet,\\nwhere you have no idea what instance will be deleted, and you can’t even specify\\nwhich one you want removed first (but this feature may be introduced in the future).\\nScaling down a StatefulSet always removes the instances with the highest ordinal index\\nfirst (shown in figure 10.7). This makes the effects of a scale-down predictable.\\nBecause certain stateful applications don’t handle rapid scale-downs nicely, Stateful-\\nSets scale down only one pod instance at a time. A distributed data store, for example,\\nmay lose data if multiple nodes go down at the same time. For example, if a replicated\\ndata store is configured to store two copies of each data entry, in cases where two\\nnodes go down at the same time, a data entry would be lost if it was stored on exactly\\nthose two nodes. If the scale-down was sequential, the distributed data store has time\\nto create an additional replica of the data entry somewhere else to replace the (single)\\nlost copy.\\n For this exact reason, StatefulSets also never permit scale-down operations if any of\\nthe instances are unhealthy. If an instance is unhealthy, and you scale down by one at\\nthe same time, you’ve effectively lost two cluster members at once.\\n10.2.3 Providing stable dedicated storage to each stateful instance\\nYou’ve seen how StatefulSets ensure stateful pods have a stable identity, but what\\nabout storage? Each stateful pod instance needs to use its own storage, plus if a state-\\nful pod is rescheduled (replaced with a new instance but with the same identity as\\nbefore), the new instance must have the same storage attached to it. How do Stateful-\\nSets achieve this?\\nPod\\nA-0\\nPod\\nA-1\\nPod\\nA-2\\nStatefulSet A\\nReplicas: 3\\nPod\\nA-0\\nPod\\nA-1\\nPod\\nA-2\\nStatefulSet A\\nReplicas: 2\\nPod\\nA-0\\nPod\\nA-1\\nStatefulSet A\\nReplicas: 1\\nScale down\\nScale down\\nFigure 10.7\\nScaling down a StatefulSet always removes the pod with the highest ordinal index first.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSets',\n",
       "    'description': 'a Kubernetes concept for managing stateful applications',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'a single instance of a running container in Kubernetes',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'a Kubernetes object that manages replicas of a pod',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'Ordinal Index',\n",
       "    'description': 'a unique identifier for each pod instance in a StatefulSet',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'Scaling',\n",
       "    'description': 'the process of increasing or decreasing the number of pod instances in a StatefulSet',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'Stateful application',\n",
       "    'description': 'an application that maintains its own state and requires persistent storage',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Scaling\", \"description\": \"Creates a new pod instance with the next unused ordinal index.\", \"destination_entity\": \"Ordinal Index\"},\\n  \\n  {\"source_entity\": \"StatefulSets\", \"description\": \"Scale down only one pod instance at a time to prevent data loss in distributed data stores.\", \"destination_entity\": \"Pod\"},\\n  \\n  {\"source_entity\": \"Scaling\", \"description\": \"Always removes the instances with the highest ordinal index first.\", \"destination_entity\": \"Ordinal Index\"},\\n  \\n  {\"source_entity\": \"StatefulSets\", \"description\": \"Ensures stateful pods have a stable identity and use their own storage.\", \"destination_entity\": \"Pod\"},\\n  \\n  {\"source_entity\": \"Scaling\", \"description\": \"Does not permit scale-down operations if any of the instances are unhealthy.\", \"destination_entity\": \"Pod\"},\\n  \\n  {\"source_entity\": \"StatefulSets\", \"description\": \"Permits sequential scale-down to prevent data loss in distributed data stores.\", \"destination_entity\": \"ReplicaSet\"},\\n  \\n  {\"source_entity\": \"Stateful application\", \"description\": \"Does not handle rapid scale-downs nicely and may lose data if multiple nodes go down at the same time.\", \"destination_entity\": \"Pod\"},\\n  \\n  {\"source_entity\": \"StatefulSets\", \"description\": \"Provides stable dedicated storage to each stateful instance, which is essential for rescheduling pods without losing data.\", \"destination_entity\": \"Storage\"}\\n]\\n```'},\n",
       " {'page': 320,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '288\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\n Obviously, storage for stateful pods needs to be persistent and decoupled from\\nthe pods. In chapter 6 you learned about PersistentVolumes and PersistentVolume-\\nClaims, which allow persistent storage to be attached to a pod by referencing the\\nPersistentVolumeClaim in the pod by name. Because PersistentVolumeClaims map\\nto PersistentVolumes one-to-one, each pod of a StatefulSet needs to reference a dif-\\nferent PersistentVolumeClaim to have its own separate PersistentVolume. Because\\nall pod instances are stamped from the same pod template, how can they each refer\\nto a different PersistentVolumeClaim? And who creates these claims? Surely you’re\\nnot expected to create as many PersistentVolumeClaims as the number of pods you\\nplan to have in the StatefulSet upfront? Of course not.\\nTEAMING UP POD TEMPLATES WITH VOLUME CLAIM TEMPLATES\\nThe StatefulSet has to create the PersistentVolumeClaims as well, the same way it’s cre-\\nating the pods. For this reason, a StatefulSet can also have one or more volume claim\\ntemplates, which enable it to stamp out PersistentVolumeClaims along with each pod\\ninstance (see figure 10.8).\\nThe PersistentVolumes for the claims can either be provisioned up-front by an admin-\\nistrator or just in time through dynamic provisioning of PersistentVolumes, as explained\\nat the end of chapter 6. \\nUNDERSTANDING THE CREATION AND DELETION OF PERSISTENTVOLUMECLAIMS\\nScaling up a StatefulSet by one creates two or more API objects (the pod and one or\\nmore PersistentVolumeClaims referenced by the pod). Scaling down, however, deletes\\nonly the pod, leaving the claims alone. The reason for this is obvious, if you consider\\nwhat happens when a claim is deleted. After a claim is deleted, the PersistentVolume it\\nwas bound to gets recycled or deleted and its contents are lost. \\n Because stateful pods are meant to run stateful applications, which implies that the\\ndata they store in the volume is important, deleting the claim on scale-down of a Stateful-\\nSet could be catastrophic—especially since triggering a scale-down is as simple as\\ndecreasing the replicas field of the StatefulSet. For this reason, you’re required to\\ndelete PersistentVolumeClaims manually to release the underlying PersistentVolume.\\nPVC A-0\\nPV\\nPod A-0\\nPVC A-1\\nPV\\nPod A-1\\nPVC A-2\\nPV\\nPod A-2\\nStatefulSet A\\nPod\\ntemplate\\nVolume claim\\ntemplate\\nFigure 10.8\\nA StatefulSet creates both pods and PersistentVolumeClaims.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PersistentVolumes',\n",
       "    'description': 'persistent storage to be attached to a pod',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'PersistentVolumeClaims',\n",
       "    'description': 'allow persistent storage to be attached to a pod by referencing the PersistentVolume in the pod by name',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'StatefulSets',\n",
       "    'description': 'deploying replicated stateful applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a single instance of an application',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'reference a different PersistentVolume for each pod instance',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'volume claim templates',\n",
       "    'description': 'enable StatefulSet to stamp out PersistentVolumeClaims along with each pod instance',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Pod template',\n",
       "    'description': 'template used by StatefulSet to create pods',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'PersistentVolumeClaim template',\n",
       "    'description': 'template used by StatefulSet to create PersistentVolumeClaims',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Scaling',\n",
       "    'description': 'increase or decrease the number of replicas in a StatefulSet',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'API objects',\n",
       "    'description': 'pod and one or more PersistentVolumeClaims referenced by the pod',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\":\"StatefulSet\",\"description\":\"creates\",\"destination_entity\":\"PersistentVolumeClaim\"},{\"source_entity\":\"StatefulSet\",\"description\":\"stamps out\",\"destination_entity\":\"PersistentVolumeClaim template\"},{\"source_entity\":\"StatefulSet\",\"description\":\"references\",\"destination_entity\":\"Pod template\"},{\"source_entity\":\"PersistentVolumeClaim\",\"description\":\"maps to\",\"destination_entity\":\"PersistentVolumes\"},{\"source_entity\":\"Scaling\",\"description\":\"creates\",\"destination_entity\":\"API objects\"},{\"source_entity\":\"Scaling\",\"description\":\"deletes\",\"destination_entity\":\"PersistentVolumeClaim\"},{\"source_entity\":\"StatefulSet\",\"description\":\"references\",\"destination_entity\":\"volume claim templates\"},{\"source_entity\":\"StatefulSet\",\"description\":\"provisions\",\"destination_entity\":\"PersistentVolumes\"},{\"source_entity\":\"administraror\",\"description\":\"provisions\",\"destination_entity\":\"PersistentVolumes\"},{\"source_entity\":\"Scaling\",\"description\":\"leaves behind\",\"destination_entity\":\"PersistentVolumeClaims\"}]\\n\\nNote: I assumed \"administraror\" is an entity in the context of the document, as it seems to be referring to a person responsible for provisioning PersistentVolumes.'},\n",
       " {'page': 321,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '289\\nUnderstanding StatefulSets\\nREATTACHING THE PERSISTENTVOLUMECLAIM TO THE NEW INSTANCE OF THE SAME POD\\nThe fact that the PersistentVolumeClaim remains after a scale-down means a subse-\\nquent scale-up can reattach the same claim along with the bound PersistentVolume\\nand its contents to the new pod instance (shown in figure 10.9). If you accidentally\\nscale down a StatefulSet, you can undo the mistake by scaling up again and the new\\npod will get the same persisted state again (as well as the same name).\\n10.2.4 Understanding StatefulSet guarantees\\nAs you’ve seen so far, StatefulSets behave differently from ReplicaSets or Replication-\\nControllers. But this doesn’t end with the pods having a stable identity and storage.\\nStatefulSets also have different guarantees regarding their pods. \\nUNDERSTANDING THE IMPLICATIONS OF STABLE IDENTITY AND STORAGE\\nWhile regular, stateless pods are fungible, stateful pods aren’t. We’ve already seen how\\na stateful pod is always replaced with an identical pod (one having the same name and\\nhostname, using the same persistent storage, and so on). This happens when Kuber-\\nnetes sees that the old pod is no longer there (for example, when you delete the pod\\nmanually). \\n But what if Kubernetes can’t be sure about the state of the pod? If it creates a\\nreplacement pod with the same identity, two instances of the app with the same iden-\\ntity might be running in the system. The two would also be bound to the same storage,\\nPod\\nA-0\\nPod\\nA-1\\nStatefulSet A\\nReplicas: 2\\nScale\\ndown\\nScale\\nup\\nNew pod instance created\\nwith same identity as before\\nPVC is\\nre-attached\\nPVC\\nA-0\\nPV\\nPVC\\nA-1\\nPV\\nPod\\nA-0\\nStatefulSet A\\nReplicas: 1\\nPVC\\nA-0\\nPV\\nPVC\\nA-1\\nPV\\nPod\\nA-0\\nPod has been deleted\\nPod\\nA-1\\nStatefulSet A\\nReplicas: 2\\nPVC\\nA-0\\nPV\\nPVC\\nA-1\\nPVC has not\\nbeen deleted\\nPV\\nFigure 10.9\\nStatefulSets don’t delete PersistentVolumeClaims when scaling down; then they \\nreattach them when scaling back up.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [PVC has not PVC is\n",
       "   been deleted re-attached\n",
       "   PV PV PV PV PV PV\n",
       "   PVC PVC PVC PVC PVC PVC\n",
       "   A-0 A-1 A-0 A-1 A-0 A-1\n",
       "   Pod Pod Pod Pod Pod\n",
       "   A-0 A-1 A-0 A-0 A-1\n",
       "   Scale Scale\n",
       "   down up\n",
       "   StatefulSet A StatefulSet A StatefulSet A\n",
       "   Replicas: 2 Replicas: 1 Replicas: 2\n",
       "   Pod has been deleted New pod instance created\n",
       "   with same identity as before, Col1]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'A request for storage resources',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'A deployment strategy that preserves state',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A collection of containers running on a host',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A deployment strategy for creating multiple replicas of an application',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'A storage resource that can be attached to a pod',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Storage',\n",
       "    'description': 'Resources for storing data',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"re-attach PersistentVolumeClaim to new instance of same pod\", \"destination_entity\": \"PersistentVolumeClaim\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"scale up and re-attach same claim along with bound PersistentVolume and its contents to new pod instance\", \"destination_entity\": \"PersistentVolumeClaim\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"guarantee stable identity of StatefulSet pods\", \"destination_entity\": \"StatefulSet\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"guarantee storage for StatefulSet pods\", \"destination_entity\": \"Storage\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"behave differently from ReplicaSets or Replication-Controllers\", \"destination_entity\": \"ReplicaSet\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"always replace old pod with identical pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"ensure new pod has same identity as before when scaling up\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"PersistentVolumeClaim\", \"description\": \"remain after scale-down and can be re-attached to new pod instance\", \"destination_entity\": \"StatefulSet\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"not delete PersistentVolumeClaims when scaling down\", \"destination_entity\": \"PersistentVolumeClaim\"}\\n]\\n```'},\n",
       " {'page': 322,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '290\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\nso two processes with the same identity would be writing over the same files. With pods\\nmanaged by a ReplicaSet, this isn’t a problem, because the apps are obviously made to\\nwork on the same files. Also, ReplicaSets create pods with a randomly generated iden-\\ntity, so there’s no way for two processes to run with the same identity. \\nINTRODUCING STATEFULSET’S AT-MOST-ONE SEMANTICS\\nKubernetes must thus take great care to ensure two stateful pod instances are never\\nrunning with the same identity and are bound to the same PersistentVolumeClaim. A\\nStatefulSet must guarantee at-most-one semantics for stateful pod instances. \\n This means a StatefulSet must be absolutely certain that a pod is no longer run-\\nning before it can create a replacement pod. This has a big effect on how node fail-\\nures are handled. We’ll demonstrate this later in the chapter. Before we can do that,\\nhowever, you need to create a StatefulSet and see how it behaves. You’ll also learn a\\nfew more things about them along the way.\\n10.3\\nUsing a StatefulSet\\nTo properly show StatefulSets in action, you’ll build your own little clustered data\\nstore. Nothing fancy—more like a data store from the Stone Age. \\n10.3.1 Creating the app and container image\\nYou’ll use the kubia app you’ve used throughout the book as your starting point. You’ll\\nexpand it so it allows you to store and retrieve a single data entry on each pod instance. \\n The important parts of the source code of your data store are shown in the follow-\\ning listing.\\n...\\nconst dataFile = \"/var/data/kubia.txt\";\\n...\\nvar handler = function(request, response) {\\n  if (request.method == \\'POST\\') {                \\n    var file = fs.createWriteStream(dataFile);                     \\n    file.on(\\'open\\', function (fd) {                                \\n      request.pipe(file);                                          \\n      console.log(\"New data has been received and stored.\");       \\n      response.writeHead(200);                                     \\n      response.end(\"Data stored on pod \" + os.hostname() + \"\\\\n\");  \\n    });\\n  } else {                                       \\n    var data = fileExists(dataFile)                                \\n      ? fs.readFileSync(dataFile, \\'utf8\\')                          \\n      : \"No data posted yet\";                                      \\n    response.writeHead(200);                                       \\n    response.write(\"You\\'ve hit \" + os.hostname() + \"\\\\n\");          \\n    response.end(\"Data stored on this pod: \" + data + \"\\\\n\");       \\n  }\\n};\\nListing 10.1\\nA simple stateful app: kubia-pet-image/app.js\\nOn POST \\nrequests, store \\nthe request’s \\nbody into a \\ndata file.\\nOn GET (and all \\nother types of) \\nrequests, return \\nyour hostname \\nand the contents \\nof the data file.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSets',\n",
       "    'description': 'a Kubernetes feature for deploying replicated stateful applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'a Kubernetes controller that manages a set of replicas (pods) with the same identity',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'a request for storage resources in a Kubernetes cluster',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'a Kubernetes feature that guarantees at-most-one semantics for stateful pod instances',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a lightweight and portable executable container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'a machine in a Kubernetes cluster',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'fs',\n",
       "    'description': 'a Node.js file system module',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'createWriteStream',\n",
       "    'description': 'a function that creates a writable stream for writing to a file',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'fileExists',\n",
       "    'description': 'a function that checks if a file exists',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'readFileSync',\n",
       "    'description': 'a function that reads the contents of a file synchronously',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'hostname',\n",
       "    'description': 'an environment variable that returns the hostname of the machine',\n",
       "    'category': 'environmental variable'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"must guarantee at-most-one semantics for stateful pod instances\",\\n    \"destination_entity\": \"stateful pod instances\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"must be absolutely certain that a pod is no longer running before it can create a replacement pod\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSet\",\\n    \"description\": \"create pods with a randomly generated identity, so there\\'s no way for two processes to run with the same identity\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSets\",\\n    \"description\": \"must take great care to ensure two stateful pod instances are never running with the same identity and are bound to the same PersistentVolumeClaim\",\\n    \"destination_entity\": \"PersistentVolumeClaim\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"can create a replacement pod only when a pod is no longer running\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"node\",\\n    \"description\": \"hosts a StatefulSet and ensures that two stateful pod instances are never running with the same identity\",\\n    \"destination_entity\": \"StatefulSet\"\\n  },\\n  {\\n    \"source_entity\": \"createWriteStream\",\\n    \"description\": \"writes data to a file specified by the dataFile variable\",\\n    \"destination_entity\": \"dataFile\"\\n  },\\n  {\\n    \"source_entity\": \"readFileSync\",\\n    \"description\": \"reads data from a file specified by the dataFile variable\",\\n    \"destination_entity\": \"dataFile\"\\n  },\\n  {\\n    \"source_entity\": \"fileExists\",\\n    \"description\": \"checks if a file exists at the path specified by the dataFile variable\",\\n    \"destination_entity\": \"dataFile\"\\n  },\\n  {\\n    \"source_entity\": \"hostname\",\\n    \"description\": \"returns the hostname of the node hosting the pod instance\",\\n    \"destination_entity\": \"pod instance\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSets\",\\n    \"description\": \"provides a way to manage stateful applications in Kubernetes\",\\n    \"destination_entity\": \"Kubernetes\"\\n  }\\n]\\n```\\n\\nNote that I\\'ve only extracted relations between the entities provided as input, and ignored any other information in the document.'},\n",
       " {'page': 323,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '291\\nUsing a StatefulSet\\nvar www = http.createServer(handler);\\nwww.listen(8080);\\nWhenever the app receives a POST request, it writes the data it receives in the body of\\nthe request to the file /var/data/kubia.txt. Upon a GET request, it returns the host-\\nname and the stored data (contents of the file). Simple enough, right? This is the first\\nversion of your app. It’s not clustered yet, but it’s enough to get you started. You’ll\\nexpand the app later in the chapter.\\n The Dockerfile for building the container image is shown in the following listing\\nand hasn’t changed from before.\\nFROM node:7\\nADD app.js /app.js\\nENTRYPOINT [\"node\", \"app.js\"]\\nGo ahead and build the image now, or use the one I pushed to docker.io/luksa/kubia-pet.\\n10.3.2 Deploying the app through a StatefulSet\\nTo deploy your app, you’ll need to create two (or three) different types of objects:\\n\\uf0a1PersistentVolumes for storing your data files (you’ll need to create these only if\\nthe cluster doesn’t support dynamic provisioning of PersistentVolumes).\\n\\uf0a1A governing Service required by the StatefulSet.\\n\\uf0a1The StatefulSet itself.\\nFor each pod instance, the StatefulSet will create a PersistentVolumeClaim that will\\nbind to a PersistentVolume. If your cluster supports dynamic provisioning, you don’t\\nneed to create any PersistentVolumes manually (you can skip the next section). If it\\ndoesn’t, you’ll need to create them as explained in the next section. \\nCREATING THE PERSISTENT VOLUMES\\nYou’ll need three PersistentVolumes, because you’ll be scaling the StatefulSet up to\\nthree replicas. You must create more if you plan on scaling the StatefulSet up more\\nthan that.\\n If you’re using Minikube, deploy the PersistentVolumes defined in the Chapter06/\\npersistent-volumes-hostpath.yaml file in the book’s code archive. \\n If you’re using Google Kubernetes Engine, you’ll first need to create the actual\\nGCE Persistent Disks like this:\\n$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-a\\n$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-b\\n$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-c\\nNOTE\\nMake sure to create the disks in the same zone that your nodes are\\nrunning in.\\nListing 10.2\\nDockerfile for the stateful app: kubia-pet-image/Dockerfile\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSet',\n",
       "    'description': 'A Kubernetes API resource that manages the deployment, scaling, and management of a set of pods.',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'Dockerfile',\n",
       "    'description': 'A text file used to build Docker images.',\n",
       "    'category': 'Docker'},\n",
       "   {'entity': 'http.createServer()',\n",
       "    'description': 'A function in Node.js that creates an HTTP server.',\n",
       "    'category': 'Node.js'},\n",
       "   {'entity': '/var/data/kubia.txt',\n",
       "    'description': 'A file used to store data written by the application.',\n",
       "    'category': 'Filesystem'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'A Kubernetes API resource that requests access to a Persistent Volume.',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'GCE Persistent Disk',\n",
       "    'description': 'A Google Cloud Storage service that provides persistent storage for virtual machines.',\n",
       "    'category': 'Cloud Storage'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'A tool for running a Kubernetes cluster locally on a machine.',\n",
       "    'category': 'Kubernetes Distribution'},\n",
       "   {'entity': 'Google Kubernetes Engine',\n",
       "    'description': 'A managed container environment that runs on Google Cloud Platform.',\n",
       "    'category': 'Cloud Platform'},\n",
       "   {'entity': '$ gcloud compute disks create',\n",
       "    'description': 'A command used to create a persistent disk in Google Cloud Console.',\n",
       "    'category': 'Command'},\n",
       "   {'entity': 'Persistent Volume',\n",
       "    'description': 'A Kubernetes API resource that provides persistent storage for pods.',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'StatefulSet replica',\n",
       "    'description': 'A copy of the StatefulSet that runs on a separate pod.',\n",
       "    'category': 'Kubernetes'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"The user\", \"description\": \"runs a command to create a GCE Persistent Disk\", \"destination_entity\": \"$ gcloud compute disks create\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"creates a PersistentVolumeClaim for each pod instance\", \"destination_entity\": \"PersistentVolumeClaim\"},\\n  {\"source_entity\": \"The app\", \"description\": \"writes data from POST requests to /var/data/kubia.txt\", \"destination_entity\": \"/var/data/kubia.txt\"},\\n  {\"source_entity\": \"The user\", \"description\": \"deploys the PersistentVolumes through a StatefulSet\", \"destination_entity\": \"StatefulSet\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"binds each PersistentVolumeClaim to a PersistentVolume\", \"destination_entity\": \"PersistentVolume\"},\\n  {\"source_entity\": \"$ gcloud compute disks create\", \"description\": \"creates three GCE Persistent Disks for the StatefulSet\", \"destination_entity\": \"GCE Persistent Disk\"},\\n  {\"source_entity\": \"Google Kubernetes Engine\", \"description\": \"requires creation of actual GCE Persistent Disks\", \"destination_entity\": \"$ gcloud compute disks create\"},\\n  {\"source_entity\": \"http.createServer()\", \"description\": \"creates an HTTP server that listens on port 8080\", \"destination_entity\": \"port 8080\"},\\n  {\"source_entity\": \"The user\", \"description\": \"scales the StatefulSet up to three replicas\", \"destination_entity\": \"StatefulSet replica\"},\\n  {\"source_entity\": \"Dockerfile\", \"description\": \"specifies the image for the stateful app\", \"destination_entity\": \"kubia-pet-image\"},\\n  {\"source_entity\": \"$ gcloud compute disks create\", \"description\": \"creates PersistentVolumes in Minikube through a YAML file\", \"destination_entity\": \"persistent-volumes-hostpath.yaml\"},\\n  {\"source_entity\": \"The user\", \"description\": \"uses the Dockerfile to build the container image\", \"destination_entity\": \"Dockerfile\"},\\n  {\"source_entity\": \"$ gcloud compute disks create\", \"description\": \"creates PersistentVolumes in Google Kubernetes Engine\", \"destination_entity\": \"GCE Persistent Disk\"}\\n]\\n```'},\n",
       " {'page': 324,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '292\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\nThen create the PersistentVolumes from the persistent-volumes-gcepd.yaml file,\\nwhich is shown in the following listing.\\nkind: List                     \\napiVersion: v1\\nitems:\\n- apiVersion: v1\\n  kind: PersistentVolume       \\n  metadata:\\n    name: pv-a                \\n  spec:\\n    capacity:\\n      storage: 1Mi            \\n    accessModes:\\n      - ReadWriteOnce\\n    persistentVolumeReclaimPolicy: Recycle     \\n    gcePersistentDisk:         \\n      pdName: pv-a             \\n      fsType: nfs4                         \\n- apiVersion: v1\\n  kind: PersistentVolume\\n  metadata:\\n    name: pv-b\\n ...\\nNOTE\\nIn the previous chapter you specified multiple resources in the same\\nYAML by delimiting them with a three-dash line. Here you’re using a differ-\\nent approach by defining a List object and listing the resources as items of\\nthe object. Both methods are equivalent.\\nThis manifest creates PersistentVolumes called pv-a, pv-b, and pv-c. They use GCE Per-\\nsistent Disks as the underlying storage mechanism, so they’re not appropriate for clus-\\nters that aren’t running on Google Kubernetes Engine or Google Compute Engine. If\\nyou’re running the cluster elsewhere, you must modify the PersistentVolume definition\\nand use an appropriate volume type, such as NFS (Network File System), or similar.\\nCREATING THE GOVERNING SERVICE\\nAs explained earlier, before deploying a StatefulSet, you first need to create a headless\\nService, which will be used to provide the network identity for your stateful pods. The\\nfollowing listing shows the Service manifest.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: kubia       \\nspec:\\n  clusterIP: None    \\nListing 10.3\\nThree PersistentVolumes: persistent-volumes-gcepd.yaml\\nListing 10.4\\nHeadless service to be used in the StatefulSet: kubia-service-headless.yaml\\nFile describes a list \\nof three persistent \\nvolumes\\nPersistent volumes’ names \\nare pv-a, pv-b, and pv-c\\nCapacity of each persistent \\nvolume is 1 Mebibyte\\nWhen the volume \\nis released by the \\nclaim, it’s recycled \\nto be used again.\\nThe volume uses a GCE \\nPersistent Disk as the underlying \\nstorage mechanism.\\nName of the \\nService\\nThe StatefulSet’s governing \\nService must be headless.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'List',\n",
       "    'description': 'A Kubernetes object used to define a collection of resources.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'A Kubernetes resource representing a persistent storage device.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'PersistentVolumeReclaimPolicy',\n",
       "    'description': 'A policy for how Persistent Volumes are reclaimed when they are released by a claim.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'GCE Persistent Disk',\n",
       "    'description': 'A Google Cloud Platform storage solution used as the underlying storage mechanism for Persistent Volumes.',\n",
       "    'category': 'cloud service'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A Kubernetes object representing a network identity that can be used to provide access to resources within a cluster.',\n",
       "    'category': 'network,application'},\n",
       "   {'entity': 'Headless Service',\n",
       "    'description': 'A special type of Service that is used as the governing Service for a StatefulSet, providing network identity without exposing the underlying pods directly.',\n",
       "    'category': 'network,application'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'A Kubernetes workload object representing a set of stateful applications that are deployed and managed together.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'A Kubernetes resource representing a request for storage resources, used to bind to Persistent Volumes.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'pv-a',\n",
       "    'description': \"A specific Persistent Volume with a name of 'pv-a' and a capacity of 1 MiB.\",\n",
       "    'category': 'database'},\n",
       "   {'entity': 'pv-b',\n",
       "    'description': \"A specific Persistent Volume with a name of 'pv-b' and a capacity of 1 MiB.\",\n",
       "    'category': 'database'},\n",
       "   {'entity': 'pv-c',\n",
       "    'description': \"A specific Persistent Volume with a name of 'pv-c' and a capacity of 1 MiB.\",\n",
       "    'category': 'database'},\n",
       "   {'entity': 'kubia',\n",
       "    'description': 'The name of the Headless Service used as the governing Service for the StatefulSet.',\n",
       "    'category': 'network,application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"uses to provide network identity for stateful pods\",\\n    \"destination_entity\": \"Headless Service\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"requires PersistentVolumes before deployment\",\\n    \"destination_entity\": \"PersistentVolume\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"uses to provide storage mechanism for StatefulSet\",\\n    \"destination_entity\": \"GCE Persistent Disk\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"needs to be governed by a Service before deployment\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"PersistentVolume\",\\n    \"description\": \"uses GCE Persistent Disk as underlying storage mechanism\",\\n    \"destination_entity\": \"GCE Persistent Disk\"\\n  },\\n  {\\n    \"source_entity\": \"PersistentVolume\",\\n    \"description\": \"has capacity of 1 Mebibyte\",\\n    \"destination_entity\": null\\n  },\\n  {\\n    \"source_entity\": \"PersistentVolume\",\\n    \"description\": \"recycles when released by claim\",\\n    \"destination_entity\": \"PersistentVolumeReclaimPolicy\"\\n  },\\n  {\\n    \"source_entity\": \"Headless Service\",\\n    \"description\": \"must be used as governing Service for StatefulSet\",\\n    \"destination_entity\": \"StatefulSet\"\\n  },\\n  {\\n    \"source_entity\": \"List\",\\n    \"description\": \"defines a list of PersistentVolumes\",\\n    \"destination_entity\": \"PersistentVolume\"\\n  }\\n]\\n```\\n\\nNote: I\\'ve only extracted relations between entities mentioned in the document and provided entities. Let me know if you need any further clarification!'},\n",
       " {'page': 325,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '293\\nUsing a StatefulSet\\n  selector:           \\n    app: kubia        \\n  ports:\\n  - name: http\\n    port: 80\\nYou’re setting the clusterIP field to None, which makes this a headless Service. It will\\nenable peer discovery between your pods (you’ll need this later). Once you create the\\nService, you can move on to creating the actual StatefulSet.\\nCREATING THE STATEFULSET MANIFEST\\nNow you can finally create the StatefulSet. The following listing shows the manifest.\\napiVersion: apps/v1beta1\\nkind: StatefulSet\\nmetadata:\\n  name: kubia\\nspec:\\n  serviceName: kubia\\n  replicas: 2\\n  template:\\n    metadata:\\n      labels:                  \\n        app: kubia             \\n    spec:\\n      containers:\\n      - name: kubia\\n        image: luksa/kubia-pet\\n        ports:\\n        - name: http\\n          containerPort: 8080\\n        volumeMounts:\\n        - name: data                  \\n          mountPath: /var/data        \\n  volumeClaimTemplates:\\n  - metadata:                  \\n      name: data               \\n    spec:                      \\n      resources:               \\n        requests:              \\n          storage: 1Mi         \\n      accessModes:             \\n      - ReadWriteOnce          \\nThe StatefulSet manifest isn’t that different from ReplicaSet or Deployment manifests\\nyou’ve created so far. What’s new is the volumeClaimTemplates list. In it, you’re defin-\\ning one volume claim template called data, which will be used to create a Persistent-\\nVolumeClaim for each pod. As you may remember from chapter 6, a pod references a\\nclaim by including a persistentVolumeClaim volume in the manifest. In the previous\\nListing 10.5\\nStatefulSet manifest: kubia-statefulset.yaml\\nAll pods with the app=kubia \\nlabel belong to this service.\\nPods created by the StatefulSet \\nwill have the app=kubia label.\\nThe container inside the pod will \\nmount the pvc volume at this path.\\nThe PersistentVolumeClaims \\nwill be created from this \\ntemplate.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSet',\n",
       "    'description': 'A Kubernetes resource that manages stateful applications.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A Kubernetes resource that provides a network identity and load balancing for accessing a pod or a group of pods.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'StatefulSet manifest',\n",
       "    'description': 'The YAML file that defines the configuration of a StatefulSet.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'volumeClaimTemplates',\n",
       "    'description': 'A list in the StatefulSet manifest that defines a PersistentVolumeClaim for each pod.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Persistent-VolumeClaim',\n",
       "    'description': 'A Kubernetes resource that requests storage resources from the cluster.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'The basic execution unit in a Kubernetes cluster.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A Kubernetes resource that ensures a specified number of replicas of a pod are running at any given time.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A Kubernetes resource that manages the rollout and scaling of applications.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'clusterIP',\n",
       "    'description': 'The IP address assigned to a Service by the cluster.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'Key-value pairs used to identify and select pods.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'app=kubia',\n",
       "    'description': 'A label that identifies a pod as belonging to the kubia application.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'The data that describes an object in Kubernetes.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"creates a Persistent-VolumeClaim for each pod\", \"destination_entity\": \"Persistent-VolumeClaim\"},\\n  {\"source_entity\": \"Service\", \"description\": \"enables peer discovery between pods\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"StatefulSet manifest\", \"description\": \"defines one volume claim template called data\", \"destination_entity\": \"volumeClaimTemplates\"},\\n  {\"source_entity\": \"metadata\", \"description\": \"specifies the labels for a pod\", \"destination_entity\": \"labels\"},\\n  {\"source_entity\": \"app=kubia\", \"description\": \"identifies all pods created by a StatefulSet\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"StatefulSet manifest\", \"description\": \"references a claim by including a persistentVolumeClaim volume in the manifest\", \"destination_entity\": \"Persistent-VolumeClaim\"},\\n  {\"source_entity\": \"volumeClaimTemplates\", \"description\": \"defines a Persistent-VolumeClaim for each pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"specifies the serviceName and replicas in its spec\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"has a similar manifest to StatefulSet\", \"destination_entity\": \"StatefulSet\"},\\n  {\"source_entity\": \"ReplicaSet\", \"description\": \"is related to StatefulSet manifests\", \"destination_entity\": \"StatefulSet\"},\\n  {\"source_entity\": \"clusterIP\", \"description\": \"makes this Service headless, enabling peer discovery between pods\", \"destination_entity\": \"pod\"}\\n]\\n```'},\n",
       " {'page': 326,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '294\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\npod template, you’ll find no such volume. The StatefulSet adds it to the pod specifica-\\ntion automatically and configures the volume to be bound to the claim the StatefulSet\\ncreated for the specific pod.\\nCREATING THE STATEFULSET\\nYou’ll create the StatefulSet now:\\n$ kubectl create -f kubia-statefulset.yaml \\nstatefulset \"kubia\" created\\nNow, list your pods:\\n$ kubectl get po\\nNAME      READY     STATUS              RESTARTS   AGE\\nkubia-0   0/1       ContainerCreating   0          1s\\nNotice anything strange? Remember how a ReplicationController or a ReplicaSet cre-\\nates all the pod instances at the same time? Your StatefulSet is configured to create\\ntwo replicas, but it created a single pod. \\n Don’t worry, nothing is wrong. The second pod will be created only after the first\\none is up and ready. StatefulSets behave this way because certain clustered stateful\\napps are sensitive to race conditions if two or more cluster members come up at the\\nsame time, so it’s safer to bring each member up fully before continuing to bring up\\nthe rest.\\n List the pods again to see how the pod creation is progressing:\\n$ kubectl get po\\nNAME      READY     STATUS              RESTARTS   AGE\\nkubia-0   1/1       Running             0          8s\\nkubia-1   0/1       ContainerCreating   0          2s\\nSee, the first pod is now running, and the second one has been created and is being\\nstarted. \\nEXAMINING THE GENERATED STATEFUL POD\\nLet’s take a closer look at the first pod’s spec in the following listing to see how the\\nStatefulSet has constructed the pod from the pod template and the PersistentVolume-\\nClaim template.\\n$ kubectl get po kubia-0 -o yaml\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  ...\\nspec:\\n  containers:\\n  - image: luksa/kubia-pet\\n    ...\\nListing 10.6\\nA stateful pod created by the StatefulSet\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSets',\n",
       "    'description': 'deploying replicated stateful applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod template',\n",
       "    'description': 'a template for creating a pod',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'volume',\n",
       "    'description': 'a persistent storage unit',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'claim',\n",
       "    'description': 'a request for storage resources',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'a controller for creating multiple replicas of a pod',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'a set of identical pods created by a controller',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl create',\n",
       "    'description': 'command to create a resource (in this case, a StatefulSet)',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubia-statefulset.yaml',\n",
       "    'description': 'a YAML file containing the configuration for a StatefulSet',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'a Kubernetes object that manages stateful applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a single instance of a running process (in this case, created by a StatefulSet)',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'a request for persistent storage resources',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'statefulset',\n",
       "    'description': 'a type of Kubernetes object that manages stateful applications',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"creates a pod with specific configuration\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl create\",\\n    \"description\": \"creates a new statefulset\",\\n    \"destination_entity\": \"statefulset\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"adds a volume to the pod specification automatically\",\\n    \"destination_entity\": \"volume\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"configures the volume to be bound to the claim created for the specific pod\",\\n    \"destination_entity\": \"claim\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"is configured to create two replicas\",\\n    \"destination_entity\": \"replicas\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationController or ReplicaSet\",\\n    \"description\": \"creates all pod instances at the same time\",\\n    \"destination_entity\": \"pod instances\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSets\",\\n    \"description\": \"behaves in a way that certain clustered stateful apps are sensitive to race conditions if two or more cluster members come up at the same time\",\\n    \"destination_entity\": \"clustered stateful apps\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl get po\",\\n    \"description\": \"lists the pods created by the StatefulSet\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"kubia-statefulset.yaml\",\\n    \"description\": \"is used as a template to create the StatefulSet\",\\n    \"destination_entity\": \"StatefulSet\"\\n  }\\n]\\n```'},\n",
       " {'page': 327,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '295\\nUsing a StatefulSet\\n    volumeMounts:\\n    - mountPath: /var/data           \\n      name: data                     \\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\\n      name: default-token-r2m41\\n      readOnly: true\\n  ...\\n  volumes:\\n  - name: data                       \\n    persistentVolumeClaim:           \\n      claimName: data-kubia-0            \\n  - name: default-token-r2m41\\n    secret:\\n      secretName: default-token-r2m41\\nThe PersistentVolumeClaim template was used to create the PersistentVolumeClaim\\nand the volume inside the pod, which refers to the created PersistentVolumeClaim. \\nEXAMINING THE GENERATED PERSISTENTVOLUMECLAIMS\\nNow list the generated PersistentVolumeClaims to confirm they were created:\\n$ kubectl get pvc\\nNAME           STATUS    VOLUME    CAPACITY   ACCESSMODES   AGE\\ndata-kubia-0   Bound     pv-c      0                        37s\\ndata-kubia-1   Bound     pv-a      0                        37s\\nThe names of the generated PersistentVolumeClaims are composed of the name\\ndefined in the volumeClaimTemplate and the name of each pod. You can examine the\\nclaims’ YAML to see that they match the template.\\n10.3.3 Playing with your pods\\nWith the nodes of your data store cluster now running, you can start exploring it. You\\ncan’t communicate with your pods through the Service you created because it’s head-\\nless. You’ll need to connect to individual pods directly (or create a regular Service, but\\nthat wouldn’t allow you to talk to a specific pod).\\n You’ve already seen ways to connect to a pod directly: by piggybacking on another\\npod and running curl inside it, by using port-forwarding, and so on. This time, you’ll\\ntry another option. You’ll use the API server as a proxy to the pods. \\nCOMMUNICATING WITH PODS THROUGH THE API SERVER\\nOne useful feature of the API server is the ability to proxy connections directly to indi-\\nvidual pods. If you want to perform requests against your kubia-0 pod, you hit the fol-\\nlowing URL:\\n<apiServerHost>:<port>/api/v1/namespaces/default/pods/kubia-0/proxy/<path>\\nBecause the API server is secured, sending requests to pods through the API server is\\ncumbersome (among other things, you need to pass the authorization token in each\\nrequest). Luckily, in chapter 8 you learned how to use kubectl proxy to talk to the\\nThe volume mount, as \\nspecified in the manifest\\nThe volume created \\nby the StatefulSet\\nThe claim referenced \\nby this volume\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSet',\n",
       "    'description': 'A Kubernetes concept for creating and managing stateful applications.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'volumeMounts',\n",
       "    'description': 'The specification of volumes to be mounted within a pod.',\n",
       "    'category': 'software'},\n",
       "   {'entity': '/var/data',\n",
       "    'description': \"The mount path for the volume named 'data'.\",\n",
       "    'category': 'path'},\n",
       "   {'entity': 'data',\n",
       "    'description': 'A PersistentVolumeClaim used by the StatefulSet.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'A Kubernetes resource that requests access to a Persistent Volume.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'default-token-r2m41',\n",
       "    'description': 'A secret used for authentication.',\n",
       "    'category': 'security'},\n",
       "   {'entity': '/var/run/secrets/kubernetes.io/serviceaccount',\n",
       "    'description': 'The path to the service account secrets.',\n",
       "    'category': 'path'},\n",
       "   {'entity': 'kubectl get pvc',\n",
       "    'description': 'A command used to list PersistentVolumeClaims.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pvc',\n",
       "    'description': 'Short for PersistentVolumeClaim, a Kubernetes resource that requests access to a Persistent Volume.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'data-kubia-0',\n",
       "    'description': 'A PersistentVolumeClaim used by the StatefulSet.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'pv-c',\n",
       "    'description': \"A Persistent Volume used by the PersistentVolumeClaim 'data-kubia-0'.\",\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kubia-0',\n",
       "    'description': 'The name of a pod used in this example.',\n",
       "    'category': 'container'},\n",
       "   {'entity': '/api/v1/namespaces/default/pods/kubia-0/proxy/<path>',\n",
       "    'description': 'A URL for communicating with the kubia-0 pod through the API server.',\n",
       "    'category': 'url'},\n",
       "   {'entity': 'kubectl proxy',\n",
       "    'description': 'A command used to talk to the API server and communicate with pods.',\n",
       "    'category': 'command'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"Creates PersistentVolumeClaim and volume inside pod\",\\n    \"destination_entity\": \"PersistentVolumeClaim\"\\n  },\\n  {\\n    \"source_entity\": \"PersistentVolumeClaim\",\\n    \"description\": \"References PersistentVolume\",\\n    \"destination_entity\": \"pv-c\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl get pvc\",\\n    \"description\": \"Lists generated PersistentVolumeClaims\",\\n    \"destination_entity\": \"data-kubia-0\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"Mounts volume to /var/data and /var/run/secrets/kubernetes.io/serviceaccount\",\\n    \"destination_entity\": \"/var/data\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"Mounts secret to /var/run/secrets/kubernetes.io/serviceaccount\",\\n    \"destination_entity\": \"/var/run/secrets/kubernetes.io/serviceaccount\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl proxy\",\\n    \"description\": \"Talks to kubia-0 pod through API server\",\\n    \"destination_entity\": \"kubia-0\"\\n  },\\n  {\\n    \"source_entity\": \"data-kubia-0\",\\n    \"description\": \"Is bound to PersistentVolume pv-c\",\\n    \"destination_entity\": \"pv-c\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl get pvc\",\\n    \"description\": \"Lists data-kubia-1 as a generated PersistentVolumeClaim\",\\n    \"destination_entity\": \"data-kubia-1\"\\n  },\\n  {\\n    \"source_entity\": \"/api/v1/namespaces/default/pods/kubia-0/proxy/<path>\",\\n    \"description\": \"Communicates with kubia-0 pod through API server\",\\n    \"destination_entity\": \"kubia-0\"\\n  },\\n  {\\n    \"source_entity\": \"/var/data\",\\n    \"description\": \"Is the mount path of a volume created by StatefulSet\",\\n    \"destination_entity\": \"StatefulSet\"\\n  }\\n]\\n```'},\n",
       " {'page': 328,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '296\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\nAPI server without having to deal with authentication and SSL certificates. Run the\\nproxy again:\\n$ kubectl proxy\\nStarting to serve on 127.0.0.1:8001\\nNow, because you’ll be talking to the API server through the kubectl proxy, you’ll use\\nlocalhost:8001 rather than the actual API server host and port. You’ll send a request to\\nthe kubia-0 pod like this:\\n$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\\nYou\\'ve hit kubia-0\\nData stored on this pod: No data posted yet\\nThe response shows that the request was indeed received and handled by the app run-\\nning in your pod kubia-0. \\nNOTE\\nIf you receive an empty response, make sure you haven’t left out that\\nlast slash character at the end of the URL (or make sure curl follows redirects\\nby using its -L option). \\nBecause you’re communicating with the pod through the API server, which you’re\\nconnecting to through the kubectl proxy, the request went through two different\\nproxies (the first was the kubectl proxy and the other was the API server, which prox-\\nied the request to the pod). For a clearer picture, examine figure 10.10.\\nThe request you sent to the pod was a GET request, but you can also send POST\\nrequests through the API server. This is done by sending a POST request to the same\\nproxy URL as the one you sent the GET request to. \\n When your app receives a POST request, it stores whatever’s in the request body\\ninto a local file. Send a POST request to the kubia-0 pod:\\n$ curl -X POST -d \"Hey there! This greeting was submitted to kubia-0.\"\\n➥ localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\\nData stored on pod kubia-0\\nkubectl proxy\\ncurl\\nGET localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\\nGET 192.168.99.100:8443/api/v1/namespaces/default/pods/kubia-0/proxy/\\nAuthorization: Bearer <token>\\nGET 172.17.0.3:8080/\\nAPI server\\nPod: kubia-0\\n192.168.99.100\\n172.17.0.3\\nlocalhost\\nFigure 10.10\\nConnecting to a pod through both the kubectl proxy and API server proxy\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSets',\n",
       "    'description': 'A Kubernetes resource that allows you to deploy replicated stateful applications.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool used to interact with a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server',\n",
       "    'description': \"A component of the Kubernetes system that provides an interface for interacting with the cluster's resources.\",\n",
       "    'category': 'application'},\n",
       "   {'entity': 'proxy',\n",
       "    'description': 'A process that forwards requests from one location to another, allowing communication between different components.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'A command-line tool used to transfer data to and from a web server, often used for testing or debugging purposes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'GET request',\n",
       "    'description': 'A type of HTTP request that retrieves information from a server without modifying it.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'POST request',\n",
       "    'description': 'A type of HTTP request that sends new data to a server, often used for creating or updating resources.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'The basic execution unit in Kubernetes, representing a running container or set of containers.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'localhost',\n",
       "    'description': 'A reference to the local machine, often used as a placeholder for the address of a server or other resource on the same machine.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'API server proxy',\n",
       "    'description': 'A component that acts as an intermediary between the API server and the client (in this case, kubectl).',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'namespaces',\n",
       "    'description': 'A way to isolate resources in a Kubernetes cluster, allowing multiple independent applications or systems to run on the same machine.',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\": \"kubectl\", \"description\": \"used to communicate with API server through proxy\", \"destination_entity\": \"API server\"}]\\n\\n'},\n",
       " {'page': 329,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '297\\nUsing a StatefulSet\\nThe data you sent should now be stored in that pod. Let’s see if it returns the stored\\ndata when you perform a GET request again:\\n$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\\nYou\\'ve hit kubia-0\\nData stored on this pod: Hey there! This greeting was submitted to kubia-0.\\nOkay, so far so good. Now let’s see what the other cluster node (the kubia-1 pod)\\nsays:\\n$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/\\nYou\\'ve hit kubia-1\\nData stored on this pod: No data posted yet\\nAs expected, each node has its own state. But is that state persisted? Let’s find out.\\nDELETING A STATEFUL POD TO SEE IF THE RESCHEDULED POD IS REATTACHED TO THE SAME STORAGE\\nYou’re going to delete the kubia-0 pod and wait for it to be rescheduled. Then you’ll\\nsee if it’s still serving the same data as before:\\n$ kubectl delete po kubia-0\\npod \"kubia-0\" deleted\\nIf you list the pods, you’ll see that the pod is terminating: \\n$ kubectl get po\\nNAME      READY     STATUS        RESTARTS   AGE\\nkubia-0   1/1       Terminating   0          3m\\nkubia-1   1/1       Running       0          3m\\nAs soon as it terminates successfully, a new pod with the same name is created by the\\nStatefulSet:\\n$ kubectl get po\\nNAME      READY     STATUS              RESTARTS   AGE\\nkubia-0   0/1       ContainerCreating   0          6s\\nkubia-1   1/1       Running             0          4m\\n$ kubectl get po\\nNAME      READY     STATUS    RESTARTS   AGE\\nkubia-0   1/1       Running   0          9s\\nkubia-1   1/1       Running   0          4m\\nLet me remind you again that this new pod may be scheduled to any node in the clus-\\nter, not necessarily the same node that the old pod was scheduled to. The old pod’s\\nwhole identity (the name, hostname, and the storage) is effectively moved to the new\\nnode (as shown in figure 10.11). If you’re using Minikube, you can’t see this because it\\nonly runs a single node, but in a multi-node cluster, you may see the pod scheduled to\\na different node than before.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSet',\n",
       "    'description': 'A Kubernetes concept for managing stateful applications',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A unit of compute resources in Kubernetes',\n",
       "    'category': 'Container'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool for interacting with a Kubernetes cluster',\n",
       "    'category': 'Command'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'A command-line utility for transferring data to and from a web server',\n",
       "    'category': 'Tool'},\n",
       "   {'entity': 'API',\n",
       "    'description': 'Application Programming Interface, used by Kubernetes for communication between components',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'StatefulSet controller',\n",
       "    'description': 'The component responsible for managing the state of StatefulSets in a cluster',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': 'A scope within which resources are namespaced and isolated from other namespaces',\n",
       "    'category': 'Resource'},\n",
       "   {'entity': 'Labels',\n",
       "    'description': 'Key-value pairs used to identify and select resources in Kubernetes',\n",
       "    'category': 'Metadata'},\n",
       "   {'entity': 'Pod proxy',\n",
       "    'description': \"A component that allows communication with a pod's API server\",\n",
       "    'category': 'Service'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"manages the state of the pod and persists data\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"deletes a pod with the name kubia-0\",\\n    \"destination_entity\": \"kubia-0\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet controller\",\\n    \"description\": \"reschedules a new pod with the same name when the old one terminates\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"curl\",\\n    \"description\": \"performs a GET request to the API to retrieve data from a pod\",\\n    \"destination_entity\": \"API\"\\n  },\\n  {\\n    \"source_entity\": \"Pod proxy\",\\n    \"description\": \"returns data stored on the pod when queried\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"Namespace\",\\n    \"description\": \"contains a pod with the name kubia-0\",\\n    \"destination_entity\": \"kubia-0\"\\n  },\\n  {\\n    \"source_entity\": \"Labels\",\\n    \"description\": \"are attached to a pod with the name kubia-1\",\\n    \"destination_entity\": \"kubia-1\"\\n  },\\n  {\\n    \"source_entity\": \"Pod proxy\",\\n    \"description\": \"returns no data posted yet when queried on kubia-1\",\\n    \"destination_entity\": \"kubia-1\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"gets a list of pods and their status\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet controller\",\\n    \"description\": \"maintains the identity of a pod, including its name, hostname, and storage\",\\n    \"destination_entity\": \"pod\"\\n  }\\n]\\n\\nNote: I\\'ve only included the entities provided in the list. If you want me to extract relations with other entities mentioned in the text (e.g., \"cluster\", \"node\"), please let me know!'},\n",
       " {'page': 330,\n",
       "  'img_cnt': 2,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"298\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\nWith the new pod now running, let’s check to see if it has the exact same identity as in\\nits previous incarnation. The pod’s name is the same, but what about the hostname\\nand persistent data? You can ask the pod itself to confirm:\\n$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\\nYou've hit kubia-0\\nData stored on this pod: Hey there! This greeting was submitted to kubia-0.\\nThe pod’s response shows that both the hostname and the data are the same as before,\\nconfirming that a StatefulSet always replaces a deleted pod with what’s effectively the\\nexact same pod. \\nSCALING A STATEFULSET\\nScaling down a StatefulSet and scaling it back up after an extended time period\\nshould be no different than deleting a pod and having the StatefulSet recreate it\\nimmediately. Remember that scaling down a StatefulSet only deletes the pods, but\\nleaves the PersistentVolumeClaims untouched. I’ll let you try scaling down the State-\\nfulSet yourself and confirm this behavior. \\n The key thing to remember is that scaling down (and up) is performed gradu-\\nally—similar to how individual pods are created when the StatefulSet is created ini-\\ntially. When scaling down by more than one instance, the pod with the highest ordinal\\nnumber is deleted first. Only after the pod terminates completely is the pod with the\\nsecond highest ordinal number deleted. \\nEXPOSING STATEFUL PODS THROUGH A REGULAR, NON-HEADLESS SERVICE\\nBefore you move on to the last part of this chapter, you’re going to add a proper, non-\\nheadless Service in front of your pods, because clients usually connect to the pods\\nthrough a Service rather than connecting directly.\\nNode 1\\nPod: kubia-0\\nPod: kubia-1\\nDelete kubia-0\\nStorage\\nStorage\\nStorage\\nPod: kubia-1\\nStorage\\nNode 1\\nkubia-0 rescheduled\\nNode 1\\nNode 2\\nNode 2\\nNode 2\\nStorage\\nPod: kubia-1\\nStorage\\nPod: kubia-0\\nFigure 10.11\\nA stateful pod may be rescheduled to a different node, but it retains the name, hostname, and storage.\\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSet',\n",
       "    'description': 'a Kubernetes API resource that can be used to deploy replicated stateful applications',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a lightweight and portable container that is running an application or service',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubia-0',\n",
       "    'description': 'the name of a pod',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'hostname',\n",
       "    'description': 'a unique identifier for a pod',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'persistent data',\n",
       "    'description': 'data stored on a persistent volume claim',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'PersistentVolumeClaims',\n",
       "    'description': 'a resource that provides storage for a pod',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'a Kubernetes API resource that exposes a set of pods to the network',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'headless Service',\n",
       "    'description': 'a type of Service that does not expose a DNS name or IP address',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'regular Service',\n",
       "    'description': 'a type of Service that exposes a DNS name and IP address',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ordinal number',\n",
       "    'description': 'the order in which pods are deleted when scaling down a StatefulSet',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'is created gradually and has the same identity as before when recreated, retaining its name, hostname, and storage.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\":\"StatefulSet\",\"description\":\"replaces a deleted pod with what\\'s effectively the exact same pod\",\"destination_entity\":\"pod\"},{\"source_entity\":\"StatefulSet\",\"description\":\"scales down by deleting pods, but leaves PersistentVolumeClaims untouched\",\"destination_entity\":\"PersistentVolumeClaims\"},{\"source_entity\":\"StatefulSet\",\"description\":\"scales up by creating new pods gradually\",\"destination_entity\":\"pod\"},{\"source_entity\":\"headless Service\",\"description\":\"exposes stateful pods through a regular, non-headless service\",\"destination_entity\":\"Service\"},{\"source_entity\":\"Service\",\"description\":\"clients usually connect to pods through a Service rather than connecting directly\",\"destination_entity\":\"pod\"},{\"source_entity\":\"kubia-0\",\"description\":\"reschedules a stateful pod to a different node, but retains the name, hostname, and storage\",\"destination_entity\":\"node\"},{\"source_entity\":\"ordinal number\",\"description\":\"used to determine which pod to delete when scaling down a StatefulSet\",\"destination_entity\":\"pod\"},{\"source_entity\":\"hostname\",\"description\":\"retained by a rescheduled stateful pod\",\"destination_entity\":\"kubia-0\"},{\"source_entity\":\"persistent data\",\"description\":\"retained by a rescheduled stateful pod\",\"destination_entity\":\"kubia-0\"},{\"source_entity\":\"regular Service\",\"description\":\"exposes stateful pods through a regular, non-headless service\",\"destination_entity\":\"Service\"}]\\n\\nNote: I have only extracted relations where the source entity performs an action on the destination entity.'},\n",
       " {'page': 331,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"299\\nDiscovering peers in a StatefulSet\\n You know how to create the Service by now, but in case you don’t, the following list-\\ning shows the manifest.\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: kubia-public\\nspec:\\n  selector:\\n    app: kubia\\n  ports:\\n  - port: 80\\n    targetPort: 8080\\nBecause this isn’t an externally exposed Service (it’s a regular ClusterIP Service, not\\na NodePort or a LoadBalancer-type Service), you can only access it from inside the\\ncluster. You’ll need a pod to access it from, right? Not necessarily.\\nCONNECTING TO CLUSTER-INTERNAL SERVICES THROUGH THE API SERVER\\nInstead of using a piggyback pod to access the service from inside the cluster, you can\\nuse the same proxy feature provided by the API server to access the service the way\\nyou’ve accessed individual pods.\\n The URI path for proxy-ing requests to Services is formed like this:\\n/api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>\\nTherefore, you can run curl on your local machine and access the service through the\\nkubectl proxy like this (you ran kubectl proxy earlier and it should still be running):\\n$ curl localhost:8001/api/v1/namespaces/default/services/kubia-\\n➥ public/proxy/\\nYou've hit kubia-1\\nData stored on this pod: No data posted yet\\nLikewise, clients (inside the cluster) can use the kubia-public service for storing to\\nand reading data from your clustered data store. Of course, each request lands on a\\nrandom cluster node, so you’ll get the data from a random node each time. You’ll\\nimprove this next.\\n10.4\\nDiscovering peers in a StatefulSet\\nWe still need to cover one more important thing. An important requirement of clus-\\ntered apps is peer discovery—the ability to find other members of the cluster. Each\\nmember of a StatefulSet needs to easily find all the other members. Sure, it could do\\nthat by talking to the API server, but one of Kubernetes’ aims is to expose features that\\nhelp keep applications completely Kubernetes-agnostic. Having apps talk to the Kuber-\\nnetes API is therefore undesirable.\\nListing 10.7\\nA regular Service for accessing the stateful pods: kubia-service-public.yaml\\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSet',\n",
       "    'description': '',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'a network service that can be accessed by clients within a cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pod', 'description': '', 'category': 'container'},\n",
       "   {'entity': 'ClusterIP Service',\n",
       "    'description': 'an internally exposed Service that can only be accessed from within the cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'NodePort Service',\n",
       "    'description': 'an externally exposed Service that can be accessed from outside the cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'LoadBalancer Service',\n",
       "    'description': 'an externally exposed Service that uses a load balancer to distribute traffic across multiple nodes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'API Server', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'kubectl proxy',\n",
       "    'description': 'a feature of the API server that allows clients to access Services within the cluster without running a pod',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'a command-line tool for making HTTP requests',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'namespace', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'service name', 'description': '', 'category': 'string'},\n",
       "   {'entity': 'path', 'description': '', 'category': 'string'},\n",
       "   {'entity': 'random cluster node',\n",
       "    'description': '',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[{\"source_entity\": \"StatefulSet\",\"description\": \"needs to discover peers easily\",\"destination_entity\": \"other members of the cluster\"},\\n{\"source_entity\": \"Service\",\"description\": \"is used by pods to access\",\"destination_entity\": \"pods\"},\\n{\"source_entity\": \"ClusterIP Service\",\"description\": \"can only be accessed from inside the cluster\",\"destination_entity\": \"inside the cluster\"},\\n{\"source_entity\": \"path\",\"description\": \"forms the URI for proxy-ing requests\",\"destination_entity\": \"Services\"},\\n{\"source_entity\": \"service name\",\"description\": \"is used in the URI path to access Service\",\"destination_entity\": \"Service\"},\\n{\"source_entity\": \"namespace\",\"description\": \"is part of the URI path to access Service\",\"destination_entity\": \"Service\"},\\n{\"source_entity\": \"random cluster node\",\"description\": \"each request lands on a random node\",\"destination_entity\": \"clustered data store\"},\\n{\"source_entity\": \"NodePort Service\",\"description\": \"exposes ports to external traffic\",\"destination_entity\": \"external traffic\"},\\n{\"source_entity\": \"LoadBalancer Service\",\"description\": \"exposes load balancing for external traffic\",\"destination_entity\": \"external traffic\"},\\n{\"source_entity\": \"curl\",\"description\": \"can be used to access Service through API server\",\"destination_entity\": \"API Server\"},\\n{\"source_entity\": \"Pod\",\"description\": \"uses Service to access data store\",\"destination_entity\": \"data store\"},\\n{\"source_entity\": \"kubectl proxy\",\"description\": \"provides a proxy feature to access Services\",\"destination_entity\": \"Services\"},\\n{\"source_entity\": \"API Server\",\"description\": \"exposes features that help keep applications Kubernetes-agnostic\",\"destination_entity\": \"applications\"}]'},\n",
       " {'page': 332,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '300\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\n How can a pod discover its peers without talking to the API? Is there an existing,\\nwell-known technology you can use that makes this possible? How about the Domain\\nName System (DNS)? Depending on how much you know about DNS, you probably\\nunderstand what an A, CNAME, or MX record is used for. Other lesser-known types of\\nDNS records also exist. One of them is the SRV record.\\nINTRODUCING SRV RECORDS\\nSRV records are used to point to hostnames and ports of servers providing a specific\\nservice. Kubernetes creates SRV records to point to the hostnames of the pods back-\\ning a headless service. \\n You’re going to list the SRV records for your stateful pods by running the dig DNS\\nlookup tool inside a new temporary pod. This is the command you’ll use:\\n$ kubectl run -it srvlookup --image=tutum/dnsutils --rm \\n➥ --restart=Never -- dig SRV kubia.default.svc.cluster.local\\nThe command runs a one-off pod (--restart=Never) called srvlookup, which is\\nattached to the console (-it) and is deleted as soon as it terminates (--rm). The\\npod runs a single container from the tutum/dnsutils image and runs the following\\ncommand:\\ndig SRV kubia.default.svc.cluster.local\\nThe following listing shows what the command prints out.\\n...\\n;; ANSWER SECTION:\\nk.d.s.c.l. 30 IN  SRV     10 33 0 kubia-0.kubia.default.svc.cluster.local.\\nk.d.s.c.l. 30 IN  SRV     10 33 0 kubia-1.kubia.default.svc.cluster.local.\\n;; ADDITIONAL SECTION:\\nkubia-0.kubia.default.svc.cluster.local. 30 IN A 172.17.0.4\\nkubia-1.kubia.default.svc.cluster.local. 30 IN A 172.17.0.6\\n...\\nNOTE\\nI’ve had to shorten the actual name to get records to fit into a single\\nline, so kubia.d.s.c.l is actually kubia.default.svc.cluster.local.\\nThe ANSWER SECTION shows two SRV records pointing to the two pods backing your head-\\nless service. Each pod also gets its own A record, as shown in ADDITIONAL SECTION.\\n For a pod to get a list of all the other pods of a StatefulSet, all you need to do is\\nperform an SRV DNS lookup. In Node.js, for example, the lookup is performed\\nlike this:\\ndns.resolveSrv(\"kubia.default.svc.cluster.local\", callBackFunction);\\nYou’ll use this command in your app to enable each pod to discover its peers.\\nListing 10.8\\nListing DNS SRV records of your headless Service\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'SRV record',\n",
       "    'description': 'DNS record used to point to hostnames and ports of servers providing a specific service',\n",
       "    'category': 'database/records'},\n",
       "   {'entity': 'DNS',\n",
       "    'description': 'Domain Name System, a technology used for hostname-to-IP address translation',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'A record',\n",
       "    'description': 'DNS record that maps a hostname to an IP address',\n",
       "    'category': 'database/records'},\n",
       "   {'entity': 'CNAME record',\n",
       "    'description': 'DNS record that maps one hostname to another',\n",
       "    'category': 'database/records'},\n",
       "   {'entity': 'MX record',\n",
       "    'description': 'DNS record used for mail server lookup',\n",
       "    'category': 'database/records'},\n",
       "   {'entity': 'dig DNS lookup tool',\n",
       "    'description': 'tool used to query DNS records',\n",
       "    'category': 'software/command'},\n",
       "   {'entity': 'kubia.default.svc.cluster.local',\n",
       "    'description': 'fully qualified domain name of a Kubernetes service',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'containerized instance of an application',\n",
       "    'category': 'application/container'},\n",
       "   {'entity': 'headless service',\n",
       "    'description': 'Kubernetes service that does not expose a load balancer',\n",
       "    'category': 'application/service'},\n",
       "   {'entity': 'Node.js',\n",
       "    'description': 'JavaScript runtime environment for server-side programming',\n",
       "    'category': 'software/runtime'},\n",
       "   {'entity': 'callBackFunction',\n",
       "    'description': 'function called as a callback in Node.js',\n",
       "    'category': 'software/function'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"creates SRV records to point to the hostnames of the pods backing a headless service.\",\\n    \"destination_entity\": \"SRV record\"\\n  },\\n  {\\n    \"source_entity\": \"DNS\",\\n    \"description\": \"is used to perform an SRV DNS lookup and get a list of all the other pods of a StatefulSet.\",\\n    \"destination_entity\": \"StatefulSet\"\\n  },\\n  {\\n    \"source_entity\": \"dig DNS lookup tool\",\\n    \"description\": \"is used to list the SRV records for your stateful pods by running inside a new temporary pod.\",\\n    \"destination_entity\": \"SRV record\"\\n  },\\n  {\\n    \"source_entity\": \"Node.js\",\\n    \"description\": \"performs an SRV DNS lookup using dns.resolveSrv() function to enable each pod to discover its peers.\",\\n    \"destination_entity\": \"headless service\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"gets a list of all the other pods of a StatefulSet by performing an SRV DNS lookup.\",\\n    \"destination_entity\": \"StatefulSet\"\\n  },\\n  {\\n    \"source_entity\": \"SRV record\",\\n    \"description\": \"is used to point to the hostnames and ports of servers providing a specific service.\",\\n    \"destination_entity\": \"server\"\\n  },\\n  {\\n    \"source_entity\": \"A record\",\\n    \"description\": \"is created for each pod, showing its IP address.\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"headless service\",\\n    \"description\": \"uses SRV records to point to the hostnames of the pods backing it.\",\\n    \"destination_entity\": \"SRV record\"\\n  },\\n  {\\n    \"source_entity\": \"callBackFunction\",\\n    \"description\": \"is used in Node.js to perform an SRV DNS lookup and get a list of all the other pods of a StatefulSet.\",\\n    \"destination_entity\": \"headless service\"\\n  },\\n  {\\n    \"source_entity\": \"CNAME record\",\\n    \"description\": \"and MX record are types of DNS records, but SRV record is used to point to the hostnames and ports of servers providing a specific service.\",\\n    \"destination_entity\": \"SRV record\"\\n  }\\n]'},\n",
       " {'page': 333,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '301\\nDiscovering peers in a StatefulSet\\nNOTE\\nThe order of the returned SRV records is random, because they all have\\nthe same priority. Don’t expect to always see kubia-0 listed before kubia-1.\\n10.4.1 Implementing peer discovery through DNS\\nYour Stone Age data store isn’t clustered yet. Each data store node runs completely\\nindependently of all the others—no communication exists between them. You’ll get\\nthem talking to each other next.\\n Data posted by clients connecting to your data store cluster through the kubia-\\npublic Service lands on a random cluster node. The cluster can store multiple data\\nentries, but clients currently have no good way to see all those entries. Because ser-\\nvices forward requests to pods randomly, a client would need to perform many\\nrequests until it hit all the pods if it wanted to get the data from all the pods. \\n You can improve this by having the node respond with data from all the cluster\\nnodes. To do this, the node needs to find all its peers. You’re going to use what you\\nlearned about StatefulSets and SRV records to do this.\\n You’ll modify your app’s source code as shown in the following listing (the full\\nsource is available in the book’s code archive; the listing shows only the important\\nparts).\\n...\\nconst dns = require(\\'dns\\');\\nconst dataFile = \"/var/data/kubia.txt\";\\nconst serviceName = \"kubia.default.svc.cluster.local\";\\nconst port = 8080;\\n...\\nvar handler = function(request, response) {\\n  if (request.method == \\'POST\\') {\\n    ...\\n  } else {\\n    response.writeHead(200);\\n    if (request.url == \\'/data\\') {\\n      var data = fileExists(dataFile) \\n        ? fs.readFileSync(dataFile, \\'utf8\\') \\n        : \"No data posted yet\";\\n      response.end(data);\\n    } else {\\n      response.write(\"You\\'ve hit \" + os.hostname() + \"\\\\n\");\\n      response.write(\"Data stored in the cluster:\\\\n\");\\n      dns.resolveSrv(serviceName, function (err, addresses) {    \\n        if (err) {\\n          response.end(\"Could not look up DNS SRV records: \" + err);\\n          return;\\n        }\\n        var numResponses = 0;\\n        if (addresses.length == 0) {\\n          response.end(\"No peers discovered.\");\\n        } else {\\nListing 10.9\\nDiscovering peers in a sample app: kubia-pet-peers-image/app.js\\nThe app \\nperforms a DNS \\nlookup to obtain \\nSRV records.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSet',\n",
       "    'description': 'A Kubernetes API resource that manages stateful applications.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'DNS',\n",
       "    'description': 'A system for translating domain names into IP addresses.',\n",
       "    'category': 'hardware,software,network'},\n",
       "   {'entity': 'SRV records',\n",
       "    'description': 'A type of DNS record that specifies the service location.',\n",
       "    'category': 'hardware,software,network'},\n",
       "   {'entity': 'kubia-0 and kubia-1',\n",
       "    'description': 'Pods in a StatefulSet with different names.',\n",
       "    'category': 'software,application,process'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A lightweight and portable runtime environment for applications.',\n",
       "    'category': 'hardware,software,application'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'A machine running a Kubernetes cluster.',\n",
       "    'category': 'hardware,software,network'},\n",
       "   {'entity': 'client',\n",
       "    'description': 'An application connecting to the data store cluster through the kubia-public Service.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'kubia-public Service',\n",
       "    'description': 'A Service in Kubernetes that exposes an application.',\n",
       "    'category': 'hardware,software,application,network'},\n",
       "   {'entity': 'request and response',\n",
       "    'description': 'HTTP requests and responses between clients and servers.',\n",
       "    'category': 'software,application,process'},\n",
       "   {'entity': 'handler function',\n",
       "    'description': 'A JavaScript function that handles HTTP requests.',\n",
       "    'category': 'software,application,process'},\n",
       "   {'entity': 'file system',\n",
       "    'description': 'A system for storing and retrieving files on a computer.',\n",
       "    'category': 'hardware,software,application'},\n",
       "   {'entity': 'fs module',\n",
       "    'description': 'A Node.js module for interacting with the file system.',\n",
       "    'category': 'software,application,process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"node\",\\n    \"description\": \"discovers peers\",\\n    \"destination_entity\": \"StatefulSet\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"uses SRV records for peer discovery\",\\n    \"destination_entity\": \"SRV records\"\\n  },\\n  {\\n    \"source_entity\": \"kubia-public Service\",\\n    \"description\": \"allows clients to connect to data store cluster\",\\n    \"destination_entity\": \"client\"\\n  },\\n  {\\n    \"source_entity\": \"data store node\",\\n    \"description\": \"responds with data from all cluster nodes\",\\n    \"destination_entity\": \"cluster node\"\\n  },\\n  {\\n    \"source_entity\": \"node\",\\n    \"description\": \"needs to find all its peers\",\\n    \"destination_entity\": \"peers\"\\n  },\\n  {\\n    \"source_entity\": \"app\\'s source code\",\\n    \"description\": \"modified to use DNS lookup for peer discovery\",\\n    \"destination_entity\": \"DNS\"\\n  },\\n  {\\n    \"source_entity\": \"dns module\",\\n    \"description\": \"used to perform DNS lookup\",\\n    \"destination_entity\": \"SRV records\"\\n  },\\n  {\\n    \"source_entity\": \"handler function\",\\n    \"description\": \"sends response to client based on request type\",\\n    \"destination_entity\": \"client\"\\n  },\\n  {\\n    \"source_entity\": \"request and response\",\\n    \"description\": \"handled by handler function\",\\n    \"destination_entity\": \"handler function\"\\n  }\\n]\\n```'},\n",
       " {'page': 334,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '302\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\n          addresses.forEach(function (item) {                   \\n            var requestOptions = {\\n              host: item.name, \\n              port: port, \\n              path: \\'/data\\'\\n            };\\n            httpGet(requestOptions, function (returnedData) {   \\n              numResponses++;\\n              response.write(\"- \" + item.name + \": \" + returnedData);\\n              response.write(\"\\\\n\");\\n              if (numResponses == addresses.length) {\\n                response.end();\\n              }\\n            });\\n          });\\n        }\\n      });\\n    }\\n  }\\n};\\n...\\nFigure 10.12 shows what happens when a GET request is received by your app. The\\nserver that receives the request first performs a lookup of SRV records for the head-\\nless kubia service and then sends a GET request to each of the pods backing the ser-\\nvice (even to itself, which obviously isn’t necessary, but I wanted to keep the code as\\nsimple as possible). It then returns a list of all the nodes along with the data stored on\\neach of them.\\nThe container image containing this new version of the app is available at docker.io/\\nluksa/kubia-pet-peers.\\n10.4.2 Updating a StatefulSet\\nYour StatefulSet is already running, so let’s see how to update its pod template so the\\npods use the new image. You’ll also set the replica count to 3 at the same time. To\\nEach pod \\npointed to by \\nan SRV record is \\nthen contacted \\nto get its data.\\ncurl\\nDNS\\n1. GET /\\n4. GET /data\\n5. GET /data\\n2. SRV lookup\\n6. Return collated data\\nkubia-0\\nkubia-1\\nkubia-2\\n3. GET /data\\nFigure 10.12\\nThe operation of your simplistic distributed data store\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'addresses',\n",
       "    'description': 'an array of addresses to contact for data',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'httpGet',\n",
       "    'description': 'a function to make an HTTP GET request',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'returnedData',\n",
       "    'description': 'the data returned from the GET request',\n",
       "    'category': 'data'},\n",
       "   {'entity': 'response',\n",
       "    'description': 'an object to write output to',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'requestOptions',\n",
       "    'description': 'options for the HTTP GET request',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'numResponses',\n",
       "    'description': 'a counter for the number of responses received',\n",
       "    'category': 'variable'},\n",
       "   {'entity': 'addresses.length',\n",
       "    'description': 'the length of the addresses array',\n",
       "    'category': 'expression'},\n",
       "   {'entity': 'docker.io/luksa/kubia-pet-peers',\n",
       "    'description': 'a container image URL',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'a Kubernetes resource for managing stateful applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod template',\n",
       "    'description': 'the template for the pods in a StatefulSet',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'replica count',\n",
       "    'description': 'the number of replicas in a StatefulSet',\n",
       "    'category': 'variable'},\n",
       "   {'entity': 'SRV record',\n",
       "    'description': 'a DNS record for resolving service names to hostnames and ports',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'a command-line tool for making HTTP requests',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'DNS',\n",
       "    'description': 'the Domain Name System for resolving hostnames to IP addresses',\n",
       "    'category': 'network'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"deploys replicated stateful applications\",\\n    \"destination_entity\": null\\n  },\\n  {\\n    \"source_entity\": \"addresses.forEach\",\\n    \"description\": \"iterates over a list of addresses to perform an action on each item\",\\n    \"destination_entity\": \"addresses\"\\n  },\\n  {\\n    \"source_entity\": \"requestOptions\",\\n    \"description\": \"defines the options for an HTTP request\",\\n    \"destination_entity\": null\\n  },\\n  {\\n    \"source_entity\": \"httpGet\",\\n    \"description\": \"sends a GET request to each pod backing a service\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"httpGet\",\\n    \"description\": \"returns data stored on each node\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"uses the new image from docker.io/luksa/kubia-pet-peers\",\\n    \"destination_entity\": \"docker.io/luksa/kubia-pet-peers\"\\n  },\\n  {\\n    \"source_entity\": \"DNS\",\\n    \"description\": \"performs a lookup of SRV records for the headless kubia service\",\\n    \"destination_entity\": \"SRV record\"\\n  },\\n  {\\n    \"source_entity\": \"curl\",\\n    \"description\": \"contacts each pod pointed to by an SRV record to get its data\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"addresses.length\",\\n    \"description\": \"defines the number of addresses to iterate over\",\\n    \"destination_entity\": null\\n  },\\n  {\\n    \"source_entity\": \"pod template\",\\n    \"description\": \"updates the pod template to use the new image\",\\n    \"destination_entity\": null\\n  },\\n  {\\n    \"source_entity\": \"returnedData\",\\n    \"description\": \"returns the collated data from each node\",\\n    \"destination_entity\": null\\n  },\\n  {\\n    \"source_entity\": \"SRV record\",\\n    \"description\": \"points to each pod that is contacted to get its data\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"replica count\",\\n    \"description\": \"sets the replica count to 3\",\\n    \"destination_entity\": null\\n  },\\n  {\\n    \"source_entity\": \"numResponses\",\\n    \"description\": \"defines the number of responses expected from each node\",\\n    \"destination_entity\": null\\n  }\\n]'},\n",
       " {'page': 335,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '303\\nDiscovering peers in a StatefulSet\\nupdate the StatefulSet, use the kubectl edit command (the patch command would\\nbe another option):\\n$ kubectl edit statefulset kubia\\nThis opens the StatefulSet definition in your default editor. In the definition, change\\nspec.replicas to 3 and modify the spec.template.spec.containers.image attri-\\nbute so it points to the new image (luksa/kubia-pet-peers instead of luksa/kubia-\\npet). Save the file and exit the editor to update the StatefulSet. Two replicas were\\nrunning previously, so you should now see an additional replica called kubia-2 start-\\ning. List the pods to confirm:\\n$ kubectl get po\\nNAME      READY     STATUS              RESTARTS   AGE\\nkubia-0   1/1       Running             0          25m\\nkubia-1   1/1       Running             0          26m\\nkubia-2   0/1       ContainerCreating   0          4s\\nThe new pod instance is running the new image. But what about the existing two rep-\\nlicas? Judging from their age, they don’t seem to have been updated. This is expected,\\nbecause initially, StatefulSets were more like ReplicaSets and not like Deployments,\\nso they don’t perform a rollout when the template is modified. You need to delete\\nthe replicas manually and the StatefulSet will bring them up again based on the new\\ntemplate:\\n$ kubectl delete po kubia-0 kubia-1\\npod \"kubia-0\" deleted\\npod \"kubia-1\" deleted\\nNOTE\\nStarting from Kubernetes version 1.7, StatefulSets support rolling\\nupdates the same way Deployments and DaemonSets do. See the StatefulSet’s\\nspec.updateStrategy field documentation using kubectl explain for more\\ninformation.\\n10.4.3 Trying out your clustered data store\\nOnce the two pods are up, you can see if your shiny new Stone Age data store works as\\nexpected. Post a few requests to the cluster, as shown in the following listing.\\n$ curl -X POST -d \"The sun is shining\" \\\\\\n➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/\\nData stored on pod kubia-1\\n$ curl -X POST -d \"The weather is sweet\" \\\\\\n➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/\\nData stored on pod kubia-0\\nNow, read the stored data, as shown in the following listing.\\nListing 10.10\\nWriting to the clustered data store through the service\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSet',\n",
       "    'description': 'A Kubernetes resource for managing stateful applications',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Kubernetes command-line tool',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'edit',\n",
       "    'description': 'Command to edit a StatefulSet definition',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'replicas',\n",
       "    'description': 'Number of replicas in a StatefulSet',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'template.spec.containers.image',\n",
       "    'description': 'Image attribute for a container',\n",
       "    'category': 'property'},\n",
       "   {'entity': 'kubia',\n",
       "    'description': 'Pod name or service name',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'Kubernetes resources representing running processes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl get po',\n",
       "    'description': 'Command to list pods',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'po kubia-0',\n",
       "    'description': 'Pod name or ID',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod kubia-1',\n",
       "    'description': 'Pod name or ID',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'StatefulSets updateStrategy',\n",
       "    'description': 'Field for specifying an update strategy',\n",
       "    'category': 'property'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'HTTP command-line tool',\n",
       "    'category': 'software'},\n",
       "   {'entity': '-X POST -d \"The sun is shining\" ',\n",
       "    'description': 'HTTP request with data',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/',\n",
       "    'description': 'URL for a Kubernetes service proxy',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'pod kubia-0',\n",
       "    'description': 'Pod name or ID',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod kubia-1',\n",
       "    'description': 'Pod name or ID',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\": \"kubectl\", \"description\": \"used to edit StatefulSet\", \"destination_entity\": \"StatefulSet\"}, \\n {\"source_entity\": \"kubectl\", \"description\": \"used to delete pod kubia-0\", \"destination_entity\": \"pod kubia-0\"}, \\n {\"source_entity\": \"kubectl\", \"description\": \"used to get pods\", \"destination_entity\": \"pods\"}, \\n {\"source_entity\": \"StatefulSet\", \"description\": \"modified to update replicas\", \"destination_entity\": \"replicas\"}, \\n {\"source_entity\": \"template.spec.containers.image\", \"description\": \"updated to new image\", \"destination_entity\": \"image\"}, \\n {\"source_entity\": \"pod kubia-1\", \"description\": \"used to store data\", \"destination_entity\": \"data\"}, \\n {\"source_entity\": \"curl\", \"description\": \"used to post request\", \"destination_entity\": \"service\"}, \\n {\"source_entity\": \"StatefulSets updateStrategy\", \"description\": \"supported for rolling updates\", \"destination_entity\": \"Deployments\"}, \\n {\"source_entity\": \"kubectl get po\", \"description\": \"used to list pods\", \"destination_entity\": \"pods\"}, \\n {\"source_entity\": \"edit\", \"description\": \"used to edit StatefulSet\", \"destination_entity\": \"StatefulSet\"}, \\n {\"source_entity\": \"po kubia-0\", \"description\": \"deleted manually\", \"destination_entity\": \"StatefulSet\"}, \\n {\"source_entity\": \"pod kubia-1\", \"description\": \"running the new image\", \"destination_entity\": \"image\"}, \\n {\"source_entity\": \"curl -X POST -d \", \"description\": \"posted request to service\", \"destination_entity\": \"service\"}]'},\n",
       " {'page': 336,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"304\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\n$ curl localhost:8001/api/v1/namespaces/default/services\\n➥ /kubia-public/proxy/\\nYou've hit kubia-2\\nData stored on each cluster node:\\n- kubia-0.kubia.default.svc.cluster.local: The weather is sweet\\n- kubia-1.kubia.default.svc.cluster.local: The sun is shining\\n- kubia-2.kubia.default.svc.cluster.local: No data posted yet\\nNice! When a client request reaches one of your cluster nodes, it discovers all its\\npeers, gathers data from them, and sends all the data back to the client. Even if you\\nscale the StatefulSet up or down, the pod servicing the client’s request can always find\\nall the peers running at that time. \\n The app itself isn’t that useful, but I hope you found it a fun way to show how\\ninstances of a replicated stateful app can discover their peers and handle horizontal\\nscaling with ease.\\n10.5\\nUnderstanding how StatefulSets deal with node \\nfailures\\nIn section 10.2.4 we stated that Kubernetes must be absolutely sure that a stateful\\npod is no longer running before creating its replacement. When a node fails\\nabruptly, Kubernetes can’t know the state of the node or its pods. It can’t know\\nwhether the pods are no longer running, or if they still are and are possibly even still\\nreachable, and it’s only the Kubelet that has stopped reporting the node’s state to\\nthe master.\\n Because a StatefulSet guarantees that there will never be two pods running with\\nthe same identity and storage, when a node appears to have failed, the StatefulSet can-\\nnot and should not create a replacement pod until it knows for certain that the pod is\\nno longer running. \\n It can only know that when the cluster administrator tells it so. To do that, the\\nadmin needs to either delete the pod or delete the whole node (doing so then deletes\\nall the pods scheduled to the node).\\n As your final exercise in this chapter, you’ll look at what happens to StatefulSets\\nand their pods when one of the cluster nodes gets disconnected from the network.\\n10.5.1 Simulating a node’s disconnection from the network \\nAs in chapter 4, you’ll simulate the node disconnecting from the network by shutting\\ndown the node’s eth0 network interface. Because this example requires multiple\\nnodes, you can’t run it on Minikube. You’ll use Google Kubernetes Engine instead.\\nSHUTTING DOWN THE NODE’S NETWORK ADAPTER\\nTo shut down a node’s eth0 interface, you need to ssh into one of the nodes like this:\\n$ gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1\\nListing 10.11\\nReading from the data store\\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSets',\n",
       "    'description': 'deploying replicated stateful applications',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubia-0.kubia.default.svc.cluster.local',\n",
       "    'description': \"pod servicing a client's request\",\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubia-1.kubia.default.svc.cluster.local',\n",
       "    'description': 'pod running on another cluster node',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubia-2.kubia.default.svc.cluster.local',\n",
       "    'description': \"current pod servicing a client's request\",\n",
       "    'category': 'application'},\n",
       "   {'entity': '$ curl localhost:8001/api/v1/namespaces/default/services /kubia-public/proxy/',\n",
       "    'description': 'command to get service details',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'eth0',\n",
       "    'description': \"node's network interface\",\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'component responsible for reporting node state to the master',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'guarantees that there will never be two pods running with the same identity and storage',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1',\n",
       "    'description': 'command to ssh into a node',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Listing 10.11',\n",
       "    'description': 'code listing for reading from the data store',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"guarantees that there will never be two pods running with the same identity and storage\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"can\\'t know whether the pods are no longer running, or if they still are and are possibly even still reachable\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"can not create a replacement pod until it knows for certain that the pod is no longer running\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"cluster administrator\",\\n    \"description\": \"needs to either delete the pod or delete the whole node\",\\n    \"destination_entity\": \"StatefulSet\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"must be absolutely sure that a stateful pod is no longer running before creating its replacement\",\\n    \"destination_entity\": \"stateful pod\"\\n  },\\n  {\\n    \"source_entity\": \"$ curl localhost:8001/api/v1/namespaces/default/services /kubia-public/proxy/\",\\n    \"description\": \"displays information about available services\",\\n    \"destination_entity\": \"available services\"\\n  },\\n  {\\n    \"source_entity\": \"gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1\",\\n    \"description\": \"shuts down the node\\'s eth0 network interface\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"has stopped reporting the node\\'s state to the master\",\\n    \"destination_entity\": \"master\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSets\",\\n    \"description\": \"can discover their peers and handle horizontal scaling with ease\",\\n    \"destination_entity\": \"peers\"\\n  },\\n  {\\n    \"source_entity\": \"Listing 10.11\",\\n    \"description\": \"reads from the data store\",\\n    \"destination_entity\": \"data store\"\\n  },\\n  {\\n    \"source_entity\": \"$ curl localhost:8001/api/v1/namespaces/default/services /kubia-public/proxy/\",\\n    \"description\": \"displays information about kubia-2 pod\",\\n    \"destination_entity\": \"kubia-2 pod\"\\n  },\\n  {\\n    \"source_entity\": \"eth0\",\\n    \"description\": \"network interface being shut down\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"kubia-1.kubia.default.svc.cluster.local\",\\n    \"description\": \"stores data on each cluster node\",\\n    \"destination_entity\": \"cluster nodes\"\\n  },\\n  {\\n    \"source_entity\": \"kubia-0.kubia.default.svc.cluster.local\",\\n    \"description\": \"stores data on each cluster node\",\\n    \"destination_entity\": \"cluster nodes\"\\n  }\\n]\\n```\\n\\nNote: I have considered all the entities mentioned in the text and extracted relevant relations. Let me know if you need any further assistance!'},\n",
       " {'page': 337,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '305\\nUnderstanding how StatefulSets deal with node failures\\nThen, inside the node, run the following command:\\n$ sudo ifconfig eth0 down\\nYour ssh session will stop working, so you’ll need to open another terminal to continue.\\nCHECKING THE NODE’S STATUS AS SEEN BY THE KUBERNETES MASTER\\nWith the node’s network interface down, the Kubelet running on the node can no\\nlonger contact the Kubernetes API server and let it know that the node and all its pods\\nare still running.\\n After a while, the control plane will mark the node as NotReady. You can see this\\nwhen listing nodes, as the following listing shows.\\n$ kubectl get node\\nNAME                                   STATUS     AGE       VERSION\\ngke-kubia-default-pool-32a2cac8-596v   Ready      16m       v1.6.2\\ngke-kubia-default-pool-32a2cac8-m0g1   NotReady   16m       v1.6.2\\ngke-kubia-default-pool-32a2cac8-sgl7   Ready      16m       v1.6.2\\nBecause the control plane is no longer getting status updates from the node, the\\nstatus of all pods on that node is Unknown. This is shown in the pod list in the follow-\\ning listing.\\n$ kubectl get po\\nNAME      READY     STATUS    RESTARTS   AGE\\nkubia-0   1/1       Unknown   0          15m\\nkubia-1   1/1       Running   0          14m\\nkubia-2   1/1       Running   0          13m\\nAs you can see, the kubia-0 pod’s status is no longer known because the pod was (and\\nstill is) running on the node whose network interface you shut down.\\nUNDERSTANDING WHAT HAPPENS TO PODS WHOSE STATUS IS UNKNOWN\\nIf the node were to come back online and report its and its pod statuses again, the pod\\nwould again be marked as Running. But if the pod’s status remains unknown for more\\nthan a few minutes (this time is configurable), the pod is automatically evicted from\\nthe node. This is done by the master (the Kubernetes control plane). It evicts the pod\\nby deleting the pod resource. \\n When the Kubelet sees that the pod has been marked for deletion, it starts ter-\\nminating the pod. In your case, the Kubelet can no longer reach the master (because\\nyou disconnected the node from the network), which means the pod will keep\\nrunning.\\nListing 10.12\\nObserving a failed node’s status change to NotReady\\nListing 10.13\\nObserving the pod’s status change after its node becomes NotReady\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'StatefulSets',\n",
       "    'description': 'A Kubernetes feature for managing stateful applications.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'node failures',\n",
       "    'description': 'A situation where a node in a Kubernetes cluster fails.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ifconfig eth0 down',\n",
       "    'description': 'A command used to shut down a network interface on a Linux system.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'The agent that runs on each node in a Kubernetes cluster and is responsible for running containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes API server',\n",
       "    'description': 'The component of the Kubernetes control plane that manages communication between nodes and the master.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl get node',\n",
       "    'description': 'A command used to list all nodes in a Kubernetes cluster.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'NotReady',\n",
       "    'description': 'The status assigned to a node when it is not functioning properly.',\n",
       "    'category': 'status'},\n",
       "   {'entity': 'gke-kubia-default-pool-32a2cac8-596v',\n",
       "    'description': 'A specific node in the Kubernetes cluster.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kubia-0',\n",
       "    'description': 'A pod running on a node in the Kubernetes cluster.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'pod list',\n",
       "    'description': 'A command used to list all pods in a Kubernetes cluster.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Unknown',\n",
       "    'description': 'The status assigned to a pod when its status is not known.',\n",
       "    'category': 'status'},\n",
       "   {'entity': 'kubectl get po',\n",
       "    'description': 'A command used to list all pods in a Kubernetes cluster.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubia-1',\n",
       "    'description': 'A pod running on a node in the Kubernetes cluster.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubia-2',\n",
       "    'description': 'A pod running on a node in the Kubernetes cluster.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'eviction',\n",
       "    'description': 'The process of removing a pod from a node when its status is unknown for too long.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pod deletion',\n",
       "    'description': 'The process of deleting a pod resource in Kubernetes.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes API server\", \\n   \"description\": \"not contact\", \\n   \"destination_entity\": \"node\"},\\n  \\n  {\"source_entity\": \"control plane\", \\n   \"description\": \"mark node as NotReady\", \\n   \"destination_entity\": \"node\"},\\n  \\n  {\"source_entity\": \"control plane\", \\n   \"description\": \"mark pod status as Unknown\", \\n   \"destination_entity\": \"pod list\"},\\n  \\n  {\"source_entity\": \"master\", \\n   \"description\": \"evict pod from node\", \\n   \"destination_entity\": \"eviction\"},\\n  \\n  {\"source_entity\": \"Kubelet\", \\n   \"description\": \"start terminating pod\", \\n   \"destination_entity\": \"pod deletion\"},\\n  \\n  {\"source_entity\": \"node\", \\n   \"description\": \"report its and its pod statuses again\", \\n   \"destination_entity\": \"control plane\"},\\n  \\n  {\"source_entity\": \"node\", \\n   \"description\": \"come back online\", \\n   \"destination_entity\": \"control plane\"},\\n  \\n  {\"source_entity\": \"Kubelet\", \\n   \"description\": \"reach master\", \\n   \"destination_entity\": \"master\"},\\n  \\n  {\"source_entity\": \"pod\", \\n   \"description\": \"keep running\", \\n   \"destination_entity\": \"node\"},\\n  \\n  {\"source_entity\": \"ifconfig eth0 down\", \\n   \"description\": \"stop working\", \\n   \"destination_entity\": \"ssh session\"},\\n  \\n  {\"source_entity\": \"Kubelet\", \\n   \"description\": \"no longer reach master\", \\n   \"destination_entity\": \"pod deletion\"},\\n  \\n  {\"source_entity\": \"node failures\", \\n   \"description\": \"deal with stateful sets\", \\n   \"destination_entity\": \"StatefulSets\"}\\n]\\n```'},\n",
       " {'page': 338,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '306\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\n Let’s examine the current situation. Use kubectl describe to display details about\\nthe kubia-0 pod, as shown in the following listing.\\n$ kubectl describe po kubia-0\\nName:        kubia-0\\nNamespace:   default\\nNode:        gke-kubia-default-pool-32a2cac8-m0g1/10.132.0.2\\n...\\nStatus:      Terminating (expires Tue, 23 May 2017 15:06:09 +0200)\\nReason:      NodeLost\\nMessage:     Node gke-kubia-default-pool-32a2cac8-m0g1 which was \\n             running pod kubia-0 is unresponsive\\nThe pod is shown as Terminating, with NodeLost listed as the reason for the termina-\\ntion. The message says the node is considered lost because it’s unresponsive.\\nNOTE\\nWhat’s shown here is the control plane’s view of the world. In reality,\\nthe pod’s container is still running perfectly fine. It isn’t terminating at all.\\n10.5.2 Deleting the pod manually\\nYou know the node isn’t coming back, but you need all three pods running to handle\\nclients properly. You need to get the kubia-0 pod rescheduled to a healthy node. As\\nmentioned earlier, you need to delete the node or the pod manually. \\nDELETING THE POD IN THE USUAL WAY\\nDelete the pod the way you’ve always deleted pods:\\n$ kubectl delete po kubia-0\\npod \"kubia-0\" deleted\\nAll done, right? By deleting the pod, the StatefulSet should immediately create a\\nreplacement pod, which will get scheduled to one of the remaining nodes. List the\\npods again to confirm: \\n$ kubectl get po\\nNAME      READY     STATUS    RESTARTS   AGE\\nkubia-0   1/1       Unknown   0          15m\\nkubia-1   1/1       Running   0          14m\\nkubia-2   1/1       Running   0          13m\\nThat’s strange. You deleted the pod a moment ago and kubectl said it had deleted it.\\nWhy is the same pod still there? \\nNOTE\\nThe kubia-0 pod in the listing isn’t a new pod with the same name—\\nthis is clear by looking at the AGE column. If it were new, its age would be\\nmerely a few seconds.\\nListing 10.14\\nDisplaying details of the pod with the unknown status\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'describe',\n",
       "    'description': 'Option used to display details about a pod',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'po',\n",
       "    'description': \"Short form of 'pod' used in kubectl commands\",\n",
       "    'category': 'keyword'},\n",
       "   {'entity': 'kubia-0',\n",
       "    'description': 'Name of the pod being examined',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'Kubernetes concept for deploying replicated stateful applications',\n",
       "    'category': 'framework'},\n",
       "   {'entity': 'NodeLost',\n",
       "    'description': 'Reason for the termination of a pod',\n",
       "    'category': 'error'},\n",
       "   {'entity': 'kubectl delete',\n",
       "    'description': 'Command used to delete a pod',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'Kubernetes concept for deploying replicated stateful applications',\n",
       "    'category': 'framework'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Basic execution unit in Kubernetes',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kubia-0',\n",
       "    'description': 'Name of the pod being examined',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kubectl get',\n",
       "    'description': 'Command used to list pods',\n",
       "    'category': 'command'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"use to display details about the kubia-0 pod\",\\n    \"destination_entity\": \"kubia-0\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"deploying replicated stateful applications is done by StatefulSets\",\\n    \"destination_entity\": null\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"display the current situation of the kubia-0 pod\",\\n    \"destination_entity\": \"kubia-0\"\\n  },\\n  {\\n    \"source_entity\": \"NodeLost\",\\n    \"description\": \"is the reason for the termination of the kubia-0 pod\",\\n    \"destination_entity\": \"kubectl\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl delete\",\\n    \"description\": \"delete the kubia-0 pod manually\",\\n    \"destination_entity\": \"kubia-0\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl get\",\\n    \"description\": \"list all pods, including the kubia-0 pod with unknown status\",\\n    \"destination_entity\": \"kubia-0\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl describe\",\\n    \"description\": \"display details about a pod (in this case, kubia-0)\",\\n    \"destination_entity\": \"kubia-0\"\\n  }\\n]\\n\\nNote: The `destination_entity` for the relation with `StatefulSet` is set to `null` because StatefulSet is not an entity that directly interacts with another entity in this context. It\\'s more of a concept or a category.'},\n",
       " {'page': 339,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '307\\nSummary\\nUNDERSTANDING WHY THE POD ISN’T DELETED\\nThe pod was marked for deletion even before you deleted it. That’s because the con-\\ntrol plane itself already deleted it (in order to evict it from the node). \\n If you look at listing 10.14 again, you’ll see that the pod’s status is Terminating.\\nThe pod was already marked for deletion earlier and will be removed as soon as the\\nKubelet on its node notifies the API server that the pod’s containers have terminated.\\nBecause the node’s network is down, this will never happen. \\nFORCIBLY DELETING THE POD\\nThe only thing you can do is tell the API server to delete the pod without waiting for\\nthe Kubelet to confirm that the pod is no longer running. You do that like this:\\n$ kubectl delete po kubia-0 --force --grace-period 0\\nwarning: Immediate deletion does not wait for confirmation that the running \\nresource has been terminated. The resource may continue to run on the \\ncluster indefinitely.\\npod \"kubia-0\" deleted\\nYou need to use both the --force and --grace-period 0 options. The warning dis-\\nplayed by kubectl notifies you of what you did. If you list the pods again, you’ll finally\\nsee a new kubia-0 pod created:\\n$ kubectl get po\\nNAME          READY     STATUS              RESTARTS   AGE\\nkubia-0       0/1       ContainerCreating   0          8s\\nkubia-1       1/1       Running             0          20m\\nkubia-2       1/1       Running             0          19m\\nWARNING\\nDon’t delete stateful pods forcibly unless you know the node is no\\nlonger running or is unreachable (and will remain so forever). \\nBefore continuing, you may want to bring the node you disconnected back online.\\nYou can do that by restarting the node through the GCE web console or in a terminal\\nby issuing the following command:\\n$ gcloud compute instances reset <node name>\\n10.6\\nSummary\\nThis concludes the chapter on using StatefulSets to deploy stateful apps. This chapter\\nhas shown you how to\\n\\uf0a1Give replicated pods individual storage\\n\\uf0a1Provide a stable identity to a pod\\n\\uf0a1Create a StatefulSet and a corresponding headless governing Service\\n\\uf0a1Scale and update a StatefulSet\\n\\uf0a1Discover other members of the StatefulSet through DNS\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A pod is the basic execution unit in Kubernetes.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'The Kubelet is an agent that runs on each node in a cluster and is responsible for scheduling containers and pods.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': \"The API server is the central component of the Kubernetes control plane, responsible for storing and retrieving data about the cluster's resources.\",\n",
       "    'category': 'service'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'A node is a machine in a Kubernetes cluster that runs pods and serves as a worker for the control plane.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Kubernetes Control Plane',\n",
       "    'description': 'The control plane is responsible for managing the cluster, including scheduling workloads, managing network policies, and providing logging and monitoring.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'A StatefulSet is a Kubernetes resource that manages stateful applications, providing a stable identity to each pod in the set.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Headless Service',\n",
       "    'description': 'A headless service is a type of Kubernetes service that does not have an IP address associated with it, typically used for DNS-based discovery and communication within a StatefulSet.',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'DNS',\n",
       "    'description': 'DNS (Domain Name System) is a system used to map human-readable domain names to IP addresses, allowing resources to be accessed by name rather than IP address.',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'Stateful Apps',\n",
       "    'description': 'Stateful applications are those that maintain state across multiple requests and interactions with users.',\n",
       "    'category': 'application'},\n",
       "   {'entity': '--force option',\n",
       "    'description': 'The --force option is used to immediately delete a resource without waiting for confirmation from the Kubelet, potentially leaving resources running indefinitely.',\n",
       "    'category': 'command'},\n",
       "   {'entity': '--grace-period option',\n",
       "    'description': 'The --grace-period option sets the time period during which Kubernetes will wait before deleting a resource after it has been marked for deletion.',\n",
       "    'category': 'command'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Control Plane\",\\n    \"description\": \"already deleted the pod from node\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"notifies API server that pod\\'s containers have terminated\",\\n    \"destination_entity\": \"API Server\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"deletes the pod without waiting for Kubelet confirmation\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl delete command\",\\n    \"description\": \"forces deletion of pod with --force option and --grace-period 0\",\\n    \"destination_entity\": \"API Server\"\\n  },\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"removes pod\\'s containers on node\",\\n    \"destination_entity\": \"Node\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"provides individual storage to pods\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"provides stable identity to pods\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"DNS\",\\n    \"description\": \"allows discovery of other members in StatefulSet\",\\n    \"destination_entity\": \"StatefulSet\"\\n  },\\n  {\\n    \"source_entity\": \"Headless Service\",\\n    \"description\": \"corresponds to StatefulSet and allows scaling and updating\",\\n    \"destination_entity\": \"StatefulSet\"\\n  },\\n  {\\n    \"source_entity\": \"Node\",\\n    \"description\": \"has network issues, preventing Kubelet from notifying API Server\",\\n    \"destination_entity\": \"API Server\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes Control Plane\",\\n    \"description\": \"evicts pod from node due to termination request\",\\n    \"destination_entity\": \"Pod\"\\n  }\\n]\\n```'},\n",
       " {'page': 340,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '308\\nCHAPTER 10\\nStatefulSets: deploying replicated stateful applications\\n\\uf0a1Connect to other members through their host names\\n\\uf0a1Forcibly delete stateful pods\\nNow that you know the major building blocks you can use to have Kubernetes run and\\nmanage your apps, we can look more closely at how it does that. In the next chapter,\\nyou’ll learn about the individual components that control the Kubernetes cluster and\\nkeep your apps running.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'StatefulSets',\n",
       "    'description': 'Replicated stateful applications deployment mechanism in Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Basic execution unit in Kubernetes, can contain one or more containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Host names',\n",
       "    'description': 'Unique identifier for a host machine',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Stateful pods',\n",
       "    'description': 'Persistent state is maintained across restarts of the pod',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"StatefulSets\", \"description\": \"deploying replicated applications\", \"destination_entity\": \"applications\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"run and manage apps\", \"destination_entity\": \"apps\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"forcibly delete stateful pods\", \"destination_entity\": \"stateful pods\"},\\n  {\"source_entity\": \"Members\", \"description\": \"connect through host names\", \"destination_entity\": \"host names\"}\\n]'},\n",
       " {'page': 341,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '309\\nUnderstanding\\nKubernetes internals\\nBy reading this book up to this point, you’ve become familiar with what Kubernetes\\nhas to offer and what it does. But so far, I’ve intentionally not spent much time\\nexplaining exactly how it does all this because, in my opinion, it makes no sense to\\ngo into details of how a system works until you have a good understanding of what\\nthe system does. That’s why we haven’t talked about exactly how a pod is scheduled\\nor how the various controllers running inside the Controller Manager make deployed\\nresources come to life. Because you now know most resources that can be deployed in\\nKubernetes, it’s time to dive into how they’re implemented.\\nThis chapter covers\\n\\uf0a1What components make up a Kubernetes cluster\\n\\uf0a1What each component does and how it does it\\n\\uf0a1How creating a Deployment object results in a \\nrunning pod\\n\\uf0a1What a running pod is\\n\\uf0a1How the network between pods works\\n\\uf0a1How Kubernetes Services work\\n\\uf0a1How high-availability is achieved\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Lightweight and ephemeral container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'Component responsible for running controllers',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Deployment object',\n",
       "    'description': 'Resource that automates the deployment of applications',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Kubernetes cluster',\n",
       "    'description': 'Group of machines that run Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'Resource that provides a network interface to access an application',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Network',\n",
       "    'description': 'Communication mechanism between pods',\n",
       "    'category': 'network'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"makes up a cluster\", \"destination_entity\": \"Kubernetes cluster\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"implements deployed resources\", \"destination_entity\": \"Deployment object\"},\\n  {\"source_entity\": \"Controller Manager\", \"description\": \"makes deployed resources come to life\", \"destination_entity\": \"Deployment object\"},\\n  {\"source_entity\": \"Deployment object\", \"description\": \"results in a running pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Kubernetes cluster\", \"description\": \"contains various components\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"Deployment object\", \"description\": \"results in a running pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Controller Manager\", \"description\": \"runs inside the Controller Manager\", \"destination_entity\": \"Kubernetes cluster\"},\\n  {\"source_entity\": \"Service\", \"description\": \"works with the network between pods\", \"destination_entity\": \"Network\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"has a running network\", \"destination_entity\": \"Network\"},\\n  {\"source_entity\": \"Kubernetes cluster\", \"description\": \"achieves high-availability\", \"destination_entity\": \"Service\"}\\n]'},\n",
       " {'page': 342,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '310\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\n11.1\\nUnderstanding the architecture\\nBefore you look at how Kubernetes does what it does, let’s take a closer look at the\\ncomponents that make up a Kubernetes cluster. In chapter 1, you saw that a Kuberne-\\ntes cluster is split into two parts:\\n\\uf0a1The Kubernetes Control Plane\\n\\uf0a1The (worker) nodes\\nLet’s look more closely at what these two parts do and what’s running inside them.\\nCOMPONENTS OF THE CONTROL PLANE\\nThe Control Plane is what controls and makes the whole cluster function. To refresh\\nyour memory, the components that make up the Control Plane are\\n\\uf0a1The etcd distributed persistent storage\\n\\uf0a1The API server\\n\\uf0a1The Scheduler\\n\\uf0a1The Controller Manager\\nThese components store and manage the state of the cluster, but they aren’t what runs\\nthe application containers. \\nCOMPONENTS RUNNING ON THE WORKER NODES\\nThe task of running your containers is up to the components running on each\\nworker node:\\n\\uf0a1The Kubelet\\n\\uf0a1The Kubernetes Service Proxy (kube-proxy)\\n\\uf0a1The Container Runtime (Docker, rkt, or others)\\nADD-ON COMPONENTS\\nBeside the Control Plane components and the components running on the nodes, a\\nfew add-on components are required for the cluster to provide everything discussed\\nso far. This includes\\n\\uf0a1The Kubernetes DNS server\\n\\uf0a1The Dashboard\\n\\uf0a1An Ingress controller\\n\\uf0a1Heapster, which we’ll talk about in chapter 14\\n\\uf0a1The Container Network Interface network plugin (we’ll explain it later in this\\nchapter)\\n11.1.1 The distributed nature of Kubernetes components\\nThe previously mentioned components all run as individual processes. The compo-\\nnents and their inter-dependencies are shown in figure 11.1.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes Control Plane',\n",
       "    'description': 'The part of a Kubernetes cluster that controls and makes it function.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'etcd distributed persistent storage',\n",
       "    'description': 'A component of the Control Plane that stores and manages the state of the cluster.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'A component of the Control Plane that handles API requests to the cluster.',\n",
       "    'category': 'server'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'A component of the Control Plane that schedules containers on nodes.',\n",
       "    'category': 'scheduler'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'A component of the Control Plane that manages and updates container states.',\n",
       "    'category': 'controller'},\n",
       "   {'entity': 'worker node',\n",
       "    'description': 'The part of a Kubernetes cluster where application containers run.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'A component running on each worker node that runs containers and provides a way to communicate with the API server.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubernetes Service Proxy (kube-proxy)',\n",
       "    'description': 'A component running on each worker node that manages network traffic for services.',\n",
       "    'category': 'proxy'},\n",
       "   {'entity': 'Container Runtime',\n",
       "    'description': 'A component running on each worker node that runs containers, e.g., Docker or rkt.',\n",
       "    'category': 'container engine'},\n",
       "   {'entity': 'Kubernetes DNS server',\n",
       "    'description': 'An add-on component that provides a DNS service for the cluster.',\n",
       "    'category': 'server'},\n",
       "   {'entity': 'Dashboard',\n",
       "    'description': 'An add-on component that provides a web-based interface to interact with the cluster.',\n",
       "    'category': 'web application'},\n",
       "   {'entity': 'Ingress controller',\n",
       "    'description': 'An add-on component that manages incoming HTTP requests and routes them to the appropriate service.',\n",
       "    'category': 'controller'},\n",
       "   {'entity': 'Heapster',\n",
       "    'description': 'An add-on component that provides monitoring and logging capabilities for the cluster.',\n",
       "    'category': 'monitoring tool'},\n",
       "   {'entity': 'Container Network Interface network plugin',\n",
       "    'description': 'An add-on component that provides a networking solution for the cluster.',\n",
       "    'category': 'network plugin'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"etcd distributed persistent storage\",\\n    \"description\": \"stores and manages cluster state\",\\n    \"destination_entity\": \"Kubernetes Control Plane\"\\n  },\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"handles incoming requests to the control plane\",\\n    \"destination_entity\": \"Kubernetes Control Plane\"\\n  },\\n  {\\n    \"source_entity\": \"Scheduler\",\\n    \"description\": \"schedules workload on available nodes\",\\n    \"destination_entity\": \"worker node\"\\n  },\\n  {\\n    \"source_entity\": \"Controller Manager\",\\n    \"description\": \"manages and orchestrates containers\",\\n    \"destination_entity\": \"Kubernetes Control Plane\"\\n  },\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"runs and manages application containers on worker nodes\",\\n    \"destination_entity\": \"worker node\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes Service Proxy (kube-proxy)\",\\n    \"description\": \"manages network traffic between services\",\\n    \"destination_entity\": \"Kubernetes Control Plane\"\\n  },\\n  {\\n    \"source_entity\": \"Container Runtime\",\\n    \"description\": \"runs containers on worker nodes\",\\n    \"destination_entity\": \"worker node\"\\n  },\\n  {\\n    \"source_entity\": \"Ingress controller\",\\n    \"description\": \"manages incoming traffic to the cluster\",\\n    \"destination_entity\": \"Kubernetes Control Plane\"\\n  },\\n  {\\n    \"source_entity\": \"Heapster\",\\n    \"description\": \"monitors and manages container resource usage\",\\n    \"destination_entity\": \"Kubernetes Control Plane\"\\n  },\\n  {\\n    \"source_entity\": \"Dashboard\",\\n    \"description\": \"provides a web-based interface for cluster management\",\\n    \"destination_entity\": \"Kubernetes Control Plane\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes DNS server\",\\n    \"description\": \"manages and resolves service names within the cluster\",\\n    \"destination_entity\": \"Kubernetes Control Plane\"\\n  },\\n  {\\n    \"source_entity\": \"Container Network Interface network plugin\",\\n    \"description\": \"manages container networking on worker nodes\",\\n    \"destination_entity\": \"worker node\"\\n  }\\n]'},\n",
       " {'page': 343,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '311\\nUnderstanding the architecture\\nTo get all the features Kubernetes provides, all these components need to be running.\\nBut several can also perform useful work individually without the other components.\\nYou’ll see how as we examine each of them.\\nHOW THESE COMPONENTS COMMUNICATE\\nKubernetes system components communicate only with the API server. They don’t\\ntalk to each other directly. The API server is the only component that communicates\\nwith etcd. None of the other components communicate with etcd directly, but instead\\nmodify the cluster state by talking to the API server.\\n Connections between the API server and the other components are almost always\\ninitiated by the components, as shown in figure 11.1. But the API server does connect\\nto the Kubelet when you use kubectl to fetch logs, use kubectl attach to connect to\\na running container, or use the kubectl port-forward command.\\nNOTE\\nThe kubectl attach command is similar to kubectl exec, but it attaches\\nto the main process running in the container instead of running an addi-\\ntional one.\\nRUNNING MULTIPLE INSTANCES OF INDIVIDUAL COMPONENTS\\nAlthough the components on the worker nodes all need to run on the same node,\\nthe components of the Control Plane can easily be split across multiple servers. There\\nChecking the status of the Control Plane components\\nThe API server exposes an API resource called ComponentStatus, which shows the\\nhealth status of each Control Plane component. You can list the components and\\ntheir statuses with kubectl:\\n$ kubectl get componentstatuses\\nNAME                 STATUS    MESSAGE              ERROR\\nscheduler            Healthy   ok\\ncontroller-manager   Healthy   ok\\netcd-0               Healthy   {\"health\": \"true\"}\\nControl Plane (master node)\\nWorker node(s)\\netcd\\nAPI server\\nkube-proxy\\nKubelet\\nScheduler\\nController\\nManager\\nController\\nRuntime\\nFigure 11.1\\nKubernetes \\ncomponents of the Control \\nPlane and the worker nodes\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Control Plane (master node) Worker node(s)\\nkube-proxy\\netcd API server\\nKubelet\\nController\\nScheduler\\nManager Controller\\nRuntime  \\\n",
       "   0                                               None                                                                                     \n",
       "   1  Control Plane (master node)\\netcd API server\\n...                                                                                     \n",
       "   2                                               None                                                                                     \n",
       "   3                                               None                                                                                     \n",
       "   4                                               None                                                                                     \n",
       "   \n",
       "      Col1  Col2 Worker node(s)\\nkube-proxy\\nKubelet\\nController\\nRuntime  Col4  \n",
       "   0  None        Worker node(s)\\nkube-proxy\\nKubelet\\nControlle...        None  \n",
       "   1              Worker node(s)\\nkube-proxy\\nKubelet\\nControlle...        None  \n",
       "   2                                                           None        None  \n",
       "   3  None  None                                Controller\\nRuntime        None  \n",
       "   4  None  None                                               None              ],\n",
       "  'entities': [{'entity': 'API Server',\n",
       "    'description': 'The component that communicates with etcd and initiates connections to other components.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'A distributed key-value store used by Kubernetes to store its cluster state.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'A component that runs on each worker node and is responsible for managing containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'A component that schedules container workloads to run on specific nodes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'A component that manages the state of the cluster and performs actions based on that state.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kube-proxy',\n",
       "    'description': 'A component that provides network proxying and load balancing for containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool used to interact with the Kubernetes API server.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubeadm', 'description': '', 'category': ''},\n",
       "   {'entity': 'etcd-0',\n",
       "    'description': 'A instance of etcd running on a specific node.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ComponentStatus',\n",
       "    'description': 'An API resource used to display the health status of each Control Plane component.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"communicates with etcd indirectly by modifying cluster state through API server\",\\n    \"destination_entity\": \"etcd\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"communicates directly with other components, initiated by components themselves\",\\n    \"destination_entity\": \"Kube-proxy\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"communicates directly with other components, initiated by components themselves\",\\n    \"destination_entity\": \"Kubelet\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"communicates directly with other components, initiated by components themselves\",\\n    \"destination_entity\": \"Controller Manager\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"exposes ComponentStatus API resource to show health status of each Control Plane component\",\\n    \"destination_entity\": \"ComponentStatus\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"fetches logs from Kubelet, uses kubectl attach to connect to running container, or uses kubectl port-forward command\",\\n    \"destination_entity\": \"API Server\"\\n  },\\n  {\\n    \"source_entity\": \"kubeadm\",\\n    \"description\": \"not explicitly mentioned in the document, but as an entity, it might be related to API Server or etcd-0\",\\n    \"destination_entity\": \"API Server\"\\n  },\\n  {\\n    \"source_entity\": \"etcd-0\",\\n    \"description\": \"health status shown by ComponentStatus API resource exposed by API Server\",\\n    \"destination_entity\": \"ComponentStatus\"\\n  },\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"communicates directly with other components, initiated by components themselves\",\\n    \"destination_entity\": \"API Server\"\\n  },\\n  {\\n    \"source_entity\": \"Controller Manager\",\\n    \"description\": \"communicates indirectly with etcd through API server\",\\n    \"destination_entity\": \"etcd\"\\n  },\\n  {\\n    \"source_entity\": \"Scheduler\",\\n    \"description\": \"not explicitly mentioned in the document, but as an entity, it might be related to API Server or Controller Manager\",\\n    \"destination_entity\": \"API Server\"\\n  }\\n]\\n```'},\n",
       " {'page': 344,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '312\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\ncan be more than one instance of each Control Plane component running to ensure\\nhigh availability. While multiple instances of etcd and API server can be active at the\\nsame time and do perform their jobs in parallel, only a single instance of the Sched-\\nuler and the Controller Manager may be active at a given time—with the others in\\nstandby mode.\\nHOW COMPONENTS ARE RUN\\nThe Control Plane components, as well as kube-proxy, can either be deployed on the\\nsystem directly or they can run as pods (as shown in listing 11.1). You may be surprised\\nto hear this, but it will all make sense later when we talk about the Kubelet. \\n The Kubelet is the only component that always runs as a regular system compo-\\nnent, and it’s the Kubelet that then runs all the other components as pods. To run the\\nControl Plane components as pods, the Kubelet is also deployed on the master. The\\nnext listing shows pods in the kube-system namespace in a cluster created with\\nkubeadm, which is explained in appendix B.\\n$ kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName \\n➥ --sort-by spec.nodeName -n kube-system\\nPOD                              NODE\\nkube-controller-manager-master   master      \\nkube-dns-2334855451-37d9k        master      \\netcd-master                      master      \\nkube-apiserver-master            master      \\nkube-scheduler-master            master      \\nkube-flannel-ds-tgj9k            node1      \\nkube-proxy-ny3xm                 node1      \\nkube-flannel-ds-0eek8            node2      \\nkube-proxy-sp362                 node2      \\nkube-flannel-ds-r5yf4            node3      \\nkube-proxy-og9ac                 node3      \\nAs you can see in the listing, all the Control Plane components are running as pods on\\nthe master node. There are three worker nodes, and each one runs the kube-proxy\\nand a Flannel pod, which provides the overlay network for the pods (we’ll talk about\\nFlannel later). \\nTIP\\nAs shown in the listing, you can tell kubectl to display custom columns\\nwith the -o custom-columns option and sort the resource list with --sort-by.\\nNow, let’s look at each of the components up close, starting with the lowest level com-\\nponent of the Control Plane—the persistent storage.\\n11.1.2 How Kubernetes uses etcd\\nAll the objects you’ve created throughout this book—Pods, ReplicationControllers,\\nServices, Secrets, and so on—need to be stored somewhere in a persistent manner so\\ntheir manifests survive API server restarts and failures. For this, Kubernetes uses etcd,\\nListing 11.1\\nKubernetes components running as pods\\netcd, API server, Scheduler, \\nController Manager, and \\nthe DNS server are running \\non the master.\\nThe three nodes each run \\na Kube Proxy pod and a \\nFlannel networking pod.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Control Plane',\n",
       "    'description': 'Components that manage the cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'Distributed key-value store for persistent storage',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Server that exposes the Kubernetes API',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'Component that schedules pods to run on nodes',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'Component that manages the control plane components',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'Agent that runs on each node and manages pod lifecycle',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kube-proxy',\n",
       "    'description': 'Proxy server that provides network connectivity for pods',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Flannel',\n",
       "    'description': 'Networking component that provides an overlay network for pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Lightweight and portable container runtime',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ReplicationControllers',\n",
       "    'description': 'Component that manages pod replicas',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'Abstracted networking concept for accessing pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'Component that stores sensitive information',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with the Kubernetes API',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubeadm',\n",
       "    'description': 'Tool for creating and managing Kubernetes clusters',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"uses to store objects persistently\", \"destination_entity\": \"etcd\"},\\n  {\"source_entity\": \"Control Plane\", \"description\": \"has Scheduler and Controller Manager running simultaneously\", \"destination_entity\": \"Scheduler\"},\\n  {\"source_entity\": \"Control Plane\", \"description\": \"has Scheduler and Controller Manager running simultaneously\", \"destination_entity\": \"Controller Manager\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"runs Control Plane components as pods\", \"destination_entity\": \"Control Plane\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"can have multiple instances of each component for high availability\", \"destination_entity\": \"etcd\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"can have multiple instances of each component for high availability\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"runs Kube-proxy as a pod\", \"destination_entity\": \"kube-proxy\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"uses kubectl to display custom columns and sort resource lists\", \"destination_entity\": \"kubectl\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"uses kubeadm to create clusters\", \"destination_entity\": \"kubeadm\"},\\n  {\"source_entity\": \"Control Plane\", \"description\": \"has kube-apiserver running on the master node\", \"destination_entity\": \"kube-apiserver\"},\\n  {\"source_entity\": \"Control Plane\", \"description\": \"has etcd running on the master node\", \"destination_entity\": \"etcd\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"uses etcd to store objects persistently for ReplicationControllers, Services, and Secrets\", \"destination_entity\": \"ReplicationControllers\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"uses etcd to store objects persistently for Pods, ReplicationControllers, Services, and Secrets\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"uses etcd to store objects persistently for Pods, ReplicationControllers, Services, and Secrets\", \"destination_entity\": \"Secrets\"}\\n]\\n```'},\n",
       " {'page': 345,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '313\\nUnderstanding the architecture\\nwhich is a fast, distributed, and consistent key-value store. Because it’s distributed,\\nyou can run more than one etcd instance to provide both high availability and bet-\\nter performance.\\n The only component that talks to etcd directly is the Kubernetes API server. All\\nother components read and write data to etcd indirectly through the API server. This\\nbrings a few benefits, among them a more robust optimistic locking system as well as\\nvalidation; and, by abstracting away the actual storage mechanism from all the other\\ncomponents, it’s much simpler to replace it in the future. It’s worth emphasizing that\\netcd is the only place Kubernetes stores cluster state and metadata.\\nHOW RESOURCES ARE STORED IN ETCD\\nAs I’m writing this, Kubernetes can use either etcd version 2 or version 3, but version 3\\nis now recommended because of improved performance. etcd v2 stores keys in a hier-\\narchical key space, which makes key-value pairs similar to files in a file system. Each\\nkey in etcd is either a directory, which contains other keys, or is a regular key with a\\ncorresponding value. etcd v3 doesn’t support directories, but because the key format\\nremains the same (keys can include slashes), you can still think of them as being\\ngrouped into directories. Kubernetes stores all its data in etcd under /registry. The\\nfollowing listing shows a list of keys stored under /registry.\\n$ etcdctl ls /registry\\n/registry/configmaps\\n/registry/daemonsets\\n/registry/deployments\\n/registry/events\\n/registry/namespaces\\n/registry/pods\\n...\\nAbout optimistic concurrency control\\nOptimistic concurrency control (sometimes referred to as optimistic locking) is a\\nmethod where instead of locking a piece of data and preventing it from being read or\\nupdated while the lock is in place, the piece of data includes a version number. Every\\ntime the data is updated, the version number increases. When updating the data, the\\nversion number is checked to see if it has increased between the time the client read\\nthe data and the time it submits the update. If this happens, the update is rejected\\nand the client must re-read the new data and try to update it again. \\nThe result is that when two clients try to update the same data entry, only the first\\none succeeds.\\nAll Kubernetes resources include a metadata.resourceVersion field, which clients\\nneed to pass back to the API server when updating an object. If the version doesn’t\\nmatch the one stored in etcd, the API server rejects the update.\\nListing 11.2\\nTop-level entries stored in etcd by Kubernetes\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'etcd',\n",
       "    'description': 'a fast, distributed, and consistent key-value store',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Kubernetes API server',\n",
       "    'description': 'component that talks to etcd directly',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'abstracts away the actual storage mechanism from all components',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'etcdctl',\n",
       "    'description': 'tool for managing etcd cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': '/registry',\n",
       "    'description': 'directory where Kubernetes stores all its data in etcd',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'configmaps',\n",
       "    'description': 'resource stored under /registry/configmaps',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'daemonsets',\n",
       "    'description': 'resource stored under /registry/daemonsets',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'deployments',\n",
       "    'description': 'resource stored under /registry/deployments',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'events',\n",
       "    'description': 'resource stored under /registry/events',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'namespaces',\n",
       "    'description': 'resource stored under /registry/namespaces',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'resource stored under /registry/pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'optimistic concurrency control',\n",
       "    'description': 'method for preventing concurrent updates to the same data entry',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'metadata.resourceVersion',\n",
       "    'description': 'field that clients need to pass back to API server when updating an object',\n",
       "    'category': 'database'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"etcd\",\\n    \"description\": \"stores keys in a hierarchical key space\",\\n    \"destination_entity\": \"keys\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes API server\",\\n    \"description\": \"talks to etcd directly\",\\n    \"destination_entity\": \"etcd\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes API server\",\\n    \"description\": \"abstracts away the actual storage mechanism from other components\",\\n    \"destination_entity\": \"other components\"\\n  },\\n  {\\n    \"source_entity\": \"etcd\",\\n    \"description\": \"provides a robust optimistic locking system\",\\n    \"destination_entity\": \"Kubernetes resources\"\\n  },\\n  {\\n    \"source_entity\": \"etcd\",\\n    \"description\": \"validates Kubernetes resources\",\\n    \"destination_entity\": \"Kubernetes resources\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes API server\",\\n    \"description\": \"stores cluster state and metadata in etcd\",\\n    \"destination_entity\": \"etcd\"\\n  },\\n  {\\n    \"source_entity\": \"etcd\",\\n    \"description\": \"supports high availability and better performance\",\\n    \"destination_entity\": \"Kubernetes components\"\\n  },\\n  {\\n    \"source_entity\": \"kubernetes resources\",\\n    \"description\": \"include a metadata.resourceVersion field\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"optimistic concurrency control\",\\n    \"description\": \"checks version number to prevent concurrent updates\",\\n    \"destination_entity\": \"Kubernetes resources\"\\n  },\\n  {\\n    \"source_entity\": \"etcdctl\",\\n    \"description\": \"lists top-level entries stored in etcd by Kubernetes\",\\n    \"destination_entity\": \"Kubernetes resources\"\\n  }\\n]\\n```'},\n",
       " {'page': 346,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '314\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\nYou’ll recognize that these keys correspond to the resource types you learned about in\\nthe previous chapters. \\nNOTE\\nIf you’re using v3 of the etcd API, you can’t use the ls command to see\\nthe contents of a directory. Instead, you can list all keys that start with a given\\nprefix with etcdctl get /registry --prefix=true.\\nThe following listing shows the contents of the /registry/pods directory.\\n$ etcdctl ls /registry/pods\\n/registry/pods/default\\n/registry/pods/kube-system\\nAs you can infer from the names, these two entries correspond to the default and the\\nkube-system namespaces, which means pods are stored per namespace. The follow-\\ning listing shows the entries in the /registry/pods/default directory.\\n$ etcdctl ls /registry/pods/default\\n/registry/pods/default/kubia-159041347-xk0vc\\n/registry/pods/default/kubia-159041347-wt6ga\\n/registry/pods/default/kubia-159041347-hp2o5\\nEach entry corresponds to an individual pod. These aren’t directories, but key-value\\nentries. The following listing shows what’s stored in one of them.\\n$ etcdctl get /registry/pods/default/kubia-159041347-wt6ga\\n{\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"kubia-159041347-wt6ga\",\\n\"generateName\":\"kubia-159041347-\",\"namespace\":\"default\",\"selfLink\":...\\nYou’ll recognize that this is nothing other than a pod definition in JSON format. The\\nAPI server stores the complete JSON representation of a resource in etcd. Because of\\netcd’s hierarchical key space, you can think of all the stored resources as JSON files in\\na filesystem. Simple, right?\\nWARNING\\nPrior to Kubernetes version 1.7, the JSON manifest of a Secret\\nresource was also stored like this (it wasn’t encrypted). If someone got direct\\naccess to etcd, they knew all your Secrets. From version 1.7, Secrets are\\nencrypted and thus stored much more securely.\\nENSURING THE CONSISTENCY AND VALIDITY OF STORED OBJECTS\\nRemember Google’s Borg and Omega systems mentioned in chapter 1, which are\\nwhat Kubernetes is based on? Like Kubernetes, Omega also uses a centralized store to\\nhold the state of the cluster, but in contrast, multiple Control Plane components\\naccess the store directly. All these components need to make sure they all adhere to\\nListing 11.3\\nKeys in the /registry/pods directory\\nListing 11.4\\netcd entries for pods in the default namespace\\nListing 11.5\\nAn etcd entry representing a pod\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'Distributed key-value store',\n",
       "    'category': 'software'},\n",
       "   {'entity': '/registry/pods',\n",
       "    'description': 'Directory in etcd storing pod resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ls command',\n",
       "    'description': 'Command to list contents of a directory in etcd',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'etcdctl get',\n",
       "    'description': 'Command to retrieve key-value entries from etcd',\n",
       "    'category': 'command'},\n",
       "   {'entity': '/registry/pods/default',\n",
       "    'description': 'Directory in etcd storing pods for default namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Containerized application instance in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'JSON manifest',\n",
       "    'description': 'Serialized representation of a resource in JSON format',\n",
       "    'category': 'format'},\n",
       "   {'entity': '/registry/pods/default/kubia-159041347-wt6ga',\n",
       "    'description': 'Key-value entry in etcd representing a pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes API server',\n",
       "    'description': 'Component responsible for storing resources in etcd',\n",
       "    'category': 'application'},\n",
       "   {'entity': '/registry/pods/default/kubia-159041347-xk0vc',\n",
       "    'description': 'Key-value entry in etcd representing a pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': '/registry/pods/default/kubia-159041347-hp2o5',\n",
       "    'description': 'Key-value entry in etcd representing a pod',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ls command\", \"description\": \"list keys that start with a given prefix\", \"destination_entity\": \"/registry/pods/default\"},\\n  {\"source_entity\": \"/registry/pods/default/kubia-159041347-wt6ga\", \"description\": \"store pod definition in JSON format\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"etcdctl get\", \"description\": \"get the contents of a directory or key-value entry\", \"destination_entity\": \"/registry/pods/default/kubia-159041347-wt6ga\"},\\n  {\"source_entity\": \"etcdctl ls\", \"description\": \"list all keys that start with a given prefix\", \"destination_entity\": \"/registry/pods/default\"},\\n  {\"source_entity\": \"/registry/pods/default\", \"description\": \"store pods per namespace\", \"destination_entity\": \"/registry/pods\"},\\n  {\"source_entity\": \"/registry/pods/default/kubia-159041347-xk0vc\", \"description\": \"store individual pod entries\", \"destination_entity\": \"/registry/pods/default\"},\\n  {\"source_entity\": \"Kubernetes API server\", \"description\": \"store the complete JSON representation of a resource in etcd\", \"destination_entity\": \"etcd\"},\\n  {\"source_entity\": \"/registry/pods/default/kubia-159041347-hp2o5\", \"description\": \"store individual pod entries\", \"destination_entity\": \"/registry/pods/default\"},\\n  {\"source_entity\": \"JSON manifest\", \"description\": \"store Secret resource in etcd like a JSON file\", \"destination_entity\": \"etcd\"}\\n]'},\n",
       " {'page': 347,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '315\\nUnderstanding the architecture\\nthe same optimistic locking mechanism to handle conflicts properly. A single compo-\\nnent not adhering fully to the mechanism may lead to inconsistent data. \\n Kubernetes improves this by requiring all other Control Plane components to go\\nthrough the API server. This way updates to the cluster state are always consistent, because\\nthe optimistic locking mechanism is implemented in a single place, so less chance exists,\\nif any, of error. The API server also makes sure that the data written to the store is always\\nvalid and that changes to the data are only performed by authorized clients. \\nENSURING CONSISTENCY WHEN ETCD IS CLUSTERED\\nFor ensuring high availability, you’ll usually run more than a single instance of etcd.\\nMultiple etcd instances will need to remain consistent. Such a distributed system\\nneeds to reach a consensus on what the actual state is. etcd uses the RAFT consensus\\nalgorithm to achieve this, which ensures that at any given moment, each node’s state is\\neither what the majority of the nodes agrees is the current state or is one of the previ-\\nously agreed upon states. \\n Clients connecting to different nodes of an etcd cluster will either see the actual\\ncurrent state or one of the states from the past (in Kubernetes, the only etcd client is\\nthe API server, but there may be multiple instances). \\n The consensus algorithm requires a majority (or quorum) for the cluster to progress\\nto the next state. As a result, if the cluster splits into two disconnected groups of nodes,\\nthe state in the two groups can never diverge, because to transition from the previous\\nstate to the new one, there needs to be more than half of the nodes taking part in\\nthe state change. If one group contains the majority of all nodes, the other one obvi-\\nously doesn’t. The first group can modify the cluster state, whereas the other one can’t.\\nWhen the two groups reconnect, the second group can catch up with the state in the\\nfirst group (see figure 11.2).\\nClients(s)\\nClients(s)\\nClients(s)\\netcd-0\\netcd-1\\netcd-2\\nThe nodes know\\nthere are three nodes\\nin the etcd cluster.\\netcd-0\\netcd-1\\nThese two nodes know\\nthey still have quorum\\nand can accept state\\nchanges from clients.\\netcd-2\\nThis node knows it does\\nnot have quorum and\\nshould therefore not\\nallow state changes.\\nNetwork\\nsplit\\nFigure 11.2\\nIn a split-brain scenario, only the side which still has the majority (quorum) accepts \\nstate changes.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Control Plane',\n",
       "    'description': 'Components that manage the cluster state',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Centralized entry point for updates to the cluster state',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'Distributed key-value store used by Kubernetes',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'RAFT consensus algorithm',\n",
       "    'description': 'Consensus protocol used by etcd to ensure consistency across nodes',\n",
       "    'category': 'algorithm'},\n",
       "   {'entity': 'quorum',\n",
       "    'description': 'Majority of nodes required for state changes in a distributed system',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'state change',\n",
       "    'description': 'Update to the cluster state, requiring consensus among nodes',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'clients',\n",
       "    'description': 'Applications connecting to the etcd cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'etcd-0',\n",
       "    'description': 'Node in an etcd cluster',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'etcd-1',\n",
       "    'description': 'Node in an etcd cluster',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'etcd-2',\n",
       "    'description': 'Node in an etcd cluster',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[{\"source_entity\": \"etcd\", \"description\": \"uses\", \"destination_entity\": \"RAFT consensus algorithm\"}, \\n {\"source_entity\": \"etcd\", \"description\": \"needs to remain consistent with\", \"destination_entity\": \"other etcd instances\"}, \\n {\"source_entity\": \"RAFT consensus algorithm\", \"description\": \"ensures that each node\\'s state is either the current or a previously agreed-upon state\", \"destination_entity\": \"etcd nodes\"}, \\n {\"source_entity\": \"API server\", \"description\": \"connects to and sees the actual current state of\", \"destination_entity\": \"etcd cluster\"}, \\n {\"source_entity\": \"API server\", \"description\": \"makes sure that data written to the store is always valid\", \"destination_entity\": \"data written to etcd store\"}, \\n {\"source_entity\": \"RAFT consensus algorithm\", \"description\": \"requires a majority (or quorum) for the cluster to progress\", \"destination_entity\": \"etcd nodes\"}, \\n {\"source_entity\": \"API server\", \"description\": \"only allows authorized clients to make changes to\", \"destination_entity\": \"data written to etcd store\"}, \\n {\"source_entity\": \"Control Plane\", \"description\": \"goes through the API server\", \"destination_entity\": \"API server\"}, \\n {\"source_entity\": \"etcd-0\", \"description\": \"has quorum and can accept state changes from clients\", \"destination_entity\": \"clients\"}, \\n {\"source_entity\": \"etcd-2\", \"description\": \"does not have quorum and should not allow state changes from\", \"destination_entity\": \"clients\"}, \\n {\"source_entity\": \"etcd-0\", \"description\": \"can modify the cluster state because it has majority (quorum)\", \"destination_entity\": \"cluster state\"}, \\n {\"source_entity\": \"etcd-1\", \"description\": \"has quorum and can accept state changes from clients\", \"destination_entity\": \"clients\"}]'},\n",
       " {'page': 348,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '316\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\nWHY THE NUMBER OF ETCD INSTANCES SHOULD BE AN ODD NUMBER\\netcd is usually deployed with an odd number of instances. I’m sure you’d like to know\\nwhy. Let’s compare having two vs. having one instance. Having two instances requires\\nboth instances to be present to have a majority. If either of them fails, the etcd cluster\\ncan’t transition to a new state because no majority exists. Having two instances is worse\\nthan having only a single instance. By having two, the chance of the whole cluster fail-\\ning has increased by 100%, compared to that of a single-node cluster failing. \\n The same applies when comparing three vs. four etcd instances. With three instances,\\none instance can fail and a majority (of two) still exists. With four instances, you need\\nthree nodes for a majority (two aren’t enough). In both three- and four-instance clus-\\nters, only a single instance may fail. But when running four instances, if one fails, a\\nhigher possibility exists of an additional instance of the three remaining instances fail-\\ning (compared to a three-node cluster with one failed node and two remaining nodes).\\n Usually, for large clusters, an etcd cluster of five or seven nodes is sufficient. It can\\nhandle a two- or a three-node failure, respectively, which suffices in almost all situations. \\n11.1.3 What the API server does\\nThe Kubernetes API server is the central component used by all other components\\nand by clients, such as kubectl. It provides a CRUD (Create, Read, Update, Delete)\\ninterface for querying and modifying the cluster state over a RESTful API. It stores\\nthat state in etcd.\\n In addition to providing a consistent way of storing objects in etcd, it also performs\\nvalidation of those objects, so clients can’t store improperly configured objects (which\\nthey could if they were writing to the store directly). Along with validation, it also han-\\ndles optimistic locking, so changes to an object are never overridden by other clients\\nin the event of concurrent updates.\\n One of the API server’s clients is the command-line tool kubectl you’ve been\\nusing from the beginning of the book. When creating a resource from a JSON file, for\\nexample, kubectl posts the file’s contents to the API server through an HTTP POST\\nrequest. Figure 11.3 shows what happens inside the API server when it receives the\\nrequest. This is explained in more detail in the next few paragraphs.\\nAPI server\\netcd\\nAuthentication\\nplugin 1\\nAuthentication\\nplugin 2\\nAuthentication\\nplugin 3\\nClient\\n(\\n)\\nkubectl\\nHTTP POST\\nrequest\\nAuthorization\\nplugin 1\\nAuthorization\\nplugin 2\\nAuthorization\\nplugin 3\\nAdmission\\ncontrol plugin 1\\nAdmission\\ncontrol plugin 2\\nAdmission\\ncontrol plugin 3\\nResource\\nvalidation\\nFigure 11.3\\nThe operation of the API server\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [HTTP POST\n",
       "   request\n",
       "   Client\n",
       "   (kubectl), API server\n",
       "   Authentication Authorization Admission\n",
       "   plugin 1 plugin 1 control plugin 1\n",
       "   Authentication Authorization Admission Resource\n",
       "   plugin 2 plugin 2 control plugin 2 validation\n",
       "   Authentication Authorization Admission\n",
       "   plugin 3 plugin 3 control plugin 3, etcd]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'etcd',\n",
       "    'description': 'a distributed key-value store used in Kubernetes',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'an open-source container orchestration system',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'the central component of Kubernetes that provides a CRUD interface over a RESTful API',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'a command-line tool used to interact with the Kubernetes API server',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'HTTP POST request',\n",
       "    'description': 'an HTTP request method used to create a resource in Kubernetes',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'RESTful API',\n",
       "    'description': 'an architectural style for designing networked APIs that follow the principles of resource-based addressing',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'etcd cluster',\n",
       "    'description': 'a group of etcd instances working together to store and retrieve data',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'majority',\n",
       "    'description': 'the minimum number of instances required for an etcd cluster to reach a consensus on a state change',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'API server clients',\n",
       "    'description': 'components that interact with the API server, such as kubectl',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"etcd\",\\n    \"description\": \"should be deployed with an odd number of instances\",\\n    \"destination_entity\": \"instance\"\\n  },\\n  {\\n    \"source_entity\": \"two etcd instances\",\\n    \"description\": \"requires both instances to be present to have a majority\",\\n    \"destination_entity\": \"majority\"\\n  },\\n  {\\n    \"source_entity\": \"etcd cluster with two instances\",\\n    \"description\": \"can\\'t transition to a new state if either instance fails\",\\n    \"destination_entity\": \"state\"\\n  },\\n  {\\n    \"source_entity\": \"three etcd instances\",\\n    \"description\": \"one instance can fail and a majority still exists\",\\n    \"destination_entity\": \"majority\"\\n  },\\n  {\\n    \"source_entity\": \"etcd cluster with four instances\",\\n    \"description\": \"need three nodes for a majority (two aren\\'t enough)\",\\n    \"destination_entity\": \"majority\"\\n  },\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"provides a CRUD interface for querying and modifying the cluster state\",\\n    \"destination_entity\": \"cluster state\"\\n  },\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"stores the cluster state in etcd\",\\n    \"destination_entity\": \"etcd\"\\n  },\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"performs validation of objects stored in etcd\",\\n    \"destination_entity\": \"object\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"posts the file\\'s contents to the API server through an HTTP POST request\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes API server\",\\n    \"description\": \"handles optimistic locking, so changes to an object are never overridden by other clients\",\\n    \"destination_entity\": \"client\"\\n  },\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"authorizes client requests through multiple plugins\",\\n    \"destination_entity\": \"client request\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"uses the API server to create a resource from a JSON file\",\\n    \"destination_entity\": \"resource\"\\n  }\\n]'},\n",
       " {'page': 349,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '317\\nUnderstanding the architecture\\nAUTHENTICATING THE CLIENT WITH AUTHENTICATION PLUGINS\\nFirst, the API server needs to authenticate the client sending the request. This is per-\\nformed by one or more authentication plugins configured in the API server. The API\\nserver calls these plugins in turn, until one of them determines who is sending the\\nrequest. It does this by inspecting the HTTP request. \\n Depending on the authentication method, the user can be extracted from the cli-\\nent’s certificate or an HTTP header, such as Authorization, which you used in chap-\\nter 8. The plugin extracts the client’s username, user ID, and groups the user belongs\\nto. This data is then used in the next stage, which is authorization.\\nAUTHORIZING THE CLIENT WITH AUTHORIZATION PLUGINS\\nBesides authentication plugins, the API server is also configured to use one or more\\nauthorization plugins. Their job is to determine whether the authenticated user can\\nperform the requested action on the requested resource. For example, when creating\\npods, the API server consults all authorization plugins in turn, to determine whether\\nthe user can create pods in the requested namespace. As soon as a plugin says the user\\ncan perform the action, the API server progresses to the next stage.\\nVALIDATING AND/OR MODIFYING THE RESOURCE IN THE REQUEST WITH ADMISSION CONTROL PLUGINS\\nIf the request is trying to create, modify, or delete a resource, the request is sent\\nthrough Admission Control. Again, the server is configured with multiple Admission\\nControl plugins. These plugins can modify the resource for different reasons. They\\nmay initialize fields missing from the resource specification to the configured default\\nvalues or even override them. They may even modify other related resources, which\\naren’t in the request, and can also reject a request for whatever reason. The resource\\npasses through all Admission Control plugins.\\nNOTE\\nWhen the request is only trying to read data, the request doesn’t go\\nthrough the Admission Control.\\nExamples of Admission Control plugins include\\n\\uf0a1\\nAlwaysPullImages—Overrides the pod’s imagePullPolicy to Always, forcing\\nthe image to be pulled every time the pod is deployed.\\n\\uf0a1\\nServiceAccount—Applies the default service account to pods that don’t specify\\nit explicitly.\\n\\uf0a1\\nNamespaceLifecycle—Prevents creation of pods in namespaces that are in the\\nprocess of being deleted, as well as in non-existing namespaces.\\n\\uf0a1\\nResourceQuota—Ensures pods in a certain namespace only use as much CPU\\nand memory as has been allotted to the namespace. We’ll learn more about this\\nin chapter 14.\\nYou’ll find a list of additional Admission Control plugins in the Kubernetes documen-\\ntation at https:/\\n/kubernetes.io/docs/admin/admission-controllers/.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'API server',\n",
       "    'description': 'Server responsible for handling API requests.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Authentication plugins',\n",
       "    'description': 'Plugins used by the API server to authenticate clients.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'HTTP request',\n",
       "    'description': 'Request sent from a client to the API server.',\n",
       "    'category': 'network'},\n",
       "   {'entity': \"Client's certificate\",\n",
       "    'description': 'Certificate presented by the client to the API server for authentication.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Authorization plugins',\n",
       "    'description': 'Plugins used by the API server to determine whether a client can perform an action.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'User ID',\n",
       "    'description': 'Unique identifier for a user.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Groups',\n",
       "    'description': 'Collection of users with shared permissions.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Admission Control plugins',\n",
       "    'description': 'Plugins used by the API server to validate and modify resources in requests.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Entities that represent running processes in Kubernetes.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Namespaces',\n",
       "    'description': 'Logical grouping of resources in Kubernetes.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Resources',\n",
       "    'description': 'Entities that represent data and services in Kubernetes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ImagePullPolicy',\n",
       "    'description': 'Policy governing how images are pulled for pods.',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'Service account',\n",
       "    'description': 'Account used by pods to authenticate with the API server.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Resource quota',\n",
       "    'description': 'Limit on resources (CPU and memory) that can be used by a namespace.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'AlwaysPullImages plugin',\n",
       "    'description': 'Admission Control plugin that forces images to be pulled every time a pod is deployed.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceAccount plugin',\n",
       "    'description': 'Admission Control plugin that applies the default service account to pods without one explicitly specified.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'NamespaceLifecycle plugin',\n",
       "    'description': 'Admission Control plugin that prevents creation of pods in namespaces being deleted or not existing.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ResourceQuota plugin',\n",
       "    'description': 'Admission Control plugin that ensures pods in a namespace only use allocated CPU and memory.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"calls authentication plugins to determine who is sending the request\",\\n    \"destination_entity\": \"Authentication plugins\"\\n  },\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"consults authorization plugins to determine whether the authenticated user can perform the requested action\",\\n    \"destination_entity\": \"Authorization plugins\"\\n  },\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"passes the request through Admission Control plugins\",\\n    \"destination_entity\": \"Admission Control plugins\"\\n  },\\n  {\\n    \"source_entity\": \"Authentication plugins\",\\n    \"description\": \"extracts client\\'s username, user ID, and groups from HTTP request or client\\'s certificate\",\\n    \"destination_entity\": \"User ID\"\\n  },\\n  {\\n    \"source_entity\": \"Authorization plugins\",\\n    \"description\": \"determines whether the authenticated user can perform the requested action on the requested resource\",\\n    \"destination_entity\": \"Resources\"\\n  },\\n  {\\n    \"source_entity\": \"Admission Control plugins\",\\n    \"description\": \"modifies or rejects a request for creating, modifying, or deleting a resource\",\\n    \"destination_entity\": \"ResourceQuota plugin\"\\n  },\\n  {\\n    \"source_entity\": \"AlwaysPullImages plugin\",\\n    \"description\": \"overrides the pod\\'s imagePullPolicy to Always\",\\n    \"destination_entity\": \"ImagePullPolicy\"\\n  },\\n  {\\n    \"source_entity\": \"ServiceAccount plugin\",\\n    \"description\": \"applies the default service account to pods that don’t specify it explicitly\",\\n    \"destination_entity\": \"Service account\"\\n  },\\n  {\\n    \"source_entity\": \"NamespaceLifecycle plugin\",\\n    \"description\": \"prevents creation of pods in namespaces that are in the process of being deleted or non-existing\",\\n    \"destination_entity\": \"Namespaces\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota plugin\",\\n    \"description\": \"ensures pods in a certain namespace only use as much CPU and memory as has been allotted to the namespace\",\\n    \"destination_entity\": \"Resources\"\\n  },\\n  {\\n    \"source_entity\": \"Client\\'s certificate\",\\n    \"description\": \"used by authentication plugins to extract client’s username, user ID, and groups\",\\n    \"destination_entity\": \"User ID\"\\n  },\\n  {\\n    \"source_entity\": \"HTTP request\",\\n    \"description\": \"carries information about the client sending the request\",\\n    \"destination_entity\": \"API server\"\\n  }\\n]\\n```'},\n",
       " {'page': 350,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '318\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\nVALIDATING THE RESOURCE AND STORING IT PERSISTENTLY\\nAfter letting the request pass through all the Admission Control plugins, the API server\\nthen validates the object, stores it in etcd, and returns a response to the client.\\n11.1.4 Understanding how the API server notifies clients of resource \\nchanges\\nThe API server doesn’t do anything else except what we’ve discussed. For example, it\\ndoesn’t create pods when you create a ReplicaSet resource and it doesn’t manage the\\nendpoints of a service. That’s what controllers in the Controller Manager do. \\n But the API server doesn’t even tell these controllers what to do. All it does is\\nenable those controllers and other components to observe changes to deployed\\nresources. A Control Plane component can request to be notified when a resource is\\ncreated, modified, or deleted. This enables the component to perform whatever task\\nit needs in response to a change of the cluster metadata.\\n Clients watch for changes by opening an HTTP connection to the API server.\\nThrough this connection, the client will then receive a stream of modifications to the\\nwatched objects. Every time an object is updated, the server sends the new version of\\nthe object to all connected clients watching the object. Figure 11.4 shows how clients\\ncan watch for changes to pods and how a change to one of the pods is stored into etcd\\nand then relayed to all clients watching pods at that moment.\\nOne of the API server’s clients is the kubectl tool, which also supports watching\\nresources. For example, when deploying a pod, you don’t need to constantly poll the list\\nof pods by repeatedly executing kubectl get pods. Instead, you can use the --watch\\nflag and be notified of each creation, modification, or deletion of a pod, as shown in\\nthe following listing.\\n$ kubectl get pods --watch\\nNAME                    READY     STATUS              RESTARTS   AGE\\nListing 11.6\\nWatching a pod being created and then deleted\\nVarious\\nclients\\nkubectl\\nAPI server\\n1. GET /.../pods?watch=true\\n2. POST /.../pods/pod-xyz\\n5. Send updated object\\nto all watchers\\n3. Update object\\nin etcd\\n4. Modiﬁcation\\nnotiﬁcation\\netcd\\nFigure 11.4\\nWhen an object is updated, the API server sends the updated object to all interested \\nwatchers.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Various\\nclients              Col1              Col2 Col3\n",
       "   0             None  Various\\nclients              None     \n",
       "   1             None              None  Various\\nclients     ],\n",
       "  'entities': [{'entity': 'API Server',\n",
       "    'description': 'The central component that exposes the APIs for accessing cluster resources.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Admission Control plugins',\n",
       "    'description': 'Components that validate incoming requests and ensure they adhere to policies.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'A distributed key-value store used for storing cluster metadata.',\n",
       "    'category': 'Database'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'A component that manages the lifecycle of resources, such as pods and services.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'ReplicaSet resource',\n",
       "    'description': 'A resource that defines a desired number of replicas for a pod or group of pods.',\n",
       "    'category': 'Resource'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'An abstraction for exposing a microservice to clients, regardless of its physical location.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'kubernetes client (kubectl)',\n",
       "    'description': 'A tool for interacting with the Kubernetes cluster, allowing users to create, update, and delete resources.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': '--watch flag',\n",
       "    'description': 'An option used with kubectl to enable watching of resource changes in real-time.',\n",
       "    'category': 'Command'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'A fundamental resource in Kubernetes, representing a container or group of containers.',\n",
       "    'category': 'Resource'},\n",
       "   {'entity': 'etcd notifications',\n",
       "    'description': 'Events triggered when resources are updated in the etcd store.',\n",
       "    'category': 'Process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"validate and store object persistently\",\\n    \"destination_entity\": \"Resource\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"notify clients of resource changes\",\\n    \"destination_entity\": \"Clients (e.g. kubectl)\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"enable controllers to observe changes to deployed resources\",\\n    \"destination_entity\": \"Controller Manager\"\\n  },\\n  {\\n    \"source_entity\": \"Client (e.g. kubectl)\",\\n    \"description\": \"watch for changes to objects\",\\n    \"destination_entity\": \"API Server\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"send updated object to all interested watchers\",\\n    \"destination_entity\": \"Kubernetes Client (kubectl)\"\\n  },\\n  {\\n    \"source_entity\": \"--watch flag\",\\n    \"description\": \"notify kubectl of changes to pods\",\\n    \"destination_entity\": \"Kubernetes Client (kubectl)\"\\n  },\\n  {\\n    \"source_entity\": \"Controller Manager\",\\n    \"description\": \"enable controllers to perform tasks in response to resource changes\",\\n    \"destination_entity\": \"Control Plane component\"\\n  },\\n  {\\n    \"source_entity\": \"Admission Control plugins\",\\n    \"description\": \"validate requests passed through by API Server\",\\n    \"destination_entity\": \"API Server\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl (Kubernetes Client)\",\\n    \"description\": \"use --watch flag to watch for changes to pods\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"store updated object in etcd\",\\n    \"destination_entity\": \"etcd notifications\"\\n  }\\n]\\n```'},\n",
       " {'page': 351,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '319\\nUnderstanding the architecture\\nkubia-159041347-14j3i   0/1       Pending             0          0s\\nkubia-159041347-14j3i   0/1       Pending             0          0s\\nkubia-159041347-14j3i   0/1       ContainerCreating   0          1s\\nkubia-159041347-14j3i   0/1       Running             0          3s\\nkubia-159041347-14j3i   1/1       Running             0          5s\\nkubia-159041347-14j3i   1/1       Terminating         0          9s\\nkubia-159041347-14j3i   0/1       Terminating         0          17s\\nkubia-159041347-14j3i   0/1       Terminating         0          17s\\nkubia-159041347-14j3i   0/1       Terminating         0          17s\\nYou can even have kubectl print out the whole YAML on each watch event like this:\\n$ kubectl get pods -o yaml --watch\\nThe watch mechanism is also used by the Scheduler, which is the next Control Plane\\ncomponent you’re going to learn more about.\\n11.1.5 Understanding the Scheduler\\nYou’ve already learned that you don’t usually specify which cluster node a pod should\\nrun on. This is left to the Scheduler. From afar, the operation of the Scheduler looks\\nsimple. All it does is wait for newly created pods through the API server’s watch mech-\\nanism and assign a node to each new pod that doesn’t already have the node set. \\n The Scheduler doesn’t instruct the selected node (or the Kubelet running on that\\nnode) to run the pod. All the Scheduler does is update the pod definition through the\\nAPI server. The API server then notifies the Kubelet (again, through the watch mech-\\nanism described previously) that the pod has been scheduled. As soon as the Kubelet\\non the target node sees the pod has been scheduled to its node, it creates and runs the\\npod’s containers.\\n Although a coarse-grained view of the scheduling process seems trivial, the actual\\ntask of selecting the best node for the pod isn’t that simple. Sure, the simplest\\nScheduler could pick a random node and not care about the pods already running on\\nthat node. On the other side of the spectrum, the Scheduler could use advanced tech-\\nniques such as machine learning to anticipate what kind of pods are about to be\\nscheduled in the next minutes or hours and schedule pods to maximize future hard-\\nware utilization without requiring any rescheduling of existing pods. Kubernetes’\\ndefault Scheduler falls somewhere in between. \\nUNDERSTANDING THE DEFAULT SCHEDULING ALGORITHM\\nThe selection of a node can be broken down into two parts, as shown in figure 11.5:\\n\\uf0a1Filtering the list of all nodes to obtain a list of acceptable nodes the pod can be\\nscheduled to.\\n\\uf0a1Prioritizing the acceptable nodes and choosing the best one. If multiple nodes\\nhave the highest score, round-robin is used to ensure pods are deployed across\\nall of them evenly.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubia-159041347-14j3i',\n",
       "    'description': 'Pod name',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': \"API server's watch mechanism\",\n",
       "    'description': 'Mechanism for notifying the Kubelet when a pod is scheduled or updated',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'Control Plane component responsible for assigning nodes to pods',\n",
       "    'category': 'controller'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'Agent running on each node that manages the containers and pods on that node',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pod definition',\n",
       "    'description': 'Definition of a pod, including its name, labels, and container specifications',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'Physical or virtual machine running on which the Kubelet is executing',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'Isolated execution environment for an application',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'watch mechanism',\n",
       "    'description': 'Mechanism used by the Scheduler and API server to notify each other of updates or new pods',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'machine learning',\n",
       "    'description': 'Technique used by advanced Schedulers to anticipate future hardware utilization',\n",
       "    'category': 'technology'},\n",
       "   {'entity': 'default scheduling algorithm',\n",
       "    'description': \"Algorithm used by Kubernetes' Scheduler to select a node for a pod\",\n",
       "    'category': 'algorithm'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Scheduler\",\\n    \"description\": \"updates pod definition through API server\",\\n    \"destination_entity\": \"pod definition\"\\n  },\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"creates and runs containers based on updated pod definition\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"Scheduler\",\\n    \"description\": \"filters list of nodes to obtain acceptable nodes for scheduling\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"Scheduler\",\\n    \"description\": \"prioritizes and chooses best node from list of acceptable nodes\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"prints out whole YAML on each watch event through API server\\'s watch mechanism\",\\n    \"destination_entity\": \"API server\\'s watch mechanism\"\\n  },\\n  {\\n    \"source_entity\": \"Scheduler\",\\n    \"description\": \"uses machine learning to anticipate future hardware utilization\",\\n    \"destination_entity\": \"machine learning\"\\n  },\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"runs pod\\'s containers on selected node\",\\n    \"destination_entity\": \"kubia-159041347-14j3i\"\\n  },\\n  {\\n    \"source_entity\": \"Scheduler\",\\n    \"description\": \"instructs Kubelet to run pod on assigned node\",\\n    \"destination_entity\": \"Kubelet\"\\n  },\\n  {\\n    \"source_entity\": \"API server\\'s watch mechanism\",\\n    \"description\": \"notifies Kubelet of scheduled pods\",\\n    \"destination_entity\": \"Kubelet\"\\n  },\\n  {\\n    \"source_entity\": \"default scheduling algorithm\",\\n    \"description\": \"chooses best node from list of acceptable nodes using round-robin if necessary\",\\n    \"destination_entity\": \"node\"\\n  }\\n]\\n```'},\n",
       " {'page': 352,\n",
       "  'img_cnt': 1,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '320\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\nFINDING ACCEPTABLE NODES\\nTo determine which nodes are acceptable for the pod, the Scheduler passes each\\nnode through a list of configured predicate functions. These check various things\\nsuch as\\n\\uf0a1Can the node fulfill the pod’s requests for hardware resources? You’ll learn how\\nto specify them in chapter 14.\\n\\uf0a1Is the node running out of resources (is it reporting a memory or a disk pres-\\nsure condition)? \\n\\uf0a1If the pod requests to be scheduled to a specific node (by name), is this the node?\\n\\uf0a1Does the node have a label that matches the node selector in the pod specifica-\\ntion (if one is defined)?\\n\\uf0a1If the pod requests to be bound to a specific host port (discussed in chapter 13),\\nis that port already taken on this node or not? \\n\\uf0a1If the pod requests a certain type of volume, can this volume be mounted for\\nthis pod on this node, or is another pod on the node already using the same\\nvolume?\\n\\uf0a1Does the pod tolerate the taints of the node? Taints and tolerations are explained\\nin chapter 16.\\n\\uf0a1Does the pod specify node and/or pod affinity or anti-affinity rules? If yes,\\nwould scheduling the pod to this node break those rules? This is also explained\\nin chapter 16.\\nAll these checks must pass for the node to be eligible to host the pod. After perform-\\ning these checks on every node, the Scheduler ends up with a subset of the nodes. Any\\nof these nodes could run the pod, because they have enough available resources for\\nthe pod and conform to all requirements you’ve specified in the pod definition.\\nSELECTING THE BEST NODE FOR THE POD\\nEven though all these nodes are acceptable and can run the pod, several may be a\\nbetter choice than others. Suppose you have a two-node cluster. Both nodes are eli-\\ngible, but one is already running 10 pods, while the other, for whatever reason, isn’t\\nrunning any pods right now. It’s obvious the Scheduler should favor the second\\nnode in this case. \\nNode 1\\nNode 2\\nNode 3\\nNode 4\\nNode 5\\n...\\nFind acceptable\\nnodes\\nNode 1\\nNode 2\\nNode 3\\nNode 4\\nNode 5\\n...\\nPrioritize nodes\\nand select the\\ntop one\\nNode 3\\nNode 1\\nNode 4\\nFigure 11.5\\nThe Scheduler finds acceptable nodes for a pod and then selects the best node \\nfor the pod.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Predicate Functions',\n",
       "    'description': 'A list of configured functions that check various things to determine if a node is acceptable for the pod.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'The component that passes each node through a list of configured predicate functions to find acceptable nodes for the pod.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A container that runs an application, which can be scheduled on a node by the Scheduler.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'A machine or instance in a Kubernetes cluster where pods can be run.',\n",
       "    'category': 'Hardware'},\n",
       "   {'entity': 'Memory Pressure Condition',\n",
       "    'description': 'A condition reported by a node when it is running out of memory resources.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Disk Pressure Condition',\n",
       "    'description': 'A condition reported by a node when it is running out of disk storage resources.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Node Selector',\n",
       "    'description': 'A label that matches the node selector in the pod specification to determine if the node has the required characteristics.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Host Port',\n",
       "    'description': 'A specific port on a node where pods can be bound, which can be used by other pods as well.',\n",
       "    'category': 'Hardware'},\n",
       "   {'entity': 'Volume',\n",
       "    'description': 'A shared resource that can be mounted to multiple pods running on the same node.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Taints and Tolerations',\n",
       "    'description': 'Mechanisms used to control pod placement based on specific conditions, explained in chapter 16.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Affinity or Anti-affinity Rules',\n",
       "    'description': 'Rules that determine if a pod can be scheduled on a node based on the presence of other pods with similar characteristics.',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[{\"source_entity\": \"Scheduler\", \"description\": \"passes each node through a list of configured predicate functions to check various things\", \"destination_entity\": \"Predicate Functions\"},\\n {\"source_entity\": \"Scheduler\", \"description\": \"checks if the node can fulfill the pod\\'s requests for hardware resources\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"Scheduler\", \"description\": \"checks if the node is running out of resources (memory or disk pressure condition)\", \"destination_entity\": \"Memory Pressure Condition\"},\\n {\"source_entity\": \"Scheduler\", \"description\": \"checks if the node has a label that matches the node selector in the pod specification\", \"destination_entity\": \"Node Selector\"},\\n {\"source_entity\": \"Scheduler\", \"description\": \"checks if the node has a host port available for the pod to be bound to\", \"destination_entity\": \"Host Port\"},\\n {\"source_entity\": \"Scheduler\", \"description\": \"checks if the node can mount a volume requested by the pod\", \"destination_entity\": \"Volume\"},\\n {\"source_entity\": \"Scheduler\", \"description\": \"checks if the pod tolerates the taints of the node\", \"destination_entity\": \"Taints and Tolerations\"},\\n {\"source_entity\": \"Scheduler\", \"description\": \"checks if scheduling the pod would break affinity or anti-affinity rules with other nodes or pods\", \"destination_entity\": \"Affinity or Anti-affinity Rules\"},\\n {\"source_entity\": \"Scheduler\", \"description\": \"selects the best node for the pod based on various criteria such as available resources and requirements specified in the pod definition\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"Predicate Functions\", \"description\": \"check if the node is running out of resources (disk pressure condition)\", \"destination_entity\": \"Disk Pressure Condition\"},\\n {\"source_entity\": \"Node\", \"description\": \"has a label that matches the node selector in the pod specification\", \"destination_entity\": \"Node Selector\"}]'},\n",
       " {'page': 353,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '321\\nUnderstanding the architecture\\n Or is it? If these two nodes are provided by the cloud infrastructure, it may be bet-\\nter to schedule the pod to the first node and relinquish the second node back to the\\ncloud provider to save money. \\nADVANCED SCHEDULING OF PODS\\nConsider another example. Imagine having multiple replicas of a pod. Ideally, you’d\\nwant them spread across as many nodes as possible instead of having them all sched-\\nuled to a single one. Failure of that node would cause the service backed by those\\npods to become unavailable. But if the pods were spread across different nodes, a sin-\\ngle node failure would barely leave a dent in the service’s capacity. \\n Pods belonging to the same Service or ReplicaSet are spread across multiple nodes\\nby default. It’s not guaranteed that this is always the case. But you can force pods to be\\nspread around the cluster or kept close together by defining pod affinity and anti-\\naffinity rules, which are explained in chapter 16. \\n Even these two simple cases show how complex scheduling can be, because it\\ndepends on a multitude of factors. Because of this, the Scheduler can either be config-\\nured to suit your specific needs or infrastructure specifics, or it can even be replaced\\nwith a custom implementation altogether. You could also run a Kubernetes cluster\\nwithout a Scheduler, but then you’d have to perform the scheduling manually.\\nUSING MULTIPLE SCHEDULERS\\nInstead of running a single Scheduler in the cluster, you can run multiple Schedulers.\\nThen, for each pod, you specify the Scheduler that should schedule this particular\\npod by setting the schedulerName property in the pod spec.\\n Pods without this property set are scheduled using the default Scheduler, and so\\nare pods with schedulerName set to default-scheduler. All other pods are ignored by\\nthe default Scheduler, so they need to be scheduled either manually or by another\\nScheduler watching for such pods. \\n You can implement your own Schedulers and deploy them in the cluster, or you\\ncan deploy an additional instance of Kubernetes’ Scheduler with different configura-\\ntion options.\\n11.1.6 Introducing the controllers running in the Controller Manager\\nAs previously mentioned, the API server doesn’t do anything except store resources in\\netcd and notify clients about the change. The Scheduler only assigns a node to the\\npod, so you need other active components to make sure the actual state of the system\\nconverges toward the desired state, as specified in the resources deployed through the\\nAPI server. This work is done by controllers running inside the Controller Manager. \\n The single Controller Manager process currently combines a multitude of control-\\nlers performing various reconciliation tasks. Eventually those controllers will be split\\nup into separate processes, enabling you to replace each one with a custom imple-\\nmentation if necessary. The list of these controllers includes the\\n\\uf0a1Replication Manager (a controller for ReplicationController resources)\\n\\uf0a1ReplicaSet, DaemonSet, and Job controllers\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Scheduler',\n",
       "    'description': 'Assigns a node to a pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A pod is a logical host for containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A service is an abstraction which defines a set of pods and provides network access to them',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A ReplicaSet controller ensures that the specified number of replicas are running at any given time',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'DaemonSet',\n",
       "    'description': 'A DaemonSet is a deployment strategy where one or more pods are run on every node in a cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Job',\n",
       "    'description': 'A Job controller manages the execution of a batch process, which completes when all tasks are done',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicationManager',\n",
       "    'description': 'A controller for ReplicationController resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'a distributed key-value store used by Kubernetes',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The primary interface with the cluster, handles operations such as creating/deleting resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'Runs multiple controllers in a single process, responsible for converging the actual state of the system to its desired state',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Scheduler\", \"description\": \"assigns a node to the pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Controller Manager\", \"description\": \"makes sure the actual state of the system converges toward the desired state\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"ReplicationManager\", \"description\": \"manages ReplicationController resources\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"Controller Manager\", \"description\": \"combines multiple controllers performing reconciliation tasks\", \"destination_entity\": \"controllers\"},\\n  {\"source_entity\": \"Scheduler\", \"description\": \"schedules pods to a single node or spreads them across different nodes\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"API server\", \"description\": \"stores resources in etcd and notifies clients about changes\", \"destination_entity\": \"etcd\"},\\n  {\"source_entity\": \"Controller Manager\", \"description\": \"runs active components to make sure the actual state of the system converges toward the desired state\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"ReplicaSet\", \"description\": \"spreads multiple replicas of a pod across different nodes\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Service\", \"description\": \"is backed by pods, which can be spread across different nodes for high availability\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"DaemonSet\", \"description\": \"runs on every node in the cluster and ensures that a pod is running on each one\", \"destination_entity\": \"node\"},\\n  {\"source_entity\": \"Job\", \"description\": \"is run by the Controller Manager to perform reconciliation tasks\", \"destination_entity\": \"Controller Manager\"},\\n  {\"source_entity\": \"pod\", \"description\": \"can be scheduled using the default Scheduler or a custom Scheduler\", \"destination_entity\": \"Scheduler\"}\\n]\\n```'},\n",
       " {'page': 354,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '322\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\n\\uf0a1Deployment controller\\n\\uf0a1StatefulSet controller\\n\\uf0a1Node controller\\n\\uf0a1Service controller\\n\\uf0a1Endpoints controller\\n\\uf0a1Namespace controller\\n\\uf0a1PersistentVolume controller\\n\\uf0a1Others\\nWhat each of these controllers does should be evident from its name. From the list,\\nyou can tell there’s a controller for almost every resource you can create. Resources\\nare descriptions of what should be running in the cluster, whereas the controllers are\\nthe active Kubernetes components that perform actual work as a result of the deployed\\nresources.\\nUNDERSTANDING WHAT CONTROLLERS DO AND HOW THEY DO IT\\nControllers do many different things, but they all watch the API server for changes to\\nresources (Deployments, Services, and so on) and perform operations for each change,\\nwhether it’s a creation of a new object or an update or deletion of an existing object.\\nMost of the time, these operations include creating other resources or updating the\\nwatched resources themselves (to update the object’s status, for example).\\n In general, controllers run a reconciliation loop, which reconciles the actual state\\nwith the desired state (specified in the resource’s spec section) and writes the new\\nactual state to the resource’s status section. Controllers use the watch mechanism to\\nbe notified of changes, but because using watches doesn’t guarantee the controller\\nwon’t miss an event, they also perform a re-list operation periodically to make sure\\nthey haven’t missed anything.\\n Controllers never talk to each other directly. They don’t even know any other con-\\ntrollers exist. Each controller connects to the API server and, through the watch\\nmechanism described in section 11.1.3, asks to be notified when a change occurs in\\nthe list of resources of any type the controller is responsible for. \\n We’ll briefly look at what each of the controllers does, but if you’d like an in-depth\\nview of what they do, I suggest you look at their source code directly. The sidebar\\nexplains how to get started.\\nA few pointers on exploring the controllers’ source code\\nIf you’re interested in seeing exactly how these controllers operate, I strongly encour-\\nage you to browse through their source code. To make it easier, here are a few tips:\\nThe source code for the controllers is available at https:/\\n/github.com/kubernetes/\\nkubernetes/blob/master/pkg/controller.\\nEach controller usually has a constructor in which it creates an Informer, which is\\nbasically a listener that gets called every time an API object gets updated. Usually,\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Deployment controller',\n",
       "    'description': 'Kubernetes component responsible for deployment management',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'StatefulSet controller',\n",
       "    'description': 'Kubernetes component responsible for stateful set management',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Node controller',\n",
       "    'description': 'Kubernetes component responsible for node management',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Service controller',\n",
       "    'description': 'Kubernetes component responsible for service management',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Endpoints controller',\n",
       "    'description': 'Kubernetes component responsible for endpoint management',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Namespace controller',\n",
       "    'description': 'Kubernetes component responsible for namespace management',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'PersistentVolume controller',\n",
       "    'description': 'Kubernetes component responsible for persistent volume management',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Kubernetes component responsible for API requests and responses',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Watch mechanism',\n",
       "    'description': 'Mechanism used by controllers to be notified of changes in resources',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'Re-list operation',\n",
       "    'description': 'Periodic operation performed by controllers to ensure no changes are missed',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'Informer',\n",
       "    'description': 'Listener used by controllers to get notified of API object updates',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[{\"source_entity\":\"Deployment controller\",\"description\":\"watches API server for changes to Deployments and performs operations based on those changes\",\"destination_entity\":\"API server\"},{\"source_entity\":\"Endpoints controller\",\"description\":\"watches API server for changes to Endpoints and performs operations based on those changes\",\"destination_entity\":\"API server\"},{\"source_entity\":\"Service controller\",\"description\":\"watches API server for changes to Services and performs operations based on those changes\",\"destination_entity\":\"API server\"},{\"source_entity\":\"Namespace controller\",\"description\":\"watches API server for changes to Namespaces and performs operations based on those changes\",\"destination_entity\":\"API server\"},{\"source_entity\":\"PersistentVolume controller\",\"description\":\"watches API server for changes to Persistent Volumes and performs operations based on those changes\",\"destination_entity\":\"API server\"},{\"source_entity\":\"Node controller\",\"description\":\"watches API server for changes to Nodes and performs operations based on those changes\",\"destination_entity\":\"API server\"},{\"source_entity\":\"StatefulSet controller\",\"description\":\"watches API server for changes to StatefulSets and performs operations based on those changes\",\"destination_entity\":\"API server\"},{\"source_entity\":\"Watch mechanism\",\"description\":\"notifies controllers when a change occurs in resources they are responsible for\",\"destination_entity\":\"controllers\"},{\"source_entity\":\"Re-list operation\",\"description\":\"periodically checks with API server to make sure no events were missed\",\"destination_entity\":\"API server\"},{\"source_entity\":\"Informer\",\"description\":\"listens for updates to API objects and notifies the controller\",\"destination_entity\":\"controller\"},{\"source_entity\":\"Deployment controller\",\"description\":\"uses Informer to get notified of changes to Deployments\",\"destination_entity\":\"Informer\"},{\"source_entity\":\"Endpoints controller\",\"description\":\"uses Informer to get notified of changes to Endpoints\",\"destination_entity\":\"Informer\"},{\"source_entity\":\"Service controller\",\"description\":\"uses Informer to get notified of changes to Services\",\"destination_entity\":\"Informer\"},{\"source_entity\":\"Namespace controller\",\"description\":\"uses Informer to get notified of changes to Namespaces\",\"destination_entity\":\"Informer\"},{\"source_entity\":\"PersistentVolume controller\",\"description\":\"uses Informer to get notified of changes to Persistent Volumes\",\"destination_entity\":\"Informer\"},{\"source_entity\":\"Node controller\",\"description\":\"uses Informer to get notified of changes to Nodes\",\"destination_entity\":\"Informer\"},{\"source_entity\":\"StatefulSet controller\",\"description\":\"uses Informer to get notified of changes to StatefulSets\",\"destination_entity\":\"Informer\"}]'},\n",
       " {'page': 355,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '323\\nUnderstanding the architecture\\nTHE REPLICATION MANAGER\\nThe controller that makes ReplicationController resources come to life is called the\\nReplication Manager. We talked about how ReplicationControllers work in chapter 4.\\nIt’s not the ReplicationControllers that do the actual work, but the Replication Man-\\nager. Let’s quickly review what the controller does, because this will help you under-\\nstand the rest of the controllers.\\n In chapter 4, we said that the operation of a ReplicationController could be\\nthought of as an infinite loop, where in each iteration, the controller finds the num-\\nber of pods matching its pod selector and compares the number to the desired replica\\ncount. \\n Now that you know how the API server can notify clients through the watch\\nmechanism, it’s clear that the controller doesn’t poll the pods in every iteration, but\\nis instead notified by the watch mechanism of each change that may affect the\\ndesired replica count or the number of matched pods (see figure 11.6). Any such\\nchanges trigger the controller to recheck the desired vs. actual replica count and act\\naccordingly.\\n You already know that when too few pod instances are running, the Replication-\\nController runs additional instances. But it doesn’t actually run them itself. It creates\\nan Informer listens for changes to a specific type of resource. Looking at the con-\\nstructor will show you which resources the controller is watching.\\nNext, go look for the worker() method. In it, you’ll find the method that gets invoked\\neach time the controller needs to do something. The actual function is often stored\\nin a field called syncHandler or something similar. This field is also initialized in the\\nconstructor, so that’s where you’ll find the name of the function that gets called. That\\nfunction is the place where all the magic happens.\\nController Manager\\nWatches\\nCreates and\\ndeletes\\nReplication\\nManager\\nAPI server\\nReplicationController\\nresources\\nPod resources\\nOther resources\\nFigure 11.6\\nThe Replication Manager watches for changes to API \\nobjects.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Controller Manager\\nReplication\\nManager               Watches  \\\n",
       "   0                                     None  Creates and\\ndeletes   \n",
       "   \n",
       "     API server\\nReplicationController\\nresources\\nPod resources\\nOther resources  \n",
       "   0                                               None                            ],\n",
       "  'entities': [{'entity': 'Replication Manager',\n",
       "    'description': 'A controller that makes ReplicationController resources come to life',\n",
       "    'category': 'software/controller'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'A resource that manages the number of replicas of a pod',\n",
       "    'category': 'software/resource'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The server that provides access to Kubernetes API objects',\n",
       "    'category': 'software/server'},\n",
       "   {'entity': 'Watch mechanism',\n",
       "    'description': 'A way for the controller to be notified of changes to resources',\n",
       "    'category': 'software/mechanism'},\n",
       "   {'entity': 'Informer',\n",
       "    'description': 'A component that listens for changes to a specific type of resource',\n",
       "    'category': 'software/component'},\n",
       "   {'entity': 'syncHandler',\n",
       "    'description': 'A function that gets called when the controller needs to do something',\n",
       "    'category': 'software/function'},\n",
       "   {'entity': 'Replica count',\n",
       "    'description': 'The desired number of replicas of a pod',\n",
       "    'category': 'software/resource'},\n",
       "   {'entity': 'Pod selector',\n",
       "    'description': 'A label that selects specific pods to manage',\n",
       "    'category': 'software/resource'}],\n",
       "  'relationships': '[\\n    {\\n        \"source_entity\": \"Replication Manager\",\\n        \"description\": \"Creates ReplicationController resources\",\\n        \"destination_entity\": \"ReplicationController\"\\n    },\\n    {\\n        \"source_entity\": \"Replication Manager\",\\n        \"description\": \"Listens for changes to specific resources\",\\n        \"destination_entity\": \"Informer\"\\n    },\\n    {\\n        \"source_entity\": \"Replication Manager\",\\n        \"description\": \"Gets notified of changes through watch mechanism\",\\n        \"destination_entity\": \"API server\"\\n    },\\n    {\\n        \"source_entity\": \"Replication Manager\",\\n        \"description\": \"Creates or deletes ReplicationController resources based on replica count\",\\n        \"destination_entity\": \"Pod selector\"\\n    },\\n    {\\n        \"source_entity\": \"Watch mechanism\",\\n        \"description\": \"Notifies the controller of changes to desired replica count or number of matched pods\",\\n        \"destination_entity\": \"Replication Manager\"\\n    },\\n    {\\n        \"source_entity\": \"SyncHandler\",\\n        \"description\": \"Gets invoked each time the controller needs to do something\",\\n        \"destination_entity\": \"Replication Manager\"\\n    },\\n    {\\n        \"source_entity\": \"Informer\",\\n        \"description\": \"Listens for changes to specific resources\",\\n        \"destination_entity\": \"API server\"\\n    }\\n]\\n```'},\n",
       " {'page': 356,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '324\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\nnew Pod manifests, posts them to the API server, and lets the Scheduler and the\\nKubelet do their job of scheduling and running the pod.\\n The Replication Manager performs its work by manipulating Pod API objects\\nthrough the API server. This is how all controllers operate.\\nTHE REPLICASET, THE DAEMONSET, AND THE JOB CONTROLLERS\\nThe ReplicaSet controller does almost the same thing as the Replication Manager\\ndescribed previously, so we don’t have much to add here. The DaemonSet and Job\\ncontrollers are similar. They create Pod resources from the pod template defined in\\ntheir respective resources. Like the Replication Manager, these controllers don’t run\\nthe pods, but post Pod definitions to the API server, letting the Kubelet create their\\ncontainers and run them.\\nTHE DEPLOYMENT CONTROLLER\\nThe Deployment controller takes care of keeping the actual state of a deployment in\\nsync with the desired state specified in the corresponding Deployment API object. \\n The Deployment controller performs a rollout of a new version each time a\\nDeployment object is modified (if the modification should affect the deployed pods).\\nIt does this by creating a ReplicaSet and then appropriately scaling both the old and\\nthe new ReplicaSet based on the strategy specified in the Deployment, until all the old\\npods have been replaced with new ones. It doesn’t create any pods directly.\\nTHE STATEFULSET CONTROLLER\\nThe StatefulSet controller, similarly to the ReplicaSet controller and other related\\ncontrollers, creates, manages, and deletes Pods according to the spec of a StatefulSet\\nresource. But while those other controllers only manage Pods, the StatefulSet control-\\nler also instantiates and manages PersistentVolumeClaims for each Pod instance.\\nTHE NODE CONTROLLER\\nThe Node controller manages the Node resources, which describe the cluster’s worker\\nnodes. Among other things, a Node controller keeps the list of Node objects in sync\\nwith the actual list of machines running in the cluster. It also monitors each node’s\\nhealth and evicts pods from unreachable nodes.\\n The Node controller isn’t the only component making changes to Node objects.\\nThey’re also changed by the Kubelet, and can obviously also be modified by users\\nthrough REST API calls. \\nTHE SERVICE CONTROLLER\\nIn chapter 5, when we talked about Services, you learned that a few different types\\nexist. One of them was the LoadBalancer service, which requests a load balancer from\\nthe infrastructure to make the service available externally. The Service controller is\\nthe one requesting and releasing a load balancer from the infrastructure, when a\\nLoadBalancer-type Service is created or deleted.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A pod is a logical host in Kubernetes that can contain multiple containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The central management service in Kubernetes that handles requests from clients and manages the cluster state.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'A component in Kubernetes that decides where to place a pod based on resource availability, affinity, anti-affinity, etc.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'The agent running on each node in the cluster that is responsible for starting and maintaining containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Replication Manager',\n",
       "    'description': 'A component that performs its work by manipulating Pod API objects through the API server, similar to all controllers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A controller in Kubernetes that ensures a specified number of replicas of a pod is running at any given time.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'DaemonSet',\n",
       "    'description': 'A controller in Kubernetes that runs a single instance of a pod on every node in the cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Job',\n",
       "    'description': 'A controller in Kubernetes that creates Pods from templates and manages their execution, usually used for batch processing.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A resource in Kubernetes that represents a set of rolling updates to an application, allowing zero-downtime deployments.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'A controller in Kubernetes that creates and manages Pods according to the spec of a StatefulSet resource, also managing PersistentVolumeClaims for each Pod instance.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Node controller',\n",
       "    'description': 'A component in Kubernetes that manages Node resources, keeping their list in sync with actual machines running in the cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service controller',\n",
       "    'description': 'A component in Kubernetes that requests and releases a load balancer from infrastructure when creating or deleting LoadBalancer-type Services.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"Replication Manager\", \"description\": \"manipulates Pod API objects through the API server\", \"destination_entity\": \"API server\"},\\n {\"source_entity\": \"Scheduler\", \"description\": \"schedules and runs the pod\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"Kubelet\", \"description\": \"creates containers and runs them\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"ReplicaSet controller\", \"description\": \"does almost the same thing as the Replication Manager\", \"destination_entity\": \"Replication Manager\"},\\n {\"source_entity\": \"DaemonSet controller\", \"description\": \"similar to ReplicaSet and Job controllers\", \"destination_entity\": \"ReplicaSet controller\"},\\n {\"source_entity\": \"Job controller\", \"description\": \"similar to ReplicaSet and DaemonSet controllers\", \"destination_entity\": \"ReplicaSet controller\"},\\n {\"source_entity\": \"Deployment controller\", \"description\": \"keeps the actual state of a deployment in sync with the desired state specified in the corresponding Deployment API object\", \"destination_entity\": \"Deployment\"},\\n {\"source_entity\": \"Deployment controller\", \"description\": \"performs a rollout of a new version each time a Deployment object is modified\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"Deployment controller\", \"description\": \"scales both the old and the new ReplicaSet based on the strategy specified in the Deployment\", \"destination_entity\": \"ReplicaSet\"},\\n {\"source_entity\": \"StatefulSet controller\", \"description\": \"creates, manages, and deletes Pods according to the spec of a StatefulSet resource\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"StatefulSet controller\", \"description\": \"instantiates and manages PersistentVolumeClaims for each Pod instance\", \"destination_entity\": \"PersistentVolumeClaim\"},\\n {\"source_entity\": \"Node controller\", \"description\": \"manages the Node resources, which describe the cluster\\'s worker nodes\", \"destination_entity\": \"Node\"},\\n {\"source_entity\": \"Node controller\", \"description\": \"monitors each node\\'s health and evicts pods from unreachable nodes\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"Service controller\", \"description\": \"requests and releases a load balancer from the infrastructure\", \"destination_entity\": \"LoadBalancer service\"}]'},\n",
       " {'page': 357,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '325\\nUnderstanding the architecture\\nTHE ENDPOINTS CONTROLLER\\nYou’ll remember that Services aren’t linked directly to pods, but instead contain a list\\nof endpoints (IPs and ports), which is created and updated either manually or auto-\\nmatically according to the pod selector defined on the Service. The Endpoints con-\\ntroller is the active component that keeps the endpoint list constantly updated with\\nthe IPs and ports of pods matching the label selector.\\n As figure 11.7 shows, the controller watches both Services and Pods. When\\nServices are added or updated or Pods are added, updated, or deleted, it selects Pods\\nmatching the Service’s pod selector and adds their IPs and ports to the Endpoints\\nresource. Remember, the Endpoints object is a standalone object, so the controller\\ncreates it if necessary. Likewise, it also deletes the Endpoints object when the Service is\\ndeleted.\\nTHE NAMESPACE CONTROLLER\\nRemember namespaces (we talked about them in chapter 3)? Most resources belong\\nto a specific namespace. When a Namespace resource is deleted, all the resources in\\nthat namespace must also be deleted. This is what the Namespace controller does.\\nWhen it’s notified of the deletion of a Namespace object, it deletes all the resources\\nbelonging to the namespace through the API server. \\nTHE PERSISTENTVOLUME CONTROLLER\\nIn chapter 6 you learned about PersistentVolumes and PersistentVolumeClaims.\\nOnce a user creates a PersistentVolumeClaim, Kubernetes must find an appropriate\\nPersistentVolume and bind it to the claim. This is performed by the PersistentVolume\\ncontroller. \\n When a PersistentVolumeClaim pops up, the controller finds the best match for\\nthe claim by selecting the smallest PersistentVolume with the access mode matching\\nthe one requested in the claim and the declared capacity above the capacity requested\\nController Manager\\nWatches\\nCreates, modiﬁes,\\nand deletes\\nEndpoints\\ncontroller\\nAPI server\\nService resources\\nPod resources\\nEndpoints resources\\nFigure 11.7\\nThe Endpoints controller watches Service and Pod resources, \\nand manages Endpoints.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Controller Manager\\nEndpoints\\ncontroller                          Watches  \\\n",
       "   0                                      None  Creates, modifies,\\nand deletes   \n",
       "   \n",
       "     API server\\nService resources\\nPod resources\\nEndpoints resources  \n",
       "   0                                               None                 ],\n",
       "  'entities': [{'entity': 'THE ENDPOINTS CONTROLLER',\n",
       "    'description': 'Kubernetes component that keeps the endpoint list constantly updated with the IPs and ports of pods matching the label selector.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'Not linked directly to pods, but contain a list of endpoints (IPs and ports) created and updated manually or automatically according to the pod selector defined on the Service.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': \"Contain IPs and ports that are added to the Endpoints resource when matching the Service's pod selector.\",\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Endpoints',\n",
       "    'description': 'A standalone object created by the Endpoints controller with the IPs and ports of pods matching the label selector.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'THE NAMESPACE CONTROLLER',\n",
       "    'description': 'Deletes all resources belonging to a namespace when the Namespace resource is deleted.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Namespaces',\n",
       "    'description': 'Resources belong to a specific namespace and are deleted with the Namespace controller when the Namespace object is deleted.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'PersistentVolumes',\n",
       "    'description': 'Must be bound to a PersistentVolumeClaim by the PersistentVolume controller once a user creates a claim.',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'PersistentVolumeClaims',\n",
       "    'description': 'Can be created and are automatically matched with a PersistentVolume by the PersistentVolume controller.',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'Watches, creates, modifies, and deletes resources such as Endpoints, Service, Pod, and Namespace objects.',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"The Endpoints controller\",\\n    \"description\": \"keeps endpoint list constantly updated with IPs and ports of pods matching label selector\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"The Namespace controller\",\\n    \"description\": \"deletes all resources belonging to namespace when namespace object is deleted\",\\n    \"destination_entity\": \"Resources\"\\n  },\\n  {\\n    \"source_entity\": \"PersistentVolume controller\",\\n    \"description\": \"finds appropriate PersistentVolume and binds it to PersistentVolumeClaim\",\\n    \"destination_entity\": \"PersistentVolumeClaims\"\\n  },\\n  {\\n    \"source_entity\": \"Controller Manager\",\\n    \"description\": \"watches Endpoints, Service, and Pod resources\",\\n    \"destination_entity\": \"Endpoints controller\"\\n  },\\n  {\\n    \"source_entity\": \"Controller Manager\",\\n    \"description\": \"manages creation, modification, and deletion of Endpoints\",\\n    \"destination_entity\": \"Endpoints\"\\n  },\\n  {\\n    \"source_entity\": \"The ENDPOINTS CONTROLLER\",\\n    \"description\": \"watches Service and Pod resources\",\\n    \"destination_entity\": \"Service resources\"\\n  },\\n  {\\n    \"source_entity\": \"The ENDPOINTS CONTROLLER\",\\n    \"description\": \"manages Endpoints resource\",\\n    \"destination_entity\": \"Endpoints resources\"\\n  },\\n  {\\n    \"source_entity\": \"PersistentVolume controller\",\\n    \"description\": \"selects smallest PersistentVolume with matching access mode and capacity\",\\n    \"destination_entity\": \"PersistentVolumes\"\\n  },\\n  {\\n    \"source_entity\": \"The Namespace controller\",\\n    \"description\": \"deletes resources belonging to namespace when namespace object is deleted\",\\n    \"destination_entity\": \"Namespaces\"\\n  }\\n]\\n```'},\n",
       " {'page': 358,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '326\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\nin the claim. It does this by keeping an ordered list of PersistentVolumes for each\\naccess mode by ascending capacity and returning the first volume from the list.\\n Then, when the user deletes the PersistentVolumeClaim, the volume is unbound\\nand reclaimed according to the volume’s reclaim policy (left as is, deleted, or emptied).\\nCONTROLLER WRAP-UP\\nYou should now have a good feel for what each controller does and how controllers\\nwork in general. Again, all these controllers operate on the API objects through the\\nAPI server. They don’t communicate with the Kubelets directly or issue any kind of\\ninstructions to them. In fact, they don’t even know Kubelets exist. After a controller\\nupdates a resource in the API server, the Kubelets and Kubernetes Service Proxies,\\nalso oblivious of the controllers’ existence, perform their work, such as spinning up a\\npod’s containers and attaching network storage to them, or in the case of services, set-\\nting up the actual load balancing across pods. \\n The Control Plane handles one part of the operation of the whole system, so to\\nfully understand how things unfold in a Kubernetes cluster, you also need to under-\\nstand what the Kubelet and the Kubernetes Service Proxy do. We’ll learn that next.\\n11.1.7 What the Kubelet does\\nIn contrast to all the controllers, which are part of the Kubernetes Control Plane and\\nrun on the master node(s), the Kubelet and the Service Proxy both run on the worker\\nnodes, where the actual pods containers run. What does the Kubelet do exactly?\\nUNDERSTANDING THE KUBELET’S JOB\\nIn a nutshell, the Kubelet is the component responsible for everything running on a\\nworker node. Its initial job is to register the node it’s running on by creating a Node\\nresource in the API server. Then it needs to continuously monitor the API server for\\nPods that have been scheduled to the node, and start the pod’s containers. It does this\\nby telling the configured container runtime (which is Docker, CoreOS’ rkt, or some-\\nthing else) to run a container from a specific container image. The Kubelet then con-\\nstantly monitors running containers and reports their status, events, and resource\\nconsumption to the API server. \\n The Kubelet is also the component that runs the container liveness probes, restart-\\ning containers when the probes fail. Lastly, it terminates containers when their Pod is\\ndeleted from the API server and notifies the server that the pod has terminated.\\nRUNNING STATIC PODS WITHOUT THE API SERVER\\nAlthough the Kubelet talks to the Kubernetes API server and gets the pod manifests\\nfrom there, it can also run pods based on pod manifest files in a specific local direc-\\ntory as shown in figure 11.8. This feature is used to run the containerized versions of\\nthe Control Plane components as pods, as you saw in the beginning of the chapter.\\n Instead of running Kubernetes system components natively, you can put their pod\\nmanifests into the Kubelet’s manifest directory and have the Kubelet run and manage\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'PersistentVolumes',\n",
       "    'description': 'Storage resources in Kubernetes',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'Request for storage resources',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Kubernetes server that manages API objects',\n",
       "    'category': 'server'},\n",
       "   {'entity': 'Kubelets',\n",
       "    'description': 'Components that run on worker nodes and manage pods',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubernetes Service Proxies',\n",
       "    'description': 'Components that set up load balancing across pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Control Plane',\n",
       "    'description': 'Part of the Kubernetes system that handles controller updates',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Entities in Kubernetes that represent running containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Containers',\n",
       "    'description': 'Run-time environments for applications',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'Physical or virtual machine that runs a Kubelet',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Container runtime',\n",
       "    'description': 'Software that runs containers (e.g. Docker)',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API objects',\n",
       "    'description': 'Resources managed by the Kubernetes API server',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Kubelet manifest directory',\n",
       "    'description': 'Local directory where pod manifests are stored',\n",
       "    'category': 'directory'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Controller\", \"description\": \"updates a resource\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"starts the pod\\'s containers\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"monitors running containers and reports their status\", \"destination_entity\": \"Containers\"},\\n  {\"source_entity\": \"Controller\", \"description\": \"reclaims the volume according to the volume’s reclaim policy\", \"destination_entity\": \"PersistentVolumeClaim\"},\\n  {\"source_entity\": \"API server\", \"description\": \"performs work, such as spinning up a pod\\'s containers\", \"destination_entity\": \"Kubelets\"},\\n  {\"source_entity\": \"Control Plane\", \"description\": \"handles one part of the operation of the whole system\", \"destination_entity\": \"Kubernetes cluster\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"registers the node it’s running on by creating a Node resource in the API server\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"runs the container liveness probes, restarting containers when the probes fail\", \"destination_entity\": \"Containers\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"terminates containers when their Pod is deleted from the API server\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"API server\", \"description\": \"gets the pod manifests from there\", \"destination_entity\": \"Kubelet\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"runs pods based on pod manifest files in a specific local directory\", \"destination_entity\": \"Kubernetes system components\"},\\n  {\"source_entity\": \"Controller\", \"description\": \"operates on the API objects through the API server\", \"destination_entity\": \"API objects\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"monitors the API server for Pods that have been scheduled to the node\", \"destination_entity\": \"Pods\"}\\n]\\n```'},\n",
       " {'page': 359,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '327\\nUnderstanding the architecture\\nthem. You can also use the same method to run your custom system containers, but\\ndoing it through a DaemonSet is the recommended method.\\n11.1.8 The role of the Kubernetes Service Proxy\\nBeside the Kubelet, every worker node also runs the kube-proxy, whose purpose is to\\nmake sure clients can connect to the services you define through the Kubernetes API.\\nThe kube-proxy makes sure connections to the service IP and port end up at one of\\nthe pods backing that service (or other, non-pod service endpoints). When a service is\\nbacked by more than one pod, the proxy performs load balancing across those pods. \\nWHY IT’S CALLED A PROXY\\nThe initial implementation of the kube-proxy was the userspace proxy. It used an\\nactual server process to accept connections and proxy them to the pods. To inter-\\ncept connections destined to the service IPs, the proxy configured iptables rules\\n(iptables is the tool for managing the Linux kernel’s packet filtering features) to\\nredirect the connections to the proxy server. A rough diagram of the userspace proxy\\nmode is shown in figure 11.9.\\nContainer Runtime\\n(Docker, rkt, ...)\\nKubelet\\nAPI server\\nWorker node\\nRuns, monitors,\\nand manages\\ncontainers\\nPod resource\\nContainer A\\nContainer B\\nContainer A\\nContainer B\\nContainer C\\nPod manifest (ﬁle)\\nLocal manifest directory\\nContainer C\\nFigure 11.8\\nThe Kubelet runs pods based on pod specs from the API server and a local file directory.\\nClient\\nkube-proxy\\nConﬁgures\\n:\\niptables\\nredirect through proxy server\\niptables\\nPod\\nFigure 11.9\\nThe userspace proxy mode\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [API server\n",
       "   Pod resource\n",
       "   Container A\n",
       "   Container B, Col1, Worker node\n",
       "   Runs, monitors,\n",
       "   and manages\n",
       "   containers Container Runtime\n",
       "   Kubelet (Docker, rkt, ...)\n",
       "   Container A\n",
       "   Container B\n",
       "   Pod manifest (file)\n",
       "   Container C\n",
       "   Container C\n",
       "   Local manifest directory]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Kubelet',\n",
       "    'description': 'runs, monitors, and manages containers on a worker node',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'serves as an entry point for clients to interact with Kubernetes resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Worker node',\n",
       "    'description': 'a machine that runs the Kubelet and manages pods',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Kubernetes Service Proxy',\n",
       "    'description': 'ensures clients can connect to services through the Kubernetes API',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'DaemonSet',\n",
       "    'description': 'a controller that ensures a specified number of replicas of a pod are running at all times',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Pod resource',\n",
       "    'description': 'a container runtime (e.g. Docker, rkt) runs containers within pods',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Container A',\n",
       "    'description': 'a container that can be run within a pod',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Container B',\n",
       "    'description': 'a container that can be run within a pod',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Pod manifest (file)',\n",
       "    'description': 'a file that defines the configuration for a pod',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Local manifest directory',\n",
       "    'description': 'a directory where pod manifests are stored',\n",
       "    'category': 'directory'},\n",
       "   {'entity': 'Client',\n",
       "    'description': 'an entity that interacts with the Kubernetes API server',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kube-proxy',\n",
       "    'description': 'ensures connections to services end up at one of the pods backing that service',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'iptables',\n",
       "    'description': 'a tool for managing packet filtering features on a Linux kernel',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'redirect through proxy server',\n",
       "    'description': 'the process by which kube-proxy configures iptables rules to redirect connections',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"Runs and manages containers based on pod specs from the API server\",\\n    \"destination_entity\": \"Container A\"\\n  },\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"Runs and manages containers based on pod specs from the API server\",\\n    \"destination_entity\": \"Container B\"\\n  },\\n  {\\n    \"source_entity\": \"kube-proxy\",\\n    \"description\": \"Makes sure connections to the service IP and port end up at one of the pods backing that service\",\\n    \"destination_entity\": \"Pod resource\"\\n  },\\n  {\\n    \"source_entity\": \"kube-proxy\",\\n    \"description\": \"Performs load balancing across those pods when a service is backed by more than one pod\",\\n    \"destination_entity\": \"Multiple Pod resources\"\\n  },\\n  {\\n    \"source_entity\": \"Client\",\\n    \"description\": \"Connects to the services you define through the Kubernetes API using kube-proxy\",\\n    \"destination_entity\": \"Kubernetes Service Proxy\"\\n  },\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"Provides pod specs to the Kubelet for running and managing containers\",\\n    \"destination_entity\": \"Kubelet\"\\n  },\\n  {\\n    \"source_entity\": \"DaemonSet\",\\n    \"description\": \"Recommended method for running custom system containers on worker nodes\",\\n    \"destination_entity\": \"Worker node\"\\n  },\\n  {\\n    \"source_entity\": \"iptables\",\\n    \"description\": \"Configured to redirect connections to the proxy server\",\\n    \"destination_entity\": \"kube-proxy\"\\n  },\\n  {\\n    \"source_entity\": \"Pod manifest (file)\",\\n    \"description\": \"Used by the Kubelet to run and manage containers based on pod specs from the API server\",\\n    \"destination_entity\": \"Kubelet\"\\n  },\\n  {\\n    \"source_entity\": \"Local manifest directory\",\\n    \"description\": \"Contains a copy of the Pod manifest (file) for quick access by the Kubelet\",\\n    \"destination_entity\": \"Pod manifest (file)\"\\n  }\\n]\\n```'},\n",
       " {'page': 360,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '328\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\nThe kube-proxy got its name because it was an actual proxy, but the current, much\\nbetter performing implementation only uses iptables rules to redirect packets to a\\nrandomly selected backend pod without passing them through an actual proxy server.\\nThis mode is called the iptables proxy mode and is shown in figure 11.10.\\nThe major difference between these two modes is whether packets pass through the\\nkube-proxy and must be handled in user space, or whether they’re handled only by\\nthe Kernel (in kernel space). This has a major impact on performance. \\n Another smaller difference is that the userspace proxy mode balanced connec-\\ntions across pods in a true round-robin fashion, while the iptables proxy mode\\ndoesn’t—it selects pods randomly. When only a few clients use a service, they may not\\nbe spread evenly across pods. For example, if a service has two backing pods but only\\nfive or so clients, don’t be surprised if you see four clients connect to pod A and only\\none client connect to pod B. With a higher number of clients or pods, this problem\\nisn’t so apparent.\\n You’ll learn exactly how iptables proxy mode works in section 11.5. \\n11.1.9 Introducing Kubernetes add-ons\\nWe’ve now discussed the core components that make a Kubernetes cluster work. But\\nin the beginning of the chapter, we also listed a few add-ons, which although not\\nalways required, enable features such as DNS lookup of Kubernetes services, exposing\\nmultiple HTTP services through a single external IP address, the Kubernetes web\\ndashboard, and so on.\\nHOW ADD-ONS ARE DEPLOYED\\nThese components are available as add-ons and are deployed as pods by submitting\\nYAML manifests to the API server, the way you’ve been doing throughout the book.\\nSome of these components are deployed through a Deployment resource or a Repli-\\ncationController resource, and some through a DaemonSet. \\n For example, as I’m writing this, in Minikube, the Ingress controller and the\\ndashboard add-ons are deployed as ReplicationControllers, as shown in the follow-\\ning listing.\\n \\nClient\\nConﬁgures\\n:\\niptables\\nredirect straight to pod\\n(no proxy server in-between)\\niptables\\nPod\\nkube-proxy\\nFigure 11.10\\nThe iptables proxy mode\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kube-proxy',\n",
       "    'description': 'a component that redirects packets to a randomly selected backend pod without passing them through an actual proxy server',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'iptables',\n",
       "    'description': 'a Linux utility for managing network packet filtering and network address translation',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'backend pod',\n",
       "    'description': 'a container that runs on a node in the Kubernetes cluster and provides a service',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kube-proxy modes',\n",
       "    'description': 'two operating modes of kube-proxy: userspace proxy mode and iptables proxy mode',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'userspace proxy mode',\n",
       "    'description': 'a mode where packets pass through the kube-proxy and must be handled in user space',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'iptables proxy mode',\n",
       "    'description': 'a mode where packets are handled only by the Kernel (in kernel space)',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes add-ons',\n",
       "    'description': 'components that enable features such as DNS lookup of Kubernetes services and exposing multiple HTTP services through a single external IP address',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Deployment resource',\n",
       "    'description': 'a Kubernetes object that manages rollout and updates of Pods in a cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'a Kubernetes object that ensures a specified number of replicas (Pods) are running at any given time',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'DaemonSet',\n",
       "    'description': 'a Kubernetes object that ensures a specified number of copies of a Pod are running across all nodes in the cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Ingress controller',\n",
       "    'description': 'a component that enables feature such as DNS lookup of Kubernetes services and exposing multiple HTTP services through a single external IP address',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'dashboard add-ons',\n",
       "    'description': 'components that enable feature such as web dashboard for Kubernetes cluster',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kube-proxy\", \"description\": \"uses iptables rules to redirect packets to a randomly selected backend pod\", \"destination_entity\": \"backend pod\"},\\n  {\"source_entity\": \"iptables proxy mode\", \"description\": \"doesn\\'t pass packets through an actual proxy server, instead using iptables rules to redirect packets\", \"destination_entity\": \"kube-proxy\"},\\n  {\"source_entity\": \"userspace proxy mode\", \"description\": \"balances connections across pods in a true round-robin fashion\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"iptables proxy mode\", \"description\": \"selects pods randomly\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"kube-proxy\", \"description\": \"must handle packets in user space\", \"destination_entity\": \"Kernel\"},\\n  {\"source_entity\": \"userspace proxy mode\", \"description\": \"handles connections across pods in a true round-robin fashion\", \"destination_entity\": \"clients\"},\\n  {\"source_entity\": \"iptables proxy mode\", \"description\": \"may not spread clients evenly across pods\", \"destination_entity\": \"clients\"},\\n  {\"source_entity\": \"Kubernetes add-ons\", \"description\": \"enable features such as DNS lookup of Kubernetes services\", \"destination_entity\": \"services\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"deploys components through a ReplicationController resource\", \"destination_entity\": \"components\"},\\n  {\"source_entity\": \"DaemonSet\", \"description\": \"deploys components through a DaemonSet resource\", \"destination_entity\": \"components\"},\\n  {\"source_entity\": \"Ingress controller\", \"description\": \"is deployed as a ReplicationController in Minikube\", \"destination_entity\": \"Minikube\"},\\n  {\"source_entity\": \"dashboard add-ons\", \"description\": \"are deployed as ReplicationControllers in Minikube\", \"destination_entity\": \"Minikube\"}\\n]\\n```'},\n",
       " {'page': 361,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '329\\nUnderstanding the architecture\\n$ kubectl get rc -n kube-system\\nNAME                       DESIRED   CURRENT   READY     AGE\\ndefault-http-backend       1         1         1         6d\\nkubernetes-dashboard       1         1         1         6d\\nnginx-ingress-controller   1         1         1         6d\\nThe DNS add-on is deployed as a Deployment, as shown in the following listing.\\n$ kubectl get deploy -n kube-system\\nNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\\nkube-dns   1         1         1            1           6d\\nLet’s see how DNS and the Ingress controllers work.\\nHOW THE DNS SERVER WORKS\\nAll the pods in the cluster are configured to use the cluster’s internal DNS server by\\ndefault. This allows pods to easily look up services by name or even the pod’s IP\\naddresses in the case of headless services.\\n The DNS server pod is exposed through the kube-dns service, allowing the pod to\\nbe moved around the cluster, like any other pod. The service’s IP address is specified\\nas the nameserver in the /etc/resolv.conf file inside every container deployed in the\\ncluster. The kube-dns pod uses the API server’s watch mechanism to observe changes\\nto Services and Endpoints and updates its DNS records with every change, allowing its\\nclients to always get (fairly) up-to-date DNS information. I say fairly because during\\nthe time between the update of the Service or Endpoints resource and the time the\\nDNS pod receives the watch notification, the DNS records may be invalid.\\nHOW (MOST) INGRESS CONTROLLERS WORK\\nUnlike the DNS add-on, you’ll find a few different implementations of Ingress con-\\ntrollers, but most of them work in the same way. An Ingress controller runs a reverse\\nproxy server (like Nginx, for example), and keeps it configured according to the\\nIngress, Service, and Endpoints resources defined in the cluster. The controller thus\\nneeds to observe those resources (again, through the watch mechanism) and change\\nthe proxy server’s config every time one of them changes. \\n Although the Ingress resource’s definition points to a Service, Ingress controllers\\nforward traffic to the service’s pod directly instead of going through the service IP.\\nThis affects the preservation of client IPs when external clients connect through the\\nIngress controller, which makes them preferred over Services in certain use cases.\\nUSING OTHER ADD-ONS\\nYou’ve seen how both the DNS server and the Ingress controller add-ons are similar to\\nthe controllers running in the Controller Manager, except that they also accept client\\nconnections instead of only observing and modifying resources through the API server. \\nListing 11.7\\nAdd-ons deployed with ReplicationControllers in Minikube\\nListing 11.8\\nThe kube-dns Deployment \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for managing Kubernetes resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'rc',\n",
       "    'description': 'ReplicaController resource in Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'default-http-backend',\n",
       "    'description': 'Default HTTP backend service in Kubernetes',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'kubernetes-dashboard',\n",
       "    'description': 'Kubernetes dashboard service',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'nginx-ingress-controller',\n",
       "    'description': 'Nginx Ingress controller service',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'deploy',\n",
       "    'description': 'Deployment resource in Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kube-dns',\n",
       "    'description': 'Kube DNS deployment',\n",
       "    'category': 'deployment'},\n",
       "   {'entity': 'DNS server pod',\n",
       "    'description': 'Pod running the DNS server service',\n",
       "    'category': 'pod'},\n",
       "   {'entity': 'kube-dns service',\n",
       "    'description': 'Service exposing the DNS server pod',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'nameserver',\n",
       "    'description': 'Nameserver configuration for containers',\n",
       "    'category': 'process'},\n",
       "   {'entity': \"API server's watch mechanism\",\n",
       "    'description': 'Mechanism for observing changes to resources',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Ingress controller',\n",
       "    'description': 'Reverse proxy server running in the cluster',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'Nginx reverse proxy',\n",
       "    'description': 'Specific implementation of an Ingress controller',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'watch mechanism',\n",
       "    'description': 'Mechanism for observing changes to resources',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Service and Endpoints resources',\n",
       "    'description': 'Resources observed by the DNS server pod',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\":\"nameserver\",\"description\":\"resolves DNS queries for pods\",\"destination_entity\":\"pods\"},{\"source_entity\":\"nameserver\",\"description\":\"gets updated with Services and Endpoints changes via API server\\'s watch mechanism\",\"destination_entity\":\"API server\\'s watch mechanism\"},{\"source_entity\":\"nameserver\",\"description\":\"provides up-to-date DNS information to clients\",\"destination_entity\":\"clients\"},{\"source_entity\":\"kubectl\",\"description\":\"gets resource information from cluster\",\"destination_entity\":\"cluster\"},{\"source_entity\":\"kubectl\",\"description\":\"displays resource information in a readable format\",\"destination_entity\":\"users\"},{\"source_entity\":\"kube-dns\",\"description\":\"runs as a Deployment and is updated with Services and Endpoints changes via API server\\'s watch mechanism\",\"destination_entity\":\"API server\\'s watch mechanism\"},{\"source_entity\":\"kube-dns\",\"description\":\"uses cluster\\'s internal DNS server by default to resolve service names or pod IP addresses\",\"destination_entity\":\"cluster\"},{\"source_entity\":\"rc\",\"description\":\"deploys add-ons with ReplicationControllers in Minikube\",\"destination_entity\":\"Minikube\"},{\"source_entity\":\"Nginx reverse proxy\",\"description\":\"runs as an Ingress controller and forwards traffic directly to service\\'s pod instead of going through the service IP\",\"destination_entity\":\"service\\'s pod\"},{\"source_entity\":\"Nginx reverse proxy\",\"description\":\"keeps itself configured according to Ingress, Service, and Endpoints resources defined in the cluster\",\"destination_entity\":\"Ingress, Service, and Endpoints resources\"},{\"source_entity\":\"default-http-backend\",\"description\":\"runs as a Deployment with 1 replica in kube-system namespace\",\"destination_entity\":\"kube-system namespace\"},{\"source_entity\":\"API server\\'s watch mechanism\",\"description\":\"notifies DNS server pod of Services and Endpoints changes\",\"destination_entity\":\"DNS server pod\"},{\"source_entity\":\"nginx-ingress-controller\",\"description\":\"runs as an Ingress controller and observes Service and Endpoints resources through the API server\\'s watch mechanism\",\"destination_entity\":\"Service and Endpoints resources\"},{\"source_entity\":\"kubernetes-dashboard\",\"description\":\"runs as a Deployment with 1 replica in kube-system namespace\",\"destination_entity\":\"kube-system namespace\"},{\"source_entity\":\"watch mechanism\",\"description\":\"updates Ingress controllers with changes to Service and Endpoints resources\",\"destination_entity\":\"Ingress controllers\"},{\"source_entity\":\"kube-dns service\",\"description\":\"exposes the DNS server pod through its IP address\",\"destination_entity\":\"DNS server pod\"},{\"source_entity\":\"DNS server pod\",\"description\":\"uses cluster\\'s internal DNS server by default to resolve service names or pod IP addresses\",\"destination_entity\":\"cluster\"},{\"source_entity\":\"Service and Endpoints resources\",\"description\":\"are observed by Ingress controllers through the API server\\'s watch mechanism\",\"destination_entity\":\"Ingress controllers\"},{\"source_entity\":\"Ingress controller\",\"description\":\"observes Service and Endpoints resources through the API server\\'s watch mechanism\",\"destination_entity\":\"Service and Endpoints resources\"},{\"source_entity\":\"deploy\",\"description\":\"deploys add-ons with ReplicationControllers in Minikube\",\"destination_entity\":\"Minikube\"}]'},\n",
       " {'page': 362,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '330\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\n Other add-ons are similar. They all need to observe the cluster state and perform\\nthe necessary actions when that changes. We’ll introduce a few other add-ons in this\\nand the remaining chapters.\\n11.1.10Bringing it all together\\nYou’ve now learned that the whole Kubernetes system is composed of relatively small,\\nloosely coupled components with good separation of concerns. The API server, the\\nScheduler, the individual controllers running inside the Controller Manager, the\\nKubelet, and the kube-proxy all work together to keep the actual state of the system\\nsynchronized with what you specify as the desired state. \\n For example, submitting a pod manifest to the API server triggers a coordinated\\ndance of various Kubernetes components, which eventually results in the pod’s con-\\ntainers running. You’ll learn how this dance unfolds in the next section. \\n11.2\\nHow controllers cooperate\\nYou now know about all the components that a Kubernetes cluster is comprised of.\\nNow, to solidify your understanding of how Kubernetes works, let’s go over what hap-\\npens when a Pod resource is created. Because you normally don’t create Pods directly,\\nyou’re going to create a Deployment resource instead and see everything that must\\nhappen for the pod’s containers to be started.\\n11.2.1 Understanding which components are involved\\nEven before you start the whole process, the controllers, the Scheduler, and the\\nKubelet are watching the API server for changes to their respective resource types.\\nThis is shown in figure 11.11. The components depicted in the figure will each play a\\npart in the process you’re about to trigger. The diagram doesn’t include etcd, because\\nit’s hidden behind the API server, and you can think of the API server as the place\\nwhere objects are stored.\\nMaster node\\nController Manager\\nWatches\\nDeployment\\ncontroller\\nScheduler\\nReplicaSet\\ncontroller\\nAPI server\\nDeployments\\nPods\\nReplicaSets\\nWatches\\nWatches\\nNode X\\nWatches\\nDocker\\nKubelet\\nFigure 11.11\\nKubernetes components watching API objects through the API server\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Master node\n",
       "   Controller Manager API server\n",
       "   Watches\n",
       "   Deployment\n",
       "   Deployments\n",
       "   controller\n",
       "   Watches\n",
       "   ReplicaSet\n",
       "   ReplicaSets\n",
       "   controller\n",
       "   Watches\n",
       "   Scheduler Pods, Watches, Node X\n",
       "   Kubelet\n",
       "   Docker]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'API Server',\n",
       "    'description': 'The central component that stores and manages all Kubernetes resources.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'A component responsible for scheduling pods onto nodes in a cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'A component that runs multiple controllers, which are responsible for managing specific resources.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': \"The agent running on each node in a cluster, responsible for ensuring the node's state is consistent with the desired state.\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kube-proxy',\n",
       "    'description': 'A network proxy that enables communication between pods and services across a cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A container running in the cluster, which can be managed by various controllers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A resource that manages pods and ensures a specified number of replicas are running at any given time.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A resource that ensures a specified number of identical pods are running at any given time.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'A distributed key-value store used by the API server to store and manage cluster state.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'A container runtime engine used to run containers in a cluster.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"API Server\", \"description\": \"stores objects\", \"destination_entity\": \"etcd\"},\\n  {\"source_entity\": \"API Server\", \"description\": \"watches for changes to resource types\", \"destination_entity\": \"controllers\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"watches the API server for changes to pods\", \"destination_entity\": \"API Server\"},\\n  {\"source_entity\": \"Scheduler\", \"description\": \"watches the API server for changes to pods and deployments\", \"destination_entity\": \"API Server\"},\\n  {\"source_entity\": \"Controller Manager\", \"description\": \"runs individual controllers that watch the cluster state\", \"destination_entity\": \"Cluster State\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"is created as a desired state\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"ReplicaSet\", \"description\": \"watches for changes to deployments and pods\", \"destination_entity\": \"API Server\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"starts the pod\\'s containers\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Docker\", \"description\": \"is used by Kubelet to start containers\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"ReplicaSet controller\", \"description\": \"ensures a desired number of replicas for a pod or deployment\", \"destination_entity\": \"API Server\"}\\n]\\n```\\n\\nI have extracted the following relations based on the provided text and entities:\\n\\n1. The API server stores objects in etcd.\\n2. The API server watches for changes to resource types, which are observed by controllers.\\n3. Kubelet watches the API server for changes to pods.\\n4. The Scheduler watches the API server for changes to pods and deployments.\\n5. Controller Manager runs individual controllers that watch the cluster state.\\n6. A Deployment is created as a desired state, which affects Pods.\\n7. ReplicaSet watches for changes to Deployments and Pods.\\n8. Kubelet starts the pod\\'s containers.\\n9. Docker is used by Kubelet to start containers.\\n10. The ReplicaSet controller ensures a desired number of replicas for a Pod or Deployment.\\n\\nNote that I have only extracted relations between entities mentioned in the provided text, and not all possible relations may be included.'},\n",
       " {'page': 363,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '331\\nHow controllers cooperate\\n11.2.2 The chain of events\\nImagine you prepared the YAML file containing the Deployment manifest and you’re\\nabout to submit it to Kubernetes through kubectl. kubectl sends the manifest to the\\nKubernetes API server in an HTTP POST request. The API server validates the Deploy-\\nment specification, stores it in etcd, and returns a response to kubectl. Now a chain\\nof events starts to unfold, as shown in figure 11.12.\\nTHE DEPLOYMENT CONTROLLER CREATES THE REPLICASET\\nAll API server clients watching the list of Deployments through the API server’s watch\\nmechanism are notified of the newly created Deployment resource immediately after\\nit’s created. One of those clients is the Deployment controller, which, as we discussed\\nearlier, is the active component responsible for handling Deployments. \\n As you may remember from chapter 9, a Deployment is backed by one or more\\nReplicaSets, which then create the actual pods. As a new Deployment object is\\ndetected by the Deployment controller, it creates a ReplicaSet for the current speci-\\nfication of the Deployment. This involves creating a new ReplicaSet resource\\nthrough the Kubernetes API. The Deployment controller doesn’t deal with individ-\\nual pods at all.\\nMaster node\\nController\\nManager\\n2. Notiﬁcation\\nthrough watch\\n3. Creates\\nReplicaSet\\n4. Notiﬁcation\\n5. Creates pod\\n6. Notiﬁcation\\nthrough watch\\n7. Assigns pod to node\\n1. Creates Deployment\\nresource\\nDeployment\\ncontroller\\nScheduler\\nkubectl\\nReplicaSet\\ncontroller\\nAPI server\\nDeployment A\\nDeployments\\nReplicaSets\\nPod A\\nPods\\nReplicaSet A\\nNode X\\n8. Notiﬁcation\\nthrough watch\\n9. Tells Docker to\\nrun containers\\nDocker\\n10. Runs\\ncontainers\\nContainer(s)\\nKubelet\\nFigure 11.12\\nThe chain of events that unfolds when a Deployment resource is posted to the API server\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Master node\n",
       "   1. Creates Deployment\n",
       "   resource\n",
       "   kubectl\n",
       "   API server\n",
       "   Controller\n",
       "   Manager 2. Notification Deployments\n",
       "   through watch\n",
       "   Deployment Deployment A\n",
       "   controller\n",
       "   3. Creates\n",
       "   ReplicaSet\n",
       "   ReplicaSets\n",
       "   4. Notification\n",
       "   ReplicaSet ReplicaSet A\n",
       "   controller\n",
       "   5. Creates pod\n",
       "   Pods\n",
       "   6. Notification\n",
       "   through watch\n",
       "   Scheduler Pod A\n",
       "   7. Assigns pod to node, Node X\n",
       "   8. Notification\n",
       "   through watch\n",
       "   Kubelet\n",
       "   9. Tells Docker to\n",
       "   run containers\n",
       "   Docker\n",
       "   10. Runs\n",
       "   containers\n",
       "   Container(s)]\n",
       "   Index: [],\n",
       "                                         Col0  Col1  Col2\n",
       "   0  3. Creates\\nReplicaSet\\n4. Notification  None  None\n",
       "   1                                     None            ,\n",
       "      Col0 Container(s)          Col2 Col3\n",
       "   0  None         None  Container(s)     ],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'Command line tool for interacting with Kubernetes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Kubernetes API server responsible for validating and storing deployment specifications.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment controller',\n",
       "    'description': 'Active component responsible for handling Deployments in Kubernetes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSet controller',\n",
       "    'description': 'Component responsible for creating ReplicaSets for Deployments in Kubernetes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployments',\n",
       "    'description': 'Kubernetes resource representing a set of running Pods.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicaSets',\n",
       "    'description': 'Kubernetes resource used to manage a specified number of replicas (running instances) of a Pod.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Lightweight and highly portable Linux containers in Kubernetes.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'Containerization platform used by Kubernetes to run Containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'Agent that runs on each node in a cluster, responsible for starting and stopping containers.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"sends manifest to API server\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"API server\", \"description\": \"validates Deployment specification and stores it in etcd\", \"destination_entity\": \"etcd\"},\\n  {\"source_entity\": \"Deployment controller\", \"description\": \"creates ReplicaSet for the current specification of the Deployment\", \"destination_entity\": \"ReplicaSet controller\"},\\n  {\"source_entity\": \"Deployment controller\", \"description\": \"creates a new ReplicaSet resource through the Kubernetes API\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"Deployment controller\", \"description\": \"notifies all API server clients watching the list of Deployments\", \"destination_entity\": \"ReplicaSet controller\"},\\n  {\"source_entity\": \"ReplicaSet controller\", \"description\": \"creates a new ReplicaSet resource through the Kubernetes API\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"ReplicaSet controller\", \"description\": \"notifies all API server clients watching the list of Deployments\", \"destination_entity\": \"Deployment controller\"},\\n  {\"source_entity\": \"Deployment controller\", \"description\": \"doesn\\'t deal with individual pods at all\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"ReplicaSet controller\", \"description\": \"creates a new pod through the Kubernetes API\", \"destination_entity\": \"Kubelet\"},\\n  {\"source_entity\": \"API server\", \"description\": \"notifies ReplicaSet controller of newly created Deployment resource\", \"destination_entity\": \"ReplicaSet controller\"},\\n  {\"source_entity\": \"Deployment controller\", \"description\": \"notifies all API server clients watching the list of Deployments\", \"destination_entity\": \"Kubelet\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"runs containers on Docker\", \"destination_entity\": \"Docker\"},\\n  {\"source_entity\": \"ReplicaSet controller\", \"description\": \"assigns pod to node through Scheduler\", \"destination_entity\": \"Node X\"},\\n  {\"source_entity\": \"Deployment controller\", \"description\": \"notifies all API server clients watching the list of Deployments\", \"destination_entity\": \"Deployments\"}\\n]'},\n",
       " {'page': 364,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '332\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\nTHE REPLICASET CONTROLLER CREATES THE POD RESOURCES\\nThe newly created ReplicaSet is then picked up by the ReplicaSet controller, which\\nwatches for creations, modifications, and deletions of ReplicaSet resources in the\\nAPI server. The controller takes into consideration the replica count and pod selec-\\ntor defined in the ReplicaSet and verifies whether enough existing Pods match\\nthe selector.\\n The controller then creates the Pod resources based on the pod template in the\\nReplicaSet (the pod template was copied over from the Deployment when the Deploy-\\nment controller created the ReplicaSet). \\nTHE SCHEDULER ASSIGNS A NODE TO THE NEWLY CREATED PODS\\nThese newly created Pods are now stored in etcd, but they each still lack one import-\\nant thing—they don’t have an associated node yet. Their nodeName attribute isn’t set.\\nThe Scheduler watches for Pods like this, and when it encounters one, chooses the\\nbest node for the Pod and assigns the Pod to the node. The Pod’s definition now\\nincludes the name of the node it should be running on.\\n Everything so far has been happening in the Kubernetes Control Plane. None of\\nthe controllers that have taken part in this whole process have done anything tangible\\nexcept update the resources through the API server. \\nTHE KUBELET RUNS THE POD’S CONTAINERS\\nThe worker nodes haven’t done anything up to this point. The pod’s containers\\nhaven’t been started yet. The images for the pod’s containers haven’t even been down-\\nloaded yet. \\n But with the Pod now scheduled to a specific node, the Kubelet on that node can\\nfinally get to work. The Kubelet, watching for changes to Pods on the API server, sees a\\nnew Pod scheduled to its node, so it inspects the Pod definition and instructs Docker,\\nor whatever container runtime it’s using, to start the pod’s containers. The container\\nruntime then runs the containers.\\n11.2.3 Observing cluster events\\nBoth the Control Plane components and the Kubelet emit events to the API server as\\nthey perform these actions. They do this by creating Event resources, which are like\\nany other Kubernetes resource. You’ve already seen events pertaining to specific\\nresources every time you used kubectl describe to inspect those resources, but you\\ncan also retrieve events directly with kubectl get events.\\n Maybe it’s me, but using kubectl get to inspect events is painful, because they’re\\nnot shown in proper temporal order. Instead, if an event occurs multiple times, the\\nevent is displayed only once, showing when it was first seen, when it was last seen, and\\nthe number of times it occurred. Luckily, watching events with the --watch option is\\nmuch easier on the eyes and useful for seeing what’s happening in the cluster. \\n The following listing shows the events emitted in the process described previously\\n(some columns have been removed and the output is edited heavily to make it legible\\nin the limited space on the page).\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 365,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '333\\nUnderstanding what a running pod is\\n$ kubectl get events --watch\\n    NAME             KIND         REASON              SOURCE \\n... kubia            Deployment   ScalingReplicaSet   deployment-controller  \\n                     ➥ Scaled up replica set kubia-193 to 3\\n... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  \\n                     ➥ Created pod: kubia-193-w7ll2\\n... kubia-193-tpg6j  Pod          Scheduled           default-scheduler   \\n                     ➥ Successfully assigned kubia-193-tpg6j to node1\\n... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  \\n                     ➥ Created pod: kubia-193-39590\\n... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  \\n                     ➥ Created pod: kubia-193-tpg6j\\n... kubia-193-39590  Pod          Scheduled           default-scheduler  \\n                     ➥ Successfully assigned kubia-193-39590 to node2\\n... kubia-193-w7ll2  Pod          Scheduled           default-scheduler  \\n                     ➥ Successfully assigned kubia-193-w7ll2 to node2\\n... kubia-193-tpg6j  Pod          Pulled              kubelet, node1  \\n                     ➥ Container image already present on machine\\n... kubia-193-tpg6j  Pod          Created             kubelet, node1  \\n                     ➥ Created container with id 13da752\\n... kubia-193-39590  Pod          Pulled              kubelet, node2  \\n                     ➥ Container image already present on machine\\n... kubia-193-tpg6j  Pod          Started             kubelet, node1  \\n                     ➥ Started container with id 13da752\\n... kubia-193-w7ll2  Pod          Pulled              kubelet, node2  \\n                     ➥ Container image already present on machine\\n... kubia-193-39590  Pod          Created             kubelet, node2  \\n                     ➥ Created container with id 8850184\\n...\\nAs you can see, the SOURCE column shows the controller performing the action, and\\nthe NAME and KIND columns show the resource the controller is acting on. The REASON\\ncolumn and the MESSAGE column (shown in every second line) give more details\\nabout what the controller has done.\\n11.3\\nUnderstanding what a running pod is\\nWith the pod now running, let’s look more closely at what a running pod even is. If a\\npod contains a single container, do you think that the Kubelet just runs this single\\ncontainer, or is there more to it?\\n You’ve run several pods throughout this book. If you’re the investigative type, you\\nmay have already snuck a peek at what exactly Docker ran when you created a pod. If\\nnot, let me explain what you’d see.\\n Imagine you run a single container pod. Let’s say you create an Nginx pod:\\n$ kubectl run nginx --image=nginx\\ndeployment \"nginx\" created\\nYou can now ssh into the worker node running the pod and inspect the list of run-\\nning Docker containers. I’m using Minikube to test this out, so to ssh into the single\\nListing 11.9\\nWatching events emitted by the controllers\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A pod is a logical host for one or more application containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A deployment is a way to manage replicasets and their pods.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A ReplicaSet is a controller that ensures a specified number of replicas (identical Pods) are running at any given time.',\n",
       "    'category': 'controller'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'The Kubelet is an agent that runs on each node in the cluster, responsible for starting and stopping containers.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'A container runtime that allows you to run applications in isolated environments.',\n",
       "    'category': 'container runtime'},\n",
       "   {'entity': 'minikube',\n",
       "    'description': 'A tool for running a single-node Kubernetes cluster on your laptop or local machine.',\n",
       "    'category': 'hypervisor'},\n",
       "   {'entity': 'SSH',\n",
       "    'description': 'A protocol for secure remote access to a node in the cluster.',\n",
       "    'category': 'network protocol'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"deployment-controller\", \"description\": \"Scaled up\", \"destination_entity\": \"replica set kubia-193\"},\\n  {\"source_entity\": \"replicaset-controller\", \"description\": \"Created pod\", \"destination_entity\": \"pod: kubia-193-w7ll2\"},\\n  {\"source_entity\": \"default-scheduler\", \"description\": \"Scheduled\", \"destination_entity\": \"pod kubia-193-tpg6j\"},\\n  {\"source_entity\": \"replicaset-controller\", \"description\": \"Created pod\", \"destination_entity\": \"pod: kubia-193-39590\"},\\n  {\"source_entity\": \"default-scheduler\", \"description\": \"Scheduled\", \"destination_entity\": \"pod kubia-193-39590\"},\\n  {\"source_entity\": \"default-scheduler\", \"description\": \"Scheduled\", \"destination_entity\": \"pod kubia-193-w7ll2\"},\\n  {\"source_entity\": \"kubelet, node1\", \"description\": \"Pulled container image\", \"destination_entity\": \"pod kubia-193-tpg6j\"},\\n  {\"source_entity\": \"kubelet, node1\", \"description\": \"Created container\", \"destination_entity\": \"container with id 13da752\"},\\n  {\"source_entity\": \"kubelet, node2\", \"description\": \"Pulled container image\", \"destination_entity\": \"pod kubia-193-39590\"},\\n  {\"source_entity\": \"kubelet, node2\", \"description\": \"Created container\", \"destination_entity\": \"container with id 8850184\"},\\n  {\"source_entity\": \"minikube\", \"description\": \"Runs Docker containers\", \"destination_entity\": \"Docker\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"Creates pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"SSH\", \"description\": \"Inspects running Docker containers\", \"destination_entity\": \"Docker\"},\\n  {\"source_entity\": \"deployment-controller\", \"description\": \"Scales up replica set\", \"destination_entity\": \"ReplicaSet\"},\\n  {\"source_entity\": \"replicaset-controller\", \"description\": \"Creates pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"default-scheduler\", \"description\": \"Schedules pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Docker\", \"description\": \"Runs container\", \"destination_entity\": \"Container\"},\\n  {\"source_entity\": \"minikube\", \"description\": \"Creates deployment\", \"destination_entity\": \"Deployment\"}\\n]\\n```\\n\\nNote that I\\'ve extracted all the relations mentioned in the document, even if they seem trivial or obvious. Let me know if you\\'d like me to filter them out!'},\n",
       " {'page': 366,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '334\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\nnode, I use minikube ssh. If you’re using GKE, you can ssh into a node with gcloud\\ncompute ssh <node name>.\\n Once you’re inside the node, you can list all the running containers with docker\\nps, as shown in the following listing.\\ndocker@minikubeVM:~$ docker ps\\nCONTAINER ID   IMAGE                  COMMAND                 CREATED\\nc917a6f3c3f7   nginx                  \"nginx -g \\'daemon off\"  4 seconds ago \\n98b8bf797174   gcr.io/.../pause:3.0   \"/pause\"                7 seconds ago\\n...\\nNOTE\\nI’ve removed irrelevant information from the previous listing—this\\nincludes both columns and rows. I’ve also removed all the other running con-\\ntainers. If you’re trying this out yourself, pay attention to the two containers\\nthat were created a few seconds ago. \\nAs expected, you see the Nginx container, but also an additional container. Judging\\nfrom the COMMAND column, this additional container isn’t doing anything (the con-\\ntainer’s command is \"pause\"). If you look closely, you’ll see that this container was\\ncreated a few seconds before the Nginx container. What’s its role?\\n This pause container is the container that holds all the containers of a pod\\ntogether. Remember how all containers of a pod share the same network and other\\nLinux namespaces? The pause container is an infrastructure container whose sole\\npurpose is to hold all these namespaces. All other user-defined containers of the pod\\nthen use the namespaces of the pod infrastructure container (see figure 11.13).\\nActual application containers may die and get restarted. When such a container starts\\nup again, it needs to become part of the same Linux namespaces as before. The infra-\\nstructure container makes this possible since its lifecycle is tied to that of the pod—the\\ncontainer runs from the time the pod is scheduled until the pod is deleted. If the\\ninfrastructure pod is killed in the meantime, the Kubelet recreates it and all the pod’s\\ncontainers.\\nListing 11.10\\nListing running Docker containers\\nPod\\nContainer A\\nContainer A\\nPod infrastructure\\ncontainer\\nContainer B\\nContainer B\\nUses Linux\\nnamespaces from\\nUses Linux\\nnamespaces from\\nFigure 11.13\\nA two-container pod results in three running containers \\nsharing the same Linux namespaces.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'node', 'description': '', 'category': 'container'},\n",
       "   {'entity': 'minikube ssh', 'description': '', 'category': 'command'},\n",
       "   {'entity': 'gcloud compute ssh <node name>',\n",
       "    'description': '',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'docker ps',\n",
       "    'description': 'list all running containers',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'CONTAINER ID',\n",
       "    'description': 'unique identifier for a container',\n",
       "    'category': 'attribute'},\n",
       "   {'entity': 'IMAGE',\n",
       "    'description': 'image used to create a container',\n",
       "    'category': 'attribute'},\n",
       "   {'entity': 'COMMAND',\n",
       "    'description': 'command run by a container',\n",
       "    'category': 'attribute'},\n",
       "   {'entity': 'pause', 'description': '', 'category': 'container'},\n",
       "   {'entity': 'Nginx',\n",
       "    'description': 'web server software',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod infrastructure container',\n",
       "    'description': 'holds all containers of a pod together',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Kubelet', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'Docker', 'description': '', 'category': 'container engine'},\n",
       "   {'entity': 'gcr.io/.../pause:3.0', 'description': '', 'category': 'image'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"docker\", \"description\": \"list running containers\", \"destination_entity\": \"docker ps\"},\\n  {\"source_entity\": \"docker\", \"description\": \"list running containers\", \"destination_entity\": \"CONTAINER ID\"},\\n  {\"source_entity\": \"docker\", \"description\": \"list running containers\", \"destination_entity\": \"IMAGE\"},\\n  {\"source_entity\": \"docker\", \"description\": \"list running containers\", \"destination_entity\": \"COMMAND\"},\\n  {\"source_entity\": \"pause\", \"description\": \"holds all namespaces for a pod\", \"destination_entity\": \"pod infrastructure container\"},\\n  {\"source_entity\": \"pause\", \"description\": \"recreated by Kubelet if killed\", \"destination_entity\": \"Kubelet\"},\\n  {\"source_entity\": \"pause\", \"description\": \"creates namespace for Container A\", \"destination_entity\": \"Container A\"},\\n  {\"source_entity\": \"pause\", \"description\": \"creates namespace for Container B\", \"destination_entity\": \"Container B\"},\\n  {\"source_entity\": \"docker ps\", \"description\": \"lists running containers created a few seconds ago\", \"destination_entity\": \"Nginx container\"},\\n  {\"source_entity\": \"docker ps\", \"description\": \"lists running containers created a few seconds ago\", \"destination_entity\": \"pause container\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"recreates pause container if killed\", \"destination_entity\": \"pause container\"}\\n]'},\n",
       " {'page': 367,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '335\\nInter-pod networking\\n11.4\\nInter-pod networking\\nBy now, you know that each pod gets its own unique IP address and can communicate\\nwith all other pods through a flat, NAT-less network. How exactly does Kubernetes\\nachieve this? In short, it doesn’t. The network is set up by the system administrator or\\nby a Container Network Interface (CNI) plugin, not by Kubernetes itself. \\n11.4.1 What the network must be like\\nKubernetes doesn’t require you to use a specific networking technology, but it does\\nmandate that the pods (or to be more precise, their containers) can communicate\\nwith each other, regardless if they’re running on the same worker node or not. The\\nnetwork the pods use to communicate must be such that the IP address a pod sees as\\nits own is the exact same address that all other pods see as the IP address of the pod in\\nquestion. \\n Look at figure 11.14. When pod A connects to (sends a network packet to) pod B,\\nthe source IP pod B sees must be the same IP that pod A sees as its own. There should\\nbe no network address translation (NAT) performed in between—the packet sent by\\npod A must reach pod B with both the source and destination address unchanged.\\nThis is important, because it makes networking for applications running inside pods\\nsimple and exactly as if they were running on machines connected to the same net-\\nwork switch. The absence of NAT between pods enables applications running inside\\nthem to self-register in other pods. \\nNode 1\\nPod A\\nIP: 10.1.1.1\\nsrcIP: 10.1.1.1\\ndstIP: 10.1.2.1\\nsrcIP: 10.1.1.1\\ndstIP: 10.1.2.1\\nPacket\\nNode 2\\nPod B\\nIP: 10.1.2.1\\nsrcIP: 10.1.1.1\\ndstIP: 10.1.2.1\\nPacket\\nNetwork\\nNo NAT (IPs\\nare preserved)\\nFigure 11.14\\nKubernetes mandates pods are connected through a NAT-less \\nnetwork.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  No NAT (IPs\\nare preserved)\\nNode 1 Node 2\\nPod A Pod B\\nIP: 10.1.1.1 IP: 10.1.2.1\\nPacket Packet\\nsrcIP: 10.1.1.1 srcIP: 10.1.1.1\\ndstIP: 10.1.2.1 dstIP: 10.1.2.1  \\\n",
       "   0  Node 1\\nPod A\\nIP: 10.1.1.1\\nPacket\\nsrcIP: 10...                                                                                                                    \n",
       "   1                                                                                                                                                                       \n",
       "   2                                            Network                                                                                                                    \n",
       "   3                   srcIP: 10.1.1.1\\ndstIP: 10.1.2.1                                                                                                                    \n",
       "   \n",
       "                                                   Col1  Col2  \n",
       "   0  Node 2\\nPod B\\nIP: 10.1.2.1\\nPacket\\nsrcIP: 10...  None  \n",
       "   1                                               None        \n",
       "   2                                               None  None  \n",
       "   3                                               None  None  ],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Lightweight and portable executable container for application code',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'IP address',\n",
       "    'description': 'Unique numerical identifier assigned to each pod',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Network Address Translation (NAT)',\n",
       "    'description': 'Process of modifying IP addresses in network packets',\n",
       "    'category': 'networking'},\n",
       "   {'entity': 'CNI plugin',\n",
       "    'description': 'Container Network Interface plugin for setting up networking infrastructure',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Container Network Interface (CNI)',\n",
       "    'description': 'API and runtime for container network interfaces',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod A',\n",
       "    'description': 'First pod in communication scenario',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Pod B',\n",
       "    'description': 'Second pod in communication scenario',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Node 1',\n",
       "    'description': 'First worker node',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Node 2',\n",
       "    'description': 'Second worker node',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n    {\"source_entity\": \"Kubernetes\", \"description\": \"mandates that pods communicate with each other\", \"destination_entity\": \"pods\"},\\n    {\"source_entity\": \"system administrator\", \"description\": \"sets up network for pods\", \"destination_entity\": \"network\"},\\n    {\"source_entity\": \"Container Network Interface (CNI)\", \"description\": \"plugin sets up network for pods\", \"destination_entity\": \"network\"},\\n    {\"source_entity\": \"Pod A\", \"description\": \"sends packet to Pod B with same source and destination IP\", \"destination_entity\": \"Pod B\"},\\n    {\"source_entity\": \"Pod A\", \"description\": \"uses own IP address as source IP\", \"destination_entity\": \"Pod B\"},\\n    {\"source_entity\": \"pod\", \"description\": \"registers itself in other pods\", \"destination_entity\": \"other pods\"},\\n    {\"source_entity\": \"Node 1\", \"description\": \"runs Pod A with IP address 10.1.1.1\", \"destination_entity\": \"Pod A\"},\\n    {\"source_entity\": \"Node 2\", \"description\": \"runs Pod B with IP address 10.1.2.1\", \"destination_entity\": \"Pod B\"},\\n    {\"source_entity\": \"CNI plugin\", \"description\": \"sets up NAT-less network for pods\", \"destination_entity\": \"network\"},\\n    {\"source_entity\": \"Kubernetes\", \"description\": \"does not set up network itself, but mandates that it must be NAT-less\", \"destination_entity\": \"Network Address Translation (NAT)\"},\\n    {\"source_entity\": \"Pod A\", \"description\": \"connects to Pod B through a NAT-less network\", \"destination_entity\": \"Pod B\"}\\n]\\n```'},\n",
       " {'page': 368,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '336\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\n For example, say you have a client pod X and pod Y, which provides a kind of noti-\\nfication service to all pods that register with it. Pod X connects to pod Y and tells it,\\n“Hey, I’m pod X, available at IP 1.2.3.4; please send updates to me at this IP address.”\\nThe pod providing the service can connect to the first pod by using the received\\nIP address. \\n The requirement for NAT-less communication between pods also extends to pod-\\nto-node and node-to-pod communication. But when a pod communicates with ser-\\nvices out on the internet, the source IP of the packets the pod sends does need to be\\nchanged, because the pod’s IP is private. The source IP of outbound packets is\\nchanged to the host worker node’s IP address.\\n Building a proper Kubernetes cluster involves setting up the networking according\\nto these requirements. There are various methods and technologies available to do\\nthis, each with its own benefits or drawbacks in a given scenario. Because of this, we’re\\nnot going to go into specific technologies. Instead, let’s explain how inter-pod net-\\nworking works in general. \\n11.4.2 Diving deeper into how networking works\\nIn section 11.3, we saw that a pod’s IP address and network namespace are set up and\\nheld by the infrastructure container (the pause container). The pod’s containers then\\nuse its network namespace. A pod’s network interface is thus whatever is set up in the\\ninfrastructure container. Let’s see how the interface is created and how it’s connected\\nto the interfaces in all the other pods. Look at figure 11.15. We’ll discuss it next.\\nENABLING COMMUNICATION BETWEEN PODS ON THE SAME NODE\\nBefore the infrastructure container is started, a virtual Ethernet interface pair (a veth\\npair) is created for the container. One interface of the pair remains in the host’s\\nnamespace (you’ll see it listed as vethXXX when you run ifconfig on the node),\\nwhereas the other is moved into the container’s network namespace and renamed\\neth0. The two virtual interfaces are like two ends of a pipe (or like two network\\ndevices connected by an Ethernet cable)—what goes in on one side comes out on the\\nother, and vice-versa. \\nNode\\nPod A\\neth0\\n10.1.1.1\\nveth123\\nPod B\\neth0\\n10.1.1.2\\nveth234\\nBridge\\n10.1.1.0/24\\nThis is pod A’s\\nveth pair.\\nThis is pod B’s\\nveth pair.\\nFigure 11.15\\nPods on a node are \\nconnected to the same bridge through \\nvirtual Ethernet interface pairs.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'Container runtime engine',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Lightweight and portable container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'veth pair',\n",
       "    'description': 'Virtual Ethernet interface pair for pod-to-host communication',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Infrastructure container',\n",
       "    'description': \"Container that holds a pod's IP address and network namespace\",\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Pause container',\n",
       "    'description': 'Special type of infrastructure container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Network interface',\n",
       "    'description': 'Interface for communication between pods on the same node',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Bridge',\n",
       "    'description': 'Networking component that connects pods on the same node',\n",
       "    'category': 'networking'},\n",
       "   {'entity': 'IP address',\n",
       "    'description': \"Unique identifier for a pod's network interface\",\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Network namespace',\n",
       "    'description': 'Isolated network environment for each pod',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Infrastructure container\", \"description\": \"sets up and holds a pod\\'s IP address and network namespace\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"uses its network namespace to communicate with other pods\", \"destination_entity\": \"Network namespace\"},\\n  {\"source_entity\": \"Pause container\", \"description\": \"holds the infrastructure container\\'s network interface\", \"destination_entity\": \"Infrastructure container\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"communicates with other pods through a virtual Ethernet interface pair (veth pair)\", \"destination_entity\": \"Bridge\"},\\n  {\"source_entity\": \"Bridge\", \"description\": \"connects multiple pods on the same node to each other\", \"destination_entity\": \"Multiple pods\"},\\n  {\"source_entity\": \"Pod A\", \"description\": \"uses its veth pair to connect to Pod B through a bridge\", \"destination_entity\": \"Pod B\"},\\n  {\"source_entity\": \"veth pair\", \"description\": \"allows communication between two endpoints, in this case, Pod A and Pod B\", \"destination_entity\": \"Pod A and Pod B\"},\\n  {\"source_entity\": \"Infrastructure container\", \"description\": \"creates a veth pair for each pod to communicate with the bridge\", \"destination_entity\": \"veth pair\"}\\n]\\n```\\n\\nNote that I\\'ve kept only the entities provided in the input list, even if they are not explicitly mentioned in the text but can be inferred from the context. Let me know if you have any further questions!'},\n",
       " {'page': 369,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '337\\nInter-pod networking\\n The interface in the host’s network namespace is attached to a network bridge that\\nthe container runtime is configured to use. The eth0 interface in the container is\\nassigned an IP address from the bridge’s address range. Anything that an application\\nrunning inside the container sends to the eth0 network interface (the one in the con-\\ntainer’s namespace), comes out at the other veth interface in the host’s namespace\\nand is sent to the bridge. This means it can be received by any network interface that’s\\nconnected to the bridge. \\n If pod A sends a network packet to pod B, the packet first goes through pod A’s\\nveth pair to the bridge and then through pod B’s veth pair. All containers on a node\\nare connected to the same bridge, which means they can all communicate with each\\nother. But to enable communication between containers running on different nodes,\\nthe bridges on those nodes need to be connected somehow. \\nENABLING COMMUNICATION BETWEEN PODS ON DIFFERENT NODES\\nYou have many ways to connect bridges on different nodes. This can be done with\\noverlay or underlay networks or by regular layer 3 routing, which we’ll look at next.\\n You know pod IP addresses must be unique across the whole cluster, so the bridges\\nacross the nodes must use non-overlapping address ranges to prevent pods on differ-\\nent nodes from getting the same IP. In the example shown in figure 11.16, the bridge\\non node A is using the 10.1.1.0/24 IP range and the bridge on node B is using\\n10.1.2.0/24, which ensures no IP address conflicts exist.\\n Figure 11.16 shows that to enable communication between pods across two nodes\\nwith plain layer 3 networking, the node’s physical network interface needs to be con-\\nnected to the bridge as well. Routing tables on node A need to be configured so all\\npackets destined for 10.1.2.0/24 are routed to node B, whereas node B’s routing\\ntables need to be configured so packets sent to 10.1.1.0/24 are routed to node A.\\n With this type of setup, when a packet is sent by a container on one of the nodes\\nto a container on the other node, the packet first goes through the veth pair, then\\nNode A\\nPod A\\nNetwork\\neth0\\n10.1.1.1\\nveth123\\nPod B\\neth0\\n10.1.1.2\\nveth234\\nBridge\\n10.1.1.0/24\\neth0\\n10.100.0.1\\nNode B\\nPod C\\neth0\\n10.1.2.1\\nveth345\\nPod D\\neth0\\n10.1.2.2\\nveth456\\nBridge\\n10.1.2.0/24\\neth0\\n10.100.0.2\\nFigure 11.16\\nFor pods on different nodes to communicate, the bridges need to be connected \\nsomehow.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Node A\\nPod A\\neth0\\nveth123\\n10.1.1.1\\nBridge\\n10.1.1.0/24\\nPod B\\neth0 eth0\\nveth234\\n10.1.1.2 10.100.0.1  \\\n",
       "   0                                                                                                               \n",
       "   1                                            Network                                                            \n",
       "   \n",
       "     Node B\\nPod C\\neth0\\nveth345\\n10.1.2.1\\nBridge\\n10.1.2.0/24\\nPod D\\neth0 eth0\\nveth456\\n10.100.0.2 10.1.2.2  \\\n",
       "   0                                               None                                                            \n",
       "   1                                               None                                                            \n",
       "   \n",
       "      Col2  \n",
       "   0        \n",
       "   1  None  ],\n",
       "  'entities': [{'entity': 'network bridge',\n",
       "    'description': 'A network bridge that connects multiple networks together.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'container runtime',\n",
       "    'description': 'A software component that manages containers on a node.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'eth0 interface',\n",
       "    'description': \"A network interface in the container's namespace.\",\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'veth pair',\n",
       "    'description': \"A virtual Ethernet pair that connects a container to the host's network namespace.\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod A',\n",
       "    'description': 'A Kubernetes pod running on node A.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod B',\n",
       "    'description': 'A Kubernetes pod running on node B.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'overlay network',\n",
       "    'description': 'A type of network that connects multiple networks together using a virtual layer.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'underlay network',\n",
       "    'description': 'The underlying physical network that connects multiple nodes together.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'layer 3 routing',\n",
       "    'description': 'A method of routing packets between networks based on their destination IP address.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'routing table',\n",
       "    'description': 'A data structure that stores the rules for forwarding packets between networks.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'node A',\n",
       "    'description': 'A Kubernetes node running on host A.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'node B',\n",
       "    'description': 'A Kubernetes node running on host B.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'pod C',\n",
       "    'description': 'A Kubernetes pod running on node B.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod D',\n",
       "    'description': 'A Kubernetes pod running on node A.',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Node B\",\\n    \"description\": \"Uses a bridge with non-overlapping address range to prevent IP conflicts\",\\n    \"destination_entity\": \"Bridge\"\\n  },\\n  {\\n    \"source_entity\": \"Pod C\",\\n    \"description\": \"Connected to the same bridge as pod D\",\\n    \"destination_entity\": \"Pod D\"\\n  },\\n  {\\n    \"source_entity\": \"Underlay network\",\\n    \"description\": \"Used to connect bridges on different nodes\",\\n    \"destination_entity\": \"Bridge\"\\n  },\\n  {\\n    \"source_entity\": \"Pod D\",\\n    \"description\": \"Connected to the same bridge as pod C\",\\n    \"destination_entity\": \"Pod C\"\\n  },\\n  {\\n    \"source_entity\": \"Eth0 interface\",\\n    \"description\": \"Attached to a network bridge used by container runtime\",\\n    \"destination_entity\": \"Network Bridge\"\\n  },\\n  {\\n    \"source_entity\": \"Pod B\",\\n    \"description\": \"Connected to the same bridge as pod A\",\\n    \"destination_entity\": \"Pod A\"\\n  },\\n  {\\n    \"source_entity\": \"Node A\",\\n    \"description\": \"Has a routing table configured for packets destined for node B\",\\n    \"destination_entity\": \"Routing Table\"\\n  },\\n  {\\n    \"source_entity\": \"Node A\",\\n    \"description\": \"Connected to the same bridge as pod A and pod B\",\\n    \"destination_entity\": \"Pod A\"\\n  },\\n  {\\n    \"source_entity\": \"Node A\",\\n    \"description\": \"Has a routing table configured for packets destined for node B\\'s bridge\",\\n    \"destination_entity\": \"Bridge\"\\n  },\\n  {\\n    \"source_entity\": \"Layer 3 routing\",\\n    \"description\": \"Used to enable communication between pods on different nodes\",\\n    \"destination_entity\": \"Node A\"\\n  },\\n  {\\n    \"source_entity\": \"Container runtime\",\\n    \"description\": \"Configured to use a network bridge attached to eth0 interface\",\\n    \"destination_entity\": \"Eth0 Interface\"\\n  },\\n  {\\n    \"source_entity\": \"Veth pair\",\\n    \"description\": \"Used by pods to communicate with each other\\'s bridges\",\\n    \"destination_entity\": \"Bridge\"\\n  },\\n  {\\n    \"source_entity\": \"Node A\",\\n    \"description\": \"Has a routing table configured for packets destined for node B\\'s bridge address range\",\\n    \"destination_entity\": \"Routing Table\"\\n  },\\n  {\\n    \"source_entity\": \"Overlay network\",\\n    \"description\": \"Used to connect bridges on different nodes\",\\n    \"destination_entity\": \"Bridge\"\\n  }\\n]\\n```'},\n",
       " {'page': 370,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '338\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\nthrough the bridge to the node’s physical adapter, then over the wire to the other\\nnode’s physical adapter, through the other node’s bridge, and finally through the veth\\npair of the destination container.\\n This works only when nodes are connected to the same network switch, without\\nany routers in between; otherwise those routers would drop the packets because\\nthey refer to pod IPs, which are private. Sure, the routers in between could be con-\\nfigured to route packets between the nodes, but this becomes increasingly difficult\\nand error-prone as the number of routers between the nodes increases. Because of\\nthis, it’s easier to use a Software Defined Network (SDN), which makes the nodes\\nappear as though they’re connected to the same network switch, regardless of the\\nactual underlying network topology, no matter how complex it is. Packets sent\\nfrom the pod are encapsulated and sent over the network to the node running the\\nother pod, where they are de-encapsulated and delivered to the pod in their origi-\\nnal form.\\n11.4.3 Introducing the Container Network Interface\\nTo make it easier to connect containers into a network, a project called Container\\nNetwork Interface (CNI) was started. The CNI allows Kubernetes to be configured to\\nuse any CNI plugin that’s out there. These plugins include\\n\\uf0a1Calico\\n\\uf0a1Flannel\\n\\uf0a1Romana\\n\\uf0a1Weave Net \\n\\uf0a1And others\\nWe’re not going to go into the details of these plugins; if you want to learn more about\\nthem, refer to https:/\\n/kubernetes.io/docs/concepts/cluster-administration/addons/.\\n Installing a network plugin isn’t difficult. You only need to deploy a YAML con-\\ntaining a DaemonSet and a few other supporting resources. This YAML is provided\\non each plugin’s project page. As you can imagine, the DaemonSet is used to deploy\\na network agent on all cluster nodes. It then ties into the CNI interface on the node,\\nbut be aware that the Kubelet needs to be started with --network-plugin=cni to\\nuse CNI. \\n11.5\\nHow services are implemented\\nIn chapter 5 you learned about Services, which allow exposing a set of pods at a long-\\nlived, stable IP address and port. In order to focus on what Services are meant for and\\nhow they can be used, we intentionally didn’t go into how they work. But to truly\\nunderstand Services and have a better feel for where to look when things don’t behave\\nthe way you expect, you need to understand how they are implemented. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Bridge',\n",
       "    'description': 'Network bridge used to connect nodes to the same network switch.',\n",
       "    'category': 'Hardware'},\n",
       "   {'entity': 'veth pair',\n",
       "    'description': 'Virtual Ethernet interface used for communication between containers.',\n",
       "    'category': 'Hardware'},\n",
       "   {'entity': 'Pod IPs',\n",
       "    'description': 'Private IP addresses assigned to pods.',\n",
       "    'category': 'Network'},\n",
       "   {'entity': 'Software Defined Network (SDN)',\n",
       "    'description': 'Technology that makes nodes appear connected to the same network switch, regardless of underlying topology.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'CNI (Container Network Interface)',\n",
       "    'description': 'Project that allows Kubernetes to be configured with various CNI plugins.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Calico',\n",
       "    'description': 'CNI plugin for network connectivity.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Flannel',\n",
       "    'description': 'CNI plugin for network connectivity.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Romana',\n",
       "    'description': 'CNI plugin for network connectivity.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Weave Net',\n",
       "    'description': 'CNI plugin for network connectivity.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'DaemonSet',\n",
       "    'description': 'Kubernetes resource used to deploy a network agent on all cluster nodes.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'YAML',\n",
       "    'description': 'Configuration file format used for deploying CNI plugins.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'Component of Kubernetes that interacts with the CNI interface.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'Kubernetes resource used to expose a set of pods at a long-lived, stable IP address and port.',\n",
       "    'category': 'Application'}],\n",
       "  'relationships': '[{\"source_entity\": \"Kubernetes\", \"description\": \"uses\", \"destination_entity\": \"CNI (Container Network Interface)\"}, \\n{\"source_entity\": \"CNI (Container Network Interface)\", \"description\": \"allows\", \"destination_entity\": \"multiple CNI plugins\"}, \\n{\"source_entity\": \"Calico\", \"description\": \"is one of the CNI plugins\", \"destination_entity\": \"CNI (Container Network Interface)\"}, \\n{\"source_entity\": \"Flannel\", \"description\": \"is another CNI plugin\", \"destination_entity\": \"CNI (Container Network Interface)\"}, \\n{\"source_entity\": \"Romana\", \"description\": \"is also a CNI plugin\", \"destination_entity\": \"CNI (Container Network Interface)\"}, \\n{\"source_entity\": \"Weave Net\", \"description\": \"is yet another CNI plugin\", \"destination_entity\": \"CNI (Container Network Interface)\"}, \\n{\"source_entity\": \"Kubernetes\", \"description\": \"requires\", \"destination_entity\": \"--network-plugin=cni\"}, \\n{\"source_entity\": \"DaemonSet\", \"description\": \"deploys a network agent on all cluster nodes\", \"destination_entity\": \"cluster nodes\"}, \\n{\"source_entity\": \"CNI interface\", \"description\": \"is tied into by the DaemonSet\", \"destination_entity\": \"node\"}, \\n{\"source_entity\": \"Kubelet\", \"description\": \"needs to be started with --network-plugin=cni to use CNI\", \"destination_entity\": \"--network-plugin=cni\"}, \\n{\"source_entity\": \"Pod IPs\", \"description\": \"are private and dropped by routers\", \"destination_entity\": \"routers\"}, \\n{\"source_entity\": \"Software Defined Network (SDN)\", \"description\": \"makes nodes appear as though they\\'re connected to the same network switch\", \"destination_entity\": \"nodes\"}, \\n{\"source_entity\": \"Bridge\", \"description\": \"connects containers into a network\", \"destination_entity\": \"containers\"}, \\n{\"source_entity\": \"CNI (Container Network Interface)\", \"description\": \"is used by Kubernetes to configure networking\", \"destination_entity\": \"Kubernetes\"}, \\n{\"source_entity\": \"Services\", \"description\": \"are implemented using Services\", \"destination_entity\": \"Kubernetes\"}, \\n{\"source_entity\": \"YAML\", \"description\": \"contains a DaemonSet and other supporting resources for installing a network plugin\", \"destination_entity\": \"DaemonSet\"}]'},\n",
       " {'page': 371,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '339\\nHow services are implemented\\n11.5.1 Introducing the kube-proxy\\nEverything related to Services is handled by the kube-proxy process running on each\\nnode. Initially, the kube-proxy was an actual proxy waiting for connections and for\\neach incoming connection, opening a new connection to one of the pods. This was\\ncalled the userspace proxy mode. Later, a better-performing iptables proxy mode\\nreplaced it. This is now the default, but you can still configure Kubernetes to use the\\nold mode if you want.\\n Before we continue, let’s quickly review a few things about Services, which are rele-\\nvant for understanding the next few paragraphs.\\n We’ve learned that each Service gets its own stable IP address and port. Clients\\n(usually pods) use the service by connecting to this IP address and port. The IP\\naddress is virtual—it’s not assigned to any network interfaces and is never listed as\\neither the source or the destination IP address in a network packet when the packet\\nleaves the node. A key detail of Services is that they consist of an IP and port pair (or\\nmultiple IP and port pairs in the case of multi-port Services), so the service IP by itself\\ndoesn’t represent anything. That’s why you can’t ping them. \\n11.5.2 How kube-proxy uses iptables\\nWhen a service is created in the API server, the virtual IP address is assigned to it\\nimmediately. Soon afterward, the API server notifies all kube-proxy agents running on\\nthe worker nodes that a new Service has been created. Then, each kube-proxy makes\\nthat service addressable on the node it’s running on. It does this by setting up a few\\niptables rules, which make sure each packet destined for the service IP/port pair is\\nintercepted and its destination address modified, so the packet is redirected to one of\\nthe pods backing the service. \\n Besides watching the API server for changes to Services, kube-proxy also watches\\nfor changes to Endpoints objects. We talked about them in chapter 5, but let me\\nrefresh your memory, as it’s easy to forget they even exist, because you rarely create\\nthem manually. An Endpoints object holds the IP/port pairs of all the pods that back\\nthe service (an IP/port pair can also point to something other than a pod). That’s\\nwhy the kube-proxy must also watch all Endpoints objects. After all, an Endpoints\\nobject changes every time a new backing pod is created or deleted, and when the\\npod’s readiness status changes or the pod’s labels change and it falls in or out of scope\\nof the service. \\n Now let’s see how kube-proxy enables clients to connect to those pods through the\\nService. This is shown in figure 11.17.\\n The figure shows what the kube-proxy does and how a packet sent by a client pod\\nreaches one of the pods backing the Service. Let’s examine what happens to the\\npacket when it’s sent by the client pod (pod A in the figure). \\n The packet’s destination is initially set to the IP and port of the Service (in the\\nexample, the Service is at 172.30.0.1:80). Before being sent to the network, the\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kube-proxy',\n",
       "    'description': 'The process that handles everything related to Services in Kubernetes.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'A stable IP address and port for a group of pods.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'IPtables',\n",
       "    'description': 'A set of rules used by kube-proxy to intercept and redirect packets destined for the service IP/port pair.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Endpoints objects',\n",
       "    'description': 'Holds the IP/port pairs of all the pods that back a Service.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The component that assigns virtual IP addresses to Services and notifies kube-proxy agents when a new Service is created.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kube-proxy agents',\n",
       "    'description': 'Processes running on worker nodes that make the service addressable on each node.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'iptables rules',\n",
       "    'description': 'Rules set up by kube-proxy to intercept and redirect packets destined for the service IP/port pair.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'packets',\n",
       "    'description': 'Data units that are sent between nodes in a network.',\n",
       "    'category': 'data'},\n",
       "   {'entity': 'nodes',\n",
       "    'description': 'Computers in a Kubernetes cluster.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'The smallest deployable unit in Kubernetes, which can be a single container or multiple containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'Lightweight and standalone execution environments for applications.',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kube-proxy\",\\n    \"description\": \"sets up iptables rules to redirect packets destined for service IP/port pair to one of the pods backing the service\",\\n    \"destination_entity\": \"iptables rules\"\\n  },\\n  {\\n    \"source_entity\": \"kube-proxy\",\\n    \"description\": \"watches API server for changes to Services and Endpoints objects\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"kube-proxy agents\",\\n    \"description\": \"receive notification from API server that a new Service has been created\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"kube-proxy\",\\n    \"description\": \"makes service addressable on the node it\\'s running on by setting up iptables rules\",\\n    \"destination_entity\": \"nodes\"\\n  },\\n  {\\n    \"source_entity\": \"pods\",\\n    \"description\": \"use the service by connecting to its IP address and port\",\\n    \"destination_entity\": \"Services\"\\n  },\\n  {\\n    \"source_entity\": \"kube-proxy\",\\n    \"description\": \"modifies destination address of packets destined for service IP/port pair so they are redirected to one of the pods backing the service\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"Endpoints objects\",\\n    \"description\": \"hold the IP/port pairs of all the pods that back the service\",\\n    \"destination_entity\": \"Services\"\\n  },\\n  {\\n    \"source_entity\": \"kube-proxy\",\\n    \"description\": \"watches Endpoints objects for changes to backing pods and updates iptables rules accordingly\",\\n    \"destination_entity\": \"Endpoints objects\"\\n  },\\n  {\\n    \"source_entity\": \"client pod\",\\n    \"description\": \"sends packet with destination set to service IP and port\",\\n    \"destination_entity\": \"Services\"\\n  },\\n  {\\n    \"source_entity\": \"kube-proxy\",\\n    \"description\": \"intercepts packets destined for service IP/port pair and redirects them to one of the pods backing the service\",\\n    \"destination_entity\": \"pods\"\\n  }\\n]\\n```\\n\\nNote that I\\'ve assumed that the entities \\'packets\\', \\'containers\\', and \\'IPtables\\' are not directly related to any specific action or process in the document, so I haven\\'t included them in the relations list. If you\\'d like me to include them, please let me know!'},\n",
       " {'page': 372,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '340\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\npacket is first handled by node A’s kernel according to the iptables rules set up on\\nthe node. \\n The kernel checks if the packet matches any of those iptables rules. One of them\\nsays that if any packet has the destination IP equal to 172.30.0.1 and destination port\\nequal to 80, the packet’s destination IP and port should be replaced with the IP and\\nport of a randomly selected pod. \\n The packet in the example matches that rule and so its destination IP/port is\\nchanged. In the example, pod B2 was randomly selected, so the packet’s destination\\nIP is changed to 10.1.2.1 (pod B2’s IP) and the port to 8080 (the target port specified\\nin the Service spec). From here on, it’s exactly as if the client pod had sent the packet\\nto pod B directly instead of through the service. \\n It’s slightly more complicated than that, but that’s the most important part you\\nneed to understand.\\n \\nNode A\\nNode B\\nAPI server\\nPod A\\nPod B1\\nPod B2\\nPod B3\\nPacket X\\nSource:\\n10.1.1.1\\nDestination:\\n172.30.0.1:80\\n10.1.2.1:8080\\niptables\\nService B\\n172.30.0.1:80\\nConﬁgures\\niptables\\nPacket X\\nSource:\\n10.1.1.1\\nDestination:\\n172.30.0.1:80\\nkube-proxy\\nEndpoints B\\nPod A\\nIP: 10.1.1.1\\nPod B1\\nIP: 10.1.1.2\\nPod B2\\nIP: 10.1.2.1\\nPod B3\\nIP: 10.1.2.2\\nWatches for changes to\\nservices and endpoints\\nFigure 11.17\\nNetwork packets sent to a Service’s virtual IP/port pair are \\nmodified and redirected to a randomly selected backend pod.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Node A\\nPod A Pod B1\\nkube-proxy\\nIP: 10.1.1.1 IP: 10.1.1.2\\nConfigures\\niptables\\nSource:\\nSource:\\n10.1.1.1\\n10.1.1.1\\nDestination:\\nDestination:\\n172.30.0.1:8\\n172.30.0.1:80 iptables 10.1.2.1:808\\nPacket X Packet X  \\\n",
       "   0                                               None                                                                                                                                                                          \n",
       "   1                                               None                                                                                                                                                                          \n",
       "   2                                               None                                                                                                                                                                          \n",
       "   \n",
       "                                                   Col1  Col2  \\\n",
       "   0  Source:\\n10.1.1.1\\nDestination:\\n172.30.0.1:8\\...  None   \n",
       "   1                                               None  0\\n0   \n",
       "   2                                               None         \n",
       "   \n",
       "     Node B\\nPod B2\\nIP: 10.1.2.1\\nPod B3\\nIP: 10.1.2.2  \n",
       "   0                                               None  \n",
       "   1                                               None  \n",
       "   2                                               None  ],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': '',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'iptables',\n",
       "    'description': 'A firewall system for Linux',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Node A',\n",
       "    'description': 'A Kubernetes node',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Node B',\n",
       "    'description': 'A Kubernetes node',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The central management service of a Kubernetes cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pod A', 'description': '', 'category': 'container'},\n",
       "   {'entity': 'Pod B1', 'description': '', 'category': 'container'},\n",
       "   {'entity': 'Pod B2',\n",
       "    'description': 'Selected pod to receive packet',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Pod B3', 'description': '', 'category': 'container'},\n",
       "   {'entity': 'Packet X',\n",
       "    'description': 'A network packet',\n",
       "    'category': 'packet'},\n",
       "   {'entity': 'Service B', 'description': '', 'category': 'application'},\n",
       "   {'entity': 'kube-proxy',\n",
       "    'description': 'A Kubernetes networking component',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Endpoints B',\n",
       "    'description': 'A resource that holds a list of IP addresses and ports',\n",
       "    'category': 'resource'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Node A\", \"description\": \"handles packet according to iptables rules\", \"destination_entity\": \"Packet X\"},\\n  {\"source_entity\": \"iptables\", \"description\": \"checks if packet matches any of its rules\", \"destination_entity\": \"Packet X\"},\\n  {\"source_entity\": \"packet\", \"description\": \"matches an iptables rule that replaces destination IP and port\", \"destination_entity\": \"Pod B2\"},\\n  {\"source_entity\": \"iptables\", \"description\": \"replaces destination IP and port with pod B2\\'s IP and port\", \"destination_entity\": \"Packet X\"},\\n  {\"source_entity\": \"Packet X\", \"description\": \"has its destination IP and port changed to pod B2\\'s IP and port\", \"destination_entity\": \"Pod B2\"},\\n  {\"source_entity\": \"kube-proxy\", \"description\": \"redirects network packets to a randomly selected backend pod\", \"destination_entity\": \"Endpoints B\"},\\n  {\"source_entity\": \"Endpoints B\", \"description\": \"contains information about available endpoints\", \"destination_entity\": \"Service B\"},\\n  {\"source_entity\": \"Pod A\", \"description\": \"watches for changes to services and endpoints\", \"destination_entity\": \"Kubernetes\"},\\n  {\"source_entity\": \"iptables\", \"description\": \"configures rules for packet handling\", \"destination_entity\": \"Node A\"},\\n  {\"source_entity\": \"API server\", \"description\": \"manages Kubernetes cluster resources\", \"destination_entity\": \"Pod A\"}\\n]\\n```'},\n",
       " {'page': 373,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '341\\nRunning highly available clusters\\n11.6\\nRunning highly available clusters\\nOne of the reasons for running apps inside Kubernetes is to keep them running with-\\nout interruption with no or limited manual intervention in case of infrastructure\\nfailures. For running services without interruption it’s not only the apps that need to\\nbe up all the time, but also the Kubernetes Control Plane components. We’ll look at\\nwhat’s involved in achieving high availability next.\\n11.6.1 Making your apps highly available\\nWhen running apps in Kubernetes, the various controllers make sure your app keeps\\nrunning smoothly and at the specified scale even when nodes fail. To ensure your app\\nis highly available, you only need to run them through a Deployment resource and\\nconfigure an appropriate number of replicas; everything else is taken care of by\\nKubernetes. \\nRUNNING MULTIPLE INSTANCES TO REDUCE THE LIKELIHOOD OF DOWNTIME\\nThis requires your apps to be horizontally scalable, but even if that’s not the case in\\nyour app, you should still use a Deployment with its replica count set to one. If the\\nreplica becomes unavailable, it will be replaced with a new one quickly, although that\\ndoesn’t happen instantaneously. It takes time for all the involved controllers to notice\\nthat a node has failed, create the new pod replica, and start the pod’s containers.\\nThere will inevitably be a short period of downtime in between. \\nUSING LEADER-ELECTION FOR NON-HORIZONTALLY SCALABLE APPS\\nTo avoid the downtime, you need to run additional inactive replicas along with the\\nactive one and use a fast-acting lease or leader-election mechanism to make sure only\\none is active. In case you’re unfamiliar with leader election, it’s a way for multiple app\\ninstances running in a distributed environment to come to an agreement on which is\\nthe leader. That leader is either the only one performing tasks, while all others are\\nwaiting for the leader to fail and then becoming leaders themselves, or they can all be\\nactive, with the leader being the only instance performing writes, while all the others\\nare providing read-only access to their data, for example. This ensures two instances\\nare never doing the same job, if that would lead to unpredictable system behavior due\\nto race conditions.\\n The mechanism doesn’t need to be incorporated into the app itself. You can use a\\nsidecar container that performs all the leader-election operations and signals the\\nmain container when it should become active. You’ll find an example of leader elec-\\ntion in Kubernetes at https:/\\n/github.com/kubernetes/contrib/tree/master/election.\\n Ensuring your apps are highly available is relatively simple, because Kubernetes\\ntakes care of almost everything. But what if Kubernetes itself fails? What if the servers\\nrunning the Kubernetes Control Plane components go down? How are those compo-\\nnents made highly available?\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration platform',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'Resource for managing app replicas',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Replica',\n",
       "    'description': 'Instance of an application or service',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Controller',\n",
       "    'description': 'Component responsible for making decisions in Kubernetes',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'Physical or virtual machine running a container',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Logical host for one or more containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Leader Election',\n",
       "    'description': 'Mechanism for selecting a single instance to perform tasks',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Lease',\n",
       "    'description': 'Temporary lock on a resource used by leader election',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Cluster',\n",
       "    'description': 'Collection of machines running Kubernetes',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Control Plane',\n",
       "    'description': 'Components responsible for managing the cluster',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"takes care of almost everything to ensure apps are highly available\",\\n    \"destination_entity\": \"Apps\"\\n  },\\n  {\\n    \"source_entity\": \"Deployment\",\\n    \"description\": \"makes sure app keeps running smoothly and at the specified scale even when nodes fail\",\\n    \"destination_entity\": \"App\"\\n  },\\n  {\\n    \"source_entity\": \"Replica\",\\n    \"description\": \"will be replaced with a new one quickly if it becomes unavailable\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Leader Election\",\\n    \"description\": \"makes sure only one instance is active and avoids downtime\",\\n    \"destination_entity\": \"App Instances\"\\n  },\\n  {\\n    \"source_entity\": \"Lease\",\\n    \"description\": \"is used to make sure only one replica is active and avoids downtime\",\\n    \"destination_entity\": \"Replica\"\\n  },\\n  {\\n    \"source_entity\": \"Controller\",\\n    \"description\": \"notifies that a node has failed, creates the new pod replica, and starts the pod\\'s containers\",\\n    \"destination_entity\": \"Node\"\\n  },\\n  {\\n    \"source_entity\": \"Control Plane\",\\n    \"description\": \"components need to be made highly available along with apps\",\\n    \"destination_entity\": \"Apps\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster\",\\n    \"description\": \"can run multiple instances of an app to reduce the likelihood of downtime\",\\n    \"destination_entity\": \"App\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"replicas can be quickly replaced if they become unavailable\",\\n    \"destination_entity\": \"Replica\"\\n  }\\n]\\n```'},\n",
       " {'page': 374,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '342\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\n11.6.2 Making Kubernetes Control Plane components highly available\\nIn the beginning of this chapter, you learned about the few components that make up\\na Kubernetes Control Plane. To make Kubernetes highly available, you need to run\\nmultiple master nodes, which run multiple instances of the following components:\\n\\uf0a1etcd, which is the distributed data store where all the API objects are kept\\n\\uf0a1API server\\n\\uf0a1Controller Manager, which is the process in which all the controllers run\\n\\uf0a1Scheduler\\nWithout going into the actual details of how to install and run these components, let’s\\nsee what’s involved in making each of these components highly available. Figure 11.18\\nshows an overview of a highly available cluster.\\nRUNNING AN ETCD CLUSTER\\nBecause etcd was designed as a distributed system, one of its key features is the ability\\nto run multiple etcd instances, so making it highly available is no big deal. All you\\nneed to do is run it on an appropriate number of machines (three, five, or seven, as\\nexplained earlier in the chapter) and make them aware of each other. You do this by\\nincluding the list of all the other instances in every instance’s configuration. For\\nexample, when starting an instance, you specify the IPs and ports where the other etcd\\ninstances can be reached. \\n etcd will replicate data across all its instances, so a failure of one of the nodes when\\nrunning a three-machine cluster will still allow the cluster to accept both read and\\nwrite operations. To increase the fault tolerance to more than a single node, you need\\nto run five or seven etcd nodes, which would allow the cluster to handle two or three\\nNode 1\\nKubelet\\nNode 2\\nKubelet\\nNode 3\\nKubelet\\nNode 4\\nKubelet\\nNode 5\\nKubelet\\n...\\nNode N\\nKubelet\\nLoad\\nbalancer\\nMaster 3\\netcd\\nAPI server\\nScheduler\\nController\\nManager\\n[standing-by]\\n[standing-by]\\nMaster 2\\netcd\\nAPI server\\nScheduler\\nController\\nManager\\n[standing-by]\\n[standing-by]\\nMaster 1\\netcd\\nAPI server\\nScheduler\\nController\\nManager\\n[active]\\n[active]\\nFigure 11.18\\nA highly-available cluster with three master nodes\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Controller\n",
       "   Manager, Scheduler]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'etcd',\n",
       "    'description': 'Distributed data store for Kubernetes API objects',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Kubernetes component that handles incoming requests',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'Process that runs all the controllers in a Kubernetes cluster',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'Component that assigns Pods to nodes in a Kubernetes cluster',\n",
       "    'category': 'scheduler'},\n",
       "   {'entity': 'etcd instance',\n",
       "    'description': 'Individual instance of the etcd distributed data store',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'Agent that runs on each node and is responsible for managing Pods',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'Physical or virtual machine in a Kubernetes cluster',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Master\", \"description\": \"runs multiple instances of the Controller Manager\", \"destination_entity\": \"Controller Manager\"},\\n  {\"source_entity\": \"Master\", \"description\": \"runs multiple instances of the Scheduler\", \"destination_entity\": \"Scheduler\"},\\n  {\"source_entity\": \"Master\", \"description\": \"runs multiple instances of the etcd instance\", \"destination_entity\": \"etcd instance\"},\\n  {\"source_entity\": \"Master\", \"description\": \"runs multiple instances of the API server\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"Controller Manager\", \"description\": \"is the process in which all controllers run\", \"destination_entity\": \"controllers\"},\\n  {\"source_entity\": \"Scheduler\", \"description\": \"schedules pods on nodes\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"runs on each node and talks to the API server to fetch the pod configuration\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"etcd instance\", \"description\": \"replicates data across all its instances\", \"destination_entity\": \"other etcd instances\"},\\n  {\"source_entity\": \"Node\", \"description\": \"runs a Kubelet which talks to the API server\", \"destination_entity\": \"Kubelet\"},\\n  {\"source_entity\": \"etcd instance\", \"description\": \"can be run on multiple machines and made aware of each other\", \"destination_entity\": \"other etcd instances\"},\\n  {\"source_entity\": \"Master\", \"description\": \"runs a highly available cluster with three master nodes\", \"destination_entity\": \"highly available cluster\"}\\n]\\n```\\n\\nNote that I\\'ve tried to match the entities provided in the list to the corresponding concepts mentioned in the document. Let me know if you\\'d like me to clarify any of these relations!'},\n",
       " {'page': 375,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '343\\nRunning highly available clusters\\nnode failures, respectively. Having more than seven etcd instances is almost never nec-\\nessary and begins impacting performance.\\nRUNNING MULTIPLE INSTANCES OF THE API SERVER\\nMaking the API server highly available is even simpler. Because the API server is (almost\\ncompletely) stateless (all the data is stored in etcd, but the API server does cache it), you\\ncan run as many API servers as you need, and they don’t need to be aware of each other\\nat all. Usually, one API server is collocated with every etcd instance. By doing this, the\\netcd instances don’t need any kind of load balancer in front of them, because every API\\nserver instance only talks to the local etcd instance. \\n The API servers, on the other hand, do need to be fronted by a load balancer, so\\nclients (kubectl, but also the Controller Manager, Scheduler, and all the Kubelets)\\nalways connect only to the healthy API server instances. \\nENSURING HIGH AVAILABILITY OF THE CONTROLLERS AND THE SCHEDULER\\nCompared to the API server, where multiple replicas can run simultaneously, run-\\nning multiple instances of the Controller Manager or the Scheduler isn’t as simple.\\nBecause controllers and the Scheduler all actively watch the cluster state and act when\\nit changes, possibly modifying the cluster state further (for example, when the desired\\nreplica count on a ReplicaSet is increased by one, the ReplicaSet controller creates an\\nadditional pod), running multiple instances of each of those components would\\nresult in all of them performing the same action. They’d be racing each other, which\\ncould cause undesired effects (creating two new pods instead of one, as mentioned in\\nthe previous example).\\n For this reason, when running multiple instances of these components, only one\\ninstance may be active at any given time. Luckily, this is all taken care of by the compo-\\nnents themselves (this is controlled with the --leader-elect option, which defaults to\\ntrue). Each individual component will only be active when it’s the elected leader. Only\\nthe leader performs actual work, whereas all other instances are standing by and waiting\\nfor the current leader to fail. When it does, the remaining instances elect a new leader,\\nwhich then takes over the work. This mechanism ensures that two components are never\\noperating at the same time and doing the same work (see figure 11.19).\\nMaster 3\\nScheduler\\nController\\nManager\\n[standing-by]\\n[standing-by]\\nMaster 1\\nScheduler\\nController\\nManager\\n[active]\\n[active]\\nMaster 2\\nScheduler\\nController\\nManager\\n[standing-by]\\n[standing-by]\\nOnly the controllers in\\nthis Controller Manager\\nare reacting to API\\nresources being created,\\nupdated, and deleted.\\nThese Controller Managers\\nand Schedulers aren’t doing\\nanything except waiting to\\nbecome leaders.\\nOnly this Scheduler\\nis scheduling pods.\\nFigure 11.19\\nOnly a single Controller Manager and a single Scheduler are active; others are standing by.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Only the controllers in These Controller Managers\\nthis Controller Manager and Schedulers aren’t doing\\nare reacting to API Only this Scheduler anything except waiting to\\nresources being created, is scheduling pods. become leaders.\\nupdated, and deleted.\\nController Controller Controller\\nScheduler Scheduler Scheduler\\nManager Manager Manager\\n[active] [active] [standing-by] [standing-by] [standing-by] [standing-by]\\nMaster 1 Master 2 Master 3  \\\n",
       "   0                                               None                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "   \n",
       "                                                   Col1  \\\n",
       "   0  Controller\\nScheduler\\nManager\\n[standing-by] ...   \n",
       "   \n",
       "                                                   Col2  \n",
       "   0  Controller\\nScheduler\\nManager\\n[standing-by] ...  ],\n",
       "  'entities': [{'entity': 'etcd',\n",
       "    'description': 'distributed key-value store',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'stateless service for managing Kubernetes cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'etcd instance',\n",
       "    'description': 'single node in etcd distributed system',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'load balancer',\n",
       "    'description': 'device for distributing incoming network traffic across multiple servers',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'Kubernetes component responsible for managing controllers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'Kubernetes component responsible for scheduling pods on nodes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'agent running on each node in Kubernetes cluster, responsible for starting and stopping containers',\n",
       "    'category': 'process'},\n",
       "   {'entity': '--leader-elect option',\n",
       "    'description': 'flag used to enable leader election among multiple instances of a component',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ReplicaSet controller',\n",
       "    'description': 'Kubernetes controller responsible for managing replica sets',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"can run multiple instances simultaneously without affecting performance\",\\n    \"destination_entity\": \"etcd instance\"\\n  },\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"does not need to be aware of other API servers\",\\n    \"destination_entity\": \"other API server\"\\n  },\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"connects only to the healthy API server instances\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"Controller Manager\",\\n    \"description\": \"can run multiple instances, but only one instance may be active at any given time\",\\n    \"destination_entity\": \"other Controller Manager instance\"\\n  },\\n  {\\n    \"source_entity\": \"Scheduler\",\\n    \"description\": \"can run multiple instances, but only one instance may be active at any given time\",\\n    \"destination_entity\": \"other Scheduler instance\"\\n  },\\n  {\\n    \"source_entity\": \"Controller Manager\",\\n    \"description\": \"only performs actual work when it\\'s the elected leader\",\\n    \"destination_entity\": \"etcd instance\"\\n  },\\n  {\\n    \"source_entity\": \"Scheduler\",\\n    \"description\": \"only performs scheduling when it\\'s the elected leader\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSet controller\",\\n    \"description\": \"creates an additional pod when desired replica count is increased by one\",\\n    \"destination_entity\": \"etcd instance\"\\n  },\\n  {\\n    \"source_entity\": \"Controller Manager\",\\n    \"description\": \"reacts to API resources being created, updated, and deleted\",\\n    \"destination_entity\": \"API resource\"\\n  },\\n  {\\n    \"source_entity\": \"load balancer\",\\n    \"description\": \"directs clients (Kubelet, Controller Manager, Scheduler) only to the healthy API server instances\",\\n    \"destination_entity\": \"API server instance\"\\n  },\\n  {\\n    \"source_entity\": \"etcd instance\",\\n    \"description\": \"stores all data, which is accessed by the API server and Controller Manager\",\\n    \"destination_entity\": \"API server or Controller Manager\"\\n  }\\n]'},\n",
       " {'page': 376,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '344\\nCHAPTER 11\\nUnderstanding Kubernetes internals\\nThe Controller Manager and Scheduler can run collocated with the API server and\\netcd, or they can run on separate machines. When collocated, they can talk to the\\nlocal API server directly; otherwise they connect to the API servers through the load\\nbalancer.\\nUNDERSTANDING THE LEADER ELECTION MECHANISM USED IN CONTROL PLANE COMPONENTS\\nWhat I find most interesting here is that these components don’t need to talk to each\\nother directly to elect a leader. The leader election mechanism works purely by creat-\\ning a resource in the API server. And it’s not even a special kind of resource—the End-\\npoints resource is used to achieve this (abused is probably a more appropriate term).\\n There’s nothing special about using an Endpoints object to do this. It’s used\\nbecause it has no side effects as long as no Service with the same name exists. Any\\nother resource could be used (in fact, the leader election mechanism will soon use\\nConfigMaps instead of Endpoints). \\n I’m sure you’re interested in how a resource can be used for this purpose. Let’s\\ntake the Scheduler, for example. All instances of the Scheduler try to create (and later\\nupdate) an Endpoints resource called kube-scheduler. You’ll find it in the kube-\\nsystem namespace, as the following listing shows.\\n$ kubectl get endpoints kube-scheduler -n kube-system -o yaml\\napiVersion: v1\\nkind: Endpoints\\nmetadata:\\n  annotations:\\n    control-plane.alpha.kubernetes.io/leader: \\'{\"holderIdentity\":\\n      ➥ \"minikube\",\"leaseDurationSeconds\":15,\"acquireTime\":\\n      ➥ \"2017-05-27T18:54:53Z\",\"renewTime\":\"2017-05-28T13:07:49Z\",\\n      ➥ \"leaderTransitions\":0}\\'\\n  creationTimestamp: 2017-05-27T18:54:53Z\\n  name: kube-scheduler\\n  namespace: kube-system\\n  resourceVersion: \"654059\"\\n  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler\\n  uid: f847bd14-430d-11e7-9720-080027f8fa4e\\nsubsets: []\\nThe control-plane.alpha.kubernetes.io/leader annotation is the important part.\\nAs you can see, it contains a field called holderIdentity, which holds the name of the\\ncurrent leader. The first instance that succeeds in putting its name there becomes\\nthe leader. Instances race each other to do that, but there’s always only one winner.\\n Remember the optimistic concurrency we explained earlier? That’s what ensures\\nthat if multiple instances try to write their name into the resource only one of them\\nsucceeds. Based on whether the write succeeded or not, each instance knows whether\\nit is or it isn’t the leader. \\n Once becoming the leader, it must periodically update the resource (every two sec-\\nonds by default), so all other instances know that it’s still alive. When the leader fails,\\nListing 11.11\\nThe kube-scheduler Endpoints resource used for leader-election\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Controller Manager',\n",
       "    'description': 'A component that can run collocated with the API server and etcd, or on separate machines.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'A component that can run collocated with the API server and etcd, or on separate machines.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': 'The server that handles incoming requests to the Kubernetes cluster.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'A distributed key-value store used by Kubernetes for storing and retrieving data.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Endpoints resource',\n",
       "    'description': 'A resource used in leader election mechanism for selecting a leader among control plane components.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'leader election mechanism',\n",
       "    'description': 'A process used to select a leader among control plane components, using the Endpoints resource.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kube-scheduler',\n",
       "    'description': 'The Scheduler instance that tries to create and update an Endpoints resource called kube-scheduler.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'minikube',\n",
       "    'description': 'A hold identity field in the leader election mechanism, used to identify the current leader.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ConfigMaps',\n",
       "    'description': 'Resources that can be used for storing and retrieving configuration data, soon to be used instead of Endpoints for leader election mechanism.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'load balancer',\n",
       "    'description': 'A component that distributes incoming requests across multiple instances of the API server.',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\": \"Controller Manager\", \"description\": \"can run collocated with the API server and etcd\", \"destination_entity\": \"API Server\"}, \\n{\"source_entity\": \"Controller Manager\", \"description\": \"can run on separate machines\", \"destination_entity\": \"machine\"}, \\n{\"source_entity\": \"Scheduler\", \"description\": \"try to create (and later update) an Endpoints resource called kube-scheduler\", \"destination_entity\": \"Endpoints resource\"}, \\n{\"source_entity\": \"Scheduler\", \"description\": \"will soon use ConfigMaps instead of Endpoints\", \"destination_entity\": \"ConfigMaps\"}, \\n{\"source_entity\": \"leader election mechanism\", \"description\": \"works purely by creating a resource in the API server\", \"destination_entity\": \"API Server\"}, \\n{\"source_entity\": \"leader election mechanism\", \"description\": \"uses the Endpoints resource to achieve this\", \"destination_entity\": \"Endpoints resource\"}, \\n{\"source_entity\": \"Scheduler\", \"description\": \"try to create (and later update) an Endpoints resource called kube-scheduler\", \"destination_entity\": \"kube-scheduler\"}, \\n{\"source_entity\": \"minikube\", \"description\": \"has a leaderIdentity field in the kube-scheduler Endpoints resource\", \"destination_entity\": \"kube-scheduler\"}, \\n{\"source_entity\": \"minikube\", \"description\": \"becomes the leader when it succeeds in putting its name there\", \"destination_entity\": \"leader election mechanism\"}, \\n{\"source_entity\": \"API Server\", \"description\": \"can talk to the local API server directly when collocated with the Controller Manager and Scheduler\", \"destination_entity\": \"local API server\"}, \\n{\"source_entity\": \"load balancer\", \"description\": \"connects to the API servers through the load balancer\", \"destination_entity\": \"API Server\"}]'},\n",
       " {'page': 377,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '345\\nSummary\\nother instances see that the resource hasn’t been updated for a while, and try to become\\nthe leader by writing their own name to the resource. Simple, right?\\n11.7\\nSummary\\nHopefully, this has been an interesting chapter that has improved your knowledge of\\nthe inner workings of Kubernetes. This chapter has shown you\\n\\uf0a1What components make up a Kubernetes cluster and what each component is\\nresponsible for\\n\\uf0a1How the API server, Scheduler, various controllers running in the Controller\\nManager, and the Kubelet work together to bring a pod to life\\n\\uf0a1How the infrastructure container binds together all the containers of a pod\\n\\uf0a1How pods communicate with other pods running on the same node through\\nthe network bridge, and how those bridges on different nodes are connected,\\nso pods running on different nodes can talk to each other\\n\\uf0a1How the kube-proxy performs load balancing across pods in the same service by\\nconfiguring iptables rules on the node\\n\\uf0a1How multiple instances of each component of the Control Plane can be run to\\nmake the cluster highly available\\nNext, we’ll look at how to secure the API server and, by extension, the cluster as a whole.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server',\n",
       "    'description': \"component that exposes Kubernetes' APIs\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'component responsible for scheduling pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'component that runs various controllers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'component that runs on each node and manages the containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Infrastructure container',\n",
       "    'description': 'container that binds together all the containers of a pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'lightweight and portable execution environment for applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Network bridge',\n",
       "    'description': 'component that allows pods to communicate with each other on the same node',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Iptables rules',\n",
       "    'description': 'configuration used by kube-proxy for load balancing across pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kube-proxy',\n",
       "    'description': 'component responsible for load balancing and performing network policies',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Other instances\", \"description\": \"try to become leader by writing their own name to the resource\", \"destination_entity\": \"Resource\"},\\n  {\"source_entity\": \"API server\", \"description\": \"bind together all containers of a pod\", \"destination_entity\": \"Infrastructure container\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"work together with API server, Scheduler, and various controllers to bring a pod to life\", \"destination_entity\": \"Controller Manager\"},\\n  {\"source_entity\": \"Kube-proxy\", \"description\": \"perform load balancing across pods in the same service by configuring iptables rules on the node\", \"destination_entity\": \"Iptables rules\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"make up a cluster and what each component is responsible for\", \"destination_entity\": \"Controller Manager\"},\\n  {\"source_entity\": \"API server\", \"description\": \"secure the API server and, by extension, the cluster as a whole\", \"destination_entity\": \"Cluster\"},\\n  {\"source_entity\": \"Scheduler\", \"description\": \"work together with API server, Kubelet, and various controllers to bring a pod to life\", \"destination_entity\": \"Controller Manager\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"work together with API server, Scheduler, and various controllers to bring a pod to life\", \"destination_entity\": \"Controller Manager\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"communicate with other pods running on the same node through the network bridge\", \"destination_entity\": \"Network bridge\"},\\n  {\"source_entity\": \"Kube-proxy\", \"description\": \"perform load balancing across pods in the same service\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Controller Manager\", \"description\": \"run multiple instances to make the cluster highly available\", \"destination_entity\": \"Cluster\"}\\n]\\n```'},\n",
       " {'page': 378,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '346\\nSecuring the\\nKubernetes API server\\nIn chapter 8 you learned how applications running in pods can talk to the API\\nserver to retrieve or change the state of resources deployed in the cluster. To\\nauthenticate with the API server, you used the ServiceAccount token mounted into\\nthe pod. In this chapter, you’ll learn what ServiceAccounts are and how to config-\\nure their permissions, as well as permissions for other subjects using the cluster. \\n12.1\\nUnderstanding authentication\\nIn the previous chapter, we said the API server can be configured with one or more\\nauthentication plugins (and the same is true for authorization plugins). When a\\nrequest is received by the API server, it goes through the list of authentication\\nThis chapter covers\\n\\uf0a1Understanding authentication\\n\\uf0a1What ServiceAccounts are and why they’re used\\n\\uf0a1Understanding the role-based access control \\n(RBAC) plugin\\n\\uf0a1Using Roles and RoleBindings\\n\\uf0a1Using ClusterRoles and ClusterRoleBindings\\n\\uf0a1Understanding the default roles and bindings\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes API server',\n",
       "    'description': 'The central management interface for a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceAccount token',\n",
       "    'description': 'A token used to authenticate with the Kubernetes API server',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'Lightweight and portable containers running in the Kubernetes cluster',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The central management interface for a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'authentication plugins',\n",
       "    'description': 'Plugins used by the API server to authenticate incoming requests',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'authorization plugins',\n",
       "    'description': 'Plugins used by the API server to manage access control',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'requests',\n",
       "    'description': 'Incoming calls to the API server',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ServiceAccounts',\n",
       "    'description': 'A way to authenticate and authorize entities in the Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Roles',\n",
       "    'description': 'A way to define permissions for users or groups in the Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'RoleBindings',\n",
       "    'description': 'A way to bind Roles to specific users or groups in the Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRoles',\n",
       "    'description': 'A way to define permissions for all users in a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRoleBindings',\n",
       "    'description': 'A way to bind ClusterRoles to specific users or groups in a Kubernetes cluster',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes API server\", \"description\": \" authenticates with \", \"destination_entity\": \"ServiceAccount token\"},\\n  {\"source_entity\": \"requests\", \"description\": \" are processed by \", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"authentication plugins\", \"description\": \" are used by \", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"authorization plugins\", \"description\": \" are used by \", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"ServiceAccount token\", \"description\": \" is mounted into \", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"Roles\", \"description\": \" define permissions for \", \"destination_entity\": \"users and groups\"},\\n  {\"source_entity\": \"RoleBindings\", \"description\": \" bind Roles to \", \"destination_entity\": \"users and groups\"},\\n  {\"source_entity\": \"ClusterRoleBindings\", \"description\": \" bind ClusterRoles to \", \"destination_entity\": \"users and groups\"},\\n  {\"source_entity\": \"ClusterRoles\", \"description\": \" define permissions for \", \"destination_entity\": \"the entire cluster\"},\\n  {\"source_entity\": \"ServiceAccounts\", \"description\": \" are used to manage access to \", \"destination_entity\": \"cluster resources\"},\\n  {\"source_entity\": \"Kubernetes API server\", \"description\": \" processes requests from \", \"destination_entity\": \"clients\"},\\n  {\"source_entity\": \"requests\", \"description\": \" can be authenticated using \", \"destination_entity\": \"authentication plugins\"}\\n]\\n```'},\n",
       " {'page': 379,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '347\\nUnderstanding authentication\\nplugins, so they can each examine the request and try to determine who’s sending the\\nrequest. The first plugin that can extract that information from the request returns\\nthe username, user ID, and the groups the client belongs to back to the API server\\ncore. The API server stops invoking the remaining authentication plugins and contin-\\nues onto the authorization phase. \\n Several authentication plugins are available. They obtain the identity of the client\\nusing the following methods:\\n\\uf0a1From the client certificate\\n\\uf0a1From an authentication token passed in an HTTP header\\n\\uf0a1Basic HTTP authentication\\n\\uf0a1Others\\nThe authentication plugins are enabled through command-line options when starting\\nthe API server. \\n12.1.1 Users and groups\\nAn authentication plugin returns the username and group(s) of the authenticated\\nuser. Kubernetes doesn’t store that information anywhere; it uses it to verify whether\\nthe user is authorized to perform an action or not.\\nUNDERSTANDING USERS\\nKubernetes distinguishes between two kinds of clients connecting to the API server:\\n\\uf0a1Actual humans (users)\\n\\uf0a1Pods (more specifically, applications running inside them)\\nBoth these types of clients are authenticated using the aforementioned authentication\\nplugins. Users are meant to be managed by an external system, such as a Single Sign\\nOn (SSO) system, but the pods use a mechanism called service accounts, which are cre-\\nated and stored in the cluster as ServiceAccount resources. In contrast, no resource\\nrepresents user accounts, which means you can’t create, update, or delete users through\\nthe API server. \\n We won’t go into any details of how to manage users, but we will explore Service-\\nAccounts in detail, because they’re essential for running pods. For more informa-\\ntion on how to configure the cluster for authentication of human users, cluster\\nadministrators should refer to the Kubernetes Cluster Administrator guide at http:/\\n/\\nkubernetes.io/docs/admin.\\nUNDERSTANDING GROUPS\\nBoth human users and ServiceAccounts can belong to one or more groups. We’ve said\\nthat the authentication plugin returns groups along with the username and user ID.\\nGroups are used to grant permissions to several users at once, instead of having to\\ngrant them to individual users. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Authentication Plugins',\n",
       "    'description': 'Plugins that obtain identity of client using methods such as client certificate, authentication token, basic HTTP authentication etc.',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'API Server Core',\n",
       "    'description': \"Server that receives requests from clients and invokes authentication plugins to determine who's sending the request\",\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Users',\n",
       "    'description': 'Actual humans connecting to API server, managed by external system such as Single Sign On (SSO) system',\n",
       "    'category': 'Hardware/Software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Applications running inside pods that connect to API server and use service accounts for authentication',\n",
       "    'category': 'Application/Container'},\n",
       "   {'entity': 'Service Accounts',\n",
       "    'description': 'Mechanism used by pods for authentication, created and stored in cluster as ServiceAccount resources',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'Groups',\n",
       "    'description': 'Used to grant permissions to several users at once instead of granting individual users, returned by authentication plugin along with username and user ID',\n",
       "    'category': 'Hardware/Software'}],\n",
       "  'relationships': '[{\"source_entity\": \"Authentication Plugins\", \"description\": \"extract identity from client certificate, authentication token, or Basic HTTP authentication\", \"destination_entity\": \"Client\"}, \\n {\"source_entity\": \"API Server Core\", \"description\": \"stop invoking remaining authentication plugins and continue onto authorization phase\", \"destination_entity\": \"Authentication Plugins\"}, \\n {\"source_entity\": \"API Server Core\", \"description\": \"invoke multiple authentication plugins to determine client identity\", \"destination_entity\": \"Multiple Authentication Plugins\"}, \\n {\"source_entity\": \"Kubernetes\", \"description\": \"verify user authorization based on username and group(s)\", \"destination_entity\": \"Users\"}, \\n {\"source_entity\": \"Authentication Plugin\", \"description\": \"return username and group(s) of authenticated user\", \"destination_entity\": \"User\"}, \\n {\"source_entity\": \"Service Accounts\", \"description\": \"created and stored in cluster as ServiceAccount resources\", \"destination_entity\": \"Cluster\"}, \\n {\"source_entity\": \"Users\", \"description\": \"managed by external system such as SSO\", \"destination_entity\": \"SSO System\"}, \\n {\"source_entity\": \"Pods\", \"description\": \"use mechanism called service accounts to authenticate\", \"destination_entity\": \"Service Accounts\"}, \\n {\"source_entity\": \"API Server Core\", \"description\": \"refer to Kubernetes Cluster Administrator guide for human user authentication configuration\", \"destination_entity\": \"Kubernetes Cluster Administrator Guide\"}, \\n {\"source_entity\": \"Users\", \"description\": \"belong to one or more groups\", \"destination_entity\": \"Groups\"}, \\n {\"source_entity\": \"Service Accounts\", \"description\": \"belong to one or more groups\", \"destination_entity\": \"Groups\"}]'},\n",
       " {'page': 380,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '348\\nCHAPTER 12\\nSecuring the Kubernetes API server\\n Groups returned by the plugin are nothing but strings, representing arbitrary\\ngroup names, but built-in groups have special meaning:\\n\\uf0a1The system:unauthenticated group is used for requests where none of the\\nauthentication plugins could authenticate the client.\\n\\uf0a1The system:authenticated group is automatically assigned to a user who was\\nauthenticated successfully.\\n\\uf0a1The system:serviceaccounts group encompasses all ServiceAccounts in the\\nsystem.\\n\\uf0a1The system:serviceaccounts:<namespace> includes all ServiceAccounts in a\\nspecific namespace.\\n12.1.2 Introducing ServiceAccounts\\nLet’s explore ServiceAccounts up close. You’ve already learned that the API server\\nrequires clients to authenticate themselves before they’re allowed to perform opera-\\ntions on the server. And you’ve already seen how pods can authenticate by sending the\\ncontents of the file/var/run/secrets/kubernetes.io/serviceaccount/token, which\\nis mounted into each container’s filesystem through a secret volume.\\n But what exactly does that file represent? Every pod is associated with a Service-\\nAccount, which represents the identity of the app running in the pod. The token file\\nholds the ServiceAccount’s authentication token. When an app uses this token to con-\\nnect to the API server, the authentication plugin authenticates the ServiceAccount\\nand passes the ServiceAccount’s username back to the API server core. Service-\\nAccount usernames are formatted like this:\\nsystem:serviceaccount:<namespace>:<service account name>\\nThe API server passes this username to the configured authorization plugins, which\\ndetermine whether the action the app is trying to perform is allowed to be performed\\nby the ServiceAccount.\\n ServiceAccounts are nothing more than a way for an application running inside a\\npod to authenticate itself with the API server. As already mentioned, applications do\\nthat by passing the ServiceAccount’s token in the request.\\nUNDERSTANDING THE SERVICEACCOUNT RESOURCE\\nServiceAccounts are resources just like Pods, Secrets, ConfigMaps, and so on, and are\\nscoped to individual namespaces. A default ServiceAccount is automatically created\\nfor each namespace (that’s the one your pods have used all along). \\n You can list ServiceAccounts like you do other resources:\\n$ kubectl get sa\\nNAME      SECRETS   AGE\\ndefault   1         1d\\nNOTE\\nThe shorthand for serviceaccount is sa.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes API server',\n",
       "    'description': 'The main component that manages and runs containerized applications.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Groups',\n",
       "    'description': 'A concept used to manage access control in Kubernetes.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'system:unauthenticated group',\n",
       "    'description': 'A built-in group for requests where none of the authentication plugins could authenticate the client.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'system:authenticated group',\n",
       "    'description': 'A built-in group automatically assigned to a user who was authenticated successfully.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'system:serviceaccounts group',\n",
       "    'description': 'A built-in group that encompasses all ServiceAccounts in the system.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'ServiceAccounts',\n",
       "    'description': 'An identity used by an application running inside a pod to authenticate itself with the API server.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'A resource in Kubernetes that represents a set of containers that share resources and networking.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'A resource in Kubernetes that stores sensitive information such as passwords or tokens.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'ConfigMaps',\n",
       "    'description': 'A resource in Kubernetes that stores configuration data used by applications.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'ServiceAccount usernames',\n",
       "    'description': 'The format of a username for ServiceAccounts, including the namespace and service account name.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'authentication plugin',\n",
       "    'description': 'A component responsible for authenticating clients to the API server.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'authorization plugins',\n",
       "    'description': \"Components that determine whether an action is allowed based on the ServiceAccount's permissions.\",\n",
       "    'category': 'Software'},\n",
       "   {'entity': '$ kubectl get sa',\n",
       "    'description': 'A command used to list ServiceAccounts in a namespace.',\n",
       "    'category': 'Command'}],\n",
       "  'relationships': '[{\"source_entity\": \"system:serviceaccounts group\", \"description\": \"represent arbitrary group names, but built-in groups have special meaning\", \"destination_entity\": \"Groups\"},\\n\\n {\"source_entity\": \"ServiceAccounts\", \"description\": \"are resources just like Pods, Secrets, ConfigMaps and are scoped to individual namespaces\", \"destination_entity\": \"Pods\"},\\n\\n {\"source_entity\": \"Kubernetes API server\", \"description\": \"requires clients to authenticate themselves before they\\'re allowed to perform operations on the server\", \"destination_entity\": \"authentication plugin\"},\\n\\n {\"source_entity\": \"ServiceAccounts\", \"description\": \"are associated with a ServiceAccount which represents the identity of the app running in the pod\", \"destination_entity\": \"Pods\"},\\n\\n {\"source_entity\": \"system:unauthenticated group\", \"description\": \"is used for requests where none of the authentication plugins could authenticate the client\", \"destination_entity\": \"$ kubectl get sa\"},\\n\\n {\"source_entity\": \"serviceaccount\", \"description\": \"is automatically created for each namespace (that\\'s the one your pods have used all along)\", \"destination_entity\": \"default ServiceAccount\"},\\n\\n {\"source_entity\": \"ServiceAccounts\", \"description\": \"are nothing more than a way for an application running inside a pod to authenticate itself with the API server\", \"destination_entity\": \"Kubernetes API server\"},\\n\\n {\"source_entity\": \"system:authenticated group\", \"description\": \"is automatically assigned to a user who was authenticated successfully\", \"destination_entity\": \"$ kubectl get sa\"},\\n\\n {\"source_entity\": \"authentication plugin\", \"description\": \"determine whether the action the app is trying to perform is allowed to be performed by the ServiceAccount\", \"destination_entity\": \"ServiceAccounts\"},\\n\\n {\"source_entity\": \"authorization plugins\", \"description\": \"pass the ServiceAccount\\'s username back to the API server core\", \"destination_entity\": \"Kubernetes API server\"},\\n\\n {\"source_entity\": \"ServiceAccount usernames\", \"description\": \"are formatted like this: system:serviceaccount:<namespace>:<service account name>\", \"destination_entity\": \"ServiceAccounts\"},\\n\\n {\"source_entity\": \"$ kubectl get sa\", \"description\": \"is used to list ServiceAccounts just like other resources\", \"destination_entity\": \"ServiceAccounts\"},\\n\\n {\"source_entity\": \"system:authenticated group\", \"description\": \"includes all ServiceAccounts in the system\", \"destination_entity\": \"ServiceAccounts\"},\\n\\n {\"source_entity\": \"serviceaccount\", \"description\": \"can be shortened to sa\", \"destination_entity\": \"Kubernetes API server\"}]'},\n",
       " {'page': 381,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '349\\nUnderstanding authentication\\nAs you can see, the current namespace only contains the default ServiceAccount. Addi-\\ntional ServiceAccounts can be added when required. Each pod is associated with exactly\\none ServiceAccount, but multiple pods can use the same ServiceAccount. As you can\\nsee in figure 12.1, a pod can only use a ServiceAccount from the same namespace.\\nUNDERSTANDING HOW SERVICEACCOUNTS TIE INTO AUTHORIZATION\\nYou can assign a ServiceAccount to a pod by specifying the account’s name in the pod\\nmanifest. If you don’t assign it explicitly, the pod will use the default ServiceAccount\\nin the namespace.\\n By assigning different ServiceAccounts to pods, you can control which resources\\neach pod has access to. When a request bearing the authentication token is received\\nby the API server, the server uses the token to authenticate the client sending the\\nrequest and then determines whether or not the related ServiceAccount is allowed to\\nperform the requested operation. The API server obtains this information from the\\nsystem-wide authorization plugin configured by the cluster administrator. One of\\nthe available authorization plugins is the role-based access control (RBAC) plugin,\\nwhich is discussed later in this chapter. From Kubernetes version 1.6 on, the RBAC\\nplugin is the plugin most clusters should use.\\n12.1.3 Creating ServiceAccounts\\nWe’ve said every namespace contains its own default ServiceAccount, but additional\\nones can be created if necessary. But why should you bother with creating Service-\\nAccounts instead of using the default one for all your pods? \\n The obvious reason is cluster security. Pods that don’t need to read any cluster\\nmetadata should run under a constrained account that doesn’t allow them to retrieve\\nor modify any resources deployed in the cluster. Pods that need to retrieve resource\\nmetadata should run under a ServiceAccount that only allows reading those objects’\\nmetadata, whereas pods that need to modify those objects should run under their own\\nServiceAccount allowing modifications of API objects. \\nPod\\nNamespace: foo\\nService-\\nAccount:\\ndefault\\nPod\\nPod\\nNamespace: baz\\nPod\\nNamespace: bar\\nPod\\nPod\\nNot possible\\nService-\\nAccount:\\ndefault\\nAnother\\nService-\\nAccount\\nMultiple pods using the\\nsame ServiceAccount\\nFigure 12.1\\nEach pod is associated with a single ServiceAccount in the pod’s namespace.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Namespace: bar\n",
       "   Pod Pod Pod\n",
       "   Service- Another\n",
       "   Account: Service-\n",
       "   default Account, Col1, Namespace: baz\n",
       "   Pod]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Namespace',\n",
       "    'description': 'A logical grouping of resources in Kubernetes',\n",
       "    'category': 'Kubernetes/Cluster'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'An identity for a pod to authenticate requests with',\n",
       "    'category': 'Kubernetes/Cluster'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A container that runs an application in Kubernetes',\n",
       "    'category': 'Kubernetes/Container'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': 'The primary component of the Kubernetes control plane',\n",
       "    'category': 'Kubernetes/ControlPlane'},\n",
       "   {'entity': 'RBAC Plugin',\n",
       "    'description': 'A system-wide authorization plugin for Kubernetes',\n",
       "    'category': 'Kubernetes/Security'},\n",
       "   {'entity': 'Role-Based Access Control (RBAC)',\n",
       "    'description': 'A method of controlling access to resources based on roles',\n",
       "    'category': 'Kubernetes/Security'},\n",
       "   {'entity': 'Token',\n",
       "    'description': 'An authentication token used by the API server',\n",
       "    'category': 'Kubernetes/Authentication'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Namespace\",\\n    \"description\": \"contains default ServiceAccount\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"Namespace\",\\n    \"description\": \"contains multiple ServiceAccounts\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"associated with a single ServiceAccount in the same namespace\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"uses default ServiceAccount if not explicitly assigned\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"authenticates client using Token and checks related ServiceAccount for permission\",\\n    \"destination_entity\": \"Token\" \\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"obtains permission information from system-wide authorization plugin configured by cluster administrator\",\\n    \"destination_entity\": \"Authorization Plugin\"\\n  },\\n  {\\n    \"source_entity\": \"Role-Based Access Control (RBAC)\",\\n    \"description\": \"provides permissions to ServiceAccounts and pods\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster Administrator\",\\n    \"description\": \"configures system-wide authorization plugin\",\\n    \"destination_entity\": \"Authorization Plugin\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"runs under constrained ServiceAccount if not needed to read cluster metadata\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"RBAC Plugin\",\\n    \"description\": \"is the plugin most clusters should use from Kubernetes version 1.6 on\",\\n    \"destination_entity\": \"Kubernetes\"\\n  }\\n]'},\n",
       " {'page': 382,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '350\\nCHAPTER 12\\nSecuring the Kubernetes API server\\n Let’s see how you can create additional ServiceAccounts, how they relate to Secrets,\\nand how you can assign them to your pods.\\nCREATING A SERVICEACCOUNT\\nCreating a ServiceAccount is incredibly easy, thanks to the dedicated kubectl create\\nserviceaccount command. Let’s create a new ServiceAccount called foo:\\n$ kubectl create serviceaccount foo\\nserviceaccount \"foo\" created\\nNow, you can inspect the ServiceAccount with the describe command, as shown in\\nthe following listing.\\n$ kubectl describe sa foo\\nName:               foo\\nNamespace:          default\\nLabels:             <none>\\nImage pull secrets: <none>             \\nMountable secrets:  foo-token-qzq7j    \\nTokens:             foo-token-qzq7j    \\nYou can see that a custom token Secret has been created and associated with the\\nServiceAccount. If you look at the Secret’s data with kubectl describe secret foo-\\ntoken-qzq7j, you’ll see it contains the same items (the CA certificate, namespace, and\\ntoken) as the default ServiceAccount’s token does (the token itself will obviously be\\ndifferent), as shown in the following listing.\\n$ kubectl describe secret foo-token-qzq7j\\n...\\nca.crt:         1066 bytes\\nnamespace:      7 bytes\\ntoken:          eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\\nNOTE\\nYou’ve probably heard of JSON Web Tokens (JWT). The authentica-\\ntion tokens used in ServiceAccounts are JWT tokens.\\nUNDERSTANDING A SERVICEACCOUNT’S MOUNTABLE SECRETS\\nThe token is shown in the Mountable secrets list when you inspect a ServiceAccount\\nwith kubectl describe. Let me explain what that list represents. In chapter 7 you\\nlearned how to create Secrets and mount them inside a pod. By default, a pod can\\nmount any Secret it wants. But the pod’s ServiceAccount can be configured to only\\nListing 12.1\\nInspecting a ServiceAccount with kubectl describe\\nListing 12.2\\nInspecting the custom ServiceAccount’s Secret\\nThese will be added \\nautomatically to all pods \\nusing this ServiceAccount.\\nPods using this ServiceAccount \\ncan only mount these Secrets if \\nmountable Secrets are enforced.\\nAuthentication token(s). \\nThe first one is mounted \\ninside the container.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ServiceAccount',\n",
       "    'description': 'A Kubernetes concept that allows you to manage authentication and authorization for pods.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool used to interact with a Kubernetes cluster.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'create serviceaccount',\n",
       "    'description': 'The command used to create a new ServiceAccount.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'foo',\n",
       "    'description': 'A custom ServiceAccount created in the document.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'ServiceAccount token',\n",
       "    'description': 'A JSON Web Token (JWT) that authenticates pods using a ServiceAccount.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'Kubernetes objects that store sensitive information, such as authentication tokens.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'foo-token-qzq7j',\n",
       "    'description': 'A custom token Secret created for the foo ServiceAccount.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'describe sa',\n",
       "    'description': 'The command used to inspect a ServiceAccount.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'describe secret',\n",
       "    'description': 'The command used to inspect a Secret.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'ca.crt',\n",
       "    'description': 'A certificate used for authentication and encryption in Kubernetes.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'A scope within a Kubernetes cluster where resources are organized.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'token',\n",
       "    'description': 'The JSON Web Token (JWT) that authenticates pods using a ServiceAccount.',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[{\"source_entity\": \"kubectl\", \"description\": \"used to create a ServiceAccount called foo\", \"destination_entity\": \"foo\"},{\"source_entity\": \"kubectl\", \"description\": \"inspects the ServiceAccount with the describe command\", \"destination_entity\": \"foo\"},{\"source_entity\": \"kubectl\", \"description\": \"describes the Secret associated with the ServiceAccount\", \"destination_entity\": \"foo-token-qzq7j\"},{\"source_entity\": \"ServiceAccount\", \"description\": \"has a custom token Secret created and associated with it\", \"destination_entity\": \"foo-token-qzq7j\"},{\"source_entity\": \"ServiceAccount\", \"description\": \"can be configured to only allow specific Secrets to be mounted by pods using this ServiceAccount\", \"destination_entity\": \"Secrets\"},{\"source_entity\": \"kubectl\", \"description\": \"used to describe the Secret associated with the ServiceAccount\", \"destination_entity\": \"foo-token-qzq7j\"},{\"source_entity\": \"ServiceAccount token\", \"description\": \"is a JSON Web Token (JWT) used for authentication\", \"destination_entity\": \"foo-token-qzq7j\"},{\"source_entity\": \"kubectl\", \"description\": \"used to create a new ServiceAccount called foo\", \"destination_entity\": \"foo\"},{\"source_entity\": \"foo\", \"description\": \"has a custom token Secret created and associated with it\", \"destination_entity\": \"foo-token-qzq7j\"},{\"source_entity\": \"foo\", \"description\": \"can be inspected with the describe command\", \"destination_entity\": \"kubectl\"},{\"source_entity\": \"foo\", \"description\": \"is a ServiceAccount created by kubectl\", \"destination_entity\": \"ServiceAccount\"},{\"source_entity\": \"create serviceaccount\", \"description\": \"is used to create a new ServiceAccount called foo\", \"destination_entity\": \"foo\"},{\"source_entity\": \"describe secret\", \"description\": \"is used to describe the Secret associated with the ServiceAccount\", \"destination_entity\": \"foo-token-qzq7j\"},{\"source_entity\": \"ServiceAccount token\", \"description\": \"is stored in a Secret called foo-token-qzq7j\", \"destination_entity\": \"foo-token-qzq7j\"},{\"source_entity\": \"namespace\", \"description\": \"is one of the items stored in the Secret associated with the ServiceAccount\", \"destination_entity\": \"foo-token-qzq7j\"}]'},\n",
       " {'page': 383,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '351\\nUnderstanding authentication\\nallow the pod to mount Secrets that are listed as mountable Secrets on the Service-\\nAccount. To enable this feature, the ServiceAccount must contain the following anno-\\ntation: kubernetes.io/enforce-mountable-secrets=\"true\". \\n If the ServiceAccount is annotated with this annotation, any pods using it can mount\\nonly the ServiceAccount’s mountable Secrets—they can’t use any other Secret.\\nUNDERSTANDING A SERVICEACCOUNT’S IMAGE PULL SECRETS\\nA ServiceAccount can also contain a list of image pull Secrets, which we examined in\\nchapter 7. In case you don’t remember, they are Secrets that hold the credentials for\\npulling container images from a private image repository. \\n The following listing shows an example of a ServiceAccount definition, which\\nincludes the image pull Secret you created in chapter 7.\\napiVersion: v1\\nkind: ServiceAccount\\nmetadata:\\n  name: my-service-account\\nimagePullSecrets:\\n- name: my-dockerhub-secret\\nA ServiceAccount’s image pull Secrets behave slightly differently than its mountable\\nSecrets. Unlike mountable Secrets, they don’t determine which image pull Secrets a\\npod can use, but which ones are added automatically to all pods using the Service-\\nAccount. Adding image pull Secrets to a ServiceAccount saves you from having to add\\nthem to each pod individually. \\n12.1.4 Assigning a ServiceAccount to a pod\\nAfter you create additional ServiceAccounts, you need to assign them to pods. This is\\ndone by setting the name of the ServiceAccount in the spec.serviceAccountName\\nfield in the pod definition. \\nNOTE\\nA pod’s ServiceAccount must be set when creating the pod. It can’t be\\nchanged later. \\nCREATING A POD WHICH USES A CUSTOM SERVICEACCOUNT\\nIn chapter 8 you deployed a pod that ran a container based on the tutum/curl image\\nand an ambassador container alongside it. You used it to explore the API server’s\\nREST interface. The ambassador container ran the kubectl proxy process, which\\nused the pod’s ServiceAccount’s token to authenticate with the API server. \\n You can now modify the pod so it uses the foo ServiceAccount you created minutes\\nago. The next listing shows the pod definition.\\n \\n \\nListing 12.3\\nServiceAccount with an image pull Secret: sa-image-pull-secrets.yaml\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Secrets',\n",
       "    'description': 'Mountable Secrets on a Service-Account',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'kubernetes.io/enforce-mountable-secrets',\n",
       "    'description': 'Annotation to enable mountable Secrets feature',\n",
       "    'category': 'annotation'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'Kubernetes concept for authentication and authorization',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'image pull Secret',\n",
       "    'description': 'Secrets that hold credentials for pulling container images',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'my-dockerhub-secret',\n",
       "    'description': 'Example of an image pull Secret',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'API version for ServiceAccount definition',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'Type of ServiceAccount resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Metadata fields for the ServiceAccount',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'spec.serviceAccountName',\n",
       "    'description': 'Field to set the name of the ServiceAccount in a pod definition',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Kubernetes resource that runs one or more containers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ServiceAccount token',\n",
       "    'description': 'Token used by a pod to authenticate with the API server',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'kubectl proxy process',\n",
       "    'description': \"Process that uses a pod's ServiceAccount token for authentication\",\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"Allows pod to mount Secrets listed as mountable on ServiceAccount\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"Can contain image pull Secrets for pulling container images from private repo\",\\n    \"destination_entity\": \"image pull Secret\"\\n  },\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"Enforces mountable Secrets on pods using it\",\\n    \"destination_entity\": \"Secrets\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"Must be assigned a ServiceAccount when creating pod\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"Can contain image pull Secret to use with pods\",\\n    \"destination_entity\": \"image pull Secret\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl proxy process\",\\n    \"description\": \"Uses pod\\'s ServiceAccount token for authentication with API server\",\\n    \"destination_entity\": \"ServiceAccount token\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"Can use image pull Secret added to ServiceAccount\",\\n    \"destination_entity\": \"image pull Secret\"\\n  },\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"Requires annotation kubernetes.io/enforce-mountable-secrets=\\'true\\' for mountable Secrets\",\\n    \"destination_entity\": \"kubernetes.io/enforce-mountable-secrets\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"Must use ServiceAccount\\'s image pull Secret to pull images from private repo\",\\n    \"destination_entity\": \"ServiceAccount\\'s image pull Secret\"\\n  }\\n]'},\n",
       " {'page': 384,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '352\\nCHAPTER 12\\nSecuring the Kubernetes API server\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: curl-custom-sa\\nspec:\\n  serviceAccountName: foo           \\n  containers:\\n  - name: main\\n    image: tutum/curl\\n    command: [\"sleep\", \"9999999\"]\\n  - name: ambassador                  \\n    image: luksa/kubectl-proxy:1.6.2\\nTo confirm that the custom ServiceAccount’s token is mounted into the two contain-\\ners, you can print the contents of the token as shown in the following listing.\\n$ kubectl exec -it curl-custom-sa -c main \\n➥ cat /var/run/secrets/kubernetes.io/serviceaccount/token\\neyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\\nYou can see the token is the one from the foo ServiceAccount by comparing the token\\nstring in listing 12.5 with the one in listing 12.2. \\nUSING THE CUSTOM SERVICEACCOUNT’S TOKEN TO TALK TO THE API SERVER\\nLet’s see if you can talk to the API server using this token. As mentioned previously,\\nthe ambassador container uses the token when talking to the server, so you can test\\nthe token by going through the ambassador, which listens on localhost:8001, as\\nshown in the following listing.\\n$ kubectl exec -it curl-custom-sa -c main curl localhost:8001/api/v1/pods\\n{\\n  \"kind\": \"PodList\",\\n  \"apiVersion\": \"v1\",\\n  \"metadata\": {\\n    \"selfLink\": \"/api/v1/pods\",\\n    \"resourceVersion\": \"433895\"\\n  },\\n  \"items\": [\\n  ...\\nOkay, you got back a proper response from the server, which means the custom\\nServiceAccount is allowed to list pods. This may be because your cluster doesn’t use\\nthe RBAC authorization plugin, or you gave all ServiceAccounts full permissions, as\\ninstructed in chapter 8. \\nListing 12.4\\nPod using a non-default ServiceAccount: curl-custom-sa.yaml\\nListing 12.5\\nInspecting the token mounted into the pod’s container(s)\\nListing 12.6\\nTalking to the API server with a custom ServiceAccount\\nThis pod uses the \\nfoo ServiceAccount \\ninstead of the default.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': 'Kubernetes API server component',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Kubernetes pod object',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'Kubernetes service account concept',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'token',\n",
       "    'description': 'Token used for authentication with API server',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'exec',\n",
       "    'description': 'Kubectl command to execute a command inside a pod',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'Command to send HTTP requests',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'localhost:8001',\n",
       "    'description': 'API server endpoint used by ambassador container',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ambassador',\n",
       "    'description': 'Kubectl proxy container for API server communication',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[{\"source_entity\": \"Kubernetes\", \"description\": \"secures the API server\", \"destination_entity\": \"API Server\"},{\"source_entity\": \"Pod\", \"description\": \"uses a custom ServiceAccount\", \"destination_entity\": \"ServiceAccount\"},{\"source_entity\": \"ambassador\", \"description\": \"listens on localhost:8001 and uses token to talk to the API server\", \"destination_entity\": \"API Server\"},{\"source_entity\": \"curl\", \"description\": \"talks to the API server using the custom ServiceAccount\\'s token\", \"destination_entity\": \"API Server\"},{\"source_entity\": \"kubectl\", \"description\": \"executes commands on the Pod and uses the custom ServiceAccount\\'s token\", \"destination_entity\": \"ServiceAccount\"},{\"source_entity\": \"token\", \"description\": \"is used by ambassador to talk to the API server and by curl to get a response from the server\", \"destination_entity\": \"API Server\"},{\"source_entity\": \"exec\", \"description\": \"is used by kubectl to execute commands on the Pod\", \"destination_entity\": \"Pod\"},{\"source_entity\": \"ServiceAccount\", \"description\": \"has its token mounted into the containers and is used by ambassador to talk to the API server\", \"destination_entity\": \"API Server\"},{\"source_entity\": \"localhost:8001\", \"description\": \"is the port where ambassador listens and talks to the API server\", \"destination_entity\": \"API Server\"}]'},\n",
       " {'page': 385,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '353\\nSecuring the cluster with role-based access control\\n When your cluster isn’t using proper authorization, creating and using additional\\nServiceAccounts doesn’t make much sense, since even the default ServiceAccount is\\nallowed to do anything. The only reason to use ServiceAccounts in that case is to\\nenforce mountable Secrets or to provide image pull Secrets through the Service-\\nAccount, as explained earlier. \\n But creating additional ServiceAccounts is practically a must when you use the\\nRBAC authorization plugin, which we’ll explore next.\\n12.2\\nSecuring the cluster with role-based access control\\nStarting with Kubernetes version 1.6.0, cluster security was ramped up considerably. In\\nearlier versions, if you managed to acquire the authentication token from one of the\\npods, you could use it to do anything you want in the cluster. If you google around,\\nyou’ll find demos showing how a path traversal (or directory traversal) attack (where clients\\ncan retrieve files located outside of the web server’s web root directory) can be used to\\nget the token and use it to run your malicious pods in an insecure Kubernetes cluster.\\n But in version 1.8.0, the RBAC authorization plugin graduated to GA (General\\nAvailability) and is now enabled by default on many clusters (for example, when\\ndeploying a cluster with kubadm, as described in appendix B). RBAC prevents unau-\\nthorized users from viewing or modifying the cluster state. The default Service-\\nAccount isn’t allowed to view cluster state, let alone modify it in any way, unless you\\ngrant it additional privileges. To write apps that communicate with the Kubernetes\\nAPI server (as described in chapter 8), you need to understand how to manage\\nauthorization through RBAC-specific resources.\\nNOTE\\nIn addition to RBAC, Kubernetes also includes other authorization\\nplugins, such as the Attribute-based access control (ABAC) plugin, a Web-\\nHook plugin and custom plugin implementations. RBAC is the standard,\\nthough.\\n12.2.1 Introducing the RBAC authorization plugin\\nThe Kubernetes API server can be configured to use an authorization plugin to check\\nwhether an action is allowed to be performed by the user requesting the action. Because\\nthe API server exposes a REST interface, users perform actions by sending HTTP\\nrequests to the server. Users authenticate themselves by including credentials in the\\nrequest (an authentication token, username and password, or a client certificate).\\nUNDERSTANDING ACTIONS\\nBut what actions are there? As you know, REST clients send GET, POST, PUT, DELETE,\\nand other types of HTTP requests to specific URL paths, which represent specific\\nREST resources. In Kubernetes, those resources are Pods, Services, Secrets, and so on.\\nHere are a few examples of actions in Kubernetes:\\n\\uf0a1Get Pods\\n\\uf0a1Create Services\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ServiceAccounts',\n",
       "    'description': 'a way to manage access to cluster resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Role-Based Access Control (RBAC)',\n",
       "    'description': 'an authorization plugin in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes API server',\n",
       "    'description': 'the central component of a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'REST interface',\n",
       "    'description': 'a way for clients to interact with the Kubernetes API server',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'HTTP requests',\n",
       "    'description': 'requests sent by clients to the Kubernetes API server',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'GET, POST, PUT, DELETE',\n",
       "    'description': 'types of HTTP requests',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'the basic execution unit in a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'a way to expose applications running in a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'a way to store sensitive data in a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'the default account used by pods to authenticate with the API server',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Authentication token',\n",
       "    'description': 'a way for clients to authenticate with the Kubernetes API server',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'Username and password',\n",
       "    'description': 'ways for clients to authenticate with the Kubernetes API server',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'Client certificate',\n",
       "    'description': 'a way for clients to authenticate with the Kubernetes API server',\n",
       "    'category': 'protocol'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"allowed to do anything without proper authorization\",\\n    \"destination_entity\": \"cluster\"\\n  },\\n  {\\n    \"source_entity\": \"RBAC authorization plugin\",\\n    \"description\": \"prevents unauthorized users from viewing or modifying cluster state\",\\n    \"destination_entity\": \"cluster\"\\n  },\\n  {\\n    \"source_entity\": \"default ServiceAccount\",\\n    \"description\": \"allowed to view and modify cluster state unless additional privileges are granted\",\\n    \"destination_entity\": \"cluster\"\\n  },\\n  {\\n    \"source_entity\": \"Authentication token\",\\n    \"description\": \"used to authenticate users and perform actions on Kubernetes resources\",\\n    \"destination_entity\": \"Kubernetes API server\"\\n  },\\n  {\\n    \"source_entity\": \"Username and password\",\\n    \"description\": \"used to authenticate users and perform actions on Kubernetes resources\",\\n    \"destination_entity\": \"Kubernetes API server\"\\n  },\\n  {\\n    \"source_entity\": \"Client certificate\",\\n    \"description\": \"used to authenticate users and perform actions on Kubernetes resources\",\\n    \"destination_entity\": \"Kubernetes API server\"\\n  },\\n  {\\n    \"source_entity\": \"RBAC authorization plugin\",\\n    \"description\": \"manages authorization through specific resources\",\\n    \"destination_entity\": \"Kubernetes resources (e.g. Pods, Services, Secrets)\"\\n  },\\n  {\\n    \"source_entity\": \"GET, POST, PUT, DELETE HTTP requests\",\\n    \"description\": \"used to perform actions on Kubernetes resources\",\\n    \"destination_entity\": \"Kubernetes API server\"\\n  },\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"can be used to enforce mountable Secrets or provide image pull Secrets\",\\n    \"destination_entity\": \"Secrets\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"are resources that can be created, updated, and deleted using HTTP requests\",\\n    \"destination_entity\": \"Kubernetes API server\"\\n  },\\n  {\\n    \"source_entity\": \"Services\",\\n    \"description\": \"are resources that can be created, updated, and deleted using HTTP requests\",\\n    \"destination_entity\": \"Kubernetes API server\"\\n  },\\n  {\\n    \"source_entity\": \"Secrets\",\\n    \"description\": \"are resources that can be created, updated, and deleted using HTTP requests\",\\n    \"destination_entity\": \"Kubernetes API server\"\\n  }\\n]\\n```'},\n",
       " {'page': 386,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '354\\nCHAPTER 12\\nSecuring the Kubernetes API server\\n\\uf0a1Update Secrets\\n\\uf0a1And so on\\nThe verbs in those examples (get, create, update) map to HTTP methods (GET, POST,\\nPUT) performed by the client (the complete mapping is shown in table 12.1). The\\nnouns (Pods, Service, Secrets) obviously map to Kubernetes resources. \\n An authorization plugin such as RBAC, which runs inside the API server, deter-\\nmines whether a client is allowed to perform the requested verb on the requested\\nresource or not.\\nNOTE\\nThe additional verb use is used for PodSecurityPolicy resources, which\\nare explained in the next chapter.\\nBesides applying security permissions to whole resource types, RBAC rules can also\\napply to specific instances of a resource (for example, a Service called myservice).\\nAnd later you’ll see that permissions can also apply to non-resource URL paths,\\nbecause not every path the API server exposes maps to a resource (such as the /api\\npath itself or the server health information at /healthz). \\nUNDERSTANDING THE RBAC PLUGIN\\nThe RBAC authorization plugin, as the name suggests, uses user roles as the key factor\\nin determining whether the user may perform the action or not. A subject (which may\\nbe a human, a ServiceAccount, or a group of users or ServiceAccounts) is associated\\nwith one or more roles and each role is allowed to perform certain verbs on certain\\nresources. \\n If a user has multiple roles, they may do anything that any of their roles allows\\nthem to do. If none of the user’s roles contains a permission to, for example, update\\nSecrets, the API server will prevent the user from performing PUT or PATCH requests\\non Secrets.\\n Managing authorization through the RBAC plugin is simple. It’s all done by creat-\\ning four RBAC-specific Kubernetes resources, which we’ll look at next.\\nTable 12.1\\nMapping HTTP methods to authorization verbs\\nHTTP method\\nVerb for single resource\\nVerb for collection\\nGET, HEAD\\nget (and watch for watching)\\nlist (and watch)\\nPOST\\ncreate\\nn/a\\nPUT\\nupdate\\nn/a\\nPATCH\\npatch\\nn/a\\nDELETE\\ndelete\\ndeletecollection\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [                           HTTP method  \\\n",
       "   0  GET, HEAD\\nPOST\\nPUT\\nPATCH\\nDELETE   \n",
       "   \n",
       "                               Verb for single resource  \\\n",
       "   0  get (and watch for watching)\\ncreate\\nupdate\\n...   \n",
       "   \n",
       "                                    Verb for collection  \n",
       "   0  list (and watch)\\nn/a\\nn/a\\nn/a\\ndeletecollection  ],\n",
       "  'entities': [{'entity': 'Kubernetes API server',\n",
       "    'description': 'The main server responsible for managing Kubernetes resources.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'A type of Kubernetes resource used to store sensitive information.',\n",
       "    'category': 'database/resource'},\n",
       "   {'entity': 'PodSecurityPolicy',\n",
       "    'description': 'A type of Kubernetes resource used to manage pod security policies.',\n",
       "    'category': 'database/resource'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'An entity that provides identity and access management for pods and containers.',\n",
       "    'category': 'process/thread'},\n",
       "   {'entity': 'RBAC plugin',\n",
       "    'description': 'A Kubernetes authorization plugin that uses user roles to determine permissions.',\n",
       "    'category': 'software/plugin'},\n",
       "   {'entity': 'Roles',\n",
       "    'description': 'A concept used by the RBAC plugin to manage access control for users and ServiceAccounts.',\n",
       "    'category': 'database/resource'},\n",
       "   {'entity': 'Verbs (get, create, update, etc.)',\n",
       "    'description': 'HTTP methods that map to Kubernetes API operations.',\n",
       "    'category': 'protocol/command'},\n",
       "   {'entity': 'Resources (Pods, Service, Secrets, etc.)',\n",
       "    'description': 'Entities in the Kubernetes universe that can be managed through the API server.',\n",
       "    'category': 'database/resource'},\n",
       "   {'entity': 'HTTP methods (GET, POST, PUT, etc.)',\n",
       "    'description': 'Standardized communication protocols used by clients to interact with servers.',\n",
       "    'category': 'protocol/command'}],\n",
       "  'relationships': '[{\"source_entity\": \"RBAC plugin\", \"description\": \"uses user roles to determine whether a client can perform actions on resources\", \"destination_entity\": \"Resources (Pods, Service, Secrets, etc.)\"}, \\n {\"source_entity\": \"RBAC plugin\", \"description\": \"applies security permissions to whole resource types or specific instances of a resource\", \"destination_entity\": \"Resources (Pods, Service, Secrets, etc.)\"}, \\n {\"source_entity\": \"User\", \"description\": \"associated with one or more roles which determine allowed actions on resources\", \"destination_entity\": \"Roles\"}, \\n {\"source_entity\": \"RBAC plugin\", \"description\": \"prevents users from performing unauthorized actions on resources\", \"destination_entity\": \"Resources (Pods, Service, Secrets, etc.)\"}, \\n {\"source_entity\": \"HTTP methods (GET, POST, PUT, etc.)\", \"description\": \"map to authorization verbs which are used by the RBAC plugin\", \"destination_entity\": \"Verbs (get, create, update, etc.)\"}, \\n {\"source_entity\": \"Kubernetes API server\", \"description\": \"determines whether a client is allowed to perform requested actions on resources\", \"destination_entity\": \"Resources (Pods, Service, Secrets, etc.)\"}, \\n {\"source_entity\": \"RBAC plugin\", \"description\": \"uses roles and permissions to manage authorization for users\", \"destination_entity\": \"User\"}, \\n {\"source_entity\": \"Roles\", \"description\": \"determine allowed actions on resources for associated users or groups\", \"destination_entity\": \"Resources (Pods, Service, Secrets, etc.)\"}, \\n {\"source_entity\": \"ServiceAccount\", \"description\": \"can be associated with roles and permissions to manage authorization\", \"destination_entity\": \"Roles\"}, \\n {\"source_entity\": \"Kubernetes API server\", \"description\": \"exposes resources which can have specific instances or be whole resource types\", \"destination_entity\": \"Resources (Pods, Service, Secrets, etc.)\"}, \\n {\"source_entity\": \"Secrets\", \"description\": \"can be updated, created, or deleted by users with appropriate permissions\", \"destination_entity\": \"User\"}, \\n {\"source_entity\": \"RBAC plugin\", \"description\": \"allows managing authorization through creation of specific Kubernetes resources\", \"destination_entity\": \"Resources (Pods, Service, Secrets, etc.)\"}]'},\n",
       " {'page': 387,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '355\\nSecuring the cluster with role-based access control\\n12.2.2 Introducing RBAC resources\\nThe RBAC authorization rules are configured through four resources, which can be\\ngrouped into two groups:\\n\\uf0a1Roles and ClusterRoles, which specify which verbs can be performed on which\\nresources.\\n\\uf0a1RoleBindings and ClusterRoleBindings, which bind the above roles to specific\\nusers, groups, or ServiceAccounts.\\nRoles define what can be done, while bindings define who can do it (this is shown in\\nfigure 12.2).\\nThe distinction between a Role and a ClusterRole, or between a RoleBinding and a\\nClusterRoleBinding, is that the Role and RoleBinding are namespaced resources,\\nwhereas the ClusterRole and ClusterRoleBinding are cluster-level resources (not\\nnamespaced). This is depicted in figure 12.3.\\n As you can see from the figure, multiple RoleBindings can exist in a single name-\\nspace (this is also true for Roles). Likewise, multiple ClusterRoleBindings and Cluster-\\nRoles can be created. Another thing shown in the figure is that although RoleBindings\\nare namespaced, they can also reference ClusterRoles, which aren’t. \\n The best way to learn about these four resources and what their effects are is by try-\\ning them out in a hands-on exercise. You’ll do that now.\\n \\n \\n \\n \\nWhat?\\nRole\\nBinding\\nSome\\nresources\\nOther\\nresources\\nRole\\nDoesn’t allow\\ndoing anything\\nwith other resources\\nUser A\\nWho?\\nAdmins group\\nAllows users\\nto access\\nService-\\nAccount:\\nx\\nFigure 12.2\\nRoles grant permissions, whereas RoleBindings bind Roles to subjects.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Role-Based Access Control',\n",
       "    'description': 'Authorization rules configured through four resources: Roles, ClusterRoles, RoleBindings, and ClusterRoleBindings.',\n",
       "    'category': 'Security'},\n",
       "   {'entity': 'Roles',\n",
       "    'description': 'Resources that specify which verbs can be performed on which resources.',\n",
       "    'category': 'Security'},\n",
       "   {'entity': 'ClusterRoles',\n",
       "    'description': 'Cluster-level resources that define what can be done at the cluster level.',\n",
       "    'category': 'Security'},\n",
       "   {'entity': 'RoleBindings',\n",
       "    'description': 'Resources that bind Roles to specific users, groups, or ServiceAccounts.',\n",
       "    'category': 'Security'},\n",
       "   {'entity': 'ClusterRoleBindings',\n",
       "    'description': 'Cluster-level resources that bind ClusterRoles to specific users, groups, or ServiceAccounts.',\n",
       "    'category': 'Security'},\n",
       "   {'entity': 'Verb',\n",
       "    'description': 'An action that can be performed on a resource (e.g., get, list, create).',\n",
       "    'category': 'Security'},\n",
       "   {'entity': 'Resource',\n",
       "    'description': 'An entity that can be acted upon (e.g., a pod, a service, a configuration map).',\n",
       "    'category': 'Security'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': 'A logical partitioning of resources within a cluster.',\n",
       "    'category': 'Storage'},\n",
       "   {'entity': 'Role',\n",
       "    'description': 'A resource that defines what can be done within a namespace.',\n",
       "    'category': 'Security'},\n",
       "   {'entity': 'ClusterRole',\n",
       "    'description': 'A cluster-level resource that defines what can be done at the cluster level.',\n",
       "    'category': 'Security'},\n",
       "   {'entity': 'Subject',\n",
       "    'description': 'An entity that can be bound to a Role or ClusterRole (e.g., a user, a group, a ServiceAccount).',\n",
       "    'category': 'Security'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'A resource that represents an identity for a pod.',\n",
       "    'category': 'Security'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Namespace\",\\n    \"description\": \"defines a scope for resources and role bindings\",\\n    \"destination_entity\": \"Resource\"\\n  },\\n  {\\n    \"source_entity\": \"Role\",\\n    \"description\": \"grants permissions to access certain resources\",\\n    \"destination_entity\": \"Resource\"\\n  },\\n  {\\n    \"source_entity\": \"Verb\",\\n    \"description\": \"specifies an action that can be performed on a resource\",\\n    \"destination_entity\": \"Resource\"\\n  },\\n  {\\n    \"source_entity\": \"Role-Based Access Control\",\\n    \"description\": \"manages permissions and access control\",\\n    \"destination_entity\": \"Namespace\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterRoles\",\\n    \"description\": \"defines a set of permissions that can be applied at the cluster level\",\\n    \"destination_entity\": \"Resource\"\\n  },\\n  {\\n    \"source_entity\": \"RoleBindings\",\\n    \"description\": \"binds a role to a subject, granting access to certain resources\",\\n    \"destination_entity\": \"Subject\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterRole\",\\n    \"description\": \"defines a set of permissions that can be applied at the cluster level\",\\n    \"destination_entity\": \"Resource\"\\n  },\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"represents an account for API access\",\\n    \"destination_entity\": \"Resource\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterRoleBindings\",\\n    \"description\": \"binds a cluster role to a subject, granting access at the cluster level\",\\n    \"destination_entity\": \"Subject\"\\n  },\\n  {\\n    \"source_entity\": \"Roles\",\\n    \"description\": \"define permissions that can be granted to subjects\",\\n    \"destination_entity\": \"Resource\"\\n  },\\n  {\\n    \"source_entity\": \"RoleBinding\",\\n    \"description\": \"binds a role to a subject, granting access to certain resources\",\\n    \"destination_entity\": \"Subject\"\\n  }\\n]'},\n",
       " {'page': 388,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '356\\nCHAPTER 12\\nSecuring the Kubernetes API server\\nSETTING UP YOUR EXERCISE\\nBefore you can explore how RBAC resources affect what you can do through the API\\nserver, you need to make sure RBAC is enabled in your cluster. First, ensure you’re\\nusing at least version 1.6 of Kubernetes and that the RBAC plugin is the only config-\\nured authorization plugin. There can be multiple plugins enabled in parallel and if\\none of them allows an action to be performed, the action is allowed.\\nNOTE\\nIf you’re using GKE 1.6 or 1.7, you need to explicitly disable legacy autho-\\nrization by creating the cluster with the --no-enable-legacy-authorization\\noption. If you’re using Minikube, you also may need to enable RBAC by start-\\ning Minikube with --extra-config=apiserver.Authorization.Mode=RBAC\\nIf you followed the instructions on how to disable RBAC in chapter 8, now’s the time\\nto re-enable it by running the following command:\\n$ kubectl delete clusterrolebinding permissive-binding\\nTo try out RBAC, you’ll run a pod through which you’ll try to talk to the API server,\\nthe way you did in chapter 8. But this time you’ll run two pods in different namespaces\\nto see how per-namespace security behaves.\\n In the examples in chapter 8, you ran two containers to demonstrate how an appli-\\ncation in one container uses the other container to talk to the API server. This time,\\nyou’ll run a single container (based on the kubectl-proxy image) and use kubectl\\nexec to run curl inside that container directly. The proxy will take care of authentica-\\ntion and HTTPS, so you can focus on the authorization aspect of API server security.\\nNamespace C\\nNamespaced\\nresources\\nCluster-level\\nresources\\nRoleBinding\\nRoleBinding\\nRole\\nNamespace B\\nNamespaced\\nresources\\nRoleBinding\\nRole\\nNamespace A\\nNamespaced\\nresources\\nRoleBinding\\nRole\\nCluster scope (resources that aren’t namespaced)\\nClusterRoleBinding\\nClusterRole\\nFigure 12.3\\nRoles and RoleBindings are namespaced; ClusterRoles and ClusterRoleBindings aren’t.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Namespace B\n",
       "   RoleBinding Role\n",
       "   Namespaced\n",
       "   resources, Namespace C\n",
       "   RoleBinding Role\n",
       "   Namespaced\n",
       "   RoleBinding resources]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Component of Kubernetes that provides a RESTful API for management',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'RBAC',\n",
       "    'description': 'Role-Based Access Control plugin for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'GKE',\n",
       "    'description': 'Google Kubernetes Engine, a managed container service',\n",
       "    'category': 'cloud platform'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'A local development environment for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with a Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'clusterrolebinding',\n",
       "    'description': 'Resource in Kubernetes that binds a role to a user or group',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'clusterrole',\n",
       "    'description': 'Resource in Kubernetes that defines permissions at the cluster level',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'namespaced resources',\n",
       "    'description': 'Resources in Kubernetes that are scoped to a namespace',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'RoleBinding',\n",
       "    'description': 'Resource in Kubernetes that binds a role to a user or group within a namespace',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Role',\n",
       "    'description': 'Resource in Kubernetes that defines permissions within a namespace',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'ClusterRoleBinding',\n",
       "    'description': 'Resource in Kubernetes that binds a cluster-wide role to a user or group',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'ClusterRole',\n",
       "    'description': 'Resource in Kubernetes that defines permissions at the cluster level',\n",
       "    'category': 'resource'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"requires at least version 1.6 to enable RBAC plugin\", \"destination_entity\": \"RBAC plugin\"},\\n  {\"source_entity\": \"GKE\", \"description\": \"needs explicit disabling of legacy authorization\", \"destination_entity\": \"legacy authorization\"},\\n  {\"source_entity\": \"Minikube\", \"description\": \"requires enabling RBAC mode through extra-config option\", \"destination_entity\": \"RBAC mode\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"deletes clusterrolebinding permissive-binding to re-enable RBAC\", \"destination_entity\": \"clusterrolebinding permissive-binding\"},\\n  {\"source_entity\": \"API server\", \"description\": \"can be secured through enabling RBAC in the cluster\", \"destination_entity\": \"RBAC plugin\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"uses exec command to run curl inside a container for API server security testing\", \"destination_entity\": \"curl\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"has different types of resources at namespace level and cluster scope\", \"destination_entity\": \"namespaced resources\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"uses RoleBinding and Role for namespaced resources, while using ClusterRoleBinding and ClusterRole for cluster scope\", \"destination_entity\": \"ClusterRoleBinding\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"can be used to delete a clusterrolebinding to re-enable RBAC in the cluster\", \"destination_entity\": \"clusterrolebinding permissive-binding\"},\\n  {\"source_entity\": \"API server\", \"description\": \"has different types of resources at namespace level and cluster scope, with roles and rolebindings being namespaced while clusterroles and clusterrolebindings are not\", \"destination_entity\": \"namespaced resources\"}\\n]\\n```\\n\\nNote: I\\'ve tried to be as accurate as possible in extracting the relations from the document page. However, if you feel that any relation is incorrect or incomplete, please let me know so I can re-evaluate it.'},\n",
       " {'page': 389,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '357\\nSecuring the cluster with role-based access control\\nCREATING THE NAMESPACES AND RUNNING THE PODS\\nYou’re going to create one pod in namespace foo and the other one in namespace\\nbar, as shown in the following listing.\\n$ kubectl create ns foo\\nnamespace \"foo\" created\\n$ kubectl run test --image=luksa/kubectl-proxy -n foo\\ndeployment \"test\" created\\n$ kubectl create ns bar\\nnamespace \"bar\" created\\n$ kubectl run test --image=luksa/kubectl-proxy -n bar\\ndeployment \"test\" created\\nNow open two terminals and use kubectl exec to run a shell inside each of the two\\npods (one in each terminal). For example, to run the shell in the pod in namespace\\nfoo, first get the name of the pod:\\n$ kubectl get po -n foo\\nNAME                   READY     STATUS    RESTARTS   AGE\\ntest-145485760-ttq36   1/1       Running   0          1m\\nThen use the name in the kubectl exec command:\\n$ kubectl exec -it test-145485760-ttq36 -n foo sh\\n/ #\\nDo the same in the other terminal, but for the pod in the bar namespace.\\nLISTING SERVICES FROM YOUR PODS\\nTo verify that RBAC is enabled and preventing the pod from reading cluster state, use\\ncurl to list Services in the foo namespace:\\n/ # curl localhost:8001/api/v1/namespaces/foo/services\\nUser \"system:serviceaccount:foo:default\" cannot list services in the \\nnamespace \"foo\".\\nYou’re connecting to localhost:8001, which is where the kubectl proxy process is\\nlistening (as explained in chapter 8). The process received your request and sent it to\\nthe API server while authenticating as the default ServiceAccount in the foo name-\\nspace (as evident from the API server’s response). \\n The API server responded that the ServiceAccount isn’t allowed to list Services in\\nthe foo namespace, even though the pod is running in that same namespace. You’re\\nseeing RBAC in action. The default permissions for a ServiceAccount don’t allow it to\\nlist or modify any resources. Now, let’s learn how to allow the ServiceAccount to do\\nthat. First, you’ll need to create a Role resource.\\nListing 12.7\\nRunning test pods in different namespaces\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command-line tool for interacting with Kubernetes clusters',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'logical grouping of resources within a cluster',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'lightweight and portable execution environment for containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'deployment',\n",
       "    'description': 'declarative way to describe the desired state of a pod',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'serviceaccount',\n",
       "    'description': 'identity and authentication for pods within a namespace',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Role',\n",
       "    'description': 'resource that defines permissions and access control for users or service accounts',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'command-line tool for transferring data to/from a server using HTTP protocol',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'localhost:8001',\n",
       "    'description': 'API endpoint for kubectl proxy process',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'kubectl proxy',\n",
       "    'description': 'process that listens for incoming requests and forwards them to the API server',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n    {\"source_entity\": \"kubectl\", \"description\": \"create a namespace\", \"destination_entity\": \"namespace\"},\\n    {\"source_entity\": \"kubectl\", \"description\": \"run a deployment in a namespace\", \"destination_entity\": \"deployment\"},\\n    {\"source_entity\": \"kubectl\", \"description\": \"run a test pod in a namespace\", \"destination_entity\": \"pod\"},\\n    {\"source_entity\": \"kubectl exec\", \"description\": \"run a shell inside a pod\", \"destination_entity\": \"pod\"},\\n    {\"source_entity\": \"curl\", \"description\": \"list services in a namespace\", \"destination_entity\": \"namespace\"},\\n    {\"source_entity\": \"system:serviceaccount\", \"description\": \"attempt to list services in a namespace\", \"destination_entity\": \"namespace\"},\\n    {\"source_entity\": \"kubectl proxy\", \"description\": \"receive and forward requests from localhost\", \"destination_entity\": \"localhost:8001\"},\\n    {\"source_entity\": \"API server\", \"description\": \"authenticate and respond to request\", \"destination_entity\": \"system:serviceaccount\"},\\n    {\"source_entity\": \"Role\", \"description\": \"define permissions for a service account\", \"destination_entity\": \"serviceaccount\"},\\n    {\"source_entity\": \"kubectl\", \"description\": \"create a role resource\", \"destination_entity\": \"Role\"}\\n]\\n```\\n\\nNote that I\\'ve tried to extract the most relevant and direct relations from the text, but there might be other possible interpretations depending on how one reads the document. Also, please note that some of these relations might not have been explicitly mentioned in the text, but can be inferred based on the context.'},\n",
       " {'page': 390,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '358\\nCHAPTER 12\\nSecuring the Kubernetes API server\\n12.2.3 Using Roles and RoleBindings\\nA Role resource defines what actions can be taken on which resources (or, as\\nexplained earlier, which types of HTTP requests can be performed on which RESTful\\nresources). The following listing defines a Role, which allows users to get and list\\nServices in the foo namespace.\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: Role\\nmetadata:\\n  namespace: foo            \\n  name: service-reader\\nrules:\\n- apiGroups: [\"\"]            \\n  verbs: [\"get\", \"list\"]     \\n  resources: [\"services\"]   \\nWARNING\\nThe plural form must be used when specifying resources.\\nThis Role resource will be created in the foo namespace. In chapter 8, you learned that\\neach resource type belongs to an API group, which you specify in the apiVersion field\\n(along with the version) in the resource’s manifest. In a Role definition, you need to spec-\\nify the apiGroup for the resources listed in each rule included in the definition. If you’re\\nallowing access to resources belonging to different API groups, you use multiple rules.\\nNOTE\\nIn the example, you’re allowing access to all Service resources, but you\\ncould also limit access only to specific Service instances by specifying their\\nnames through an additional resourceNames field.\\nFigure 12.4 shows the Role, its verbs and resources, and the namespace it will be cre-\\nated in.\\nListing 12.8\\nA definition of a Role: service-reader.yaml\\nRoles are namespaced (if namespace is \\nomitted, the current namespace is used).\\nServices are resources in the core apiGroup, \\nwhich has no name – hence the \"\".\\nGetting individual Services (by name) \\nand listing all of them is allowed.\\nThis rule pertains to services \\n(plural name must be used!).\\nAllows getting\\nAllows listing\\nServices\\nRole:\\nservice-reader\\nServices\\nNamespace: foo\\nNamespace: bar\\nDoes not allow users to\\nget or list Services in\\nother namespaces\\nFigure 12.4\\nThe service-reader Role allows getting and listing Services in the foo namespace.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Role',\n",
       "    'description': 'defines what actions can be taken on which resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Role resource',\n",
       "    'description': 'defines what actions can be taken on which resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Roles are namespaced',\n",
       "    'description': 'roles are tied to a specific namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'resources in the core apiGroup, used for getting and listing individual or all Services',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'specifies the version of the API group',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'holds metadata about the Role resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'identifies the namespace where the Role resource will be created',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'specifies the name of the Role resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'rules',\n",
       "    'description': 'defines what actions can be taken on which resources for a specific Role resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiGroups',\n",
       "    'description': 'specifies the API group(s) where access is allowed for a specific Role resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'verbs',\n",
       "    'description': 'specifies the HTTP verbs (get, list, etc.) that can be used on resources for a specific Role resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'resources',\n",
       "    'description': ' specifies the resources (services, etc.) where access is allowed for a specific Role resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'RoleBinding',\n",
       "    'description': 'binds a Role to a user or group, granting them access to resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service reader',\n",
       "    'description': 'specific role that allows users to get and list Services in the foo namespace',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Role\",\\n    \"description\": \"defines what actions can be taken on which resources (or types of HTTP requests) can be performed on which RESTful resources\",\\n    \"destination_entity\": \"resources\"\\n  },\\n  {\\n    \"source_entity\": \"apiVersion\",\\n    \"description\": \"specifies the API group for the resources listed in each rule included in the definition\",\\n    \"destination_entity\": \"resources\"\\n  },\\n  {\\n    \"source_entity\": \"Role\",\\n    \"description\": \"will be created in the foo namespace\",\\n    \"destination_entity\": \"namespace\"\\n  },\\n  {\\n    \"source_entity\": \"Roles are namespaced\",\\n    \"description\": \"pertain to resources (or types of HTTP requests) can be performed on which RESTful resources\",\\n    \"destination_entity\": \"resources\"\\n  },\\n  {\\n    \"source_entity\": \"Service reader\",\\n    \"description\": \"allows getting and listing Services in the foo namespace\",\\n    \"destination_entity\": \"Services\"\\n  },\\n  {\\n    \"source_entity\": \"Role resource\",\\n    \"description\": \"defines what actions can be taken on which resources (or types of HTTP requests) can be performed on which RESTful resources\",\\n    \"destination_entity\": \"resources\"\\n  },\\n  {\\n    \"source_entity\": \"Rule\",\\n    \"description\": \"pertains to services (plural name must be used!)\",\\n    \"destination_entity\": \"services\"\\n  },\\n  {\\n    \"source_entity\": \"Role\",\\n    \"description\": \"allows getting and listing Services in the foo namespace\",\\n    \"destination_entity\": \"Services\"\\n  },\\n  {\\n    \"source_entity\": \"Services\",\\n    \"description\": \"are resources in the core apiGroup, which has no name – hence the \"\"\",\\n    \"destination_entity\": \"apiGroup\"\\n  }\\n]'},\n",
       " {'page': 391,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '359\\nSecuring the cluster with role-based access control\\nCREATING A ROLE\\nCreate the previous Role in the foo namespace now:\\n$ kubectl create -f service-reader.yaml -n foo\\nrole \"service-reader\" created\\nNOTE\\nThe -n option is shorthand for --namespace.\\nNote that if you’re using GKE, the previous command may fail because you don’t have\\ncluster-admin rights. To grant the rights, run the following command:\\n$ kubectl create clusterrolebinding cluster-admin-binding \\n➥ --clusterrole=cluster-admin --user=your.email@address.com\\nInstead of creating the service-reader Role from a YAML file, you could also create\\nit with the special kubectl create role command. Let’s use this method to create the\\nRole in the bar namespace:\\n$ kubectl create role service-reader --verb=get --verb=list \\n➥ --resource=services -n bar\\nrole \"service-reader\" created\\nThese two Roles will allow you to list Services in the foo and bar namespaces from\\nwithin your two pods (running in the foo and bar namespace, respectively). But cre-\\nating the two Roles isn’t enough (you can check by executing the curl command\\nagain). You need to bind each of the Roles to the ServiceAccounts in their respec-\\ntive namespaces. \\nBINDING A ROLE TO A SERVICEACCOUNT\\nA Role defines what actions can be performed, but it doesn’t specify who can perform\\nthem. To do that, you must bind the Role to a subject, which can be a user, a Service-\\nAccount, or a group (of users or ServiceAccounts).\\n Binding Roles to subjects is achieved by creating a RoleBinding resource. To bind\\nthe Role to the default ServiceAccount, run the following command:\\n$ kubectl create rolebinding test --role=service-reader \\n➥ --serviceaccount=foo:default -n foo\\nrolebinding \"test\" created\\nThe command should be self-explanatory. You’re creating a RoleBinding, which binds\\nthe service-reader Role to the default ServiceAccount in namespace foo. You’re cre-\\nating the RoleBinding in namespace foo. The RoleBinding and the referenced Service-\\nAccount and Role are shown in figure 12.5.\\nNOTE\\nTo bind a Role to a user instead of a ServiceAccount, use the --user\\nargument to specify the username. To bind it to a group, use --group.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Role',\n",
       "    'description': 'A role defines what actions can be performed in a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool for interacting with a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'RoleBinding',\n",
       "    'description': 'A RoleBinding binds a role to a subject, such as a user or ServiceAccount.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'A ServiceAccount is an entity that can be bound to a role in a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRoleBinding',\n",
       "    'description': 'A ClusterRoleBinding binds a cluster-wide role to a subject, such as a user or ServiceAccount.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'cluster-admin',\n",
       "    'description': 'A cluster-wide role that grants administrative privileges in a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'A namespace is a way to organize resources in a Kubernetes cluster.',\n",
       "    'category': 'hardware/network'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A pod is the basic execution unit in a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A Service is an abstraction that provides a network identity and load balancing for accessing applications in a Kubernetes cluster.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"cluster-admin\",\\n    \"description\": \"has rights to grant cluster-admin access\",\\n    \"destination_entity\": \"GKE\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"is used to create a RoleBinding with cluster-admin access\",\\n    \"destination_entity\": \"cluster-admin-binding\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"is used to create a Role from a YAML file\",\\n    \"destination_entity\": \"service-reader.yaml\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"is used to create a Role with specific actions and resources\",\\n    \"destination_entity\": \"Role service-reader\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"is used to bind a Role to a ServiceAccount\",\\n    \"destination_entity\": \"service-account foo:default\"\\n  },\\n  {\\n    \"source_entity\": \"RoleBinding\",\\n    \"description\": \"binds a Role to a ServiceAccount\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"is used to create a RoleBinding\",\\n    \"destination_entity\": \"test RoleBinding\"\\n  },\\n  {\\n    \"source_entity\": \"Role\",\\n    \"description\": \"defines what actions can be performed by a ServiceAccount\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"Role\",\\n    \"description\": \"is bound to a ServiceAccount through a RoleBinding\",\\n    \"destination_entity\": \"RoleBinding\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"is used to create a clusterRoleBinding with specific actions and resources\",\\n    \"destination_entity\": \"cluster-admin-binding\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterRoleBinding\",\\n    \"description\": \"binds a ClusterRole to a user or group\",\\n    \"destination_entity\": \"user your.email@address.com\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"is used to create a pod in the foo namespace\",\\n    \"destination_entity\": \"pod running in foo namespace\"\\n  },\\n  {\\n    \"source_entity\": \"Role\",\\n    \"description\": \"defines what actions can be performed by a Role\",\\n    \"destination_entity\": \"service-reader Role\"\\n  }\\n]'},\n",
       " {'page': 392,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '360\\nCHAPTER 12\\nSecuring the Kubernetes API server\\nThe following listing shows the YAML of the RoleBinding you created.\\n$ kubectl get rolebinding test -n foo -o yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: test\\n  namespace: foo\\n  ...\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: Role                         \\n  name: service-reader               \\nsubjects:\\n- kind: ServiceAccount       \\n  name: default              \\n  namespace: foo             \\nAs you can see, a RoleBinding always references a single Role (as evident from the\\nroleRef property), but can bind the Role to multiple subjects (for example, one or\\nmore ServiceAccounts and any number of users or groups). Because this RoleBinding\\nbinds the Role to the ServiceAccount the pod in namespace foo is running under, you\\ncan now list Services from within that pod.\\n/ # curl localhost:8001/api/v1/namespaces/foo/services\\n{\\n  \"kind\": \"ServiceList\",\\n  \"apiVersion\": \"v1\",\\n  \"metadata\": {\\n    \"selfLink\": \"/api/v1/namespaces/foo/services\",\\nListing 12.9\\nA RoleBinding referencing a Role\\nListing 12.10\\nGetting Services from the API server\\nNamespace: foo\\nRole:\\nservice-reader\\nGet, list\\nDefault ServiceAccount\\nis allowed to get and list\\nservices in this namespace\\nServices\\nRoleBinding:\\ntest\\nService-\\nAccount:\\ndefault\\nFigure 12.5\\nThe test RoleBinding binds the default ServiceAccount with the \\nservice-reader Role.\\nThis RoleBinding references \\nthe service-reader Role.\\nAnd binds it to the \\ndefault ServiceAccount \\nin the foo namespace.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes API server',\n",
       "    'description': 'The main server component that manages and controls access to Kubernetes resources.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'RoleBinding',\n",
       "    'description': 'A resource that binds a Role to one or more subjects, allowing them to access Kubernetes resources.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Role',\n",
       "    'description': 'A set of permissions and privileges that can be applied to a subject (e.g. ServiceAccount) to control their access to Kubernetes resources.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'An object in the Kubernetes API server that represents an identity for a pod, allowing it to authenticate and authorize with other components.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A lightweight and portable container runtime environment for running application code.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'An object in the Kubernetes API server that represents a network service, providing access to an application or resource.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for interacting with the Kubernetes API server and managing Kubernetes resources.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'A command-line tool for making HTTP requests to access web services or APIs.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'RoleRef',\n",
       "    'description': 'A property of a RoleBinding that references the Role being bound, specifying its API group and kind.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'subjects',\n",
       "    'description': 'A list of subjects (e.g. ServiceAccounts) that are being bound to a Role by a RoleBinding.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'selfLink',\n",
       "    'description': 'A property of an object in the Kubernetes API server that provides a URL for accessing it, relative to the root API server URL.',\n",
       "    'category': 'database'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"used to get the YAML of a RoleBinding\",\\n    \"destination_entity\": \"RoleBinding\"\\n  },\\n  {\\n    \"source_entity\": \"RoleRef\",\\n    \"description\": \"references a single Role\",\\n    \"destination_entity\": \"Role\"\\n  },\\n  {\\n    \"source_entity\": \"RoleBinding\",\\n    \"description\": \"binds the Role to multiple subjects\",\\n    \"destination_entity\": \"subjects\"\\n  },\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"is referenced by a ServiceAccount in a RoleBinding\",\\n    \"destination_entity\": \"RoleBinding\"\\n  },\\n  {\\n    \"source_entity\": \"default ServiceAccount\",\\n    \"description\": \"is allowed to get and list services in the namespace\",\\n    \"destination_entity\": \"Services\"\\n  },\\n  {\\n    \"source_entity\": \"curl\",\\n    \"description\": \"used to get Services from the API server\",\\n    \"destination_entity\": \"Kubernetes API server\"\\n  },\\n  {\\n    \"source_entity\": \"RoleBinding\",\\n    \"description\": \"binds a Role to multiple subjects\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"selfLink\",\\n    \"description\": \"references the selfLink of a ServiceList\",\\n    \"destination_entity\": \"ServiceList\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"runs under a ServiceAccount in a namespace\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"RoleBinding\",\\n    \"description\": \"references the service-reader Role\",\\n    \"destination_entity\": \"Role\"\\n  },\\n  {\\n    \"source_entity\": \"test RoleBinding\",\\n    \"description\": \"binds the default ServiceAccount with the service-reader Role\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  }\\n]\\n```\\n\\nNote that some of these relations may seem obvious or trivial, but they are all derived from the context and entities provided in the document page.'},\n",
       " {'page': 393,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '361\\nSecuring the cluster with role-based access control\\n    \"resourceVersion\": \"24906\"\\n  },\\n  \"items\": []     \\n}\\nINCLUDING SERVICEACCOUNTS FROM OTHER NAMESPACES IN A ROLEBINDING\\nThe pod in namespace bar can’t list the Services in its own namespace, and obviously\\nalso not those in the foo namespace. But you can edit your RoleBinding in the foo\\nnamespace and add the other pod’s ServiceAccount, even though it’s in a different\\nnamespace. Run the following command:\\n$ kubectl edit rolebinding test -n foo\\nThen add the following lines to the list of subjects, as shown in the following listing.\\nsubjects:\\n- kind: ServiceAccount\\n  name: default          \\n  namespace: bar         \\nNow you can also list Services in the foo namespace from inside the pod running in\\nthe bar namespace. Run the same command as in listing 12.10, but do it in the other\\nterminal, where you’re running the shell in the other pod.\\n Before moving on to ClusterRoles and ClusterRoleBindings, let’s summarize\\nwhat RBAC resources you currently have. You have a RoleBinding in namespace\\nfoo, which references the service-reader Role (also in the foo namespace) and\\nbinds the default ServiceAccounts in both the foo and the bar namespaces, as\\ndepicted in figure 12.6.\\nListing 12.11\\nReferencing a ServiceAccount from another namespace\\nThe list of items is empty, \\nbecause no Services exist.\\nYou’re referencing the default \\nServiceAccount in the bar namespace.\\nNamespace: foo\\nRole:\\nservice-reader\\nGet, list\\nBoth ServiceAccounts are\\nallowed to get and list Services\\nin the foo namespace\\nServices\\nNamespace: bar\\nRoleBinding:\\ntest\\nService-\\nAccount:\\ndefault\\nService-\\nAccount:\\ndefault\\nFigure 12.6\\nA RoleBinding binding ServiceAccounts from different namespaces to the same Role.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Namespace: foo\n",
       "   Service- RoleBinding: Role: Get, list\n",
       "   Account: Services\n",
       "   test service-reader\n",
       "   default, Col1, Namespace: bar\n",
       "   Service-\n",
       "   Account:\n",
       "   default]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command to edit rolebinding',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'rolebinding',\n",
       "    'description': 'RBAC resource that binds subjects to roles',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Role',\n",
       "    'description': 'RBAC resource that defines permissions for a subject',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'an identity for an application',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'a way to group resources together',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'service-reader',\n",
       "    'description': 'Role that allows get and list Services in a namespace',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'clusterrolebindings',\n",
       "    'description': 'RBAC resource that binds cluster roles to subjects',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'clusterroles',\n",
       "    'description': 'RBAC resource that defines permissions for a cluster subject',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'default ServiceAccount',\n",
       "    'description': 'ServiceAccount in the bar namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'test RoleBinding',\n",
       "    'description': 'RoleBinding that binds default ServiceAccounts to service-reader Role',\n",
       "    'category': 'database'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Role\", \"description\": \"defines permissions for a Role\", \"destination_entity\": \"clusterroles\"},\\n  {\"source_entity\": \"service-reader\", \"description\": \"grants get and list permissions to ServiceAccounts\", \"destination_entity\": \"ServiceAccount\"},\\n  {\"source_entity\": \"default ServiceAccount\", \"description\": \"is referenced by RoleBinding in foo namespace\", \"destination_entity\": \"RoleBinding\"},\\n  {\"source_entity\": \"test RoleBinding\", \"description\": \"binds default ServiceAccount to service-reader Role\", \"destination_entity\": \"Role\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to edit and list Services\", \"destination_entity\": \"ServiceAccount\"},\\n  {\"source_entity\": \"namespace\", \"description\": \"contains ServiceAccounts that can access Services\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"default ServiceAccount\", \"description\": \"can get and list Services in foo namespace\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"RoleBinding\", \"description\": \"binds ServiceAccounts to Roles\", \"destination_entity\": \"ServiceAccount\"},\\n  {\"source_entity\": \"clusterroles\", \"description\": \"defines permissions for cluster-wide access\", \"destination_entity\": \"Role\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to edit RoleBindings and add ServiceAccounts\", \"destination_entity\": \"RoleBinding\"}\\n]\\n\\nNote: I have extracted relations based on the context provided in the document page. If there are any inconsistencies or errors, please let me know!'},\n",
       " {'page': 394,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '362\\nCHAPTER 12\\nSecuring the Kubernetes API server\\n12.2.4 Using ClusterRoles and ClusterRoleBindings\\nRoles and RoleBindings are namespaced resources, meaning they reside in and apply\\nto resources in a single namespace, but, as we saw, RoleBindings can refer to Service-\\nAccounts from other namespaces, too. \\n In addition to these namespaced resources, two cluster-level RBAC resources also\\nexist: ClusterRole and ClusterRoleBinding. They’re not namespaced. Let’s see why\\nyou need them.\\n A regular Role only allows access to resources in the same namespace the Role is\\nin. If you want to allow someone access to resources across different namespaces, you\\nhave to create a Role and RoleBinding in every one of those namespaces. If you want\\nto extend this to all namespaces (this is something a cluster administrator would prob-\\nably need), you need to create the same Role and RoleBinding in each namespace.\\nWhen creating an additional namespace, you have to remember to create the two\\nresources there as well. \\n As you’ve learned throughout the book, certain resources aren’t namespaced at\\nall (this includes Nodes, PersistentVolumes, Namespaces, and so on). We’ve also\\nmentioned the API server exposes some URL paths that don’t represent resources\\n(/healthz for example). Regular Roles can’t grant access to those resources or non-\\nresource URLs, but ClusterRoles can.\\n A ClusterRole is a cluster-level resource for allowing access to non-namespaced\\nresources or non-resource URLs or used as a common role to be bound inside individ-\\nual namespaces, saving you from having to redefine the same role in each of them.\\nALLOWING ACCESS TO CLUSTER-LEVEL RESOURCES\\nAs mentioned, a ClusterRole can be used to allow access to cluster-level resources.\\nLet’s look at how to allow your pod to list PersistentVolumes in your cluster. First,\\nyou’ll create a ClusterRole called pv-reader:\\n$ kubectl create clusterrole pv-reader --verb=get,list \\n➥ --resource=persistentvolumes\\nclusterrole \"pv-reader\" created\\nThe ClusterRole’s YAML is shown in the following listing.\\n$ kubectl get clusterrole pv-reader -o yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:                                       \\n  name: pv-reader                               \\n  resourceVersion: \"39932\"                      \\n  selfLink: ...                                 \\n  uid: e9ac1099-30e2-11e7-955c-080027e6b159     \\nListing 12.12\\nA ClusterRole definition\\nClusterRoles aren’t \\nnamespaced, hence \\nno namespace field.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes API server',\n",
       "    'description': 'The main server component of a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRoles',\n",
       "    'description': 'Cluster-level RBAC resources that allow access to non-namespaced resources or non-resource URLs.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRoleBindings',\n",
       "    'description': 'Resources that bind a ClusterRole to a user, group, or service account at the cluster level.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Roles',\n",
       "    'description': 'Namespaced resources that allow access to resources within a single namespace.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'RoleBindings',\n",
       "    'description': 'Resources that bind a Role to a user, group, or service account within a specific namespace.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service-Accounts',\n",
       "    'description': 'Namespaced resources that provide an identity for pods and other cluster components.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolumes',\n",
       "    'description': 'Cluster-level resources that represent persistent storage capacity in a Kubernetes cluster.',\n",
       "    'category': 'hardware/database'},\n",
       "   {'entity': 'Namespaces',\n",
       "    'description': 'Cluster-level resources that group resources and provide isolation in a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Nodes',\n",
       "    'description': 'Cluster-level resources that represent machines or VMs in a Kubernetes cluster.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line interface for interacting with a Kubernetes cluster.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"kubectl\", \"description\": \"create a ClusterRole called pv-reader to allow access to PersistentVolumes\", \"destination_entity\": \"ClusterRoles\"},\\n\\n{\"source_entity\": \"Kubernetes API server\", \"description\": \"expose URL paths that don’t represent resources, such as /healthz\", \"destination_entity\": \"Nodes\"},\\n\\n{\"source_entity\": \"Nodes\", \"description\": \"are not namespaced at all\", \"destination_entity\": \"PersistentVolumes\"},\\n\\n{\"source_entity\": \"Roles\", \"description\": \"only allow access to resources in the same namespace\", \"destination_entity\": \"Namespaces\"},\\n\\n{\"source_entity\": \"RoleBindings\", \"description\": \"can refer to Service-Accounts from other namespaces\", \"destination_entity\": \"Service-Accounts\"},\\n\\n{\"source_entity\": \"ClusterRoles\", \"description\": \"allow access to non-namespaced resources or non-resource URLs\", \"destination_entity\": \"Kubernetes API server\"},\\n\\n{\"source_entity\": \"kubectl\", \"description\": \"create a ClusterRole called pv-reader to allow your pod to list PersistentVolumes\", \"destination_entity\": \"PersistentVolumes\"},\\n\\n{\"source_entity\": \"ClusterRoles\", \"description\": \"are cluster-level resources for allowing access to non-namespaced resources or non-resource URLs\", \"destination_entity\": \"PersistentVolumes\"},\\n\\n{\"source_entity\": \"kubectl\", \"description\": \"get a ClusterRole YAML output\", \"destination_entity\": \"ClusterRoleBindings\"},\\n\\n{\"source_entity\": \"Kubernetes API server\", \"description\": \"expose some URL paths that don’t represent resources\", \"destination_entity\": \"/healthz\"}]\\n\\nNote: I have excluded the relations that are not directly related to the entities provided as input.'},\n",
       " {'page': 395,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '363\\nSecuring the cluster with role-based access control\\nrules:\\n- apiGroups:                      \\n  - \"\"                            \\n  resources:                      \\n  - persistentvolumes             \\n  verbs:                          \\n  - get                           \\n  - list                          \\nBefore you bind this ClusterRole to your pod’s ServiceAccount, verify whether the pod\\ncan list PersistentVolumes. Run the following command in the first terminal, where\\nyou’re running the shell inside the pod in the foo namespace:\\n/ # curl localhost:8001/api/v1/persistentvolumes\\nUser \"system:serviceaccount:foo:default\" cannot list persistentvolumes at the \\ncluster scope.\\nNOTE\\nThe URL contains no namespace, because PersistentVolumes aren’t\\nnamespaced. \\nAs expected, the default ServiceAccount can’t list PersistentVolumes. You need to\\nbind the ClusterRole to your ServiceAccount to allow it to do that. ClusterRoles can\\nbe bound to subjects with regular RoleBindings, so you’ll create a RoleBinding now:\\n$ kubectl create rolebinding pv-test --clusterrole=pv-reader \\n➥ --serviceaccount=foo:default -n foo\\nrolebinding \"pv-test\" created\\nCan you list PersistentVolumes now?\\n/ # curl localhost:8001/api/v1/persistentvolumes\\nUser \"system:serviceaccount:foo:default\" cannot list persistentvolumes at the \\ncluster scope.\\nHmm, that’s strange. Let’s examine the RoleBinding’s YAML in the following listing.\\nCan you tell what (if anything) is wrong with it?\\n$ kubectl get rolebindings pv-test -o yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: RoleBinding\\nmetadata:\\n  name: pv-test\\n  namespace: foo\\n  ...\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole              \\n  name: pv-reader                \\nListing 12.13\\nA RoleBinding referencing a ClusterRole\\nIn this case, the \\nrules are exactly \\nlike those in a \\nregular Role.\\nThe binding references the \\npv-reader ClusterRole.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'apiGroups',\n",
       "    'description': '',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'resources', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'verbs', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'persistentvolumes',\n",
       "    'description': 'PersistentVolumes resource in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': '/ # curl localhost:8001/api/v1/persistentvolumes',\n",
       "    'description': 'Kubectl command to list PersistentVolumes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'system:serviceaccount:foo:default',\n",
       "    'description': 'ServiceAccount in foo namespace',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ClusterRole', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'pv-reader', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'rolebinding', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'kubectl create rolebinding pv-test --clusterrole=pv-reader ',\n",
       "    'description': 'Kubectl command to create RoleBinding',\n",
       "    'category': 'command'}],\n",
       "  'relationships': '[{\"source_entity\": \"kubectl create rolebinding pv-test --clusterrole=pv-reader \", \"description\": \"binds ClusterRole to ServiceAccount\", \"destination_entity\": \"pv-reader\"}, \\n{\"source_entity\": \"persistentvolumes\", \"description\": \"Persistent Volumes resource\", \"destination_entity\": \"resources\"}, \\n{\"source_entity\": \"apiGroups\", \"description\": \"API Group identifier\", \"destination_entity\": \"ClusterRole\"}, \\n{\"source_entity\": \"verbs\", \"description\": \"Allowed verb for API access\", \"destination_entity\": \"pv-reader\"}, \\n{\"source_entity\": \"/ # curl localhost:8001/api/v1/persistentvolumes\", \"description\": \"curl command to list PersistentVolumes\", \"destination_entity\": \"persistentvolumes\"}, \\n{\"source_entity\": \"system:serviceaccount:foo:default\", \"description\": \"ServiceAccount with permissions issue\", \"destination_entity\": \"RoleBinding\"}, \\n{\"source_entity\": \"rolebinding\", \"description\": \"Role Binding for ClusterRole\", \"destination_entity\": \"pv-reader\"}, \\n{\"source_entity\": \"kubectl get rolebindings pv-test -o yaml\", \"description\": \"examining RoleBinding YAML\", \"destination_entity\": \"RoleBinding\"}, \\n{\"source_entity\": \"ClusterRole\", \"description\": \"Reference to ClusterRole in RoleBinding\", \"destination_entity\": \"rolebinding\"}, \\n{\"source_entity\": \"pv-reader\", \"description\": \"ClusterRole with permissions issue\", \"destination_entity\": \"/ # curl localhost:8001/api/v1/persistentvolumes\"}]'},\n",
       " {'page': 396,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '364\\nCHAPTER 12\\nSecuring the Kubernetes API server\\nsubjects:\\n- kind: ServiceAccount          \\n  name: default                 \\n  namespace: foo                \\nThe YAML looks perfectly fine. You’re referencing the correct ClusterRole and the\\ncorrect ServiceAccount, as shown in figure 12.7, so what’s wrong?\\nAlthough you can create a RoleBinding and have it reference a ClusterRole when you\\nwant to enable access to namespaced resources, you can’t use the same approach for\\ncluster-level (non-namespaced) resources. To grant access to cluster-level resources,\\nyou must always use a ClusterRoleBinding.\\n Luckily, creating a ClusterRoleBinding isn’t that different from creating a Role-\\nBinding, but you’ll clean up and delete the RoleBinding first:\\n$ kubectl delete rolebinding pv-test\\nrolebinding \"pv-test\" deleted\\nNow create the ClusterRoleBinding:\\n$ kubectl create clusterrolebinding pv-test --clusterrole=pv-reader \\n➥ --serviceaccount=foo:default\\nclusterrolebinding \"pv-test\" created\\nAs you can see, you replaced rolebinding with clusterrolebinding in the command\\nand didn’t (need to) specify the namespace. Figure 12.8 shows what you have now.\\n Let’s see if you can list PersistentVolumes now:\\n/ # curl localhost:8001/api/v1/persistentvolumes\\n{\\n  \"kind\": \"PersistentVolumeList\",\\n  \"apiVersion\": \"v1\",\\n...\\nThe bound subject is the \\ndefault ServiceAccount in \\nthe foo namespace.\\nNamespace: foo\\nCluster-level resources\\nClusterRole:\\npv-reader\\nGet, list\\nPersistent\\nVolumes\\nRoleBinding:\\npv-test\\nDefault ServiceAccount\\nis unable to get and list\\nPersistentVolumes\\nService-\\nAccount:\\ndefault\\nFigure 12.7\\nA RoleBinding referencing a ClusterRole doesn’t grant access to cluster-\\nlevel resources.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Namespace: foo\n",
       "   Service-\n",
       "   RoleBinding:\n",
       "   Account:\n",
       "   pv-test\n",
       "   default, Col1, Cluster-level resources\n",
       "   ClusterRole: Get, list Persistent\n",
       "   pv-reader Volumes]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Kubernetes API server',\n",
       "    'description': 'The server that provides access to Kubernetes resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'An entity that represents an identity in a Kubernetes namespace',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'default ServiceAccount',\n",
       "    'description': \"A specific instance of a ServiceAccount with the name 'default'\",\n",
       "    'category': 'application'},\n",
       "   {'entity': 'namespace foo',\n",
       "    'description': 'A namespace in Kubernetes where the default ServiceAccount is located',\n",
       "    'category': 'network/application'},\n",
       "   {'entity': 'RoleBinding',\n",
       "    'description': 'An object that grants a ServiceAccount or User access to resources',\n",
       "    'category': 'application/software'},\n",
       "   {'entity': 'pv-test RoleBinding',\n",
       "    'description': \"A specific instance of a RoleBinding with the name 'pv-test'\",\n",
       "    'category': 'application/software'},\n",
       "   {'entity': 'ClusterRole',\n",
       "    'description': 'An object that grants access to cluster-level resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pv-reader ClusterRole',\n",
       "    'description': \"A specific instance of a ClusterRole with the name 'pv-reader'\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRoleBinding',\n",
       "    'description': 'An object that grants access to cluster-level resources for a ServiceAccount or User',\n",
       "    'category': 'application/software'},\n",
       "   {'entity': 'pv-test ClusterRoleBinding',\n",
       "    'description': \"A specific instance of a ClusterRoleBinding with the name 'pv-test'\",\n",
       "    'category': 'application/software'},\n",
       "   {'entity': 'PersistentVolumes',\n",
       "    'description': 'Cluster-level resources that provide persistent storage',\n",
       "    'category': 'software/database'},\n",
       "   {'entity': 'ServiceAccount: default',\n",
       "    'description': 'A reference to the default ServiceAccount in the foo namespace',\n",
       "    'category': 'application/software'}],\n",
       "  'relationships': '[{\"source_entity\": \"ClusterRoleBinding\", \"description\": \"binds to a ClusterRole to grant access to cluster-level resources\", \"destination_entity\": \"pv-test RoleBinding\"}, \\n{\"source_entity\": \"ClusterRoleBinding\", \"description\": \"replaces RoleBinding for cluster-level resource access\", \"destination_entity\": \"RoleBinding\"}, \\n{\"source_entity\": \"ClusterRoleBinding\", \"description\": \"references a ClusterRole to enable access to non-namespaced resources\", \"destination_entity\": \"ClusterRole\"}, \\n{\"source_entity\": \"pv-test ClusterRoleBinding\", \"description\": \"grants access to cluster-level resources using a ClusterRole\", \"destination_entity\": \"pv-reader ClusterRole\"}, \\n{\"source_entity\": \"pv-test RoleBinding\", \"description\": \"disallows access to persistent volumes because it\\'s not a cluster-level resource binding\", \"destination_entity\": \"PersistentVolumes\"}, \\n{\"source_entity\": \"ClusterRoleBinding\", \"description\": \"binds the default ServiceAccount in namespace foo to the pv-reader ClusterRole\", \"destination_entity\": \"pv-reader ClusterRole\"}, \\n{\"source_entity\": \"ServiceAccount: default\", \"description\": \"is bound to the pv-reader ClusterRole using a ClusterRoleBinding\", \"destination_entity\": \"pv-reader ClusterRole\"}, \\n{\"source_entity\": \"default ServiceAccount\", \"description\": \"lacks access to PersistentVolumes due to being unable to get and list them\", \"destination_entity\": \"PersistentVolumes\"}, \\n{\"source_entity\": \"Kubernetes API server\", \"description\": \"hosts the PersistentVolumeList API endpoint\", \"destination_entity\": \"PersistentVolumes\"}, \\n{\"source_entity\": \"Namespace: foo\", \"description\": \"holds the default ServiceAccount which lacks access to PersistentVolumes\", \"destination_entity\": \"default ServiceAccount\"}, \\n{\"source_entity\": \"pv-test ClusterRoleBinding\", \"description\": \"binds to the pv-reader ClusterRole for cluster-level resource access\", \"destination_entity\": \"pv-reader ClusterRole\"}]\\n\\nNote that I have assumed some minor wording adjustments to fit the format requested.'},\n",
       " {'page': 397,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '365\\nSecuring the cluster with role-based access control\\nYou can! It turns out you must use a ClusterRole and a ClusterRoleBinding when\\ngranting access to cluster-level resources.\\nTIP\\nRemember that a RoleBinding can’t grant access to cluster-level resources,\\neven if it references a ClusterRoleBinding.\\nALLOWING ACCESS TO NON-RESOURCE URLS\\nWe’ve mentioned that the API server also exposes non-resource URLs. Access to these\\nURLs must also be granted explicitly; otherwise the API server will reject the client’s\\nrequest. Usually, this is done for you automatically through the system:discovery\\nClusterRole and the identically named ClusterRoleBinding, which appear among\\nother predefined ClusterRoles and ClusterRoleBindings (we’ll explore them in sec-\\ntion 12.2.5). \\n Let’s inspect the system:discovery ClusterRole shown in the following listing.\\n$ kubectl get clusterrole system:discovery -o yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: system:discovery\\n  ...\\nrules:\\n- nonResourceURLs:      \\n  - /api                \\n  - /api/*              \\n  - /apis               \\n  - /apis/*             \\n  - /healthz            \\n  - /swaggerapi         \\n  - /swaggerapi/*       \\n  - /version            \\nListing 12.14\\nThe default system:discovery ClusterRole\\nNamespace: foo\\nCluster-level resources\\nClusterRole:\\npv-reader\\nGet, list\\nPersistent\\nVolumes\\nClusterRoleBinding:\\npv-test\\nDefault ServiceAccount in\\nfoo namespace is now allowed\\nto get and list PersistentVolumes\\nService-\\nAccount:\\ndefault\\nFigure 12.8\\nA ClusterRoleBinding and ClusterRole must be used to grant access to cluster-\\nlevel resources.\\nInstead of referring \\nto resources, this rule \\nrefers to non-resource \\nURLs.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Namespace: foo\n",
       "   Service-\n",
       "   Account:\n",
       "   default, Col1, Cluster-level resources\n",
       "   ClusterRoleBinding: ClusterRole: Get, list Persistent\n",
       "   pv-test pv-reader Volumes]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'ClusterRole',\n",
       "    'description': 'a Kubernetes role that defines a set of permissions for accessing cluster-level resources',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'ClusterRoleBinding',\n",
       "    'description': 'a Kubernetes binding that grants access to cluster-level resources based on a ClusterRole',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'RoleBinding',\n",
       "    'description': 'a Kubernetes binding that grants access to namespace-level resources',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'ClusterLevelResources',\n",
       "    'description': 'resources that are accessible at the cluster level, such as PersistentVolumes',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'NonResourceURLs',\n",
       "    'description': 'urls that do not point to a specific resource, but rather to an API endpoint or other service',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': 'the Kubernetes component responsible for handling incoming requests and providing access to cluster resources',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'system:discovery ClusterRole',\n",
       "    'description': \"a predefined ClusterRole that grants access to non-resource URLs, such as the API server's healthz endpoint\",\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'ClusterRoles',\n",
       "    'description': 'predefined roles that define a set of permissions for accessing cluster-level resources',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'ClusterRoleBindings',\n",
       "    'description': 'predefined bindings that grant access to cluster-level resources based on a ClusterRole',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'PersistentVolumes',\n",
       "    'description': 'cluster-level resources that store data persistently, such as storage volumes or block devices',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'an identity used by pods to authenticate with the Kubernetes API server',\n",
       "    'category': 'software,application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"You\", \"description\": \"must use a ClusterRole and a ClusterRoleBinding when granting access to cluster-level resources.\", \"destination_entity\": \"ClusterLevelResources\"},\\n  \\n  {\"source_entity\": \"A RoleBinding\", \"description\": \"can’t grant access to cluster-level resources, even if it references a ClusterRoleBinding.\", \"destination_entity\": \"ClusterLevelResources\"},\\n  \\n  {\"source_entity\": \"The API server\", \"description\": \"also exposes non-resource URLs.\", \"destination_entity\": \"NonResourceURLs\"},\\n  \\n  {\"source_entity\": \"You\", \"description\": \"must also grant access to these URLs explicitly, otherwise the API server will reject the client’s request.\", \"destination_entity\": \"API Server\"},\\n  \\n  {\"source_entity\": \"The system:discovery ClusterRole\", \"description\": \"and the identically named ClusterRoleBinding appear among other predefined ClusterRoles and ClusterRoleBindings.\", \"destination_entity\": \"ClusterRoleBindings\"},\\n  \\n  {\"source_entity\": \"You\", \"description\": \"can inspect the system:discovery ClusterRole using kubectl get clusterrole system:discovery -o yaml.\", \"destination_entity\": \"system:discovery ClusterRole\"},\\n  \\n  {\"source_entity\": \"The default system:discovery ClusterRole\", \"description\": \"has rules that refer to non-resource URLs.\", \"destination_entity\": \"NonResourceURLs\"},\\n  \\n  {\"source_entity\": \"ClusterRoleBinding pv-test\", \"description\": \"now allows the Default ServiceAccount in foo namespace to get and list PersistentVolumes.\", \"destination_entity\": \"PersistentVolumes\"},\\n  \\n  {\"source_entity\": \"The ClusterRole pv-reader\", \"description\": \"has rules that refer to Persistent Volumes.\", \"destination_entity\": \"PersistentVolumes\"},\\n  \\n  {\"source_entity\": \"ClusterRole\", \"description\": \"must be used with a ClusterRoleBinding to grant access to cluster-level resources.\", \"destination_entity\": \"ClusterLevelResources\"},\\n  \\n  {\"source_entity\": \"ClusterRoleBindings\", \"description\": \"and ClusterRoles must be used together to grant access to cluster-level resources.\", \"destination_entity\": \"ClusterLevelResources\"}\\n]\\n```\\n\\nNote: The relations extracted are based on the context of the document and the entities provided.'},\n",
       " {'page': 398,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '366\\nCHAPTER 12\\nSecuring the Kubernetes API server\\n  verbs:             \\n  - get              \\nYou can see this ClusterRole refers to URLs instead of resources (field nonResource-\\nURLs is used instead of the resources field). The verbs field only allows the GET HTTP\\nmethod to be used on these URLs.\\nNOTE\\nFor non-resource URLs, plain HTTP verbs such as post, put, and\\npatch are used instead of create or update. The verbs need to be specified in\\nlowercase.\\nAs with cluster-level resources, ClusterRoles for non-resource URLs must be bound\\nwith a ClusterRoleBinding. Binding them with a RoleBinding won’t have any effect.\\nThe system:discovery ClusterRole has a corresponding system:discovery Cluster-\\nRoleBinding, so let’s see what’s in it by examining the following listing.\\n$ kubectl get clusterrolebinding system:discovery -o yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRoleBinding\\nmetadata:\\n  name: system:discovery\\n  ...\\nroleRef:\\n  apiGroup: rbac.authorization.k8s.io\\n  kind: ClusterRole                           \\n  name: system:discovery                      \\nsubjects:\\n- apiGroup: rbac.authorization.k8s.io\\n  kind: Group                                 \\n  name: system:authenticated                  \\n- apiGroup: rbac.authorization.k8s.io\\n  kind: Group                                 \\n  name: system:unauthenticated                \\nThe YAML shows the ClusterRoleBinding refers to the system:discovery ClusterRole,\\nas expected. It’s bound to two groups, system:authenticated and system:unauthenti-\\ncated, which makes it bound to all users. This means absolutely everyone can access\\nthe URLs listed in the ClusterRole. \\nNOTE\\nGroups are in the domain of the authentication plugin. When a\\nrequest is received by the API server, it calls the authentication plugin to\\nobtain the list of groups the user belongs to. This information is then used\\nin authorization.\\nYou can confirm this by accessing the /api URL path from inside the pod (through\\nthe kubectl proxy, which means you’ll be authenticated as the pod’s ServiceAccount)\\nListing 12.15\\nThe default system:discovery ClusterRoleBinding\\nOnly the HTTP GET method \\nis allowed for these URLs.\\nThis ClusterRoleBinding references \\nthe system:discovery ClusterRole.\\nIt binds the ClusterRole \\nto all authenticated and \\nunauthenticated users \\n(that is, everyone).\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ClusterRole',\n",
       "    'description': 'Kubernetes role that refers to URLs instead of resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'verbs',\n",
       "    'description': 'HTTP methods allowed for non-resource URLs',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'post',\n",
       "    'description': 'plain HTTP verb used for creating resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'put',\n",
       "    'description': 'plain HTTP verb used for updating resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'patch',\n",
       "    'description': 'plain HTTP verb used for patching resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRoleBinding',\n",
       "    'description': 'Kubernetes object that binds a ClusterRole to users or groups',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'system:discovery',\n",
       "    'description': 'default ClusterRole and ClusterRoleBinding in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'command-line tool for interacting with Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiGroup',\n",
       "    'description': 'identifier for API groups in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'type of object in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'metadata associated with a Kubernetes object',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'roleRef',\n",
       "    'description': 'reference to the ClusterRole in a ClusterRoleBinding',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'subjects',\n",
       "    'description': 'list of users or groups bound to a ClusterRole',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Group',\n",
       "    'description': 'type of subject in Kubernetes that refers to a group of users',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'authenticated',\n",
       "    'description': 'group of authenticated users in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'unauthenticated',\n",
       "    'description': 'group of unauthenticated users in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'version of the API used to create a Kubernetes object',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'type of object being created in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': '/api',\n",
       "    'description': 'URL path that can be accessed by all users due to system:discovery ClusterRoleBinding',\n",
       "    'category': 'network'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ClusterRole\", \"description\": \"refers to URLs instead of resources\", \"destination_entity\": \"nonResource-URLs\"},\\n  {\"source_entity\": \"verbs\", \"description\": \"only allows the GET HTTP method\", \"destination_entity\": \"GET HTTP method\"},\\n  {\"source_entity\": \"verbs\", \"description\": \"uses plain HTTP verbs such as post, put, and patch\", \"destination_entity\": \"post\"},\\n  {\"source_entity\": \"verbs\", \"description\": \"uses plain HTTP verbs such as post, put, and patch\", \"destination_entity\": \"put\"},\\n  {\"source_entity\": \"ClusterRoleBinding\", \"description\": \"binds the ClusterRole to all authenticated users\", \"destination_entity\": \"authenticated\"},\\n  {\"source_entity\": \"ClusterRoleBinding\", \"description\": \"binds the ClusterRole to all unauthenticated users\", \"destination_entity\": \"unauthenticated\"},\\n  {\"source_entity\": \"ClusterRoleBinding\", \"description\": \"refers to the system:discovery ClusterRole\", \"destination_entity\": \"system:discovery\"},\\n  {\"source_entity\": \"groups\", \"description\": \"are in the domain of the authentication plugin\", \"destination_entity\": \"authentication plugin\"},\\n  {\"source_entity\": \"request\", \"description\": \"is received by the API server and calls the authentication plugin\", \"destination_entity\": \"authentication plugin\"},\\n  {\"source_entity\": \"ClusterRoleBinding\", \"description\": \"binds the ClusterRole to all users\", \"destination_entity\": \"all users\"},\\n  {\"source_entity\": \"/api URL path\", \"description\": \"can be accessed by everyone through the kubectl proxy\", \"destination_entity\": \"everyone\"},\\n  {\"source_entity\": \"kubectl proxy\", \"description\": \"authenticates as the pod\\'s ServiceAccount and allows access to /api URL path\", \"destination_entity\": \"/api URL path\"}\\n]\\n```'},\n",
       " {'page': 399,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '367\\nSecuring the cluster with role-based access control\\nand from your local machine, without specifying any authentication tokens (making\\nyou an unauthenticated user):\\n$ curl https://$(minikube ip):8443/api -k\\n{\\n  \"kind\": \"APIVersions\",\\n  \"versions\": [\\n  ...\\nYou’ve now used ClusterRoles and ClusterRoleBindings to grant access to cluster-level\\nresources and non-resource URLs. Now let’s look at how ClusterRoles can be used\\nwith namespaced RoleBindings to grant access to namespaced resources in the Role-\\nBinding’s namespace.\\nUSING CLUSTERROLES TO GRANT ACCESS TO RESOURCES IN SPECIFIC NAMESPACES\\nClusterRoles don’t always need to be bound with cluster-level ClusterRoleBindings.\\nThey can also be bound with regular, namespaced RoleBindings. You’ve already\\nstarted looking at predefined ClusterRoles, so let’s look at another one called view,\\nwhich is shown in the following listing.\\n$ kubectl get clusterrole view -o yaml\\napiVersion: rbac.authorization.k8s.io/v1\\nkind: ClusterRole\\nmetadata:\\n  name: view\\n  ...\\nrules:\\n- apiGroups:\\n  - \"\"\\n  resources:                           \\n  - configmaps                         \\n  - endpoints                          \\n  - persistentvolumeclaims             \\n  - pods                               \\n  - replicationcontrollers             \\n  - replicationcontrollers/scale       \\n  - serviceaccounts                    \\n  - services                           \\n  verbs:                \\n  - get                 \\n  - list                \\n  - watch               \\n...\\nThis ClusterRole has many rules. Only the first one is shown in the listing. The rule\\nallows getting, listing, and watching resources like ConfigMaps, Endpoints, Persistent-\\nVolumeClaims, and so on. These are namespaced resources, even though you’re\\nlooking at a ClusterRole (not a regular, namespaced Role). What exactly does this\\nClusterRole do?\\nListing 12.16\\nThe default view ClusterRole\\nThis rule applies to \\nthese resources (note: \\nthey’re all namespaced \\nresources).\\nAs the ClusterRole’s name \\nsuggests, it only allows \\nreading, not writing the \\nresources listed. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ClusterRoles',\n",
       "    'description': 'a concept in Kubernetes that grants access to cluster-level resources and non-resource URLs',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'RoleBindings',\n",
       "    'description': 'a mechanism in Kubernetes that binds ClusterRoles or NamespacedRoles with RoleSubjects, granting access to specific resources within a namespace',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'ClusterRoleBindings',\n",
       "    'description': 'a type of RoleBinding in Kubernetes that grants access to cluster-level resources and non-resource URLs',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'a command-line tool used to interact with Kubernetes clusters, used here to get the ClusterRole view',\n",
       "    'category': 'Kubernetes Command Line Tool'},\n",
       "   {'entity': 'view',\n",
       "    'description': 'a predefined ClusterRole in Kubernetes that grants read-only access to namespaced resources within a specific namespace',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'configmaps',\n",
       "    'description': 'a type of namespaced resource in Kubernetes, used here as an example of a resource granted access by the view ClusterRole',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'endpoints',\n",
       "    'description': 'a type of namespaced resource in Kubernetes, used here as an example of a resource granted access by the view ClusterRole',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'persistentvolumeclaims',\n",
       "    'description': 'a type of namespaced resource in Kubernetes, used here as an example of a resource granted access by the view ClusterRole',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'a type of namespaced resource in Kubernetes, used here as an example of a resource granted access by the view ClusterRole',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'replicationcontrollers',\n",
       "    'description': 'a type of namespaced resource in Kubernetes, used here as an example of a resource granted access by the view ClusterRole',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'services',\n",
       "    'description': 'a type of namespaced resource in Kubernetes, used here as an example of a resource granted access by the view ClusterRole',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'get',\n",
       "    'description': 'an HTTP verb that allows retrieving resources from a server',\n",
       "    'category': 'HTTP Verb'},\n",
       "   {'entity': 'list',\n",
       "    'description': 'an HTTP verb that allows listing multiple resources from a server',\n",
       "    'category': 'HTTP Verb'},\n",
       "   {'entity': 'watch',\n",
       "    'description': 'an HTTP verb that allows watching a resource for changes on the server-side',\n",
       "    'category': 'HTTP Verb'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ClusterRoles\",\\n    \"description\": \"can be used to grant access to cluster-level resources\",\\n    \"destination_entity\": \"cluster-level resources\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterRoleBindings\",\\n    \"description\": \"are used to bind ClusterRoles with cluster-level resources\",\\n    \"destination_entity\": \"cluster-level resources\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterRoles\",\\n    \"description\": \"can be bound with regular, namespaced RoleBindings\",\\n    \"destination_entity\": \"namespaced RoleBindings\"\\n  },\\n  {\\n    \"source_entity\": \"view ClusterRole\",\\n    \"description\": \"allows getting, listing, and watching resources like ConfigMaps, Endpoints, PersistentVolumeClaims, and so on\",\\n    \"destination_entity\": \"ConfigMaps\"\\n  },\\n  {\\n    \"source_entity\": \"view ClusterRole\",\\n    \"description\": \"allows getting, listing, and watching resources like ConfigMaps, Endpoints, PersistentVolumeClaims, and so on\",\\n    \"destination_entity\": \"Endpoints\"\\n  },\\n  {\\n    \"source_entity\": \"view ClusterRole\",\\n    \"description\": \"allows getting, listing, and watching resources like ConfigMaps, Endpoints, PersistentVolumeClaims, and so on\",\\n    \"destination_entity\": \"PersistentVolumeClaims\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"is used to get clusterrole view\",\\n    \"destination_entity\": \"clusterrole view\"\\n  },\\n  {\\n    \"source_entity\": \"RoleBindings\",\\n    \"description\": \"are used to grant access to namespaced resources in the Role- Binding\\'s namespace\",\\n    \"destination_entity\": \"namespaced resources\"\\n  },\\n  {\\n    \"source_entity\": \"view ClusterRole\",\\n    \"description\": \"only allows reading, not writing the resources listed\",\\n    \"destination_entity\": \"resources listed\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl get clusterrole view -o yaml\",\\n    \"description\": \"is used to display the rules of view ClusterRole\",\\n    \"destination_entity\": \"rules of view ClusterRole\"\\n  },\\n  {\\n    \"source_entity\": \"pods\",\\n    \"description\": \"are namespaced resources that can be accessed by view ClusterRole\",\\n    \"destination_entity\": \"view ClusterRole\"\\n  },\\n  {\\n    \"source_entity\": \"replicationcontrollers\",\\n    \"description\": \"are namespaced resources that can be accessed by view ClusterRole\",\\n    \"destination_entity\": \"view ClusterRole\"\\n  },\\n  {\\n    \"source_entity\": \"services\",\\n    \"description\": \"are namespaced resources that can be accessed by view ClusterRole\",\\n    \"destination_entity\": \"view ClusterRole\"\\n  },\\n  {\\n    \"source_entity\": \"kubernetes\",\\n    \"description\": \"has a default view ClusterRole\",\\n    \"destination_entity\": \"default view ClusterRole\"\\n  }\\n]\\n```'},\n",
       " {'page': 400,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '368\\nCHAPTER 12\\nSecuring the Kubernetes API server\\n It depends whether it’s bound with a ClusterRoleBinding or a RoleBinding (it can\\nbe bound with either). If you create a ClusterRoleBinding and reference the Cluster-\\nRole in it, the subjects listed in the binding can view the specified resources across all\\nnamespaces. If, on the other hand, you create a RoleBinding, the subjects listed in the\\nbinding can only view resources in the namespace of the RoleBinding. You’ll try both\\noptions now.\\n You’ll see how the two options affect your test pod’s ability to list pods. First, let’s\\nsee what happens before any bindings are in place:\\n/ # curl localhost:8001/api/v1/pods\\nUser \"system:serviceaccount:foo:default\" cannot list pods at the cluster \\nscope./ #\\n/ # curl localhost:8001/api/v1/namespaces/foo/pods\\nUser \"system:serviceaccount:foo:default\" cannot list pods in the namespace \\n\"foo\".\\nWith the first command, you’re trying to list pods across all namespaces. With the sec-\\nond, you’re trying to list pods in the foo namespace. The server doesn’t allow you to\\ndo either.\\n Now, let’s see what happens when you create a ClusterRoleBinding and bind it to\\nthe pod’s ServiceAccount:\\n$ kubectl create clusterrolebinding view-test --clusterrole=view \\n➥ --serviceaccount=foo:default\\nclusterrolebinding \"view-test\" created\\nCan the pod now list pods in the foo namespace?\\n/ # curl localhost:8001/api/v1/namespaces/foo/pods\\n{\\n  \"kind\": \"PodList\",\\n  \"apiVersion\": \"v1\",\\n  ...\\nIt can! Because you created a ClusterRoleBinding, it applies across all namespaces.\\nThe pod in namespace foo can list pods in the bar namespace as well:\\n/ # curl localhost:8001/api/v1/namespaces/bar/pods\\n{\\n  \"kind\": \"PodList\",\\n  \"apiVersion\": \"v1\",\\n  ...\\nOkay, the pod is allowed to list pods in a different namespace. It can also retrieve pods\\nacross all namespaces by hitting the /api/v1/pods URL path:\\n/ # curl localhost:8001/api/v1/pods\\n{\\n  \"kind\": \"PodList\",\\n  \"apiVersion\": \"v1\",\\n  ...\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ClusterRoleBinding',\n",
       "    'description': 'binding of a ClusterRole to a user or service account for access to Kubernetes resources across all namespaces',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'RoleBinding',\n",
       "    'description': 'binding of a Role to a user or service account for access to Kubernetes resources within a namespace',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'PodList',\n",
       "    'description': 'API endpoint for listing pods in a Kubernetes cluster',\n",
       "    'category': 'Kubernetes API'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'command-line tool for interacting with a Kubernetes cluster',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'command-line utility for transferring data over HTTP/HTTPS',\n",
       "    'category': 'HTTP'},\n",
       "   {'entity': 'Role',\n",
       "    'description': 'set of permissions in a Kubernetes cluster that can be applied to users or service accounts',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'identity associated with a running pod in a Kubernetes cluster',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': 'logical isolation of resources within a Kubernetes cluster',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': '/api/v1/pods',\n",
       "    'description': 'API endpoint for listing pods across all namespaces in a Kubernetes cluster',\n",
       "    'category': 'Kubernetes API'},\n",
       "   {'entity': '/api/v1/namespaces/foo/pods',\n",
       "    'description': 'API endpoint for listing pods within the foo namespace in a Kubernetes cluster',\n",
       "    'category': 'Kubernetes API'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ClusterRole\", \"description\": \"is bound to a RoleBinding or ClusterRoleBinding\", \"destination_entity\": \"Role\"},\\n  {\"source_entity\": \"ClusterRole\", \"description\": \"defines permissions across all namespaces\", \"destination_entity\": \"Namespace\"},\\n  {\"source_entity\": \"Role\", \"description\": \"defines permissions within a namespace\", \"destination_entity\": \"Namespace\"},\\n  {\"source_entity\": \"ServiceAccount\", \"description\": \"can be bound to a Role or ClusterRoleBinding\", \"destination_entity\": \"ClusterRoleBinding\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"creates a ClusterRoleBinding to grant access across all namespaces\", \"destination_entity\": \"ClusterRoleBinding\"},\\n  {\"source_entity\": \"curl\", \"description\": \"attempts to list pods across all namespaces\", \"destination_entity\": \"/api/v1/pods\"},\\n  {\"source_entity\": \"curl\", \"description\": \"attempts to list pods within a namespace\", \"destination_entity\": \"/api/v1/namespaces/foo/pods\"},\\n  {\"source_entity\": \"RoleBinding\", \"description\": \"grants access within a specific namespace\", \"destination_entity\": \"Namespace\"},\\n  {\"source_entity\": \"ClusterRoleBinding\", \"description\": \"grants access across all namespaces\", \"destination_entity\": \"Namespace\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"binds a ServiceAccount to a ClusterRoleBinding\", \"destination_entity\": \"ServiceAccount\"},\\n  {\"source_entity\": \"/api/v1/pods\", \"description\": \"lists pods across all namespaces\", \"destination_entity\": \"PodList\"},\\n  {\"source_entity\": \"/api/v1/namespaces/foo/pods\", \"description\": \"lists pods within a specific namespace\", \"destination_entity\": \"PodList\"},\\n  {\"source_entity\": \"RoleBinding\", \"description\": \"grants access to view pods in the foo namespace\", \"destination_entity\": \"/api/v1/namespaces/foo/pods\"},\\n  {\"source_entity\": \"ClusterRoleBinding\", \"description\": \"grants access to view pods across all namespaces\", \"destination_entity\": \"/api/v1/pods\"}\\n]\\n```'},\n",
       " {'page': 401,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '369\\nSecuring the cluster with role-based access control\\nAs expected, the pod can get a list of all the pods in the cluster. To summarize, com-\\nbining a ClusterRoleBinding with a ClusterRole referring to namespaced resources\\nallows the pod to access namespaced resources in any namespace, as shown in fig-\\nure 12.9.\\nNow, let’s see what happens if you replace the ClusterRoleBinding with a RoleBinding.\\nFirst, delete the ClusterRoleBinding:\\n$ kubectl delete clusterrolebinding view-test\\nclusterrolebinding \"view-test\" deleted\\nNext create a RoleBinding instead. Because a RoleBinding is namespaced, you need\\nto specify the namespace you want to create it in. Create it in the foo namespace:\\n$ kubectl create rolebinding view-test --clusterrole=view \\n➥ --serviceaccount=foo:default -n foo\\nrolebinding \"view-test\" created\\nYou now have a RoleBinding in the foo namespace, binding the default Service-\\nAccount in that same namespace with the view ClusterRole. What can your pod\\naccess now?\\n/ # curl localhost:8001/api/v1/namespaces/foo/pods\\n{\\n  \"kind\": \"PodList\",\\n  \"apiVersion\": \"v1\",\\n  ...\\nNamespace: foo\\nCluster-level\\nresources\\nNamespace: bar\\nPods\\nPods\\nDefault\\nServiceAccount\\nin foo namespace\\nis allowed to\\nview pods in\\nany namespace\\nClusterRole:\\nview\\nAllows getting,\\nlisting, watching\\nClusterRoleBinding:\\nview-test\\nPods,\\nServices,\\nEndpoints,\\nConﬁgMaps,\\n…\\nService-\\nAccount:\\ndefault\\nFigure 12.9\\nA ClusterRoleBinding and ClusterRole grants permission to resources across all \\nnamespaces.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ClusterRoleBinding',\n",
       "    'description': 'a binding that grants a user or service account access to cluster-level resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRole',\n",
       "    'description': 'a role that grants access to cluster-level resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'a container running in a Kubernetes cluster',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': 'an isolated network and storage space within a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'RoleBinding',\n",
       "    'description': 'a binding that grants a user or service account access to namespaced resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'an identity for pods running in a Kubernetes cluster',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the command-line tool for interacting with a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Role',\n",
       "    'description': 'a set of permissions that can be granted to users or service accounts',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Namespaced resources',\n",
       "    'description': 'resources such as pods, services, and endpoints within a specific namespace',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Cluster-level resources',\n",
       "    'description': 'resources that span across all namespaces in a Kubernetes cluster',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ClusterRoleBinding\", \"description\": \"grants permission to access resources across all namespaces\", \"destination_entity\": \"Namespaces\"},\\n  {\"source_entity\": \"ClusterRole\", \"description\": \"allows getting, listing, watching cluster-level resources\", \"destination_entity\": \"ClusterLevel Resources\"},\\n  {\"source_entity\": \"RoleBinding\", \"description\": \"binds a role with specific namespace\", \"destination_entity\": \"Namespace\"},\\n  {\"source_entity\": \"kubectl delete\", \"description\": \"deletes ClusterRoleBinding view-test\", \"destination_entity\": \"ClusterRoleBinding view-test\"},\\n  {\"source_entity\": \"kubectl create\", \"description\": \"creates RoleBinding view-test in foo namespace\", \"destination_entity\": \"RoleBinding view-test\"},\\n  {\"source_entity\": \"ServiceAccount\", \"description\": \"is allowed to view pods in any namespace\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Default ServiceAccount\", \"description\": \"has permission to access resources in foo namespace\", \"destination_entity\": \"Namespaced resources\"},\\n  {\"source_entity\": \"ClusterRoleBinding\", \"description\": \"grants permission to access cluster-level resources\", \"destination_entity\": \"ClusterLevel Resources\"},\\n  {\"source_entity\": \"Namespace\", \"description\": \"has specific role bindings and cluster roles assigned\", \"destination_entity\": \"RoleBinding\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"can get a list of all pods in the cluster\", \"destination_entity\": \"Cluster-level resources\"}\\n]\\n\\nNote: Some entities were not found to have any relations in this document, these are excluded from the output.'},\n",
       " {'page': 402,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '370\\nCHAPTER 12\\nSecuring the Kubernetes API server\\n/ # curl localhost:8001/api/v1/namespaces/bar/pods\\nUser \"system:serviceaccount:foo:default\" cannot list pods in the namespace \\n\"bar\".\\n/ # curl localhost:8001/api/v1/pods\\nUser \"system:serviceaccount:foo:default\" cannot list pods at the cluster \\nscope.\\nAs you can see, your pod can list pods in the foo namespace, but not in any other spe-\\ncific namespace or across all namespaces. This is visualized in figure 12.10.\\nSUMMARIZING ROLE, CLUSTERROLE, ROLEBINDING, AND CLUSTERROLEBINDING COMBINATIONS\\nWe’ve covered many different combinations and it may be hard for you to remember\\nwhen to use each one. Let’s see if we can make sense of all these combinations by cat-\\negorizing them per specific use case. Refer to table 12.2.\\nTable 12.2\\nWhen to use specific combinations of role and binding types\\nFor accessing\\nRole type to use\\nBinding type to use\\nCluster-level resources (Nodes, PersistentVolumes, ...)\\nClusterRole\\nClusterRoleBinding\\nNon-resource URLs (/api, /healthz, ...)\\nClusterRole\\nClusterRoleBinding\\nNamespaced resources in any namespace (and \\nacross all namespaces)\\nClusterRole\\nClusterRoleBinding\\nNamespaced resources in a specific namespace (reus-\\ning the same ClusterRole in multiple namespaces)\\nClusterRole\\nRoleBinding\\nNamespaced resources in a specific namespace \\n(Role must be defined in each namespace)\\nRole\\nRoleBinding\\nNamespace: foo\\nCluster-level resources\\nNamespace: bar\\nPods\\nPods\\nClusterRole:\\nview\\nAllows getting,\\nlisting, watching\\nRoleBinding:\\nview-test\\nPods,\\nServices,\\nEndpoints,\\nConﬁgMaps,\\n…\\nDefault ServiceAccount in\\nfoo namespace is only allowed\\nto view pods in namespace foo,\\ndespite using a ClusterRole\\nService-\\nAccount:\\ndefault\\nFigure 12.10\\nA RoleBinding referring to a ClusterRole only grants access to resources inside the \\nRoleBinding’s namespace.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Table 12.2 When to use specific combinations of role  \\\n",
       "   0                                      For accessing     \n",
       "   1  Cluster-level resources (Nodes, PersistentVolu...     \n",
       "   \n",
       "                                      and binding types  \\\n",
       "   0                                   Role type to use   \n",
       "   1  ClusterRole\\nClusterRole\\nClusterRole\\nCluster...   \n",
       "   \n",
       "                                                   Col2  \n",
       "   0                                Binding type to use  \n",
       "   1  ClusterRoleBinding\\nClusterRoleBinding\\nCluste...  ],\n",
       "  'entities': [{'entity': 'curl',\n",
       "    'description': 'command used to fetch data from API server',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'localhost:8001/api/v1/namespaces/bar/pods',\n",
       "    'description': 'API endpoint for listing pods in namespace bar',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'system:serviceaccount:foo:default',\n",
       "    'description': 'user account with limited permissions',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'logical partitioning of resources within a cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'clusterrolebinding',\n",
       "    'description': 'resource binding for accessing cluster-level resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'clusterrole',\n",
       "    'description': 'resource role for accessing cluster-level resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'rolebinding',\n",
       "    'description': 'resource binding for accessing namespaced resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'role',\n",
       "    'description': 'resource role for accessing namespaced resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'lightweight, short-lived containerized application instance',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'serviceaccount',\n",
       "    'description': 'user account with default permissions',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'clusterrolebinding',\n",
       "    'description': 'resource binding for accessing cluster-level resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'view',\n",
       "    'description': 'ClusterRole for viewing resources',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'view-test',\n",
       "    'description': 'RoleBinding for viewing resources in a specific namespace',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"ServiceAccount\", \"description\": \"is only allowed to view pods\", \"destination_entity\": \"Pod\"}, \\n {\"source_entity\": \"System:serviceaccount:foo:default\", \"description\": \"cannot list pods at cluster scope\", \"destination_entity\": \"Pod\"}, \\n {\"source_entity\": \"User system:serviceaccount:foo:default\", \"description\": \"cannot list pods in namespace bar\", \"destination_entity\": \"Namespace\"}, \\n {\"source_entity\": \"RoleBinding\", \"description\": \"refers to a ClusterRole only granting access to resources inside the RoleBinding\\'s namespace\", \"destination_entity\": \"ClusterRole\"}, \\n {\"source_entity\": \"ClusterRole\", \"description\": \"grants getting, listing, watching access to pods\", \"destination_entity\": \"Pod\"}, \\n {\"source_entity\": \"Default ServiceAccount\", \"description\": \"uses a ClusterRole despite only being allowed to view pods in namespace foo\", \"destination_entity\": \"ServiceAccount\"}, \\n {\"source_entity\": \"ClusterRoleBinding\", \"description\": \"grants access to cluster-level resources\", \"destination_entity\": \"Resource\"}, \\n {\"source_entity\": \"Role\", \"description\": \"must be defined in each namespace for granting access to namedpaced resources\", \"destination_entity\": \"Namespace\"}, \\n {\"source_entity\": \"Curl\", \"description\": \"cannot list pods using ClusterRole at cluster scope\", \"destination_entity\": \"Pod\"}, \\n {\"source_entity\": \"System:serviceaccount:foo:default\", \"description\": \"can list pods in namespace foo\", \"destination_entity\": \"Namespace\"}]'},\n",
       " {'page': 403,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '371\\nSecuring the cluster with role-based access control\\nHopefully, the relationships between the four RBAC resources are much clearer\\nnow. Don’t worry if you still feel like you don’t yet grasp everything. Things may\\nclear up as we explore the pre-configured ClusterRoles and ClusterRoleBindings in\\nthe next section.\\n12.2.5 Understanding default ClusterRoles and ClusterRoleBindings\\nKubernetes comes with a default set of ClusterRoles and ClusterRoleBindings, which\\nare updated every time the API server starts. This ensures all the default roles and\\nbindings are recreated if you mistakenly delete them or if a newer version of Kuberne-\\ntes uses a different configuration of cluster roles and bindings.\\n You can see the default cluster roles and bindings in the following listing.\\n$ kubectl get clusterrolebindings\\nNAME                                           AGE\\ncluster-admin                                  1d\\nsystem:basic-user                              1d\\nsystem:controller:attachdetach-controller      1d\\n...\\nsystem:controller:ttl-controller               1d\\nsystem:discovery                               1d\\nsystem:kube-controller-manager                 1d\\nsystem:kube-dns                                1d\\nsystem:kube-scheduler                          1d\\nsystem:node                                    1d\\nsystem:node-proxier                            1d\\n$ kubectl get clusterroles\\nNAME                                           AGE\\nadmin                                          1d\\ncluster-admin                                  1d\\nedit                                           1d\\nsystem:auth-delegator                          1d\\nsystem:basic-user                              1d\\nsystem:controller:attachdetach-controller      1d\\n...\\nsystem:controller:ttl-controller               1d\\nsystem:discovery                               1d\\nsystem:heapster                                1d\\nsystem:kube-aggregator                         1d\\nsystem:kube-controller-manager                 1d\\nsystem:kube-dns                                1d\\nsystem:kube-scheduler                          1d\\nsystem:node                                    1d\\nsystem:node-bootstrapper                       1d\\nsystem:node-problem-detector                   1d\\nsystem:node-proxier                            1d\\nsystem:persistent-volume-provisioner           1d\\nview                                           1d\\nListing 12.17\\nListing all ClusterRoleBindings and ClusterRoles\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'RBAC',\n",
       "    'description': 'Role-Based Access Control',\n",
       "    'category': 'security'},\n",
       "   {'entity': 'ClusterRoles',\n",
       "    'description': 'Kubernetes resource that defines a set of permissions',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRoleBindings',\n",
       "    'description': 'Association of a ClusterRole to a user, group or service account',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes clusters',\n",
       "    'category': 'tool'},\n",
       "   {'entity': 'get clusterrolebindings',\n",
       "    'description': 'Command to retrieve ClusterRoleBindings',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'get clusterroles',\n",
       "    'description': 'Command to retrieve ClusterRoles',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'cluster-admin',\n",
       "    'description': 'Default ClusterRole with administrator privileges',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'system:basic-user',\n",
       "    'description': 'Default ClusterRole for basic users',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'cluster-admin',\n",
       "    'description': 'Default ClusterRoleBinding with administrator privileges',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'system:kube-controller-manager',\n",
       "    'description': 'Controller manager component of the Kubernetes control plane',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'system:kube-scheduler',\n",
       "    'description': 'Scheduler component of the Kubernetes control plane',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'system:node',\n",
       "    'description': 'Node component of the Kubernetes cluster',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"provides default ClusterRoles and ClusterRoleBindings\", \"destination_entity\": \"ClusterRoles\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"ensures all default roles and bindings are recreated if deleted or updated\", \"destination_entity\": \"ClusterRoleBindings\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"lists all ClusterRoleBindings\", \"destination_entity\": \"ClusterRoleBindings\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"lists all ClusterRoles\", \"destination_entity\": \"ClusterRoles\"},\\n  {\"source_entity\": \"system:kube-scheduler\", \"description\": \"is a default ClusterRoleBinding\", \"destination_entity\": \"ClusterRoleBindings\"},\\n  {\"source_entity\": \"system:node\", \"description\": \"is a default ClusterRoleBinding\", \"destination_entity\": \"ClusterRoleBindings\"},\\n  {\"source_entity\": \"system:kube-controller-manager\", \"description\": \"is a default ClusterRoleBinding\", \"destination_entity\": \"ClusterRoleBindings\"},\\n  {\"source_entity\": \"cluster-admin\", \"description\": \"is a default ClusterRoleBinding\", \"destination_entity\": \"ClusterRoleBindings\"},\\n  {\"source_entity\": \"system:basic-user\", \"description\": \"is a default ClusterRoleBinding\", \"destination_entity\": \"ClusterRoleBindings\"},\\n  {\"source_entity\": \"get clusterroles\", \"description\": \"lists all ClusterRoles\", \"destination_entity\": \"ClusterRoles\"},\\n  {\"source_entity\": \"get clusterrolebindings\", \"description\": \"lists all ClusterRoleBindings\", \"destination_entity\": \"ClusterRoleBindings\"},\\n  {\"source_entity\": \"RBAC\", \"description\": \"uses ClusterRoles and ClusterRoleBindings for access control\", \"destination_entity\": \"ClusterRoles\"},\\n  {\"source_entity\": \"RBAC\", \"description\": \"uses ClusterRoles and ClusterRoleBindings for access control\", \"destination_entity\": \"ClusterRoleBindings\"},\\n  {\"source_entity\": \"ClusterRoles\", \"description\": \"are used to manage user access\", \"destination_entity\": \"users\"},\\n  {\"source_entity\": \"ClusterRoleBindings\", \"description\": \"bind ClusterRoles to users or groups\", \"destination_entity\": \"users\"}\\n]\\n```'},\n",
       " {'page': 404,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '372\\nCHAPTER 12\\nSecuring the Kubernetes API server\\nThe most important roles are the view, edit, admin, and cluster-admin ClusterRoles.\\nThey’re meant to be bound to ServiceAccounts used by user-defined pods.\\nALLOWING READ-ONLY ACCESS TO RESOURCES WITH THE VIEW CLUSTERROLE\\nYou already used the default view ClusterRole in the previous example. It allows read-\\ning most resources in a namespace, except for Roles, RoleBindings, and Secrets. You’re\\nprobably wondering, why not Secrets? Because one of those Secrets might include an\\nauthentication token with greater privileges than those defined in the view Cluster-\\nRole and could allow the user to masquerade as a different user to gain additional\\nprivileges (privilege escalation). \\nALLOWING MODIFYING RESOURCES WITH THE EDIT CLUSTERROLE\\nNext is the edit ClusterRole, which allows you to modify resources in a namespace,\\nbut also allows both reading and modifying Secrets. It doesn’t, however, allow viewing\\nor modifying Roles or RoleBindings—again, this is to prevent privilege escalation.\\nGRANTING FULL CONTROL OF A NAMESPACE WITH THE ADMIN CLUSTERROLE\\nComplete control of the resources in a namespace is granted in the admin Cluster-\\nRole. Subjects with this ClusterRole can read and modify any resource in the name-\\nspace, except ResourceQuotas (we’ll learn what those are in chapter 14) and the\\nNamespace resource itself. The main difference between the edit and the admin Cluster-\\nRoles is in the ability to view and modify Roles and RoleBindings in the namespace.\\nNOTE\\nTo prevent privilege escalation, the API server only allows users to cre-\\nate and update Roles if they already have all the permissions listed in that\\nRole (and for the same scope). \\nALLOWING COMPLETE CONTROL WITH THE CLUSTER-ADMIN CLUSTERROLE \\nComplete control of the Kubernetes cluster can be given by assigning the cluster-\\nadmin ClusterRole to a subject. As you’ve seen before, the admin ClusterRole doesn’t\\nallow users to modify the namespace’s ResourceQuota objects or the Namespace\\nresource itself. If you want to allow a user to do that, you need to create a RoleBinding\\nthat references the cluster-admin ClusterRole. This gives the user included in the\\nRoleBinding complete control over all aspects of the namespace in which the Role-\\nBinding is created.\\n If you’ve paid attention, you probably already know how to give users complete\\ncontrol of all the namespaces in the cluster. Yes, by referencing the cluster-admin\\nClusterRole in a ClusterRoleBinding instead of a RoleBinding.\\nUNDERSTANDING THE OTHER DEFAULT CLUSTERROLES\\nThe list of default ClusterRoles includes a large number of other ClusterRoles, which\\nstart with the system: prefix. These are meant to be used by the various Kubernetes\\ncomponents. Among them, you’ll find roles such as system:kube-scheduler, which\\nis obviously used by the Scheduler, system:node, which is used by the Kubelets, and\\nso on. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes API server',\n",
       "    'description': 'The most important component that manages and secures access to the Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceAccounts',\n",
       "    'description': 'Pre-configured accounts for applications running in pods, used to authenticate requests to the Kubernetes API',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Roles',\n",
       "    'description': 'Permissions granted to a user or service account to perform specific actions on resources within a namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRoles',\n",
       "    'description': 'Pre-configured roles that can be assigned to users or service accounts, providing a set of permissions across the entire cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'view ClusterRole',\n",
       "    'description': 'A pre-configured ClusterRole that allows read-only access to most resources in a namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'edit ClusterRole',\n",
       "    'description': 'A pre-configured ClusterRole that allows modifying resources in a namespace, as well as reading and modifying Secrets',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'admin ClusterRole',\n",
       "    'description': 'A pre-configured ClusterRole that grants complete control over resources in a namespace, except for ResourceQuotas and the Namespace resource itself',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'cluster-admin ClusterRole',\n",
       "    'description': 'A pre-configured ClusterRole that grants complete control over all aspects of the Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ResourceQuotas',\n",
       "    'description': 'Limits and constraints imposed on resources within a namespace, to prevent over-allocation or abuse',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Namespace resource',\n",
       "    'description': 'The Kubernetes resource that represents a namespace, used for configuration and management of the namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Roles and RoleBindings',\n",
       "    'description': 'Components used to manage permissions and access control within a namespace or cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRoleBinding',\n",
       "    'description': 'A binding that assigns a ClusterRole to a user or service account, granting them the corresponding permissions across the entire cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'RoleBinding',\n",
       "    'description': 'A binding that assigns a Role to a user or service account, granting them the corresponding permissions within a namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'The Kubernetes component responsible for scheduling and managing workload resources (pods) on the cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubelets',\n",
       "    'description': 'The agents that run on each node in the cluster, used to manage and interact with pods running on those nodes',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"view ClusterRole\", \"description\": \"allows read-ing most resources in a namespace\", \"destination_entity\": \"Resources\"}, \\n{\"source_entity\": \"view ClusterRole\", \"description\": \"does not allow viewing or modifying Roles or RoleBindings\", \"destination_entity\": \"Roles and RoleBindings\"}, \\n{\"source_entity\": \"edit ClusterRole\", \"description\": \"allows to modify resources in a namespace\", \"destination_entity\": \"Resources\"}, \\n{\"source_entity\": \"edit ClusterRole\", \"description\": \"allows both reading and modifying Secrets\", \"destination_entity\": \"Secrets\"}, \\n{\"source_entity\": \"admin ClusterRole\", \"description\": \"grants complete control of the resources in a namespace\", \"destination_entity\": \"Namespace resource\"}, \\n{\"source_entity\": \"admin ClusterRole\", \"description\": \"does not allow viewing or modifying Roles or RoleBindings\", \"destination_entity\": \"Roles and RoleBindings\"}, \\n{\"source_entity\": \"cluster-admin ClusterRole\", \"description\": \"gives complete control of the Kubernetes cluster\", \"destination_entity\": \"Kubernetes API server\"}, \\n{\"source_entity\": \"cluster-admin ClusterRole\", \"description\": \"does not allow users to modify ResourceQuotas or Namespace resource itself\", \"destination_entity\": \"ResourceQuotas and Namespace resource\"}, \\n{\"source_entity\": \"ServiceAccounts\", \"description\": \"used by user-defined pods\", \"destination_entity\": \"pods\"}, \\n{\"source_entity\": \"RoleBinding\", \"description\": \"references the cluster-admin ClusterRole to give users complete control\", \"destination_entity\": \"cluster-admin ClusterRole\"}, \\n{\"source_entity\": \"ClusterRoles\", \"description\": \"include roles such as system:kube-scheduler used by Scheduler\", \"destination_entity\": \"Scheduler and Kubelets\"}, \\n{\"source_entity\": \"Kubernetes API server\", \"description\": \"only allows users to create and update Roles if they already have all permissions listed in that Role\", \"destination_entity\": \"Roles\"}]'},\n",
       " {'page': 405,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '373\\nSummary\\n Although the Controller Manager runs as a single pod, each controller running\\ninside it can use a separate ClusterRole and ClusterRoleBinding (they’re prefixed\\nwith system: controller:). \\n Each of these system ClusterRoles has a matching ClusterRoleBinding, which binds\\nit to the user the system component authenticates as. The system:kube-scheduler\\nClusterRoleBinding, for example, assigns the identically named ClusterRole to the\\nsystem:kube-scheduler user, which is the username the scheduler Authenticates as. \\n12.2.6 Granting authorization permissions wisely\\nBy default, the default ServiceAccount in a namespace has no permissions other than\\nthose of an unauthenticated user (as you may remember from one of the previous\\nexamples, the system:discovery ClusterRole and associated binding allow anyone to\\nmake GET requests on a few non-resource URLs). Therefore, pods, by default, can’t\\neven view cluster state. It’s up to you to grant them appropriate permissions to do that. \\n Obviously, giving all your ServiceAccounts the cluster-admin ClusterRole is a\\nbad idea. As is always the case with security, it’s best to give everyone only the permis-\\nsions they need to do their job and not a single permission more (principle of least\\nprivilege).\\nCREATING SPECIFIC SERVICEACCOUNTS FOR EACH POD\\nIt’s a good idea to create a specific ServiceAccount for each pod (or a set of pod rep-\\nlicas) and then associate it with a tailor-made Role (or a ClusterRole) through a\\nRoleBinding (not a ClusterRoleBinding, because that would give the pod access to\\nresources in other namespaces, which is probably not what you want). \\n If one of your pods (the application running within it) only needs to read pods,\\nwhile the other also needs to modify them, then create two different ServiceAccounts\\nand make those pods use them by specifying the serviceAccountName property in the\\npod spec, as you learned in the first part of this chapter. Don’t add all the necessary\\npermissions required by both pods to the default ServiceAccount in the namespace. \\nEXPECTING YOUR APPS TO BE COMPROMISED\\nYour aim is to reduce the possibility of an intruder getting hold of your cluster. Today’s\\ncomplex apps contain many vulnerabilities. You should expect unwanted persons to\\neventually get their hands on the ServiceAccount’s authentication token, so you should\\nalways constrain the ServiceAccount to prevent them from doing any real damage.\\n12.3\\nSummary\\nThis chapter has given you a foundation on how to secure the Kubernetes API server.\\nYou learned the following:\\n\\uf0a1Clients of the API server include both human users and applications running\\nin pods.\\n\\uf0a1Applications in pods are associated with a ServiceAccount. \\n\\uf0a1Both users and ServiceAccounts are associated with groups.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Controller Manager',\n",
       "    'description': 'A single pod that runs each controller inside it',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRole',\n",
       "    'description': 'A role that can be bound to a user or group',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRoleBinding',\n",
       "    'description': 'A binding that assigns a ClusterRole to a user or group',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'An account that represents an application running in a pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Role',\n",
       "    'description': 'A role that can be bound to a ServiceAccount or group',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'RoleBinding',\n",
       "    'description': 'A binding that assigns a Role to a ServiceAccount or group',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': 'A scope for resources such as pods and services',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'An application running in a container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Cluster',\n",
       "    'description': 'A group of nodes that run Kubernetes',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Controller Manager\",\\n    \"description\": \"uses separate ClusterRole and ClusterRoleBinding\",\\n    \"destination_entity\": \"ClusterRole\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterManager\",\\n    \"description\": \"binds system: controller to the user it authenticates as\",\\n    \"destination_entity\": \"user\"\\n  },\\n  {\\n    \"source_entity\": \"Default ServiceAccount\",\\n    \"description\": \"has no permissions other than those of an unauthenticated user\",\\n    \"destination_entity\": \"unauthenticated user\"\\n  },\\n  {\\n    \"source_entity\": \"Default ServiceAccount\",\\n    \"description\": \"can\\'t view cluster state by default\",\\n    \"destination_entity\": \"cluster state\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterAdmin\",\\n    \"description\": \"is a bad idea to give all ServiceAccounts this role\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"Principle of Least Privilege\",\\n    \"description\": \"gives everyone only the permissions they need to do their job\",\\n    \"destination_entity\": \"permissions\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"can\\'t even view cluster state by default\",\\n    \"destination_entity\": \"cluster state\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"need to be granted appropriate permissions\",\\n    \"destination_entity\": \"permissions\"\\n  },\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"should be created for each pod or set of pod replicas\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"Role (or ClusterRole)\",\\n    \"description\": \"should be associated with a tailor-made RoleBinding\",\\n    \"destination_entity\": \"RoleBinding\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"need to use the serviceAccountName property in their spec\",\\n    \"destination_entity\": \"serviceAccountName\"\\n  },\\n  {\\n    \"source_entity\": \"Applications in pods\",\\n    \"description\": \"are associated with a ServiceAccount\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"Users and ServiceAccounts\",\\n    \"description\": \"are associated with groups\",\\n    \"destination_entity\": \"groups\"\\n  }\\n]\\n```\\n\\nNote that I extracted relations between entities even if they are not explicitly mentioned in the document, but can be inferred from the context. Also, some of these relations might be more implicit than others, but they all seem to make sense based on the provided text.'},\n",
       " {'page': 406,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '374\\nCHAPTER 12\\nSecuring the Kubernetes API server\\n\\uf0a1By default, pods run under the default ServiceAccount, which is created for\\neach namespace automatically.\\n\\uf0a1Additional ServiceAccounts can be created manually and associated with a pod.\\n\\uf0a1ServiceAccounts can be configured to allow mounting only a constrained list of\\nSecrets in a given pod.\\n\\uf0a1A ServiceAccount can also be used to attach image pull Secrets to pods, so you\\ndon’t need to specify the Secrets in every pod.\\n\\uf0a1Roles and ClusterRoles define what actions can be performed on which resources.\\n\\uf0a1RoleBindings and ClusterRoleBindings bind Roles and ClusterRoles to users,\\ngroups, and ServiceAccounts.\\n\\uf0a1Each cluster comes with default ClusterRoles and ClusterRoleBindings.\\nIn the next chapter, you’ll learn how to protect the cluster nodes from pods and how\\nto isolate pods from each other by securing the network.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes API server',\n",
       "    'description': 'The component responsible for managing and securing Kubernetes resources.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'Lightweight and portable containers created to run applications in a cluster.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'A managed identity for pods, used to authenticate API requests.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'Sensitive information stored as key-value pairs within a pod or namespace.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Roles',\n",
       "    'description': 'Definitions of permissions that can be granted to users, groups, and ServiceAccounts.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'RoleBindings',\n",
       "    'description': 'Associations between Roles, users, groups, and ServiceAccounts, granting them permissions.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ClusterRoles',\n",
       "    'description': 'Definitions of permissions that can be granted to users, groups, and ServiceAccounts at the cluster level.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ClusterRoleBindings',\n",
       "    'description': 'Associations between ClusterRoles, users, groups, and ServiceAccounts, granting them permissions at the cluster level.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Users\",\\n    \"description\": \"are bound to Roles and ClusterRoles through RoleBindings and ClusterRoleBindings\",\\n    \"destination_entity\": \"Roles\"\\n  },\\n  {\\n    \"source_entity\": \"Users\",\\n    \"description\": \"are bound to Roles and ClusterRoles through RoleBindings and ClusterRoleBindings\",\\n    \"destination_entity\": \"ClusterRoles\"\\n  },\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"can be created manually and associated with a pod\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"can be used to attach image pull Secrets to pods\",\\n    \"destination_entity\": \"Secrets\"\\n  },\\n  {\\n    \"source_entity\": \"Roles\",\\n    \"description\": \"define what actions can be performed on which resources\",\\n    \"destination_entity\": \"resources\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterRoles\",\\n    \"description\": \"define what actions can be performed on which resources\",\\n    \"destination_entity\": \"resources\"\\n  },\\n  {\\n    \"source_entity\": \"RoleBindings\",\\n    \"description\": \"bind Roles and ClusterRoles to users, groups, and ServiceAccounts\",\\n    \"destination_entity\": \"Users\"\\n  },\\n  {\\n    \"source_entity\": \"RoleBindings\",\\n    \"description\": \"bind Roles and ClusterRoles to users, groups, and ServiceAccounts\",\\n    \"destination_entity\": \"groups\"\\n  },\\n  {\\n    \"source_entity\": \"RoleBindings\",\\n    \"description\": \"bind Roles and ClusterRoles to users, groups, and ServiceAccounts\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterRoleBindings\",\\n    \"description\": \"bind ClusterRoles to users, groups, and ServiceAccounts\",\\n    \"destination_entity\": \"Users\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterRoleBindings\",\\n    \"description\": \"bind ClusterRoles to users, groups, and ServiceAccounts\",\\n    \"destination_entity\": \"groups\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterRoleBindings\",\\n    \"description\": \"bind ClusterRoles to users, groups, and ServiceAccount\",\\n    \"destination_entity\": \"ServiceAccount\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes API server\",\\n    \"description\": \"runs pods under the default ServiceAccount\",\\n    \"destination_entity\": \"pods\"\\n  }\\n]'},\n",
       " {'page': 407,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '375\\nSecuring cluster nodes\\nand the network\\nIn the previous chapter, we talked about securing the API server. If an attacker\\ngets access to the API server, they can run whatever they like by packaging their\\ncode into a container image and running it in a pod. But can they do any real\\ndamage? Aren’t containers isolated from other containers and from the node\\nthey’re running on? \\n Not necessarily. In this chapter, you’ll learn how to allow pods to access the\\nresources of the node they’re running on. You’ll also learn how to configure the\\ncluster so users aren’t able to do whatever they want with their pods. Then, in\\nThis chapter covers\\n\\uf0a1Using the node’s default Linux namespaces \\nin pods\\n\\uf0a1Running containers as different users\\n\\uf0a1Running privileged containers\\n\\uf0a1Adding or dropping a container’s kernel \\ncapabilities\\n\\uf0a1Defining security policies to limit what pods can do\\n\\uf0a1Securing the pod network\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A collection of one or more containers that share network, storage, and other resources. ',\n",
       "    'category': 'Container'},\n",
       "   {'entity': 'Cluster Node',\n",
       "    'description': 'A machine in a Kubernetes cluster that runs pods.',\n",
       "    'category': 'Hardware'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': 'The main entry point for users to interact with a Kubernetes cluster.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Container Image',\n",
       "    'description': 'A package of software that includes everything needed to run an application, libraries, and dependencies. ',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Linux Namespaces',\n",
       "    'description': 'A feature of the Linux kernel that allows multiple processes to share resources, such as network interfaces or memory spaces.',\n",
       "    'category': 'Operating System'},\n",
       "   {'entity': 'Kernel Capabilities',\n",
       "    'description': 'Capabilities defined in the Linux kernel that allow containers to run with elevated privileges.',\n",
       "    'category': 'Operating System'},\n",
       "   {'entity': 'Security Policies',\n",
       "    'description': 'Rules and configurations that govern what pods can do within a cluster. ',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod Network',\n",
       "    'description': 'A network interface shared by all pods running on a node.',\n",
       "    'category': 'Network'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"API Server\", \"description\": \"can be compromised by attackers who get access to it\", \"destination_entity\": \"Attacker\"},\\n  {\"source_entity\": \"Attacker\", \"description\": \"can run whatever they like by packaging their code into a container image and running it in a pod\", \"destination_entity\": \"Container Image\"},\\n  {\"source_entity\": \"Container\", \"description\": \"is isolated from other containers and the node they\\'re running on\", \"destination_entity\": \"Kernel Capabilities\"},\\n  {\"source_entity\": \"Node\", \"description\": \"containers can access its resources\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"can be configured to run with different Linux namespaces\", \"destination_entity\": \"Linux Namespaces\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"can run as a different user\", \"destination_entity\": \"User\"},\\n  {\"source_entity\": \"Privileged Container\", \"description\": \"has access to the node\\'s resources\", \"destination_entity\": \"Node\"},\\n  {\"source_entity\": \"Security Policy\", \"description\": \"limits what pods can do\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Security Policy\", \"description\": \"defines what containers can and cannot do\", \"destination_entity\": \"Container Image\"},\\n  {\"source_entity\": \"Cluster Node\", \"description\": \"resources can be accessed by pods running on it\", \"destination_entity\": \"Pod Network\"},\\n  {\"source_entity\": \"API Server\", \"description\": \"can be secured to prevent attackers from accessing the cluster\", \"destination_entity\": \"Cluster\"}\\n]\\n```\\n\\nNote that I have assumed some entities as a single entity (e.g., \"Container\" instead of \"Container Image\") since they were not explicitly mentioned in the document. If you want me to use the exact entities provided, please let me know and I will adjust the output accordingly.'},\n",
       " {'page': 408,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '376\\nCHAPTER 13\\nSecuring cluster nodes and the network\\nthe last part of the chapter, you’ll also learn how to secure the network the pods use\\nto communicate.\\n13.1\\nUsing the host node’s namespaces in a pod\\nContainers in a pod usually run under separate Linux namespaces, which isolate\\ntheir processes from processes running in other containers or in the node’s default\\nnamespaces. \\n For example, we learned that each pod gets its own IP and port space, because it\\nuses its own network namespace. Likewise, each pod has its own process tree, because\\nit has its own PID namespace, and it also uses its own IPC namespace, allowing only\\nprocesses in the same pod to communicate with each other through the Inter-Process\\nCommunication mechanism (IPC).\\n13.1.1 Using the node’s network namespace in a pod\\nCertain pods (usually system pods) need to operate in the host’s default namespaces,\\nallowing them to see and manipulate node-level resources and devices. For example, a\\npod may need to use the node’s network adapters instead of its own virtual network\\nadapters. This can be achieved by setting the hostNetwork property in the pod spec\\nto true.\\n In that case, the pod gets to use the node’s network interfaces instead of having its\\nown set, as shown in figure 13.1. This means the pod doesn’t get its own IP address and\\nif it runs a process that binds to a port, the process will be bound to the node’s port.\\nYou can try running such a pod. The next listing shows an example pod manifest.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-with-host-network\\nListing 13.1\\nA pod using the node’s network namespace: pod-with-host-network.yaml\\nNode\\nPod A\\nPod’s own network\\nnamespace\\neth0\\nlo\\neth0\\ndocker0\\nlo\\neth1\\nNode’s default network\\nnamespace\\nPod B\\nhostNetwork: true\\nFigure 13.1\\nA pod \\nwith hostNetwork: \\ntrue uses the node’s \\nnetwork interfaces \\ninstead of its own.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [eth0, lo]\n",
       "   Index: [],\n",
       "      eth0  Col1 Col2  Col3     Col4 Col5\n",
       "   0  None  eth0       eth1  docker0   lo],\n",
       "  'entities': [{'entity': 'Linux namespaces',\n",
       "    'description': 'a way to isolate processes from each other',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'namespaces',\n",
       "    'description': 'a feature of Linux that allows for process isolation',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'lightweight and portable container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'lightweight and portable operating system instance',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'IP address',\n",
       "    'description': 'a unique number assigned to each pod for network communication',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'port space',\n",
       "    'description': 'a range of ports that a pod can use for network communication',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'PID namespace',\n",
       "    'description': 'a way to isolate process IDs between containers',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'IPC namespace',\n",
       "    'description': 'a way to allow processes within the same pod to communicate with each other',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Inter-Process Communication (IPC)',\n",
       "    'description': 'a mechanism for allowing processes within a pod to communicate with each other',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'hostNetwork property',\n",
       "    'description': \"a setting in the pod spec that allows a pod to use the node's network interfaces instead of its own\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod spec',\n",
       "    'description': 'the configuration file for a pod that defines its properties and behavior',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'a field in the pod spec that specifies the API version being used',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'a field in the pod spec that specifies the type of resource being defined',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'a field in the pod spec that contains metadata about the pod, such as its name and labels',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'the human-readable name given to a resource',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Pod\", \"description\": \"uses its own network namespace to get its own IP and port space\", \"destination_entity\": \"IP address\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"has its own process tree because it has its own PID namespace\", \"destination_entity\": \"PID namespace\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"uses its own IPC namespace to communicate with each other through Inter-Process Communication mechanism (IPC)\", \"destination_entity\": \"Inter-Process Communication (IPC)\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"gets to use the node\\'s network interfaces instead of having its own set\", \"destination_entity\": \"Node\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"doesn\\'t get its own IP address when using hostNetwork property\", \"destination_entity\": \"IP address\"},\\n  {\"source_entity\": \"Process\", \"description\": \"binds to the node\\'s port when running in a pod with hostNetwork property true\", \"destination_entity\": \"port space\"},\\n  {\"source_entity\": \"Pod spec\", \"description\": \"can be set to true to use the node\\'s network namespace instead of its own\", \"destination_entity\": \"Node\"},\\n  {\"source_entity\": \"apiVersion\", \"description\": \"defines the version of the API being used\", \"destination_entity\": \"pod spec\"},\\n  {\"source_entity\": \"kind\", \"description\": \"defines the type of Kubernetes object being created\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"metadata\", \"description\": \"provides metadata about the pod, such as its name and namespace\", \"destination_entity\": \"namespaces\"},\\n  {\"source_entity\": \"hostNetwork property\", \"description\": \"can be set to true to use the node\\'s network namespace instead of its own\", \"destination_entity\": \"Pod\"}\\n]\\n\\nNote: I\\'ve only extracted relations between entities from the provided text and entities list. Let me know if you\\'d like me to clarify or expand on any of these relations!'},\n",
       " {'page': 409,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '377\\nUsing the host node’s namespaces in a pod\\nspec:\\n  hostNetwork: true              \\n  containers:\\n  - name: main\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]\\nAfter you run the pod, you can use the following command to see that it’s indeed using\\nthe host’s network namespace (it sees all the host’s network adapters, for example).\\n$ kubectl exec pod-with-host-network ifconfig\\ndocker0   Link encap:Ethernet  HWaddr 02:42:14:08:23:47\\n          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0\\n          ...\\neth0      Link encap:Ethernet  HWaddr 08:00:27:F8:FA:4E\\n          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\\n          ...\\nlo        Link encap:Local Loopback\\n          inet addr:127.0.0.1  Mask:255.0.0.0\\n          ...\\nveth1178d4f Link encap:Ethernet  HWaddr 1E:03:8D:D6:E1:2C\\n          inet6 addr: fe80::1c03:8dff:fed6:e12c/64 Scope:Link\\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\\n...\\nWhen the Kubernetes Control Plane components are deployed as pods (such as when\\nyou deploy your cluster with kubeadm, as explained in appendix B), you’ll find that\\nthose pods use the hostNetwork option, effectively making them behave as if they\\nweren’t running inside a pod.\\n13.1.2 Binding to a host port without using the host’s network \\nnamespace\\nA related feature allows pods to bind to a port in the node’s default namespace, but\\nstill have their own network namespace. This is done by using the hostPort property\\nin one of the container’s ports defined in the spec.containers.ports field.\\n Don’t confuse pods using hostPort with pods exposed through a NodePort service.\\nThey’re two different things, as explained in figure 13.2.\\n The first thing you’ll notice in the figure is that when a pod is using a hostPort, a\\nconnection to the node’s port is forwarded directly to the pod running on that node,\\nwhereas with a NodePort service, a connection to the node’s port is forwarded to a\\nrandomly selected pod (possibly on another node). The other difference is that with\\npods using a hostPort, the node’s port is only bound on nodes that run such pods,\\nwhereas NodePort services bind the port on all nodes, even on those that don’t run\\nsuch a pod (as on node 3 in the figure).\\nListing 13.2\\nNetwork interfaces in a pod using the host’s network namespace\\nUsing the host node’s \\nnetwork namespace\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'hostNetwork',\n",
       "    'description': \"an option to use the host's network namespace\",\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'a command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'application,software'},\n",
       "   {'entity': 'pod-with-host-network',\n",
       "    'description': \"a pod that uses the host's network namespace\",\n",
       "    'category': 'container,pod,application'},\n",
       "   {'entity': 'ifconfig',\n",
       "    'description': 'a command to display and configure network interfaces',\n",
       "    'category': 'command,software'},\n",
       "   {'entity': 'docker0',\n",
       "    'description': 'a virtual network interface for Docker containers',\n",
       "    'category': 'hardware,networkinterface,application'},\n",
       "   {'entity': 'eth0',\n",
       "    'description': 'the primary Ethernet interface of a node',\n",
       "    'category': 'hardware,networkinterface,application'},\n",
       "   {'entity': 'lo',\n",
       "    'description': 'the loopback interface for a node',\n",
       "    'category': 'hardware,networkinterface,application'},\n",
       "   {'entity': 'veth1178d4f',\n",
       "    'description': 'a virtual Ethernet interface',\n",
       "    'category': 'hardware,networkinterface,application'},\n",
       "   {'entity': 'hostPort',\n",
       "    'description': \"a property to bind a port in the node's default namespace\",\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'NodePort service',\n",
       "    'description': 'a type of service that exposes a port on all nodes',\n",
       "    'category': 'application,service'},\n",
       "   {'entity': 'kubeadm',\n",
       "    'description': 'a tool for deploying and managing Kubernetes clusters',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'spec.containers.ports',\n",
       "    'description': 'a field in the pod specification that defines ports for containers',\n",
       "    'category': 'application,software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"hostNetwork\",\\n    \"description\": \"using the host\\'s network namespace\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"pod-with-host-network\",\\n    \"description\": \"seeing all the host\\'s network adapters\",\\n    \"destination_entity\": \"host\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"executing a command to see the pod\\'s network interfaces\",\\n    \"destination_entity\": \"pod-with-host-network\"\\n  },\\n  {\\n    \"source_entity\": \"kubeadm\",\\n    \"description\": \"deploying the Kubernetes Control Plane components as pods\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"hostPort\",\\n    \"description\": \"binding to a port in the node\\'s default namespace without using the host\\'s network namespace\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"hostPort\",\\n    \"description\": \"forwarding connections directly to the pod running on that node\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"NodePort service\",\\n    \"description\": \"binding a port on all nodes, even those that don\\'t run such a pod\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"spec.containers.ports\",\\n    \"description\": \"defining ports for containers in the spec field\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"ifconfig\",\\n    \"description\": \"displaying network interfaces for the pod-with-host-network pod\",\\n    \"destination_entity\": \"pod-with-host-network\"\\n  },\\n  {\\n    \"source_entity\": \"docker0\",\\n    \"description\": \"having an IP address of 172.17.0.1\",\\n    \"destination_entity\": \"docker0 interface\"\\n  },\\n  {\\n    \"source_entity\": \"eth0\",\\n    \"description\": \"having an IP address of 10.0.2.15\",\\n    \"destination_entity\": \"eth0 interface\"\\n  },\\n  {\\n    \"source_entity\": \"veth1178d4f\",\\n    \"description\": \"being a network interface with a specific MAC address\",\\n    \"destination_entity\": \"veth1178d4f interface\"\\n  }\\n]\\n```\\n\\nNote: The entities \\'lo\\', \\'kubeadm\\' and others which are not involved in any relation, have been excluded from the list of relations.'},\n",
       " {'page': 410,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '378\\nCHAPTER 13\\nSecuring cluster nodes and the network\\nIt’s important to understand that if a pod is using a specific host port, only one\\ninstance of the pod can be scheduled to each node, because two processes can’t bind\\nto the same host port. The Scheduler takes this into account when scheduling pods, so\\nit doesn’t schedule multiple pods to the same node, as shown in figure 13.3. If you\\nhave three nodes and want to deploy four pod replicas, only three will be scheduled\\n(one pod will remain Pending).\\nNode 1\\nPod 1\\nTwo pods using\\nhostPort\\nPort\\n8080\\nPort\\n9000\\nNode 2\\nPod 2\\nPort\\n8080\\nPort\\n9000\\nNode 3\\nNode 1\\nPod 1\\nTwo pods under\\nthe same\\nNodePort\\nservice\\nPort\\n8080\\nNode 2\\nPod 2\\nPort\\n8080\\nNode 3\\nPort\\n88\\nPort\\n88\\nPort\\n88\\nService\\n(\\n)\\niptables\\nService\\n(\\n)\\niptables\\nService\\n(\\n)\\niptables\\nFigure 13.2\\nDifference between pods using a hostPort and pods behind a NodePort service.\\nNode 1\\nPod 1\\nPort\\n8080\\nHost\\nport\\n9000\\nHost\\nport\\n9000\\nPod 2\\nPort\\n8080\\nNode 2\\nPod 3\\nPort\\n8080\\nHost\\nport\\n9000\\nNode 3\\nPod 4\\nPort\\n8080\\nCannot be scheduled to the same\\nnode, because the port is already bound\\nOnly a single\\nreplica per node\\nFigure 13.3\\nIf a host port is used, only a single pod instance can be scheduled to a node.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [   Col0 Node 2\\nService\\n(iptables) Col2  Col3\n",
       "   0                              None       None\n",
       "   1  None                                   None\n",
       "   2  None            Pod 2 Port\\n8080           ],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': '',\n",
       "    'category': 'container orchestration'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Scheduler\", \"description\": \"schedule multiple pods to the same node\", \"destination_entity\": \"node\"},\\n  {\"source_entity\": \"Scheduler\", \"description\": \"take into account host port binding\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"pod\", \"description\": \"use a specific host port\", \"destination_entity\": \"host port\"},\\n  {\"source_entity\": \"scheduler\", \"description\": \"not schedule multiple pods to the same node\", \"destination_entity\": \"multiple pods\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"secure cluster nodes and network\", \"destination_entity\": \"cluster nodes\"}\\n]\\n\\nNote: I\\'ve assumed that \"Scheduler\" is a part of the Kubernetes system, although it\\'s not explicitly mentioned as an entity in the provided list. If you\\'d like to exclude it or replace it with something else, please let me know!'},\n",
       " {'page': 411,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '379\\nUsing the host node’s namespaces in a pod\\nLet’s see how to define the hostPort in a pod’s YAML definition. The following listing\\nshows the YAML to run your kubia pod and bind it to the node’s port 9000.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: kubia-hostport\\nspec:\\n  containers:\\n  - image: luksa/kubia\\n    name: kubia\\n    ports:\\n    - containerPort: 8080    \\n      hostPort: 9000        \\n      protocol: TCP\\nAfter you create this pod, you can access it through port 9000 of the node it’s sched-\\nuled to. If you have multiple nodes, you’ll see you can’t access the pod through that\\nport on the other nodes. \\nNOTE\\nIf you’re trying this on GKE, you need to configure the firewall prop-\\nerly using gcloud compute firewall-rules, the way you did in chapter 5.\\nThe hostPort feature is primarily used for exposing system services, which are\\ndeployed to every node using DaemonSets. Initially, people also used it to ensure two\\nreplicas of the same pod were never scheduled to the same node, but now you have a\\nbetter way of achieving this—it’s explained in chapter 16.\\n13.1.3 Using the node’s PID and IPC namespaces\\nSimilar to the hostNetwork option are the hostPID and hostIPC pod spec properties.\\nWhen you set them to true, the pod’s containers will use the node’s PID and IPC\\nnamespaces, allowing processes running in the containers to see all the other pro-\\ncesses on the node or communicate with them through IPC, respectively. See the fol-\\nlowing listing for an example.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-with-host-pid-and-ipc\\nspec:\\n  hostPID: true                    \\n  hostIPC: true                     \\n  containers:\\n  - name: main\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]\\nListing 13.3\\nBinding a pod to a port in the node’s port space: kubia-hostport.yaml\\nListing 13.4\\nUsing the host’s PID and IPC namespaces: pod-with-host-pid-and-ipc.yaml\\nThe container can be \\nreached on port 8080 \\nof the pod’s IP.\\nIt can also be reached \\non port 9000 of the \\nnode it’s deployed on.\\nYou want the pod to \\nuse the host’s PID \\nnamespace.\\nYou also want the \\npod to use the host’s \\nIPC namespace.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'hostPort',\n",
       "    'description': \"A feature that allows a pod to bind to a port on the node's port space.\",\n",
       "    'category': 'network'},\n",
       "   {'entity': 'port 9000',\n",
       "    'description': 'The host port number used by the kubia pod.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'A physical or virtual machine that runs a Kubernetes cluster.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'The basic execution unit in a containerized system, similar to a process in traditional systems.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'containerPort',\n",
       "    'description': \"A property of a pod's container that specifies the port number on which it listens.\",\n",
       "    'category': 'network'},\n",
       "   {'entity': 'protocol TCP',\n",
       "    'description': 'The transport protocol used for communication between containers and hosts.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'hostNetwork',\n",
       "    'description': \"A pod spec property that allows the pod to use the host's network stack.\",\n",
       "    'category': 'network'},\n",
       "   {'entity': 'PID namespace',\n",
       "    'description': 'The set of processes visible to a process, which can be shared between multiple containers or hosts.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'IPC namespace',\n",
       "    'description': 'A mechanism for communication and synchronization between processes running on the same host.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'DaemonSets',\n",
       "    'description': 'A Kubernetes resource that ensures a specified pod is run as a daemon process, one instance per node.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'GKE',\n",
       "    'description': 'A Google Cloud-based container orchestration platform.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'firewall-rules',\n",
       "    'description': 'Rules that control the flow of traffic between containers and hosts.',\n",
       "    'category': 'network'}],\n",
       "  'relationships': '[{\"source_entity\": \"Pod\", \"description\": \"binds to node\\'s port 9000\", \"destination_entity\": \"node\"}, \\n {\"source_entity\": \"hostPort\", \"description\": \"exposes system services deployed to every node using DaemonSets\", \"destination_entity\": \"DaemonSets\"}, \\n {\"source_entity\": \"GKE\", \"description\": \"configures firewall properly using gcloud compute firewall-rules\", \"destination_entity\": \"firewall-rules\"}, \\n {\"source_entity\": \"hostPID\", \"description\": \"allows processes running in containers to see all other processes on the node\", \"destination_entity\": \"node\"}, \\n {\"source_entity\": \"hostIPC\", \"description\": \"allows processes running in containers to communicate with each other through IPC\", \"destination_entity\": \"IPC namespace\"}, \\n {\"source_entity\": \"Pod\", \"description\": \"uses host\\'s PID and IPC namespaces\", \"destination_entity\": \"PID namespace\"}, \\n {\"source_entity\": \"Pod\", \"description\": \"uses host\\'s PID and IPC namespaces\", \"destination_entity\": \"IPC namespace\"}, \\n {\"source_entity\": \"containerPort\", \"description\": \"binds to port 8080 of the pod\\'s IP\", \"destination_entity\": \"pod\"}, \\n {\"source_entity\": \"hostPort\", \"description\": \"binds to port 9000 of the node it\\'s deployed on\", \"destination_entity\": \"node\"}, \\n {\"source_entity\": \"node\", \"description\": \"has multiple nodes that can\\'t access pod through port 9000\", \"destination_entity\": \"multiple nodes\"}, \\n {\"source_entity\": \"Pod\", \"description\": \"can be reached on port 8080 of the pod\\'s IP and port 9000 of the node it\\'s deployed on\", \"destination_entity\": \"node\"}, \\n {\"source_entity\": \"hostPID\", \"description\": \"uses host\\'s PID namespace\", \"destination_entity\": \"PID namespace\"}, \\n {\"source_entity\": \"hostIPC\", \"description\": \"uses host\\'s IPC namespace\", \"destination_entity\": \"IPC namespace\"}]'},\n",
       " {'page': 412,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '380\\nCHAPTER 13\\nSecuring cluster nodes and the network\\nYou’ll remember that pods usually see only their own processes, but if you run this pod\\nand then list the processes from within its container, you’ll see all the processes run-\\nning on the host node, not only the ones running in the container, as shown in the\\nfollowing listing.\\n$ kubectl exec pod-with-host-pid-and-ipc ps aux\\nPID   USER     TIME   COMMAND\\n    1 root       0:01 /usr/lib/systemd/systemd --switched-root --system ...\\n    2 root       0:00 [kthreadd]\\n    3 root       0:00 [ksoftirqd/0]\\n    5 root       0:00 [kworker/0:0H]\\n    6 root       0:00 [kworker/u2:0]\\n    7 root       0:00 [migration/0]\\n    8 root       0:00 [rcu_bh]\\n    9 root       0:00 [rcu_sched]\\n   10 root       0:00 [watchdog/0]\\n...\\nBy setting the hostIPC property to true, processes in the pod’s containers can also\\ncommunicate with all the other processes running on the node, through Inter-Process\\nCommunication.\\n13.2\\nConfiguring the container’s security context\\nBesides allowing the pod to use the host’s Linux namespaces, other security-related\\nfeatures can also be configured on the pod and its container through the security-\\nContext properties, which can be specified under the pod spec directly and inside the\\nspec of individual containers.\\nUNDERSTANDING WHAT’S CONFIGURABLE IN THE SECURITY CONTEXT\\nConfiguring the security context allows you to do various things:\\n\\uf0a1Specify the user (the user’s ID) under which the process in the container will run.\\n\\uf0a1Prevent the container from running as root (the default user a container runs\\nas is usually defined in the container image itself, so you may want to prevent\\ncontainers from running as root).\\n\\uf0a1Run the container in privileged mode, giving it full access to the node’s kernel.\\n\\uf0a1Configure fine-grained privileges, by adding or dropping capabilities—in con-\\ntrast to giving the container all possible permissions by running it in privi-\\nleged mode.\\n\\uf0a1Set SELinux (Security Enhanced Linux) options to strongly lock down a\\ncontainer.\\n\\uf0a1Prevent the process from writing to the container’s filesystem.\\nWe’ll explore these options next. \\nListing 13.5\\nProcesses visible in a pod with hostPID: true\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'pod',\n",
       "    'description': 'A group of one or more containers that share resources and can be treated as a single entity.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for interacting with Kubernetes clusters.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'exec',\n",
       "    'description': 'A command used to execute a process in a running container or pod.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ps aux',\n",
       "    'description': 'A command used to list the processes running on a system.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'hostIPC',\n",
       "    'description': \"A property that allows processes in a pod's containers to communicate with all other processes running on the node, through Inter-Process Communication.\",\n",
       "    'category': 'property'},\n",
       "   {'entity': 'security context',\n",
       "    'description': 'A set of properties used to configure the security settings for a pod or container.',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'hostPID',\n",
       "    'description': \"A property that allows processes in a pod's containers to see all processes running on the host node.\",\n",
       "    'category': 'property'},\n",
       "   {'entity': 'Linux namespaces',\n",
       "    'description': 'A feature of the Linux kernel that allows multiple isolated systems to run on a single physical machine.',\n",
       "    'category': 'feature'},\n",
       "   {'entity': 'Inter-Process Communication',\n",
       "    'description': 'A mechanism for processes running on the same node to communicate with each other.',\n",
       "    'category': 'mechanism'},\n",
       "   {'entity': 'SELinux',\n",
       "    'description': 'A security feature of Linux that allows administrators to set fine-grained permissions and restrictions on system resources.',\n",
       "    'category': 'feature'},\n",
       "   {'entity': 'capabilities',\n",
       "    'description': 'A mechanism for granting or revoking specific privileges to processes running on a system.',\n",
       "    'category': 'mechanism'},\n",
       "   {'entity': 'privilege mode',\n",
       "    'description': \"A configuration option that allows a container to run with full access to the node's kernel.\",\n",
       "    'category': 'configuration'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"allows processes to communicate with all other processes running on the node\",\\n    \"destination_entity\": \"hostIPC\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"grants full access to the node\\'s kernel\",\\n    \"destination_entity\": \"privileged mode\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"can use the host\\'s Linux namespaces\",\\n    \"destination_entity\": \"Linux namespaces\"\\n  },\\n  {\\n    \"source_entity\": \"Kubectl\",\\n    \"description\": \"executes a command on a pod\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"can run processes as a specific user\",\\n    \"destination_entity\": \"security context\"\\n  },\\n  {\\n    \"source_entity\": \"Container\",\\n    \"description\": \"prevents running as root by default\",\\n    \"destination_entity\": \"root user\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"configures fine-grained privileges\",\\n    \"destination_entity\": \"capabilities\"\\n  },\\n  {\\n    \"source_entity\": \"SELinux\",\\n    \"description\": \"strongly locks down a container\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"prevents writing to the container\\'s filesystem\",\\n    \"destination_entity\": \"filesystem\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"can run processes with hostPID set to true\",\\n    \"destination_entity\": \"hostPID\"\\n  },\\n  {\\n    \"source_entity\": \"Kubectl\",\\n    \"description\": \"executes a command on a pod using ps aux\",\\n    \"destination_entity\": \"ps aux\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"can communicate with all other processes running on the node through Inter-Process Communication\",\\n    \"destination_entity\": \"Inter-Process Communication\"\\n  }\\n]\\n```'},\n",
       " {'page': 413,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '381\\nConfiguring the container’s security context\\nRUNNING A POD WITHOUT SPECIFYING A SECURITY CONTEXT\\nFirst, run a pod with the default security context options (by not specifying them at\\nall), so you can see how it behaves compared to pods with a custom security context:\\n$ kubectl run pod-with-defaults --image alpine --restart Never \\n➥  -- /bin/sleep 999999\\npod \"pod-with-defaults\" created\\nLet’s see what user and group ID the container is running as, and which groups it\\nbelongs to. You can see this by running the id command inside the container:\\n$ kubectl exec pod-with-defaults id\\nuid=0(root) gid=0(root) groups=0(root), 1(bin), 2(daemon), 3(sys), 4(adm), \\n6(disk), 10(wheel), 11(floppy), 20(dialout), 26(tape), 27(video)\\nThe container is running as user ID (uid) 0, which is root, and group ID (gid) 0 (also\\nroot). It’s also a member of multiple other groups. \\nNOTE\\nWhat user the container runs as is specified in the container image. In\\na Dockerfile, this is done using the USER directive. If omitted, the container\\nruns as root.\\nNow, you’ll run a pod where the container runs as a different user.\\n13.2.1 Running a container as a specific user\\nTo run a pod under a different user ID than the one that’s baked into the container\\nimage, you’ll need to set the pod’s securityContext.runAsUser property. You’ll\\nmake the container run as user guest, whose user ID in the alpine container image is\\n405, as shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-as-user-guest\\nspec:\\n  containers:\\n  - name: main\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]\\n    securityContext:\\n      runAsUser: 405      \\nNow, to see the effect of the runAsUser property, run the id command in this new\\npod, the way you did before:\\n$ kubectl exec pod-as-user-guest id\\nuid=405(guest) gid=100(users)\\nListing 13.6\\nRunning containers as a specific user: pod-as-user-guest.yaml\\nYou need to specify a user ID, not \\na username (id 405 corresponds \\nto the guest user).\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod-with-defaults',\n",
       "    'description': 'Kubernetes pod with default security context options',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'alpine',\n",
       "    'description': 'Docker image based on Alpine Linux',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'id',\n",
       "    'description': 'command for displaying user and group IDs',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'uid',\n",
       "    'description': 'user ID in the container',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'gid',\n",
       "    'description': 'group ID in the container',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'root',\n",
       "    'description': 'superuser account on Linux',\n",
       "    'category': 'account'},\n",
       "   {'entity': 'bin',\n",
       "    'description': 'Unix group for users who have permission to execute binaries',\n",
       "    'category': 'group'},\n",
       "   {'entity': 'daemon',\n",
       "    'description': 'Unix group for system services that run in the background',\n",
       "    'category': 'group'},\n",
       "   {'entity': 'sys',\n",
       "    'description': 'Unix group for system administrators',\n",
       "    'category': 'group'},\n",
       "   {'entity': 'adm',\n",
       "    'description': 'Unix group for system administrators',\n",
       "    'category': 'group'},\n",
       "   {'entity': 'disk',\n",
       "    'description': 'Unix group for users who have permission to read and write disk files',\n",
       "    'category': 'group'},\n",
       "   {'entity': 'wheel',\n",
       "    'description': 'Unix group for users with superuser privileges',\n",
       "    'category': 'group'},\n",
       "   {'entity': 'floppy',\n",
       "    'description': 'Unix group for users who have permission to access floppy disk drives',\n",
       "    'category': 'group'},\n",
       "   {'entity': 'dialout',\n",
       "    'description': 'Unix group for users who have permission to make serial connections',\n",
       "    'category': 'group'},\n",
       "   {'entity': 'tape',\n",
       "    'description': 'Unix group for users who have permission to read and write tape devices',\n",
       "    'category': 'group'},\n",
       "   {'entity': 'video',\n",
       "    'description': 'Unix group for users who have permission to access video devices',\n",
       "    'category': 'group'},\n",
       "   {'entity': 'guest',\n",
       "    'description': 'user ID in the Alpine container image',\n",
       "    'category': 'account'},\n",
       "   {'entity': 'users',\n",
       "    'description': 'Unix group for users',\n",
       "    'category': 'group'},\n",
       "   {'entity': 'Dockerfile',\n",
       "    'description': 'configuration file for Docker images',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'USER directive',\n",
       "    'description': 'instruction in a Dockerfile that sets the user ID of a container',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'securityContext.runAsUser property',\n",
       "    'description': 'Kubernetes pod configuration option that sets the user ID of a container',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"run a pod without specifying a security context\",\\n    \"destination_entity\": \"pod-with-defaults\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"see how the container behaves compared to pods with a custom security context\",\\n    \"destination_entity\": \"pod-with-defaults\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"run a pod under a different user ID than the one that\\'s baked into the container image\",\\n    \"destination_entity\": \"pod-as-user-guest\"\\n  },\\n  {\\n    \"source_entity\": \"id command\",\\n    \"description\": \"show what user and group ID the container is running as, and which groups it belongs to\",\\n    \"destination_entity\": \"pod-with-defaults\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"run the id command in this new pod\",\\n    \"destination_entity\": \"pod-as-user-guest\"\\n  },\\n  {\\n    \"source_entity\": \"USER directive\",\\n    \"description\": \"specify what user the container runs as is specified in the container image\",\\n    \"destination_entity\": \"Dockerfile\"\\n  },\\n  {\\n    \"source_entity\": \"securityContext.runAsUser property\",\\n    \"description\": \"set the pod\\'s security context to run a specific user\",\\n    \"destination_entity\": \"pod-as-user-guest\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl exec\",\\n    \"description\": \"see the effect of the runAsUser property by running the id command in this new pod\",\\n    \"destination_entity\": \"pod-as-user-guest\"\\n  },\\n  {\\n    \"source_entity\": \"runAsUser: 405\",\\n    \"description\": \"specify a user ID, not a username, to run a container as a specific user\",\\n    \"destination_entity\": \"guest\"\\n  },\\n  {\\n    \"source_entity\": \"id command\",\\n    \"description\": \"show what user and group ID the container is running as, and which groups it belongs to\",\\n    \"destination_entity\": \"pod-as-user-guest\"\\n  }\\n]'},\n",
       " {'page': 414,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '382\\nCHAPTER 13\\nSecuring cluster nodes and the network\\nAs requested, the container is running as the guest user. \\n13.2.2 Preventing a container from running as root\\nWhat if you don’t care what user the container runs as, but you still want to prevent it\\nfrom running as root? \\n Imagine having a pod deployed with a container image that was built with a USER\\ndaemon directive in the Dockerfile, which makes the container run under the daemon\\nuser. What if an attacker gets access to your image registry and pushes a different\\nimage under the same tag? The attacker’s image is configured to run as the root user.\\nWhen Kubernetes schedules a new instance of your pod, the Kubelet will download\\nthe attacker’s image and run whatever code they put into it. \\n Although containers are mostly isolated from the host system, running their pro-\\ncesses as root is still considered a bad practice. For example, when a host directory is\\nmounted into the container, if the process running in the container is running as\\nroot, it has full access to the mounted directory, whereas if it’s running as non-root,\\nit won’t. \\n To prevent the attack scenario described previously, you can specify that the pod’s\\ncontainer needs to run as a non-root user, as shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-run-as-non-root\\nspec:\\n  containers:\\n  - name: main\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]\\n    securityContext:                   \\n      runAsNonRoot: true               \\nIf you deploy this pod, it gets scheduled, but is not allowed to run:\\n$ kubectl get po pod-run-as-non-root\\nNAME                 READY  STATUS                                                  \\npod-run-as-non-root  0/1    container has runAsNonRoot and image will run \\n                            ➥  as root\\nNow, if anyone tampers with your container images, they won’t get far.\\n13.2.3 Running pods in privileged mode\\nSometimes pods need to do everything that the node they’re running on can do, such\\nas use protected system devices or other kernel features, which aren’t accessible to\\nregular containers. \\nListing 13.7\\nPreventing containers from running as root: pod-run-as-non-root.yaml\\nThis container will only \\nbe allowed to run as a \\nnon-root user.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Container',\n",
       "    'description': 'a process running in a containerized environment',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Dockerfile',\n",
       "    'description': 'a configuration file used to build Docker images',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'USER daemon directive',\n",
       "    'description': 'a command in the Dockerfile that sets the user for the container',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'an agent that runs on each node and manages pods',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'a logical grouping of one or more containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'API Version',\n",
       "    'description': 'the version of the Kubernetes API used to create a resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kind: Pod',\n",
       "    'description': 'the type of resource being created',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Metadata',\n",
       "    'description': 'data about the pod, such as its name and labels',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'Spec',\n",
       "    'description': 'the configuration for the pod, including the containers it should run',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'Containers',\n",
       "    'description': 'a list of containers that should be run in the pod',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Image',\n",
       "    'description': 'the Docker image used to create a container',\n",
       "    'category': 'image'},\n",
       "   {'entity': 'Command',\n",
       "    'description': 'the command that should be executed by the container',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Security Context',\n",
       "    'description': 'a set of settings for how the container should run, including its user and permissions',\n",
       "    'category': 'configuration'},\n",
       "   {'entity': 'RunAsNonRoot',\n",
       "    'description': 'a setting that prevents a container from running as root',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Kubectl',\n",
       "    'description': 'the command-line tool used to interact with the Kubernetes API',\n",
       "    'category': 'command'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Dockerfile\", \"description\": \"specifies that the container should run under the daemon user\", \"destination_entity\": \"daemon user\"},\\n  {\"source_entity\": \"Container\", \"description\": \"can access host system resources when running as root, but not if running as non-root\", \"destination_entity\": \"Host System Resources\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"needs to specify that the container should run as a non-root user\", \"destination_entity\": \"RunAsNonRoot\"},\\n  {\"source_entity\": \"Kubectl\", \"description\": \"gets pod status and shows error if container cannot run as non-root\", \"destination_entity\": \"pod-run-as-non-root\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"downloads attacker\\'s image and runs code in it when running as root\", \"destination_entity\": \"Attacker\\'s Image\"},\\n  {\"source_entity\": \"Security Context\", \"description\": \"specifies that the container should run as a non-root user\", \"destination_entity\": \"RunAsNonRoot\"},\\n  {\"source_entity\": \"Command\", \"description\": \"is executed in the container and runs as root by default\", \"destination_entity\": \"Container Processes\"},\\n  {\"source_entity\": \"Image\", \"description\": \"can be tampered with to run as root, but specifying RunAsNonRoot prevents this\", \"destination_entity\": \"Container Images\"},\\n  {\"source_entity\": \"API Version\", \"description\": \"specifies the version of Kubernetes being used\", \"destination_entity\": \"Kubernetes Version\"},\\n  {\"source_entity\": \"Metadata\", \"description\": \"provides information about the pod, such as its name\", \"destination_entity\": \"Pod Information\"},\\n  {\"source_entity\": \"Spec\", \"description\": \"defines the properties of the pod and its containers\", \"destination_entity\": \"Pod Properties\"},\\n  {\"source_entity\": \"Container\", \"description\": \"runs with full access to mounted host directories if running as root\", \"destination_entity\": \"Mounted Host Directories\"}\\n]\\n\\nNote: I have considered all the entities provided in the list, even though some of them may not be explicitly mentioned in the document.'},\n",
       " {'page': 415,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '383\\nConfiguring the container’s security context\\n An example of such a pod is the kube-proxy pod, which needs to modify the node’s\\niptables rules to make services work, as was explained in chapter 11. If you follow the\\ninstructions in appendix B and deploy a cluster with kubeadm, you’ll see every cluster\\nnode runs a kube-proxy pod and you can examine its YAML specification to see all the\\nspecial features it’s using. \\n To get full access to the node’s kernel, the pod’s container runs in privileged\\nmode. This is achieved by setting the privileged property in the container’s security-\\nContext property to true. You’ll create a privileged pod from the YAML in the follow-\\ning listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-privileged\\nspec:\\n  containers:\\n  - name: main\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]\\n    securityContext:\\n      privileged: true     \\nGo ahead and deploy this pod, so you can compare it with the non-privileged pod you\\nran earlier. \\n If you’re familiar with Linux, you may know it has a special file directory called /dev,\\nwhich contains device files for all the devices on the system. These aren’t regular files on\\ndisk, but are special files used to communicate with devices. Let’s see what devices are\\nvisible in the non-privileged container you deployed earlier (the pod-with-defaults\\npod), by listing files in its /dev directory, as shown in the following listing.\\n$ kubectl exec -it pod-with-defaults ls /dev\\ncore             null             stderr           urandom\\nfd               ptmx             stdin            zero\\nfull             pts              stdout\\nfuse             random           termination-log\\nmqueue           shm              tty\\nThe listing shows all the devices. The list is fairly short. Now, compare this with the fol-\\nlowing listing, which shows the device files your privileged pod can see.\\n$ kubectl exec -it pod-privileged ls /dev\\nautofs              snd                 tty46\\nbsg                 sr0                 tty47\\nListing 13.8\\nA pod with a privileged container: pod-privileged.yaml\\nListing 13.9\\nList of available devices in a non-privileged pod\\nListing 13.10\\nList of available devices in a privileged pod\\nThis container will \\nrun in privileged \\nmode\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'kube-proxy',\n",
       "    'description': \"A pod that modifies node's iptables rules to make services work\",\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'privileged mode',\n",
       "    'description': \"A container running with full access to the node's kernel\",\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'security context',\n",
       "    'description': \"The configuration for a pod or container's security settings\",\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes clusters',\n",
       "    'category': 'Software'},\n",
       "   {'entity': '/dev directory',\n",
       "    'description': 'A special file directory containing device files for all devices on the system',\n",
       "    'category': 'Filesystem'},\n",
       "   {'entity': 'device files',\n",
       "    'description': 'Special files used to communicate with devices',\n",
       "    'category': 'Filesystem'},\n",
       "   {'entity': 'Linux',\n",
       "    'description': 'An operating system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': '/bin/sleep',\n",
       "    'description': 'A command that puts the container in sleep mode',\n",
       "    'category': 'Command'},\n",
       "   {'entity': '/dev/core',\n",
       "    'description': 'A device file for the core device',\n",
       "    'category': 'Filesystem'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kube-proxy\",\\n    \"description\": \"needs to modify node\\'s iptables rules\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"runs a kube-proxy pod on every cluster node\",\\n    \"destination_entity\": \"cluster node\"\\n  },\\n  {\\n    \"source_entity\": \"security context\",\\n    \"description\": \"needs to be set to true for privileged mode\",\\n    \"destination_entity\": \"privileged mode\"\\n  },\\n  {\\n    \"source_entity\": \"/bin/sleep\",\\n    \"description\": \"used as a command in the container\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"needs to be created with privileged property set to true\",\\n    \"destination_entity\": \"privileged pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"used to list files in the /dev directory\",\\n    \"destination_entity\": \"/dev directory\"\\n  },\\n  {\\n    \"source_entity\": \"device files\",\\n    \"description\": \"visible in the /dev directory of a non-privileged container\",\\n    \"destination_entity\": \"/dev directory\"\\n  },\\n  {\\n    \"source_entity\": \"privileged pod\",\\n    \"description\": \"can see more device files than a non-privileged pod\",\\n    \"destination_entity\": \"device files\"\\n  },\\n  {\\n    \"source_entity\": \"/bin/sleep\",\\n    \"description\": \"used as a command in the privileged pod\",\\n    \"destination_entity\": \"privileged pod\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"needs to configure container\\'s security context for privileged mode\",\\n    \"destination_entity\": \"container\\'s security context\"\\n  },\\n  {\\n    \"source_entity\": \"Linux\",\\n    \"description\": \"has a /dev directory containing device files\",\\n    \"destination_entity\": \"/dev directory\"\\n  }\\n]\\n\\nNote: I have considered the entities mentioned in the text and extracted relations based on their context.'},\n",
       " {'page': 416,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '384\\nCHAPTER 13\\nSecuring cluster nodes and the network\\nbtrfs-control       stderr              tty48\\ncore                stdin               tty49\\ncpu                 stdout              tty5\\ncpu_dma_latency     termination-log     tty50\\nfd                  tty                 tty51\\nfull                tty0                tty52\\nfuse                tty1                tty53\\nhpet                tty10               tty54\\nhwrng               tty11               tty55\\n...                 ...                 ...\\nI haven’t included the whole list, because it’s too long for the book, but it’s evident\\nthat the device list is much longer than before. In fact, the privileged container sees\\nall the host node’s devices. This means it can use any device freely. \\n For example, I had to use privileged mode like this when I wanted a pod running\\non a Raspberry Pi to control LEDs connected it.\\n13.2.4 Adding individual kernel capabilities to a container\\nIn the previous section, you saw one way of giving a container unlimited power. In the\\nold days, traditional UNIX implementations only distinguished between privileged\\nand unprivileged processes, but for many years, Linux has supported a much more\\nfine-grained permission system through kernel capabilities.\\n Instead of making a container privileged and giving it unlimited permissions, a\\nmuch safer method (from a security perspective) is to give it access only to the kernel\\nfeatures it really requires. Kubernetes allows you to add capabilities to each container\\nor drop part of them, which allows you to fine-tune the container’s permissions and\\nlimit the impact of a potential intrusion by an attacker.\\n For example, a container usually isn’t allowed to change the system time (the hard-\\nware clock’s time). You can confirm this by trying to set the time in your pod-with-\\ndefaults pod:\\n$ kubectl exec -it pod-with-defaults -- date +%T -s \"12:00:00\"\\ndate: can\\'t set date: Operation not permitted\\nIf you want to allow the container to change the system time, you can add a capabil-\\nity called CAP_SYS_TIME to the container’s capabilities list, as shown in the follow-\\ning listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-add-settime-capability\\nspec:\\n  containers:\\n  - name: main\\n    image: alpine\\nListing 13.11\\nAdding the CAP_SYS_TIME capability: pod-add-settime-capability.yaml\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'btrfs-control',\n",
       "    'description': 'device',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'stderr',\n",
       "    'description': 'file descriptor',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'tty48',\n",
       "    'description': 'terminal device',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'core',\n",
       "    'description': 'CPU information file',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'stdin',\n",
       "    'description': 'standard input file descriptor',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'tty49',\n",
       "    'description': 'terminal device',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'cpu', 'description': 'CPU information', 'category': 'hardware'},\n",
       "   {'entity': 'stdout',\n",
       "    'description': 'standard output file descriptor',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'tty5',\n",
       "    'description': 'terminal device',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'cpu_dma_latency',\n",
       "    'description': 'CPU DMA latency file',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'termination-log',\n",
       "    'description': 'system log file',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'tty50',\n",
       "    'description': 'terminal device',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'fd', 'description': 'file descriptor', 'category': 'software'},\n",
       "   {'entity': 'tty', 'description': 'terminal device', 'category': 'hardware'},\n",
       "   {'entity': 'tty51',\n",
       "    'description': 'terminal device',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'full', 'description': 'device', 'category': 'hardware'},\n",
       "   {'entity': 'tty0',\n",
       "    'description': 'console terminal device',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'tty1',\n",
       "    'description': 'console terminal device',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'fuse',\n",
       "    'description': 'file system interface',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'tty10',\n",
       "    'description': 'terminal device',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'hpet',\n",
       "    'description': 'high precision event timer file',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'tty11',\n",
       "    'description': 'terminal device',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'hwrng',\n",
       "    'description': 'hardware random number generator file',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'tty12',\n",
       "    'description': 'terminal device',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'privileged container',\n",
       "    'description': 'container with elevated privileges',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Raspberry Pi',\n",
       "    'description': 'single-board computer',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'LEDs',\n",
       "    'description': 'light-emitting diodes',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kernel capabilities',\n",
       "    'description': 'fine-grained permission system for Linux',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CAP_SYS_TIME',\n",
       "    'description': 'capability to set the system time',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod-with-defaults pod',\n",
       "    'description': 'container with default permissions',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl exec',\n",
       "    'description': 'command to execute a command in a container',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'date +%T -s',\n",
       "    'description': 'shell command to set the system time',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API version v1',\n",
       "    'description': 'version of the Kubernetes API',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kind: Pod',\n",
       "    'description': 'definition for a pod in the Kubernetes API',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata: name:',\n",
       "    'description': 'metadata for a pod in the Kubernetes API',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'spec: containers:',\n",
       "    'description': 'specification for a container in the Kubernetes API',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiVersion: v1',\n",
       "    'description': 'version of the Kubernetes API',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"privileged container\", \"description\": \"can use any device freely\", \"destination_entity\": \"all host node\\'s devices\"},\\n  {\"source_entity\": \"privileged container\", \"description\": \"control LEDs connected it\", \"destination_entity\": \"LEDs connected to Raspberry Pi\"},\\n  {\"source_entity\": \"container\", \"description\": \"isn\\'t allowed to change the system time\", \"destination_entity\": \"system time\"},\\n  {\"source_entity\": \"CAP_SYS_TIME capability\", \"description\": \"allows container to change system time\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"kubectl exec command\", \"description\": \"executes a command in a pod\", \"destination_entity\": \"pod-with-defaults pod\"},\\n  {\"source_entity\": \"container\", \"description\": \"isn\\'t allowed to set date\", \"destination_entity\": \"date and time\"},\\n  {\"source_entity\": \"CAP_SYS_TIME capability\", \"description\": \"grants permission to set system time\", \"destination_entity\": \"system time\"},\\n  {\"source_entity\": \"privileged container\", \"description\": \"can use all host node\\'s devices freely\", \"destination_entity\": \"all host node\\'s devices\"},\\n  {\"source_entity\": \"Raspberry Pi\", \"description\": \"has LEDs connected to it\", \"destination_entity\": \"LEDs\"},\\n  {\"source_entity\": \"pod-add-settime-capability pod\", \"description\": \"adds CAP_SYS_TIME capability to container\", \"destination_entity\": \"container\"}\\n]\\n```\\n\\nNote: I\\'ve used the entities provided in the input list, and extracted the relations based on the context and content of the document page. Let me know if you need any further clarification or modifications!'},\n",
       " {'page': 417,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '385\\nConfiguring the container’s security context\\n    command: [\"/bin/sleep\", \"999999\"]\\n    securityContext:                     \\n      capabilities:                      \\n        add:                  \\n        - SYS_TIME            \\nNOTE\\nLinux kernel capabilities are usually prefixed with CAP_. But when\\nspecifying them in a pod spec, you must leave out the prefix.\\nIf you run the same command in this new pod’s container, the system time is changed\\nsuccessfully:\\n$ kubectl exec -it pod-add-settime-capability -- date +%T -s \"12:00:00\"\\n12:00:00\\n$ kubectl exec -it pod-add-settime-capability -- date\\nSun May  7 12:00:03 UTC 2017\\nWARNING\\nIf you try this yourself, be aware that it may cause your worker\\nnode to become unusable. In Minikube, although the system time was auto-\\nmatically reset back by the Network Time Protocol (NTP) daemon, I had to\\nreboot the VM to schedule new pods. \\nYou can confirm the node’s time has been changed by checking the time on the node\\nrunning the pod. In my case, I’m using Minikube, so I have only one node and I can\\nget its time like this:\\n$ minikube ssh date\\nSun May  7 12:00:07 UTC 2017\\nAdding capabilities like this is a much better way than giving a container full privileges\\nwith privileged: true. Admittedly, it does require you to know and understand what\\neach capability does.\\nTIP\\nYou’ll find the list of Linux kernel capabilities in the Linux man pages.\\n13.2.5 Dropping capabilities from a container\\nYou’ve seen how to add capabilities, but you can also drop capabilities that may oth-\\nerwise be available to the container. For example, the default capabilities given to a\\ncontainer include the CAP_CHOWN capability, which allows processes to change the\\nownership of files in the filesystem. \\n You can see that’s the case by changing the ownership of the /tmp directory in\\nyour pod-with-defaults pod to the guest user, for example:\\n$ kubectl exec pod-with-defaults chown guest /tmp\\n$ kubectl exec pod-with-defaults -- ls -la / | grep tmp\\ndrwxrwxrwt    2 guest    root             6 May 25 15:18 tmp\\nCapabilities are added or dropped \\nunder the securityContext property.\\nYou’re adding the \\nSYS_TIME capability.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': '/bin/sleep',\n",
       "    'description': 'Command to execute sleep for a specified time',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'SYS_TIME',\n",
       "    'description': 'Linux kernel capability to change system time',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'CAP_',\n",
       "    'description': 'Prefix for Linux kernel capabilities',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'date +%T -s',\n",
       "    'description': 'Command to display current date and time in a specified format',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl exec',\n",
       "    'description': 'Command to execute a command in a container',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod-add-settime-capability',\n",
       "    'description': 'Name of the pod created to add SYS_TIME capability',\n",
       "    'category': 'container'},\n",
       "   {'entity': '/tmp',\n",
       "    'description': 'Temporary directory in the filesystem',\n",
       "    'category': 'file system'},\n",
       "   {'entity': 'guest',\n",
       "    'description': 'User ID for guest user in the container',\n",
       "    'category': 'user'},\n",
       "   {'entity': 'securityContext',\n",
       "    'description': 'Property to configure security context of a container',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'capabilities',\n",
       "    'description': 'List of capabilities added or dropped from a container',\n",
       "    'category': 'software'},\n",
       "   {'entity': '/bin/bash',\n",
       "    'description': 'Command to open a shell in the container',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'minikube ssh',\n",
       "    'description': 'Command to access and execute commands on Minikube VM',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"/bin/sleep\", \"description\": \"sleep for 999999 seconds\", \"destination_entity\": null},\\n\\n {\"source_entity\": \"guest\", \"description\": \"change ownership of /tmp directory to guest user\", \"destination_entity\": \"/tmp\"},\\n\\n {\"source_entity\": \"minikube ssh\", \"description\": \"check time on node running pod\", \"destination_entity\": \"node\"},\\n\\n {\"source_entity\": \"kubectl exec\", \"description\": \"execute command in pod\\'s container\", \"destination_entity\": \"pod-with-defaults\"},\\n\\n {\"source_entity\": \"date +%T -s\", \"description\": \"set system time to 12:00:00\", \"destination_entity\": null},\\n\\n {\"source_entity\": \"pod-add-settime-capability\", \"description\": \"add SYS_TIME capability to container\\'s security context\", \"destination_entity\": \"securityContext\"},\\n\\n {\"source_entity\": \"/bin/bash\", \"description\": \"run bash shell in pod\\'s container\", \"destination_entity\": null},\\n\\n {\"source_entity\": \"SYS_TIME\", \"description\": \"change system time to 12:00:00\", \"destination_entity\": null},\\n\\n {\"source_entity\": \"CAP_\", \"description\": \"prefix for Linux kernel capabilities\", \"destination_entity\": null},\\n\\n {\"source_entity\": \"capabilities\", \"description\": \"add or drop capabilities from container\\'s security context\", \"destination_entity\": \"securityContext\"},\\n\\n {\"source_entity\": \"securityContext\", \"description\": \"configure container\\'s security context\", \"destination_entity\": \"container\"}]\\n\\nNote: I\\'ve considered the entities provided in the list as input and extracted relations accordingly. The destination entity for some relations might be null if there is no specific object or entity being acted upon by the source entity.'},\n",
       " {'page': 418,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '386\\nCHAPTER 13\\nSecuring cluster nodes and the network\\nTo prevent the container from doing that, you need to drop the capability by listing it\\nunder the container’s securityContext.capabilities.drop property, as shown in\\nthe following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-drop-chown-capability\\nspec:\\n  containers:\\n  - name: main\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]\\n    securityContext:\\n      capabilities:\\n        drop:                   \\n        - CHOWN                 \\nBy dropping the CHOWN capability, you’re not allowed to change the owner of the /tmp\\ndirectory in this pod:\\n$ kubectl exec pod-drop-chown-capability chown guest /tmp\\nchown: /tmp: Operation not permitted\\nYou’re almost done exploring the container’s security context options. Let’s look at\\none more.\\n13.2.6 Preventing processes from writing to the container’s filesystem\\nYou may want to prevent the processes running in the container from writing to the\\ncontainer’s filesystem, and only allow them to write to mounted volumes. You’d want\\nto do that mostly for security reasons. \\n Let’s imagine you’re running a PHP application with a hidden vulnerability, allow-\\ning an attacker to write to the filesystem. The PHP files are added to the container\\nimage at build time and are served from the container’s filesystem. Because of the vul-\\nnerability, the attacker can modify those files and inject them with malicious code. \\n These types of attacks can be thwarted by preventing the container from writing to\\nits filesystem, where the app’s executable code is normally stored. This is done by set-\\nting the container’s securityContext.readOnlyRootFilesystem property to true, as\\nshown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-with-readonly-filesystem\\nListing 13.12\\nDropping a capability from a container: pod-drop-chown-capability.yaml\\nListing 13.13\\nA container with a read-only filesystem: pod-with-readonly-filesystem.yaml\\nYou’re not allowing this container \\nto change file ownership.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'CHOWN capability',\n",
       "    'description': 'The capability to change the owner of a file or directory',\n",
       "    'category': 'capability'},\n",
       "   {'entity': '/tmp directory',\n",
       "    'description': 'A temporary directory within a pod',\n",
       "    'category': 'directory'},\n",
       "   {'entity': 'kubectl exec command',\n",
       "    'description': 'A command used to execute a shell in a running container',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'securityContext.capabilities.drop property',\n",
       "    'description': \"A configuration option that allows dropping capabilities from a container's security context\",\n",
       "    'category': 'property'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A Kubernetes resource representing a group of one or more containers running on a single node',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A lightweight and stand-alone execution environment',\n",
       "    'category': 'component'},\n",
       "   {'entity': '/bin/sleep command',\n",
       "    'description': 'A command that causes the container to sleep for a specified amount of time',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'alpine image',\n",
       "    'description': 'A lightweight Linux distribution used as a container image',\n",
       "    'category': 'image'},\n",
       "   {'entity': 'securityContext.readOnlyRootFilesystem property',\n",
       "    'description': 'A configuration option that allows setting the root filesystem of a container to read-only',\n",
       "    'category': 'property'},\n",
       "   {'entity': 'PHP application',\n",
       "    'description': 'A web framework written in PHP',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\": \"alpine image\", \"description\": \"runs a container with a specific capability dropped\", \"destination_entity\": \"CHOWN capability\"},{\"source_entity\": \"securityContext.readOnlyRootFilesystem property\", \"description\": \"prevents processes from writing to the container\\'s filesystem\", \"destination_entity\": \"container\"},{\"source_entity\": \"kubectl exec command\", \"description\": \"executes a command on a pod that drops a specific capability\", \"destination_entity\": \"Pod\"},{\"source_entity\": \"securityContext.capabilities.drop property\", \"description\": \"drops a specific capability from a container\\'s security context\", \"destination_entity\": \"CHOWN capability\"},{\"source_entity\": \"container\", \"description\": \"cannot change file ownership due to dropped CHOWN capability\", \"destination_entity\": \"/tmp directory\"},{\"source_entity\": \"PHP application\", \"description\": \"vulnerability allows attacker to modify app files in the container\\'s filesystem\", \"destination_entity\": \"container\"},{\"source_entity\": \"Pod\", \"description\": \"runs a container with a read-only filesystem\", \"destination_entity\": \"securityContext.readOnlyRootFilesystem property\"},{\"source_entity\": \"/bin/sleep command\", \"description\": \"is executed by the container to prevent it from writing to its filesystem\", \"destination_entity\": \"container\"}]'},\n",
       " {'page': 419,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '387\\nConfiguring the container’s security context\\nspec:\\n  containers:\\n  - name: main\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]\\n    securityContext:                      \\n      readOnlyRootFilesystem: true        \\n    volumeMounts:                      \\n    - name: my-volume                  \\n      mountPath: /volume               \\n      readOnly: false                  \\n  volumes:\\n  - name: my-volume\\n    emptyDir:\\nWhen you deploy this pod, the container is running as root, which has write permis-\\nsions to the / directory, but trying to write a file there fails:\\n$ kubectl exec -it pod-with-readonly-filesystem touch /new-file\\ntouch: /new-file: Read-only file system\\nOn the other hand, writing to the mounted volume is allowed:\\n$ kubectl exec -it pod-with-readonly-filesystem touch /volume/newfile\\n$ kubectl exec -it pod-with-readonly-filesystem -- ls -la /volume/newfile\\n-rw-r--r--    1 root     root       0 May  7 19:11 /mountedVolume/newfile\\nAs shown in the example, when you make the container’s filesystem read-only, you’ll\\nprobably want to mount a volume in every directory the application writes to (for\\nexample, logs, on-disk caches, and so on).\\nTIP\\nTo increase security, when running pods in production, set their con-\\ntainer’s readOnlyRootFilesystem property to true.\\nSETTING SECURITY CONTEXT OPTIONS AT THE POD LEVEL\\nIn all these examples, you’ve set the security context of an individual container. Sev-\\neral of these options can also be set at the pod level (through the pod.spec.security-\\nContext property). They serve as a default for all the pod’s containers but can be\\noverridden at the container level. The pod-level security context also allows you to set\\nadditional properties, which we’ll explain next.\\n13.2.7 Sharing volumes when containers run as different users\\nIn chapter 6, we explained how volumes are used to share data between the pod’s\\ncontainers. You had no trouble writing files in one container and reading them in\\nthe other. \\n But this was only because both containers were running as root, giving them full\\naccess to all the files in the volume. Now imagine using the runAsUser option we\\nexplained earlier. You may need to run the two containers as two different users (per-\\nhaps you’re using two third-party container images, where each one runs its process\\nThis container’s filesystem \\ncan’t be written to...\\n...but writing to /volume is \\nallowed, becase a volume \\nis mounted there.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'container',\n",
       "    'description': 'A Linux container running an Alpine image',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'Alpine Linux image used in the container',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'command',\n",
       "    'description': \"/bin/sleep command with argument '999999'\",\n",
       "    'category': 'process'},\n",
       "   {'entity': 'securityContext',\n",
       "    'description': 'Security context configuration for the container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'readOnlyRootFilesystem',\n",
       "    'description': \"Property to make the container's filesystem read-only\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'volumeMounts',\n",
       "    'description': 'Configuring volume mounts for the container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'volume',\n",
       "    'description': 'An emptyDir volume mounted at /volume',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool to execute commands on a pod',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pod-with-readonly-filesystem',\n",
       "    'description': 'A Kubernetes pod with a read-only filesystem',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'touch',\n",
       "    'description': 'Command to create a new file',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ls -la',\n",
       "    'description': 'Command to list files in the current directory',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'runAsUser',\n",
       "    'description': 'Option to run containers as different users',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"command\", \"description\": \"executes the sleep command\", \"destination_entity\": \"container\"}, \\n{\"source_entity\": \"volumeMounts\", \"description\": \"mounts a volume to /volume directory\", \"destination_entity\": \"pod-with-readonly-filesystem\"}, \\n{\"source_entity\": \"kubectl\", \"description\": \"executes touch command on /new-file\", \"destination_entity\": \"container\"}, \\n{\"source_entity\": \"container\", \"description\": \"tries to write file to / directory\", \"destination_entity\": \"/directory\"}, \\n{\"source_entity\": \"command\", \"description\": \"executes ls -la command\", \"destination_entity\": \"volume\"}, \\n{\"source_entity\": \"kubectl\", \"description\": \"lists files in /mountedVolume\", \"destination_entity\": \"volume\"}, \\n{\"source_entity\": \"container\", \"description\": \"tries to write file to /volume directory\", \"destination_entity\": \"/volume directory\"}, \\n{\"source_entity\": \"runAsUser\", \"description\": \"specifies user running the container process\", \"destination_entity\": \"container\"}, \\n{\"source_entity\": \"securityContext\", \"description\": \"sets security context for containers in pod\", \"destination_entity\": \"pod\"}, \\n{\"source_entity\": \"image\", \"description\": \"uses alpine image for the container\", \"destination_entity\": \"container\"}, \\n{\"source_entity\": \"readOnlyRootFilesystem\", \"description\": \"makes container\\'s filesystem read-only\", \"destination_entity\": \"container\"}, \\n{\"source_entity\": \"touch\", \"description\": \"creates a new file in /volume directory\", \"destination_entity\": \"/volume directory\"}]'},\n",
       " {'page': 420,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '388\\nCHAPTER 13\\nSecuring cluster nodes and the network\\nunder its own specific user). If those two containers use a volume to share files, they\\nmay not necessarily be able to read or write files of one another. \\n That’s why Kubernetes allows you to specify supplemental groups for all the pods\\nrunning in the container, allowing them to share files, regardless of the user IDs\\nthey’re running as. This is done using the following two properties:\\n\\uf0a1\\nfsGroup\\n\\uf0a1\\nsupplementalGroups\\nWhat they do is best explained in an example, so let’s see how to use them in a pod\\nand then see what their effect is. The next listing describes a pod with two containers\\nsharing the same volume.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-with-shared-volume-fsgroup\\nspec:\\n  securityContext:                       \\n    fsGroup: 555                         \\n    supplementalGroups: [666, 777]       \\n  containers:\\n  - name: first\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]\\n    securityContext:                     \\n      runAsUser: 1111                    \\n    volumeMounts:                               \\n    - name: shared-volume                       \\n      mountPath: /volume\\n      readOnly: false\\n  - name: second\\n    image: alpine\\n    command: [\"/bin/sleep\", \"999999\"]\\n    securityContext:                     \\n      runAsUser: 2222                    \\n    volumeMounts:                               \\n    - name: shared-volume                       \\n      mountPath: /volume\\n      readOnly: false\\n  volumes:                                      \\n  - name: shared-volume                         \\n    emptyDir:\\nAfter you create this pod, run a shell in its first container and see what user and group\\nIDs the container is running as:\\n$ kubectl exec -it pod-with-shared-volume-fsgroup -c first sh\\n/ $ id\\nuid=1111 gid=0(root) groups=555,666,777\\nListing 13.14\\nfsGroup & supplementalGroups: pod-with-shared-volume-fsgroup.yaml\\nThe fsGroup and supplementalGroups \\nare defined in the security context at \\nthe pod level.\\nThe first container \\nruns as user ID 1111.\\nBoth containers \\nuse the same \\nvolume\\nThe second\\ncontainer\\nruns as user\\nID 2222.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'A container orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'Lightweight and standalone software packages that run in a completely isolated process environment.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'fsGroup',\n",
       "    'description': 'A property used to specify supplemental groups for all the pods running in a container, allowing them to share files regardless of user IDs.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'supplementalGroups',\n",
       "    'description': 'A property that allows Kubernetes to specify additional groups for all the pods running in a container.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'The smallest unit of deployment in Kubernetes, which represents a single instance of an application.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'securityContext',\n",
       "    'description': 'A property used to configure the security settings for a pod or container, such as user ID and group IDs.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'runAsUser',\n",
       "    'description': 'A property that specifies the user ID under which a container will run.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'volumeMounts',\n",
       "    'description': 'A configuration option used to specify where and how a volume is mounted in a pod or container.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'shared-volume',\n",
       "    'description': 'A shared volume used by multiple containers within a single pod, allowing them to share files and resources.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'emptyDir',\n",
       "    'description': 'A type of storage that is allocated dynamically on the host machine when the container is created.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line interface for Kubernetes, used to execute commands and manage resources within a cluster.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'exec',\n",
       "    'description': 'A command used with kubectl to execute a command inside a running container or pod.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'id',\n",
       "    'description': 'A Unix command that displays the user and group IDs for a given username or ID number.',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"fsGroup\",\\n    \"description\": \"specifies the user ID that owns the shared volume\",\\n    \"destination_entity\": \"shared-volume\"\\n  },\\n  {\\n    \"source_entity\": \"supplementalGroups\",\\n    \"description\": \"specifies a list of group IDs that have access to the shared volume\",\\n    \"destination_entity\": \"shared-volume\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"allows specifying supplemental groups for all pods running in containers\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"fsGroup\",\\n    \"description\": \"specifies the user ID that owns the shared volume\",\\n    \"destination_entity\": \"shared-volume\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"executes a shell command in the first container of the pod\",\\n    \"destination_entity\": \"first container\"\\n  },\\n  {\\n    \"source_entity\": \"runAsUser\",\\n    \"description\": \"specifies the user ID that runs the first container\",\\n    \"destination_entity\": \"first container\"\\n  },\\n  {\\n    \"source_entity\": \"id\",\\n    \"description\": \"displays the user and group IDs of the running container\",\\n    \"destination_entity\": \"first container\"\\n  },\\n  {\\n    \"source_entity\": \"shared-volume\",\\n    \"description\": \"is shared between two containers in the pod\",\\n    \"destination_entity\": \"second container\"\\n  },\\n  {\\n    \"source_entity\": \"emptyDir\",\\n    \"description\": \"defines an empty directory that is used as a shared volume\",\\n    \"destination_entity\": \"shared-volume\"\\n  }\\n]\\n```'},\n",
       " {'page': 421,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '389\\nRestricting the use of security-related features in pods\\nThe id command shows the container is running with user ID 1111, as specified in the\\npod definition. The effective group ID is 0(root), but group IDs 555, 666, and 777 are\\nalso associated with the user. \\n In the pod definition, you set fsGroup to 555. Because of this, the mounted volume\\nwill be owned by group ID 555, as shown here:\\n/ $ ls -l / | grep volume\\ndrwxrwsrwx    2 root     555              6 May 29 12:23 volume\\nIf you create a file in the mounted volume’s directory, the file is owned by user ID\\n1111 (that’s the user ID the container is running as) and by group ID 555:\\n/ $ echo foo > /volume/foo\\n/ $ ls -l /volume\\ntotal 4\\n-rw-r--r--    1 1111     555              4 May 29 12:25 foo\\nThis is different from how ownership is otherwise set up for newly created files. Usu-\\nally, the user’s effective group ID, which is 0 in your case, is used when a user creates\\nfiles. You can see this by creating a file in the container’s filesystem instead of in the\\nvolume:\\n/ $ echo foo > /tmp/foo\\n/ $ ls -l /tmp\\ntotal 4\\n-rw-r--r--    1 1111     root             4 May 29 12:41 foo\\nAs you can see, the fsGroup security context property is used when the process cre-\\nates files in a volume (but this depends on the volume plugin used), whereas the\\nsupplementalGroups property defines a list of additional group IDs the user is asso-\\nciated with. \\n This concludes this section about the configuration of the container’s security con-\\ntext. Next, we’ll see how a cluster administrator can restrict users from doing so.\\n13.3\\nRestricting the use of security-related features in pods\\nThe examples in the previous sections have shown how a person deploying pods can\\ndo whatever they want on any cluster node, by deploying a privileged pod to the\\nnode, for example. Obviously, a mechanism must prevent users from doing part or\\nall of what’s been explained. The cluster admin can restrict the use of the previously\\ndescribed security-related features by creating one or more PodSecurityPolicy\\nresources.\\n13.3.1 Introducing the PodSecurityPolicy resource\\nPodSecurityPolicy is a cluster-level (non-namespaced) resource, which defines what\\nsecurity-related features users can or can’t use in their pods. The job of upholding\\nthe policies configured in PodSecurityPolicy resources is performed by the\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'id command',\n",
       "    'description': 'command to show container running with user ID',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'fsGroup',\n",
       "    'description': 'pod definition security context property for file system group ownership',\n",
       "    'category': 'property'},\n",
       "   {'entity': 'supplementalGroups',\n",
       "    'description': 'list of additional group IDs the user is associated with',\n",
       "    'category': 'property'},\n",
       "   {'entity': 'PodSecurityPolicy',\n",
       "    'description': \"cluster-level resource to define security-related features users can or can't use in their pods\",\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'PodDefinition',\n",
       "    'description': 'definition of a pod, including its security context',\n",
       "    'category': 'concept'},\n",
       "   {'entity': 'security context',\n",
       "    'description': 'pod configuration that defines security-related features such as user and group IDs',\n",
       "    'category': 'concept'},\n",
       "   {'entity': 'volume plugin',\n",
       "    'description': 'plugin used to manage volumes in a cluster',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'container filesystem',\n",
       "    'description': 'filesystem of the container, where files are created with root ID',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'volume filesystem',\n",
       "    'description': 'filesystem of the volume, where files are created with specified group ID',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'user ID',\n",
       "    'description': 'ID of the user running the container',\n",
       "    'category': 'identifier'},\n",
       "   {'entity': 'group ID',\n",
       "    'description': 'ID of the group associated with the user running the container',\n",
       "    'category': 'identifier'}],\n",
       "  'relationships': '[{\"source_entity\": \"id command\", \"description\": \"runs with user ID 1111\", \"destination_entity\": \"pod definition\"}, {\"source_entity\": \"pod definition\", \"description\": \"sets fsGroup to 555\", \"destination_entity\": \"volume\"}, {\"source_entity\": \"pod definition\", \"description\": \"associates group IDs 555, 666, and 777 with user\", \"destination_entity\": \"user ID\"}, {\"source_entity\": \"fsGroup\", \"description\": \"specifies the group ID for the mounted volume\", \"destination_entity\": \"volume\"}, {\"source_entity\": \"group ID\", \"description\": \"owned by group ID 555\", \"destination_entity\": \"mounted volume\"}, {\"source_entity\": \"supplementalGroups\", \"description\": \"defines a list of additional group IDs associated with user\", \"destination_entity\": \"user ID\"}, {\"source_entity\": \"container filesystem\", \"description\": \"creates files with root as effective group ID\", \"destination_entity\": \"file system\"}, {\"source_entity\": \"fsGroup\", \"description\": \"used when process creates files in a volume\", \"destination_entity\": \"volume plugin\"}, {\"source_entity\": \"PodSecurityPolicy\", \"description\": \"defines security-related features users can or can\\'t use\", \"destination_entity\": \"cluster admin\"}, {\"source_entity\": \"PodDefinition\", \"description\": \"restricts the use of security-related features by cluster admin\", \"destination_entity\": \"users\"}, {\"source_entity\": \"user ID\", \"description\": \"associated with group IDs 555, 666, and 777\", \"destination_entity\": \"group ID\"}, {\"source_entity\": \"fsGroup\", \"description\": \"specifies the group ID for newly created files in volume\", \"destination_entity\": \"file system\"}]'},\n",
       " {'page': 422,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '390\\nCHAPTER 13\\nSecuring cluster nodes and the network\\nPodSecurityPolicy admission control plugin running in the API server (we explained\\nadmission control plugins in chapter 11).\\nNOTE\\nThe PodSecurityPolicy admission control plugin may not be enabled\\nin your cluster. Before running the following examples, ensure it’s enabled. If\\nyou’re using Minikube, refer to the next sidebar.\\nWhen someone posts a pod resource to the API server, the PodSecurityPolicy admis-\\nsion control plugin validates the pod definition against the configured PodSecurity-\\nPolicies. If the pod conforms to the cluster’s policies, it’s accepted and stored into\\netcd; otherwise it’s rejected immediately. The plugin may also modify the pod\\nresource according to defaults configured in the policy.\\nUNDERSTANDING WHAT A PODSECURITYPOLICY CAN DO\\nA PodSecurityPolicy resource defines things like the following:\\n\\uf0a1Whether a pod can use the host’s IPC, PID, or Network namespaces\\n\\uf0a1Which host ports a pod can bind to\\n\\uf0a1What user IDs a container can run as\\n\\uf0a1Whether a pod with privileged containers can be created\\nEnabling RBAC and PodSecurityPolicy admission control in Minikube\\nI’m using Minikube version v0.19.0 to run these examples. That version doesn’t\\nenable either the PodSecurityPolicy admission control plugin or RBAC authorization,\\nwhich is required in part of the exercises. One exercise also requires authenticating\\nas a different user, so you’ll also need to enable the basic authentication plugin\\nwhere users are defined in a file.\\nTo run Minikube with all these plugins enabled, you may need to use this (or a similar)\\ncommand, depending on the version you’re using: \\n$ minikube start --extra-config apiserver.Authentication.PasswordFile.\\n➥ BasicAuthFile=/etc/kubernetes/passwd --extra-config=apiserver.\\n➥ Authorization.Mode=RBAC --extra-config=apiserver.GenericServerRun\\n➥ Options.AdmissionControl=NamespaceLifecycle,LimitRanger,Service\\n➥ Account,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,\\n➥ DefaultTolerationSeconds,PodSecurityPolicy\\nThe API server won’t start up until you create the password file you specified in the\\ncommand line options. This is how to create the file:\\n$ cat <<EOF | minikube ssh sudo tee /etc/kubernetes/passwd\\npassword,alice,1000,basic-user\\npassword,bob,2000,privileged-user\\nEOF\\nYou’ll find a shell script that runs both commands in the book’s code archive in\\nChapter13/minikube-with-rbac-and-psp-enabled.sh.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PodSecurityPolicy',\n",
       "    'description': 'A resource that defines security policies for pods',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'admission control plugin',\n",
       "    'description': 'A plugin that validates pod definitions against configured PodSecurityPolicies',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The server that handles API requests and admission control plugins',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'A distributed key-value store used to store cluster data',\n",
       "    'category': 'database,storage'},\n",
       "   {'entity': 'IPC namespace',\n",
       "    'description': \"A namespace that controls access to the host's IPC mechanisms\",\n",
       "    'category': 'process,namespace'},\n",
       "   {'entity': 'PID namespace',\n",
       "    'description': \"A namespace that controls access to the host's PID mechanisms\",\n",
       "    'category': 'process,namespace'},\n",
       "   {'entity': 'Network namespace',\n",
       "    'description': \"A namespace that controls access to the host's network interfaces\",\n",
       "    'category': 'network,application'},\n",
       "   {'entity': 'host ports',\n",
       "    'description': 'Ports on the host machine that pods can bind to',\n",
       "    'category': 'hardware,port'},\n",
       "   {'entity': 'user IDs',\n",
       "    'description': 'IDs used by containers to run as a specific user',\n",
       "    'category': 'process,container'},\n",
       "   {'entity': 'privileged containers',\n",
       "    'description': 'Containers that have access to all capabilities on the host',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'RBAC (Role-Based Access Control)',\n",
       "    'description': 'A system for controlling access to cluster resources based on user roles',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'A tool for running a Kubernetes cluster on a single machine',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'RBAC authorization',\n",
       "    'description': 'A plugin that enforces RBAC policies in the API server',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'BasicAuthFile',\n",
       "    'description': 'A file used to store user passwords for basic authentication',\n",
       "    'category': 'database,storage'},\n",
       "   {'entity': 'GenericServerRunOptions',\n",
       "    'description': 'An option that controls how the API server is run',\n",
       "    'category': 'process,options'},\n",
       "   {'entity': 'AdmissionControl',\n",
       "    'description': 'An option that enables or disables admission control plugins',\n",
       "    'category': 'process,options'},\n",
       "   {'entity': 'NamespaceLifecycle',\n",
       "    'description': 'A plugin that manages namespace lifecycle events',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'LimitRanger',\n",
       "    'description': 'A plugin that enforces resource limits for pods and containers',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'A service account used to authenticate pods',\n",
       "    'category': 'database,storage'},\n",
       "   {'entity': 'PersistentVolumeLabel',\n",
       "    'description': 'A label used to identify persistent volumes',\n",
       "    'category': 'hardware,volume'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"admission control plugin\", \"description\": \"validates pod definition against configured PodSecurityPolicies\", \"destination_entity\": \"PodSecurityPolicy\"},\\n  {\"source_entity\": \"admission control plugin\", \"description\": \"modifies pod resource according to defaults configured in policy\", \"destination_entity\": \"pod resource\"},\\n  {\"source_entity\": \"PodSecurityPolicy admission control plugin\", \"description\": \"rejects pod if it doesn\\'t conform to cluster\\'s policies\", \"destination_entity\": \"pod resource\"},\\n  {\"source_entity\": \"PodSecurityPolicy admission control plugin\", \"description\": \"accepts and stores pod into etcd if it conforms to policies\", \"destination_entity\": \"etcd\"},\\n  {\"source_entity\": \"A PodSecurityPolicy resource\", \"description\": \"defines host IPC, PID, or Network namespaces usage by a pod\", \"destination_entity\": \"host namespace\"},\\n  {\"source_entity\": \"A PodSecurityPolicy resource\", \"description\": \"defines host ports that a pod can bind to\", \"destination_entity\": \"host ports\"},\\n  {\"source_entity\": \"A PodSecurityPolicy resource\", \"description\": \"defines user IDs that a container can run as\", \"destination_entity\": \"user IDs\"},\\n  {\"source_entity\": \"A PodSecurityPolicy resource\", \"description\": \"defines whether a pod with privileged containers can be created\", \"destination_entity\": \"privileged containers\"},\\n  {\"source_entity\": \"Minikube\", \"description\": \"requires enabling RBAC authorization and PodSecurityPolicy admission control plugin to run exercises\", \"destination_entity\": \"RBAC authorization\"},\\n  {\"source_entity\": \"Minikube\", \"description\": \"requires authenticating as a different user using basic authentication plugin to run exercises\", \"destination_entity\": \"basic authentication plugin\"},\\n  {\"source_entity\": \"API server\", \"description\": \"won\\'t start up until password file is created specified in command line options\", \"destination_entity\": \"password file\"},\\n  {\"source_entity\": \"API server\", \"description\": \"requires RBAC authorization to be enabled to run exercises\", \"destination_entity\": \"RBAC authorization\"},\\n  {\"source_entity\": \"admission control plugin\", \"description\": \"may not be enabled in a cluster, requiring enabling it before running examples\", \"destination_entity\": \"cluster\"},\\n  {\"source_entity\": \"GenericServerRunOptions\", \"description\": \"includes AdmissionControl option to enable PodSecurityPolicy admission control plugin\", \"destination_entity\": \"PodSecurityPolicy admission control plugin\"},\\n  {\"source_entity\": \"ServiceAccount\", \"description\": \"is included in ServiceAccount policy defined by PersistentVolumeLabel\", \"destination_entity\": \"PersistentVolumeLabel\"},\\n  {\"source_entity\": \"API server\", \"description\": \"requires Authentication.PasswordFile option to be specified when starting Minikube\", \"destination_entity\": \"Authentication.PasswordFile\"},\\n  {\"source_entity\": \"BasicAuthFile\", \"description\": \"is used by basic authentication plugin to authenticate users\", \"destination_entity\": \"basic authentication plugin\"},\\n  {\"source_entity\": \"NamespaceLifecycle\", \"description\": \"is included in AdmissionControl option of GenericServerRunOptions to enable PodSecurityPolicy admission control plugin\", \"destination_entity\": \"PodSecurityPolicy admission control plugin\"},\\n  {\"source_entity\": \"LimitRanger\", \"description\": \"is included in AdmissionControl option of GenericServerRunOptions to enable PodSecurityPolicy admission control plugin\", \"destination_entity\": \"PodSecurityPolicy admission control plugin\"},\\n  {\"source_entity\": \"Network namespace\", \"description\": \"can be defined by a PodSecurityPolicy resource as something that a pod can use\", \"destination_entity\": \"pod resource\"},\\n  {\"source_entity\": \"IPC namespace\", \"description\": \"can be defined by a PodSecurityPolicy resource as something that a pod can use\", \"destination_entity\": \"pod resource\"},\\n  {\"source_entity\": \"PID namespace\", \"description\": \"can be defined by a PodSecurityPolicy resource as something that a pod can use\", \"destination_entity\": \"pod resource\"}\\n]'},\n",
       " {'page': 423,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"391\\nRestricting the use of security-related features in pods\\n\\uf0a1Which kernel capabilities are allowed, which are added by default and which are\\nalways dropped\\n\\uf0a1What SELinux labels a container can use\\n\\uf0a1Whether a container can use a writable root filesystem or not\\n\\uf0a1Which filesystem groups the container can run as\\n\\uf0a1Which volume types a pod can use\\nIf you’ve read this chapter up to this point, everything but the last item in the previous\\nlist should be familiar. The last item should also be fairly clear. \\nEXAMINING A SAMPLE PODSECURITYPOLICY\\nThe following listing shows a sample PodSecurityPolicy, which prevents pods from\\nusing the host’s IPC, PID, and Network namespaces, and prevents running privileged\\ncontainers and the use of most host ports (except ports from 10000-11000 and 13000-\\n14000). The policy doesn’t set any constraints on what users, groups, or SELinux\\ngroups the container can run as.\\napiVersion: extensions/v1beta1\\nkind: PodSecurityPolicy\\nmetadata:\\n  name: default\\nspec:\\n  hostIPC: false                 \\n  hostPID: false                 \\n  hostNetwork: false             \\n  hostPorts:                         \\n  - min: 10000                       \\n    max: 11000                       \\n  - min: 13000                       \\n    max: 14000                       \\n  privileged: false              \\n  readOnlyRootFilesystem: true   \\n  runAsUser:                      \\n    rule: RunAsAny                \\n  fsGroup:                        \\n    rule: RunAsAny                \\n  supplementalGroups:             \\n    rule: RunAsAny                \\n  seLinux:                      \\n    rule: RunAsAny              \\n  volumes:                  \\n  - '*'                     \\nMost of the options specified in the example should be self-explanatory, especially if\\nyou’ve read the previous sections. After this PodSecurityPolicy resource is posted to\\nListing 13.15\\nAn example PodSecurityPolicy: pod-security-policy.yaml\\nContainers aren’t \\nallowed to use the \\nhost’s IPC, PID, or \\nnetwork namespace.\\nThey can only bind to host ports \\n10000 to 11000 (inclusive) or \\nhost ports 13000 to 14000.\\nContainers cannot run \\nin privileged mode.\\nContainers are forced to run \\nwith a read-only root filesystem.\\nContainers can \\nrun as any user \\nand any group.\\nThey can also use any \\nSELinux groups they want.\\nAll volume types can \\nbe used in pods.\\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kernel capabilities',\n",
       "    'description': '',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'SELinux labels', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'writable root filesystem',\n",
       "    'description': '',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'filesystem groups', 'description': '', 'category': 'hardware'},\n",
       "   {'entity': 'volume types', 'description': '', 'category': 'hardware'},\n",
       "   {'entity': 'IPC namespace',\n",
       "    'description': 'Inter-Process Communication namespace',\n",
       "    'category': 'process/thread'},\n",
       "   {'entity': 'PID namespace',\n",
       "    'description': 'Process ID namespace',\n",
       "    'category': 'process/thread'},\n",
       "   {'entity': 'Network namespace',\n",
       "    'description': 'Network interface namespace',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'privileged containers',\n",
       "    'description': '',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'host ports',\n",
       "    'description': 'Ports accessible by the host',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'PodSecurityPolicy', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'extensions/v1beta1',\n",
       "    'description': 'API version for PodSecurityPolicy',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'API version key in YAML file',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'Kind of resource, e.g. PodSecurityPolicy',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Metadata section in YAML file',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'Name of the PodSecurityPolicy resource',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'Specification section in YAML file',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'hostIPC',\n",
       "    'description': 'Allow host IPC namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'hostPID',\n",
       "    'description': 'Allow host PID namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'hostNetwork',\n",
       "    'description': 'Allow host network namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'hostPorts',\n",
       "    'description': 'Allowed host ports for container',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'min',\n",
       "    'description': 'Minimum port number allowed',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'max',\n",
       "    'description': 'Maximum port number allowed',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'privileged',\n",
       "    'description': 'Allow privileged containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'readOnlyRootFilesystem',\n",
       "    'description': 'Force read-only root filesystem for container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'runAsUser',\n",
       "    'description': 'Rule for running as user in container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'rule',\n",
       "    'description': 'Rule type, e.g. RunAsAny',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'RunAsAny',\n",
       "    'description': 'Rule allowing any user to run',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'fsGroup',\n",
       "    'description': 'Filesystem group rule for container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'supplementalGroups',\n",
       "    'description': 'Supplementary groups allowed in container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'seLinux',\n",
       "    'description': 'SELinux label rule for container',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'volumes',\n",
       "    'description': 'Allowed volume types for pod',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Privileged containers\",\\n    \"description\": \"are not allowed to use the host\\'s IPC namespace\",\\n    \"destination_entity\": \"IPC namespace\"\\n  },\\n  {\\n    \"source_entity\": \"Privileged containers\",\\n    \"description\": \"are not allowed to use the host\\'s PID namespace\",\\n    \"destination_entity\": \"PID namespace\"\\n  },\\n  {\\n    \"source_entity\": \"Privileged containers\",\\n    \"description\": \"are not allowed to use the host\\'s Network namespace\",\\n    \"destination_entity\": \"Network namespace\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"can only bind to specific host ports (10000-11000 and 13000-14000)\",\\n    \"destination_entity\": \"Host ports\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"cannot run in privileged mode\",\\n    \"destination_entity\": \"Privileged mode\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"are forced to run with a read-only root filesystem\",\\n    \"destination_entity\": \"Read-only root filesystem\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"can run as any user and group\",\\n    \"destination_entity\": \"Users and groups\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"can use any SELinux labels\",\\n    \"destination_entity\": \"SELinux labels\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"can use all volume types\",\\n    \"destination_entity\": \"Volume types\"\\n  },\\n  {\\n    \"source_entity\": \"RunAsUser rule\",\\n    \"description\": \"specifies that containers can run as any user\",\\n    \"destination_entity\": \"Users\"\\n  },\\n  {\\n    \"source_entity\": \"RunAsAny rule\",\\n    \"description\": \"specifies that containers can run as any user and group\",\\n    \"destination_entity\": \"Groups\"\\n  },\\n  {\\n    \"source_entity\": \"SELinux rule\",\\n    \"description\": \"specifies that containers can use any SELinux labels\",\\n    \"destination_entity\": \"SELinux labels\"\\n  },\\n  {\\n    \"source_entity\": \"PodSecurityPolicy\",\\n    \"description\": \"defines the security policies for a pod\",\\n    \"destination_entity\": \"Policies\"\\n  }\\n]'},\n",
       " {'page': 424,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '392\\nCHAPTER 13\\nSecuring cluster nodes and the network\\nthe cluster, the API server will no longer allow you to deploy the privileged pod used\\nearlier. For example\\n$ kubectl create -f pod-privileged.yaml\\nError from server (Forbidden): error when creating \"pod-privileged.yaml\":\\npods \"pod-privileged\" is forbidden: unable to validate against any pod \\nsecurity policy: [spec.containers[0].securityContext.privileged: Invalid \\nvalue: true: Privileged containers are not allowed]\\nLikewise, you can no longer deploy pods that want to use the host’s PID, IPC, or Net-\\nwork namespace. Also, because you set readOnlyRootFilesystem to true in the pol-\\nicy, the container filesystems in all pods will be read-only (containers can only write\\nto volumes).\\n13.3.2 Understanding runAsUser, fsGroup, and supplementalGroups \\npolicies\\nThe policy in the previous example doesn’t impose any limits on which users and\\ngroups containers can run as, because you’ve used the RunAsAny rule for the runAs-\\nUser, fsGroup, and supplementalGroups fields. If you want to constrain the list of\\nallowed user or group IDs, you change the rule to MustRunAs and specify the range of\\nallowed IDs. \\nUSING THE MUSTRUNAS RULE\\nLet’s look at an example. To only allow containers to run as user ID 2 and constrain the\\ndefault filesystem group and supplemental group IDs to be anything from 2–10 or 20–\\n30 (all inclusive), you’d include the following snippet in the PodSecurityPolicy resource.\\n  runAsUser:\\n    rule: MustRunAs\\n    ranges:\\n    - min: 2                \\n      max: 2                \\n  fsGroup:\\n    rule: MustRunAs\\n    ranges:\\n    - min: 2                \\n      max: 10               \\n    - min: 20               \\n      max: 30               \\n  supplementalGroups:\\n    rule: MustRunAs\\n    ranges:\\n    - min: 2                \\n      max: 10               \\n    - min: 20               \\n      max: 30               \\nListing 13.16\\nSpecifying IDs containers must run as: psp-must-run-as.yaml\\nAdd a single range with min equal \\nto max to set one specific ID.\\nMultiple ranges are \\nsupported—here, \\ngroup IDs can be 2–10 \\nor 20–30 (inclusive).\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PodSecurityPolicy',\n",
       "    'description': 'A Kubernetes resource that defines a set of policies for pod creation.',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': 'The component of the Kubernetes cluster that provides an interface to create, update, or delete resources within the cluster.',\n",
       "    'category': 'Kubernetes Component'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool used to interact with a Kubernetes cluster.',\n",
       "    'category': 'Kubernetes Command'},\n",
       "   {'entity': 'pod-privileged.yaml',\n",
       "    'description': 'A YAML file that defines a privileged pod configuration.',\n",
       "    'category': 'Pod Configuration File'},\n",
       "   {'entity': 'runAsUser',\n",
       "    'description': 'A field in the PodSecurityPolicy resource that specifies which user IDs containers can run as.',\n",
       "    'category': 'Field in PodSecurityPolicy Resource'},\n",
       "   {'entity': 'fsGroup',\n",
       "    'description': 'A field in the PodSecurityPolicy resource that specifies which filesystem group IDs containers can use.',\n",
       "    'category': 'Field in PodSecurityPolicy Resource'},\n",
       "   {'entity': 'supplementalGroups',\n",
       "    'description': 'A field in the PodSecurityPolicy resource that specifies which supplemental group IDs containers can use.',\n",
       "    'category': 'Field in PodSecurityPolicy Resource'},\n",
       "   {'entity': 'MustRunAs rule',\n",
       "    'description': 'A policy rule that constrains the list of allowed user or group IDs for containers to run as.',\n",
       "    'category': 'PodSecurityPolicy Rule'},\n",
       "   {'entity': 'ranges',\n",
       "    'description': 'A field in the MustRunAs rule that specifies a range of allowed user or group IDs.',\n",
       "    'category': 'Field in PodSecurityPolicy Rule'},\n",
       "   {'entity': 'min',\n",
       "    'description': 'The minimum value of a range specified in the ranges field.',\n",
       "    'category': 'Field in PodSecurityPolicy Rule'},\n",
       "   {'entity': 'max',\n",
       "    'description': 'The maximum value of a range specified in the ranges field.',\n",
       "    'category': 'Field in PodSecurityPolicy Rule'},\n",
       "   {'entity': 'Pod-privileged container',\n",
       "    'description': \"A container that requires access to the host's PID, IPC, or network namespace.\",\n",
       "    'category': 'Container Type'},\n",
       "   {'entity': 'readOnlyRootFilesystem',\n",
       "    'description': 'A policy setting that makes container filesystems read-only.',\n",
       "    'category': 'PodSecurityPolicy Setting'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"used to create a pod with privileged access\",\\n    \"destination_entity\": \"pod-privileged.yaml\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"prevents deployment of privileged pods due to security policy\",\\n    \"destination_entity\": \"pod-privileged.yaml\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"used to create a pod with host\\'s PID, IPC, or Network namespace access\",\\n    \"destination_entity\": \"Pod-privileged container\"\\n  },\\n  {\\n    \"source_entity\": \"policy\",\\n    \"description\": \"sets readOnlyRootFilesystem to true in all pods\",\\n    \"destination_entity\": \"container filesystems in all pods\"\\n  },\\n  {\\n    \"source_entity\": \"MustRunAs rule\",\\n    \"description\": \"constrains the list of allowed user or group IDs for containers\",\\n    \"destination_entity\": \"runAsUser, fsGroup, and supplementalGroups fields\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"enforces policy restrictions on container access\",\\n    \"destination_entity\": \"containers running in pods\"\\n  },\\n  {\\n    \"source_entity\": \"PodSecurityPolicy\",\\n    \"description\": \"defines the security rules for containers to run as specific user or group IDs\",\\n    \"destination_entity\": \"runAsUser, fsGroup, and supplementalGroups fields\"\\n  },\\n  {\\n    \"source_entity\": \"MustRunAs rule\",\\n    \"description\": \"specifies the allowed ID range for containers to run as user or group IDs\",\\n    \"destination_entity\": \"PodSecurityPolicy resource\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"used to create a pod with specific ID constraints using MustRunAs rule\",\\n    \"destination_entity\": \"psp-must-run-as.yaml\"\\n  },\\n  {\\n    \"source_entity\": \"API Server\",\\n    \"description\": \"prevents deployment of pods that violate security policy\",\\n    \"destination_entity\": \"Pod-privileged container\"\\n  }\\n]'},\n",
       " {'page': 425,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '393\\nRestricting the use of security-related features in pods\\nIf the pod spec tries to set either of those fields to a value outside of these ranges, the\\npod will not be accepted by the API server. To try this, delete the previous PodSecurity-\\nPolicy and create the new one from the psp-must-run-as.yaml file. \\nNOTE\\nChanging the policy has no effect on existing pods, because PodSecurity-\\nPolicies are enforced only when creating or updating pods.\\nDEPLOYING A POD WITH RUNASUSER OUTSIDE OF THE POLICY’S RANGE\\nIf you try deploying the pod-as-user-guest.yaml file from earlier, which says the con-\\ntainer should run as user ID 405, the API server rejects the pod:\\n$ kubectl create -f pod-as-user-guest.yaml\\nError from server (Forbidden): error when creating \"pod-as-user-guest.yaml\"\\n: pods \"pod-as-user-guest\" is forbidden: unable to validate against any pod \\nsecurity policy: [securityContext.runAsUser: Invalid value: 405: UID on \\ncontainer main does not match required range.  Found 405, allowed: [{2 2}]]\\nOkay, that was obvious. But what happens if you deploy a pod without setting the runAs-\\nUser property, but the user ID is baked into the container image (using the USER direc-\\ntive in the Dockerfile)?\\nDEPLOYING A POD WITH A CONTAINER IMAGE WITH AN OUT-OF-RANGE USER ID\\nI’ve created an alternative image for the Node.js app you’ve used throughout the\\nbook. The image is configured so that the container will run as user ID 5. The Docker-\\nfile for the image is shown in the following listing.\\nFROM node:7\\nADD app.js /app.js\\nUSER 5                         \\nENTRYPOINT [\"node\", \"app.js\"]\\nI pushed the image to Docker Hub as luksa/kubia-run-as-user-5. If I deploy a pod\\nwith that image, the API server doesn’t reject it:\\n$ kubectl run run-as-5 --image luksa/kubia-run-as-user-5 --restart Never\\npod \"run-as-5\" created\\nUnlike before, the API server accepted the pod and the Kubelet has run its container.\\nLet’s see what user ID the container is running as:\\n$ kubectl exec run-as-5 -- id\\nuid=2(bin) gid=2(bin) groups=2(bin)\\nAs you can see, the container is running as user ID 2, which is the ID you specified in\\nthe PodSecurityPolicy. The PodSecurityPolicy can be used to override the user ID\\nhardcoded into a container image.\\nListing 13.17\\nDockerfile with a USER directive: kubia-run-as-user-5/Dockerfile\\nContainers run from \\nthis image will run \\nas user ID 5.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PodSecurityPolicy',\n",
       "    'description': 'a Kubernetes object that defines the security settings for pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'a command-line tool to interact with a Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'the component of a Kubernetes cluster that handles incoming requests',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'PodSecurityPolicy',\n",
       "    'description': 'a Kubernetes object that defines the security settings for pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'runAsUser',\n",
       "    'description': 'a field in a pod specification that sets the user ID for a container',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'Dockerfile',\n",
       "    'description': 'a file that contains instructions for building a Docker image',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'USER directive',\n",
       "    'description': 'a command in a Dockerfile that sets the user ID for a container',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'a lightweight and stand-alone process that runs an application or service',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'the component of a Kubernetes cluster that runs containers on worker nodes',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'id command',\n",
       "    'description': 'a command used to display information about the user running a container',\n",
       "    'category': 'command'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"tries to set runAsUser field to a value outside of the policy\\'s range, but API server rejects it.\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"PodSecurityPolicy\",\\n    \"description\": \"enforces only when creating or updating pods\",\\n    \"destination_entity\": \"existing pods\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"tries to deploy a pod with runAsUser outside of the policy\\'s range\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"pod-as-user-guest.yaml\",\\n    \"description\": \"tries to set container to run as user ID 405, which is outside of the policy\\'s range\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"deploys a pod with a container image that has an out-of-range user ID\",\\n    \"destination_entity\": \"Kubelet\"\\n  },\\n  {\\n    \"source_entity\": \"USER directive in Dockerfile\",\\n    \"description\": \"sets the container to run as user ID 5, which is within the policy\\'s range\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl exec\",\\n    \"description\": \"checks what user ID the container is running as\",\\n    \"destination_entity\": \"Kubelet\"\\n  },\\n  {\\n    \"source_entity\": \"PodSecurityPolicy\",\\n    \"description\": \"can override the user ID hardcoded into a container image\",\\n    \"destination_entity\": \"container image\"\\n  },\\n  {\\n    \"source_entity\": \"Dockerfile with USER directive\",\\n    \"description\": \"sets the container to run as user ID 5, which is within the policy\\'s range\",\\n    \"destination_entity\": \"container\"\\n  }\\n]\\n```\\n\\nNote: I\\'ve kept the exact wording from the document where possible, while trying to condense it into a brief description for each relation. Let me know if you\\'d like me to clarify anything!'},\n",
       " {'page': 426,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '394\\nCHAPTER 13\\nSecuring cluster nodes and the network\\nUSING THE MUSTRUNASNONROOT RULE IN THE RUNASUSER FIELD\\nFor the runAsUser field an additional rule can be used: MustRunAsNonRoot. As the\\nname suggests, it prevents users from deploying containers that run as root. Either the\\ncontainer spec must specify a runAsUser field, which can’t be zero (zero is the root\\nuser’s ID), or the container image itself must run as a non-zero user ID. We explained\\nwhy this is a good thing earlier.\\n13.3.3 Configuring allowed, default, and disallowed capabilities\\nAs you learned, containers can run in privileged mode or not, and you can define a\\nmore fine-grained permission configuration by adding or dropping Linux kernel\\ncapabilities in each container. Three fields influence which capabilities containers can\\nor cannot use:\\n\\uf0a1\\nallowedCapabilities\\n\\uf0a1\\ndefaultAddCapabilities\\n\\uf0a1\\nrequiredDropCapabilities\\nWe’ll look at an example first, and then discuss what each of the three fields does. The\\nfollowing listing shows a snippet of a PodSecurityPolicy resource defining three fields\\nrelated to capabilities.\\napiVersion: extensions/v1beta1 \\nkind: PodSecurityPolicy\\nspec:\\n  allowedCapabilities:          \\n  - SYS_TIME                    \\n  defaultAddCapabilities:         \\n  - CHOWN                         \\n  requiredDropCapabilities:     \\n  - SYS_ADMIN                   \\n  - SYS_MODULE                  \\n  ...\\nNOTE\\nThe SYS_ADMIN capability allows a range of administrative operations,\\nand the SYS_MODULE capability allows loading and unloading of Linux kernel\\nmodules.\\nSPECIFYING WHICH CAPABILITIES CAN BE ADDED TO A CONTAINER\\nThe allowedCapabilities field is used to specify which capabilities pod authors can\\nadd in the securityContext.capabilities field in the container spec. In one of the\\nprevious examples, you added the SYS_TIME capability to your container. If the Pod-\\nSecurityPolicy admission control plugin had been enabled, you wouldn’t have been\\nable to add that capability, unless it was specified in the PodSecurityPolicy as shown\\nin listing 13.18.\\nListing 13.18\\nSpecifying capabilities in a PodSecurityPolicy: psp-capabilities.yaml\\nAllow containers to \\nadd the SYS_TIME \\ncapability.\\nAutomatically add the CHOWN \\ncapability to every container.\\nRequire containers to \\ndrop the SYS_ADMIN and \\nSYS_MODULE capabilities.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'MustRunAsNonRoot',\n",
       "    'description': 'A rule that prevents users from deploying containers that run as root.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'runAsUser field',\n",
       "    'description': 'A field in container spec that specifies a user ID for the container to run as.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'container spec',\n",
       "    'description': 'A configuration file that defines how a container should be deployed and managed.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Linux kernel capabilities',\n",
       "    'description': 'Permissions that can be added or dropped in each container to define fine-grained permission configuration.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'allowedCapabilities field',\n",
       "    'description': 'A field in PodSecurityPolicy resource that specifies which capabilities pod authors can add to containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'defaultAddCapabilities field',\n",
       "    'description': 'A field in PodSecurityPolicy resource that automatically adds a capability to every container.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'requiredDropCapabilities field',\n",
       "    'description': 'A field in PodSecurityPolicy resource that requires containers to drop specific capabilities.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'SYS_TIME capability',\n",
       "    'description': 'A Linux kernel capability that allows access to system time functions.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CHOWN capability',\n",
       "    'description': 'A Linux kernel capability that allows changing the ownership of files and directories.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'SYS_ADMIN capability',\n",
       "    'description': 'A Linux kernel capability that allows a range of administrative operations.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'SYS_MODULE capability',\n",
       "    'description': 'A Linux kernel capability that allows loading and unloading of Linux kernel modules.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Pod authors\",\\n    \"description\": \"can add capabilities to a container\",\\n    \"destination_entity\": \"allowedCapabilities field\"\\n  },\\n  {\\n    \"source_entity\": \"container spec\",\\n    \"description\": \"specifies which capabilities can be added\",\\n    \"destination_entity\": \"allowedCapabilities field\"\\n  },\\n  {\\n    \"source_entity\": \"Pod authors\",\\n    \"description\": \"can add the SYS_TIME capability to a container\",\\n    \"destination_entity\": \"SYS_TIME capability\"\\n  },\\n  {\\n    \"source_entity\": \"container spec\",\\n    \"description\": \"adds the SYS_TIME capability to a container\",\\n    \"destination_entity\": \"SYS_TIME capability\"\\n  },\\n  {\\n    \"source_entity\": \"Pod authors\",\\n    \"description\": \"can add the CHOWN capability to a container\",\\n    \"destination_entity\": \"CHOWN capability\"\\n  },\\n  {\\n    \"source_entity\": \"container spec\",\\n    \"description\": \"automatically adds the CHOWN capability to every container\",\\n    \"destination_entity\": \"CHOWN capability\"\\n  },\\n  {\\n    \"source_entity\": \"containers\",\\n    \"description\": \"require to drop the SYS_ADMIN capability\",\\n    \"destination_entity\": \"SYS_ADMIN capability\"\\n  },\\n  {\\n    \"source_entity\": \"containers\",\\n    \"description\": \"require to drop the SYS_MODULE capability\",\\n    \"destination_entity\": \"SYS_MODULE capability\"\\n  },\\n  {\\n    \"source_entity\": \"users\",\\n    \"description\": \"can’t deploy containers that run as root\",\\n    \"destination_entity\": \"runAsUser field\"\\n  },\\n  {\\n    \"source_entity\": \"container spec\",\\n    \"description\": \"specifies a runAsUser field\",\\n    \"destination_entity\": \"runAsUser field\"\\n  },\\n  {\\n    \"source_entity\": \"MustRunAsNonRoot rule\",\\n    \"description\": \"prevents users from deploying containers that run as root\",\\n    \"destination_entity\": \"users\"\\n  },\\n  {\\n    \"source_entity\": \"container spec\",\\n    \"description\": \"specifies the allowedCapabilities field\",\\n    \"destination_entity\": \"allowedCapabilities field\"\\n  },\\n  {\\n    \"source_entity\": \"defaultAddCapabilities field\",\\n    \"description\": \"automatically adds capabilities to every container\",\\n    \"destination_entity\": \"containers\"\\n  },\\n  {\\n    \"source_entity\": \"requiredDropCapabilities field\",\\n    \"description\": \"requires containers to drop specific capabilities\",\\n    \"destination_entity\": \"containers\"\\n  }\\n]\\n\\nNote: I\\'ve used the exact entity names provided in the list, even if they were mentioned multiple times in the document. If you\\'d like me to merge similar entities or adjust the relations in any way, please let me know!'},\n",
       " {'page': 427,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '395\\nRestricting the use of security-related features in pods\\nADDING CAPABILITIES TO ALL CONTAINERS\\nAll capabilities listed under the defaultAddCapabilities field will be added to\\nevery deployed pod’s containers. If a user doesn’t want certain containers to have\\nthose capabilities, they need to explicitly drop them in the specs of those containers.\\n The example in listing 13.18 enables the automatic addition of the CAP_CHOWN capa-\\nbility to every container, thus allowing processes running in the container to change the\\nownership of files in the container (with the chown command, for example).\\nDROPPING CAPABILITIES FROM A CONTAINER\\nThe final field in this example is requiredDropCapabilities. I must admit, this was a\\nsomewhat strange name for me at first, but it’s not that complicated. The capabilities\\nlisted in this field are dropped automatically from every container (the PodSecurity-\\nPolicy Admission Control plugin will add them to every container’s security-\\nContext.capabilities.drop field). \\n If a user tries to create a pod where they explicitly add one of the capabilities listed\\nin the policy’s requiredDropCapabilities field, the pod is rejected:\\n$ kubectl create -f pod-add-sysadmin-capability.yaml\\nError from server (Forbidden): error when creating \"pod-add-sysadmin-\\ncapability.yaml\": pods \"pod-add-sysadmin-capability\" is forbidden: unable \\nto validate against any pod security policy: [capabilities.add: Invalid \\nvalue: \"SYS_ADMIN\": capability may not be added]\\n13.3.4 Constraining the types of volumes pods can use\\nThe last thing a PodSecurityPolicy resource can do is define which volume types users\\ncan add to their pods. At the minimum, a PodSecurityPolicy should allow using at\\nleast the emptyDir, configMap, secret, downwardAPI, and the persistentVolume-\\nClaim volumes. The pertinent part of such a PodSecurityPolicy resource is shown in\\nthe following listing.\\nkind: PodSecurityPolicy\\nspec:\\n  volumes:\\n  - emptyDir\\n  - configMap\\n  - secret\\n  - downwardAPI\\n  - persistentVolumeClaim\\nIf multiple PodSecurityPolicy resources are in place, pods can use any volume type\\ndefined in any of the policies (the union of all volumes lists is used).\\nListing 13.19\\nA PSP snippet allowing the use of only certain volume types: \\npsp-volumes.yaml\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'CAP_CHOWN',\n",
       "    'description': 'Capability to change file ownership',\n",
       "    'category': 'capability'},\n",
       "   {'entity': 'PodSecurity-Policy Admission Control plugin',\n",
       "    'description': 'A plugin that enforces Pod Security Policy rules',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A lightweight and standalone execution environment for applications',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'chown command',\n",
       "    'description': 'A command to change file ownership',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'capabilities.drop field',\n",
       "    'description': 'A field in the security Context that drops specific capabilities',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'requiredDropCapabilities',\n",
       "    'description': 'A field in PodSecurityPolicy that defines capabilities to be dropped from containers',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'PodSecurityPolicy resource',\n",
       "    'description': 'A Kubernetes resource that defines security policies for pods',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'emptyDir',\n",
       "    'description': 'A type of volume that can be used by pods, which is an empty directory on the host filesystem',\n",
       "    'category': 'volume-type'},\n",
       "   {'entity': 'configMap',\n",
       "    'description': 'A type of volume that can be used by pods, which is a map of key-value pairs that can be used by containers',\n",
       "    'category': 'volume-type'},\n",
       "   {'entity': 'secret',\n",
       "    'description': 'A type of volume that can be used by pods, which stores sensitive information such as passwords and keys',\n",
       "    'category': 'volume-type'},\n",
       "   {'entity': 'downwardAPI',\n",
       "    'description': 'A type of volume that can be used by pods, which allows containers to access API endpoints from the host filesystem',\n",
       "    'category': 'volume-type'},\n",
       "   {'entity': 'persistentVolumeClaim',\n",
       "    'description': 'A type of volume that can be used by pods, which is a claim on a persistent volume in the cluster',\n",
       "    'category': 'volume-type'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"PodSecurityPolicy resource\",\\n    \"description\": \"defines which volume types users can add to their pods\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"CAP_CHOWN\",\\n    \"description\": \"allows processes running in the container to change ownership of files using the chown command\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"PodSecurity-Policy Admission Control plugin\",\\n    \"description\": \"adds requiredDropCapabilities to every container\\'s security context.capabilities.drop field\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"requiredDropCapabilities\",\\n    \"description\": \"drops automatically from every container capabilities listed in this field\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"PodSecurityPolicy resource\",\\n    \"description\": \"rejects pod creation if it explicitly adds one of the capabilities listed in requiredDropCapabilities\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"downwardAPI\",\\n    \"description\": \"one of the volume types allowed by a PodSecurityPolicy resource\",\\n    \"destination_entity\": \"PodSecurityPolicy resource\"\\n  },\\n  {\\n    \"source_entity\": \"configMap\",\\n    \"description\": \"one of the volume types allowed by a PodSecurityPolicy resource\",\\n    \"destination_entity\": \"PodSecurityPolicy resource\"\\n  },\\n  {\\n    \"source_entity\": \"emptyDir\",\\n    \"description\": \"one of the volume types allowed by a PodSecurityPolicy resource\",\\n    \"destination_entity\": \"PodSecurityPolicy resource\"\\n  },\\n  {\\n    \"source_entity\": \"persistentVolumeClaim\",\\n    \"description\": \"one of the volume types allowed by a PodSecurityPolicy resource\",\\n    \"destination_entity\": \"PodSecurityPolicy resource\"\\n  },\\n  {\\n    \"source_entity\": \"capabilities.drop field\",\\n    \"description\": \"drops automatically from every container capabilities listed in this field\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"secret\",\\n    \"description\": \"one of the volume types allowed by a PodSecurityPolicy resource\",\\n    \"destination_entity\": \"PodSecurityPolicy resource\"\\n  }\\n]\\n```'},\n",
       " {'page': 428,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '396\\nCHAPTER 13\\nSecuring cluster nodes and the network\\n13.3.5 Assigning different PodSecurityPolicies to different users \\nand groups\\nWe mentioned that a PodSecurityPolicy is a cluster-level resource, which means it\\ncan’t be stored in and applied to a specific namespace. Does that mean it always\\napplies across all namespaces? No, because that would make them relatively unus-\\nable. After all, system pods must often be allowed to do things that regular pods\\nshouldn’t.\\n Assigning different policies to different users is done through the RBAC mecha-\\nnism described in the previous chapter. The idea is to create as many policies as you\\nneed and make them available to individual users or groups by creating ClusterRole\\nresources and pointing them to the individual policies by name. By binding those\\nClusterRoles to specific users or groups with ClusterRoleBindings, when the Pod-\\nSecurityPolicy Admission Control plugin needs to decide whether to admit a pod defi-\\nnition or not, it will only consider the policies accessible to the user creating the pod. \\n You’ll see how to do this in the next exercise. You’ll start by creating an additional\\nPodSecurityPolicy.\\nCREATING A PODSECURITYPOLICY ALLOWING PRIVILEGED CONTAINERS TO BE DEPLOYED\\nYou’ll create a special PodSecurityPolicy that will allow privileged users to create pods\\nwith privileged containers. The following listing shows the policy’s definition.\\napiVersion: extensions/v1beta1\\nkind: PodSecurityPolicy\\nmetadata:\\n  name: privileged          \\nspec:\\n  privileged: true        \\n  runAsUser:\\n    rule: RunAsAny\\n  fsGroup:\\n    rule: RunAsAny\\n  supplementalGroups:\\n    rule: RunAsAny\\n  seLinux:\\n    rule: RunAsAny\\n  volumes:\\n  - \\'*\\'\\nAfter you post this policy to the API server, you have two policies in the cluster:\\n$ kubectl get psp\\nNAME         PRIV    CAPS   SELINUX    RUNASUSER   FSGROUP    ...  \\ndefault      false   []     RunAsAny   RunAsAny    RunAsAny   ...\\nprivileged   true    []     RunAsAny   RunAsAny    RunAsAny   ...\\nNOTE\\nThe shorthand for PodSecurityPolicy is psp.\\nListing 13.20\\nA PodSecurityPolicy for privileged users: psp-privileged.yaml\\nThe name of this \\npolicy is \"privileged.”\\nIt allows running \\nprivileged containers.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PodSecurityPolicy',\n",
       "    'description': 'a cluster-level resource that defines a set of permissions for pods to run with',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"assigns different PodSecurityPolicies to different users and groups using RBAC mechanism\",\\n    \"destination_entity\": \"ClusterRole\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"makes policies available to individual users or groups by creating ClusterRole resources and pointing them to the individual policies\",\\n    \"destination_entity\": \"PodSecurityPolicy\"\\n  },\\n  {\\n    \"source_entity\": \"PodSecurityPolicy Admission Control plugin\",\\n    \"description\": \"considers the policies accessible to the user creating the pod when deciding whether to admit a pod definition or not\",\\n    \"destination_entity\": \"User\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterRole\",\\n    \"description\": \"binds specific users or groups to the ClusterRole using ClusterRoleBindings\",\\n    \"destination_entity\": \"Group/User\"\\n  },\\n  {\\n    \"source_entity\": \"PodSecurityPolicy Admission Control plugin\",\\n    \"description\": \"decides whether to admit a pod definition based on the policies accessible to the user creating the pod\",\\n    \"destination_entity\": \"PodDefinition\"\\n  },\\n  {\\n    \"source_entity\": \"User/Group\",\\n    \"description\": \"creates pods with privileged containers using the PodSecurityPolicy allowing privileged containers to be deployed\",\\n    \"destination_entity\": \"Pod\"\\n  }\\n]\\n```\\n\\nNote: I\\'ve assumed that \"User\", \"Group\", and \"ClusterRole\" are entities, as they seem to be used as subjects or objects in the context of Kubernetes. If these are not entities, please let me know and I\\'ll adjust the output accordingly.'},\n",
       " {'page': 429,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '397\\nRestricting the use of security-related features in pods\\nAs you can see in the PRIV column, the default policy doesn’t allow running privi-\\nleged containers, whereas the privileged policy does. Because you’re currently\\nlogged in as a cluster-admin, you can see all the policies. When creating pods, if any\\npolicy allows you to deploy a pod with certain features, the API server will accept\\nyour pod.\\n Now imagine two additional users are using your cluster: Alice and Bob. You want\\nAlice to only deploy restricted (non-privileged) pods, but you want to allow Bob to\\nalso deploy privileged pods. You do this by making sure Alice can only use the default\\nPodSecurityPolicy, while allowing Bob to use both.\\nUSING RBAC TO ASSIGN DIFFERENT PODSECURITYPOLICIES TO DIFFERENT USERS\\nIn the previous chapter, you used RBAC to grant users access to only certain resource\\ntypes, but I mentioned that access can be granted to specific resource instances by ref-\\nerencing them by name. That’s what you’ll use to make users use different Pod-\\nSecurityPolicy resources.\\n First, you’ll create two ClusterRoles, each allowing the use of one of the policies.\\nYou’ll call the first one psp-default and in it allow the use of the default Pod-\\nSecurityPolicy resource. You can use kubectl create clusterrole to do that:\\n$ kubectl create clusterrole psp-default --verb=use \\n➥  --resource=podsecuritypolicies --resource-name=default\\nclusterrole \"psp-default\" created\\nNOTE\\nYou’re using the special verb use instead of get, list, watch, or similar.\\nAs you can see, you’re referring to a specific instance of a PodSecurityPolicy resource by\\nusing the --resource-name option. Now, create another ClusterRole called psp-\\nprivileged, pointing to the privileged policy:\\n$ kubectl create clusterrole psp-privileged --verb=use\\n➥  --resource=podsecuritypolicies --resource-name=privileged\\nclusterrole \"psp-privileged\" created\\nNow, you need to bind these two policies to users. As you may remember from the pre-\\nvious chapter, if you’re binding a ClusterRole that grants access to cluster-level\\nresources (which is what PodSecurityPolicy resources are), you need to use a Cluster-\\nRoleBinding instead of a (namespaced) RoleBinding. \\n You’re going to bind the psp-default ClusterRole to all authenticated users, not\\nonly to Alice. This is necessary because otherwise no one could create any pods,\\nbecause the Admission Control plugin would complain that no policy is in place.\\nAuthenticated users all belong to the system:authenticated group, so you’ll bind\\nthe ClusterRole to the group:\\n$ kubectl create clusterrolebinding psp-all-users \\n➥ --clusterrole=psp-default --group=system:authenticated\\nclusterrolebinding \"psp-all-users\" created\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PodSecurityPolicy',\n",
       "    'description': 'A Kubernetes resource that defines a set of security features for pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Privileged containers',\n",
       "    'description': 'Containers that run with root privileges.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Cluster-admin',\n",
       "    'description': 'A Kubernetes user role that has cluster-level permissions.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PodSecurityPolicy resources',\n",
       "    'description': 'Resources that define a set of security features for pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRoles',\n",
       "    'description': 'Kubernetes roles that grant access to cluster-level resources.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'psp-default ClusterRole',\n",
       "    'description': 'A ClusterRole that grants access to the default PodSecurityPolicy resource.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'psp-privileged ClusterRole',\n",
       "    'description': 'A ClusterRole that grants access to the privileged PodSecurityPolicy resource.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRoleBindings',\n",
       "    'description': 'Kubernetes bindings that grant cluster-level permissions to users or groups.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'psp-all-users ClusterRoleBinding',\n",
       "    'description': 'A ClusterRoleBinding that grants access to the default PodSecurityPolicy resource to all authenticated users.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl create clusterrole',\n",
       "    'description': 'A Kubernetes command used to create a ClusterRole.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubectl create clusterrolebinding',\n",
       "    'description': 'A Kubernetes command used to create a ClusterRoleBinding.',\n",
       "    'category': 'command'}],\n",
       "  'relationships': '[{\"source_entity\": \"psp-all-users ClusterRoleBinding\",\"description\": \"binds access to psp-default ClusterRole for all authenticated users\",\"destination_entity\": \"psp-default ClusterRole\"},{\"source_entity\": \"psp-privileged ClusterRole\",\"description\": \"grants use of privileged PodSecurityPolicy resources\",\"destination_entity\": \"PodSecurityPolicy resources\"},{\"source_entity\": \"Cluster-admin\",\"description\": \"has access to all policies, including privileged and default PodSecurityPolicy\",\"destination_entity\": \"PodSecurityPolicy\"},{\"source_entity\": \"kubectl create clusterrolebinding\",\"description\": \"creates a ClusterRoleBinding for psp-all-users ClusterRoleBinding\",\"destination_entity\": \"psp-all-users ClusterRoleBinding\"},{\"source_entity\": \"psp-default ClusterRole\",\"description\": \"grants use of default PodSecurityPolicy resources\",\"destination_entity\": \"PodSecurityPolicy\"},{\"source_entity\": \"ClusterRoles\",\"description\": \" manages ClusterRoleBindings, including psp-all-users\",\"destination_entity\": \"psp-all-users ClusterRoleBinding\"},{\"source_entity\": \"kubectl create clusterrole\",\"description\": \"creates a ClusterRole for psp-privileged\",\"destination_entity\": \"psp-privileged ClusterRole\"},{\"source_entity\": \"Cluster-admin\",\"description\": \"has access to all policies, including privileged PodSecurityPolicy\",\"destination_entity\": \"psp-privileged ClusterRole\"},{\"source_entity\": \"Alice\",\"description\": \"can deploy restricted (non-privileged) pods only\",\"destination_entity\": \"PodSecurityPolicy\"},{\"source_entity\": \"Bob\",\"description\": \"can also deploy privileged pods using psp-privileged ClusterRole\",\"destination_entity\": \"psp-privileged ClusterRole\"},{\"source_entity\": \"psp-default ClusterRole\",\"description\": \"grants use of default PodSecurityPolicy resources to Alice\",\"destination_entity\": \"Alice\"},{\"source_entity\": \"kubectl create clusterrolebinding\",\"description\": \"creates a ClusterRoleBinding for psp-all-users ClusterRoleBinding\",\"destination_entity\": \"psp-all-users ClusterRoleBinding\"},{\"source_entity\": \"ClusterRoles\",\"description\": \"manages ClusterRoleBindings, including psp-all-users\",\"destination_entity\": \"psp-privileged ClusterRole\"},{\"source_entity\": \"psp-privileged ClusterRole\",\"description\": \"grants use of privileged PodSecurityPolicy resources to Bob\",\"destination_entity\": \"Bob\"}]'},\n",
       " {'page': 430,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '398\\nCHAPTER 13\\nSecuring cluster nodes and the network\\nYou’ll bind the psp-privileged ClusterRole only to Bob:\\n$ kubectl create clusterrolebinding psp-bob \\n➥ --clusterrole=psp-privileged --user=bob\\nclusterrolebinding \"psp-bob\" created\\nAs an authenticated user, Alice should now have access to the default PodSecurity-\\nPolicy, whereas Bob should have access to both the default and the privileged Pod-\\nSecurityPolicies. Alice shouldn’t be able to create privileged pods, whereas Bob\\nshould. Let’s see if that’s true.\\nCREATING ADDITIONAL USERS FOR KUBECTL\\nBut how do you authenticate as Alice or Bob instead of whatever you’re authenticated\\nas currently? The book’s appendix A explains how kubectl can be used with multiple\\nclusters, but also with multiple contexts. A context includes the user credentials used\\nfor talking to a cluster. Turn to appendix A to find out more. Here we’ll show the bare\\ncommands enabling you to use kubectl as Alice or Bob. \\n First, you’ll create two new users in kubectl’s config with the following two\\ncommands:\\n$ kubectl config set-credentials alice --username=alice --password=password\\nUser \"alice\" set.\\n$ kubectl config set-credentials bob --username=bob --password=password\\nUser \"bob\" set.\\nIt should be obvious what the commands do. Because you’re setting username and\\npassword credentials, kubectl will use basic HTTP authentication for these two users\\n(other authentication methods include tokens, client certificates, and so on).\\nCREATING PODS AS A DIFFERENT USER\\nYou can now try creating a privileged pod while authenticating as Alice. You can tell\\nkubectl which user credentials to use by using the --user option:\\n$ kubectl --user alice create -f pod-privileged.yaml\\nError from server (Forbidden): error when creating \"pod-privileged.yaml\": \\npods \"pod-privileged\" is forbidden: unable to validate against any pod \\nsecurity policy: [spec.containers[0].securityContext.privileged: Invalid \\nvalue: true: Privileged containers are not allowed]\\nAs expected, the API server doesn’t allow Alice to create privileged pods. Now, let’s see\\nif it allows Bob to do that:\\n$ kubectl --user bob create -f pod-privileged.yaml\\npod \"pod-privileged\" created\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command-line tool for interacting with Kubernetes clusters',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterRole',\n",
       "    'description': 'a Kubernetes role that grants permissions to perform actions on a cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'clusterrolebinding',\n",
       "    'description': 'a Kubernetes object that binds a user or service account to a ClusterRole',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PodSecurityPolicy',\n",
       "    'description': 'a Kubernetes policy that defines allowed and denied containers in pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'a Kubernetes object that represents a running process',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'user credentials',\n",
       "    'description': 'username and password used for basic HTTP authentication with kubectl',\n",
       "    'category': 'software'},\n",
       "   {'entity': '--user option',\n",
       "    'description': 'kubectl flag that specifies which user credentials to use when running a command',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'appendix A',\n",
       "    'description': 'section of the book that explains how to use kubectl with multiple clusters and contexts',\n",
       "    'category': 'documentation'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Bob\", \"description\": \"binds\", \"destination_entity\": \"psp-privileged ClusterRole\"},\\n  {\"source_entity\": \"Alice\", \"description\": \"has access to\", \"destination_entity\": \"default PodSecurity-Policy\"},\\n  {\"source_entity\": \"Bob\", \"description\": \"has access to\", \"destination_entity\": \"both default and privileged Pod-SecurityPolicies\"},\\n  {\"source_entity\": \"Bob\", \"description\": \"can create\", \"destination_entity\": \"privileged pods\"},\\n  {\"source_entity\": \"Alice\", \"description\": \"cannot create\", \"destination_entity\": \"privileged pods\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"can be used with\", \"destination_entity\": \"multiple clusters\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"can be used with\", \"destination_entity\": \"multiple contexts\"},\\n  {\"source_entity\": \"Bob\", \"description\": \"is authenticated as\", \"destination_entity\": \"bob user\"},\\n  {\"source_entity\": \"Alice\", \"description\": \"is authenticated as\", \"destination_entity\": \"alice user\"},\\n  {\"source_entity\": \"kubectl config\", \"description\": \"sets credentials for\", \"destination_entity\": \"alice user\"},\\n  {\"source_entity\": \"kubectl config\", \"description\": \"sets credentials for\", \"destination_entity\": \"bob user\"},\\n  {\"source_entity\": \"--user option\", \"description\": \"specifies the user to authenticate as\", \"destination_entity\": \"Alice user\"},\\n  {\"source_entity\": \"--user option\", \"description\": \"specifies the user to authenticate as\", \"destination_entity\": \"Bob user\"},\\n  {\"source_entity\": \"Appendix A\", \"description\": \"explains how to use kubectl with multiple clusters and contexts\", \"destination_entity\": \"kubectl\"}\\n]\\n```\\n\\nNote: The relations are extracted based on the context of the document page and the list of entities provided. Some of the relations might seem trivial, but they are still extracted as per the given rules.'},\n",
       " {'page': 431,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '399\\nIsolating the pod network\\nAnd there you go. You’ve successfully used RBAC to make the Admission Control\\nplugin use different PodSecurityPolicy resources for different users.\\n13.4\\nIsolating the pod network\\nUp to now in this chapter, we’ve explored many security-related configuration options\\nthat apply to individual pods and their containers. In the remainder of this chapter,\\nwe’ll look at how the network between pods can be secured by limiting which pods can\\ntalk to which pods.\\n Whether this is configurable or not depends on which container networking\\nplugin is used in the cluster. If the networking plugin supports it, you can configure\\nnetwork isolation by creating NetworkPolicy resources. \\n A NetworkPolicy applies to pods that match its label selector and specifies either\\nwhich sources can access the matched pods or which destinations can be accessed\\nfrom the matched pods. This is configured through ingress and egress rules, respec-\\ntively. Both types of rules can match only the pods that match a pod selector, all\\npods in a namespace whose labels match a namespace selector, or a network IP\\nblock specified using Classless Inter-Domain Routing (CIDR) notation (for example,\\n192.168.1.0/24). \\n We’ll look at both ingress and egress rules and all three matching options.\\nNOTE\\nIngress rules in a NetworkPolicy have nothing to do with the Ingress\\nresource discussed in chapter 5.\\n13.4.1 Enabling network isolation in a namespace\\nBy default, pods in a given namespace can be accessed by anyone. First, you’ll need\\nto change that. You’ll create a default-deny NetworkPolicy, which will prevent all\\nclients from connecting to any pod in your namespace. The NetworkPolicy defini-\\ntion is shown in the following listing.\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: default-deny\\nspec:\\n  podSelector:        \\nWhen you create this NetworkPolicy in a certain namespace, no one can connect to\\nany pod in that namespace. \\n \\n \\n \\nListing 13.21\\nA default-deny NetworkPolicy: network-policy-default-deny.yaml\\nEmpty pod selector \\nmatches all pods in the \\nsame namespace\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'RBAC',\n",
       "    'description': 'Role-Based Access Control plugin',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Admission Control',\n",
       "    'description': 'Plugin that checks and modifies incoming requests',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PodSecurityPolicy',\n",
       "    'description': 'Resource for defining security policies for pods',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'NetworkPolicy',\n",
       "    'description': 'Resource for defining network isolation policies',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'ingress rules',\n",
       "    'description': 'Rules for incoming traffic to matched pods',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'egress rules',\n",
       "    'description': 'Rules for outgoing traffic from matched pods',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'pod selector',\n",
       "    'description': 'Selector for matching pods based on labels',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'namespace selector',\n",
       "    'description': 'Selector for matching namespaces based on labels',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Classless Inter-Domain Routing (CIDR)',\n",
       "    'description': 'Notation for specifying network IP blocks',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Ingress resource',\n",
       "    'description': 'Resource for managing incoming traffic',\n",
       "    'category': 'database'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Ingress rules\", \"description\": \"allow access to pods matching its label selector\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"NetworkPolicy\", \"description\": \"specifies which sources can access matched pods or which destinations can be accessed from the matched pods\", \"destination_entity\": \"sources and destinations\"},\\n  {\"source_entity\": \"Classless Inter-Domain Routing (CIDR)\", \"description\": \"specifies a network IP block for matching\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"Egress rules\", \"description\": \"allow access from matched pods to specified destinations\", \"destination_entity\": \"destinations\"},\\n  {\"source_entity\": \"PodSecurityPolicy\", \"description\": \"defines which resources can be used by pods\", \"destination_entity\": \"resources\"},\\n  {\"source_entity\": \"Admission Control\", \"description\": \"uses PodSecurityPolicy resources for different users\", \"destination_entity\": \"users\"},\\n  {\"source_entity\": \"Namespace selector\", \"description\": \"matches all pods in a given namespace\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"Ingress resource\", \"description\": \"has nothing to do with Ingress rules in NetworkPolicy\", \"destination_entity\": \"NetworkPolicy\"},\\n  {\"source_entity\": \"Pod selector\", \"description\": \"matches specific pods for access or egress rules\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"RBAC\", \"description\": \"makes Admission Control use different PodSecurityPolicy resources for different users\", \"destination_entity\": \"users and PodSecurityPolicy\"},\\n  {\"source_entity\": \"NetworkPolicy\", \"description\": \"prevents all clients from connecting to any pod in a namespace\", \"destination_entity\": \"clients and pods\"},\\n  {\"source_entity\": \"Default-deny NetworkPolicy\", \"description\": \"denies access to all pods in a namespace by default\", \"destination_entity\": \"pods and clients\"}\\n]\\n```\\n\\nNote: I\\'ve kept the entities as they are, without modifying them to match the exact wording of the document. If you\\'d like me to modify them, please let me know!'},\n",
       " {'page': 432,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '400\\nCHAPTER 13\\nSecuring cluster nodes and the network\\nNOTE\\nThe CNI plugin or other type of networking solution used in the clus-\\nter must support NetworkPolicy, or else there will be no effect on inter-pod\\nconnectivity.\\n13.4.2 Allowing only some pods in the namespace to connect to \\na server pod\\nTo let clients connect to the pods in the namespace, you must now explicitly say who\\ncan connect to the pods. By who I mean which pods. Let’s explore how to do this\\nthrough an example. \\n Imagine having a PostgreSQL database pod running in namespace foo and a web-\\nserver pod that uses the database. Other pods are also in the namespace, and you\\ndon’t want to allow them to connect to the database. To secure the network, you need\\nto create the NetworkPolicy resource shown in the following listing in the same name-\\nspace as the database pod.\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: postgres-netpolicy\\nspec:\\n  podSelector:                     \\n    matchLabels:                   \\n      app: database                \\n  ingress:                           \\n  - from:                            \\n    - podSelector:                   \\n        matchLabels:                 \\n          app: webserver             \\n    ports:                     \\n    - port: 5432               \\nThe example NetworkPolicy allows pods with the app=webserver label to connect to\\npods with the app=database label, and only on port 5432. Other pods can’t connect to\\nthe database pods, and no one (not even the webserver pods) can connect to anything\\nother than port 5432 of the database pods. This is shown in figure 13.4.\\n Client pods usually connect to server pods through a Service instead of directly to\\nthe pod, but that doesn’t change anything. The NetworkPolicy is enforced when con-\\nnecting through a Service, as well.\\n \\n \\n \\n \\nListing 13.22\\nA NetworkPolicy for the Postgres pod: network-policy-postgres.yaml\\nThis policy secures \\naccess to pods with \\napp=database label.\\nIt allows incoming connections \\nonly from pods with the \\napp=webserver label.\\nConnections to this \\nport are allowed.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'CNI',\n",
       "    'description': 'Container Network Interface plugin or other type of networking solution',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'NetworkPolicy',\n",
       "    'description': 'Resource for securing network connectivity between pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'podSelector',\n",
       "    'description': 'Selector for matching pods based on labels',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ingress',\n",
       "    'description': 'Rule for allowing incoming connections from specific pods or networks',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ports',\n",
       "    'description': 'Ports through which connections can be made to a pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'portSelector',\n",
       "    'description': 'Selector for matching ports based on labels',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubernetes.io/v1',\n",
       "    'description': 'API version for NetworkPolicy resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'postgress-netpolicy',\n",
       "    'description': 'Name of the NetworkPolicy resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'postgres-pod',\n",
       "    'description': 'PostgreSQL database pod running in namespace foo',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'web-server-pod',\n",
       "    'description': 'Web-server pod that uses the PostgreSQL database',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'Logical grouping of resources within a cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'Resource for exposing a service to clients',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ingress\",\\n    \"description\": \"allows incoming connections from specific pods\",\\n    \"destination_entity\": \"postgress-netpolicy\"\\n  },\\n  {\\n    \"source_entity\": \"NetworkPolicy\",\\n    \"description\": \"secures access to pods with specific labels\",\\n    \"destination_entity\": \"postgres-pod\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"connects client pods to server pods through a Service\",\\n    \"destination_entity\": \"web-server-pod\"\\n  },\\n  {\\n    \"source_entity\": \"NetworkPolicy\",\\n    \"description\": \"enforces security rules when connecting through a Service\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"postgres-netpolicy\",\\n    \"description\": \"allows connections only from pods with specific labels\",\\n    \"destination_entity\": \"web-server-pod\"\\n  },\\n  {\\n    \"source_entity\": \"portSelector\",\\n    \"description\": \"selects specific ports for incoming connections\",\\n    \"destination_entity\": \"ports\"\\n  },\\n  {\\n    \"source_entity\": \"kubernetes.io/v1\",\\n    \"description\": \"specifies the API version for NetworkPolicy resources\",\\n    \"destination_entity\": \"NetworkPolicy\"\\n  },\\n  {\\n    \"source_entity\": \"podSelector\",\\n    \"description\": \"selects specific pods based on labels\",\\n    \"destination_entity\": \"postgres-pod\"\\n  },\\n  {\\n    \"source_entity\": \"ports\",\\n    \"description\": \"specifies the ports for incoming connections\",\\n    \"destination_entity\": \"postgres-netpolicy\"\\n  },\\n  {\\n    \"source_entity\": \"web-server-pod\",\\n    \"description\": \"connects to pods in a specific namespace\",\\n    \"destination_entity\": \"namespace\"\\n  },\\n  {\\n    \"source_entity\": \"CNI\",\\n    \"description\": \"supports NetworkPolicy for inter-pod connectivity\",\\n    \"destination_entity\": \"kubernetes.io/v1\"\\n  }\\n]'},\n",
       " {'page': 433,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '401\\nIsolating the pod network\\n13.4.3 Isolating the network between Kubernetes namespaces\\nNow let’s look at another example, where multiple tenants are using the same Kuber-\\nnetes cluster. Each tenant can use multiple namespaces, and each namespace has a\\nlabel specifying the tenant it belongs to. For example, one of those tenants is Man-\\nning. All their namespaces have been labeled with tenant: manning. In one of their\\nnamespaces, they run a Shopping Cart microservice that needs to be available to all\\npods running in any of their namespaces. Obviously, they don’t want any other tenants\\nto access their microservice.\\n To secure their microservice, they create the NetworkPolicy resource shown in the\\nfollowing listing.\\napiVersion: networking.k8s.io/v1\\nkind: NetworkPolicy\\nmetadata:\\n  name: shoppingcart-netpolicy\\nspec:\\n  podSelector:                       \\n    matchLabels:                     \\n      app: shopping-cart             \\n  ingress:\\n  - from:\\n    - namespaceSelector:            \\n        matchLabels:                \\n          tenant: manning           \\n    ports:\\n    - port: 80\\nListing 13.23\\nNetworkPolicy for the shopping cart pod(s): network-policy-cart.yaml\\napp: database\\nPod:\\ndatabase\\nPort\\n5432\\nPort\\n9876\\napp: webserver\\nPod:\\nwebserver\\nPod selector:\\napp=webserver\\nPod selector:\\napp=database\\napp: webserver\\nPod:\\nwebserver\\nOther pods\\nNetworkPolicy: postgres-netpolicy\\nFigure 13.4\\nA NetworkPolicy allowing only some pods to access other pods and only on a specific \\nport\\nThis policy applies to pods \\nlabeled as microservice= \\nshopping-cart.\\nOnly pods running in namespaces \\nlabeled as tenant=manning are \\nallowed to access the microservice.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod network',\n",
       "    'description': 'Network isolation between Kubernetes namespaces',\n",
       "    'category': 'network,application'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': 'Logical isolation of resources in a cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Tenant',\n",
       "    'description': 'Authorized user or organization',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'NetworkPolicy',\n",
       "    'description': 'Resource for securing network traffic',\n",
       "    'category': 'network,application'},\n",
       "   {'entity': 'PodSelector',\n",
       "    'description': 'Selector for pods based on labels',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Ingress',\n",
       "    'description': 'Rule for allowing incoming traffic to a pod',\n",
       "    'category': 'network,application'},\n",
       "   {'entity': 'Ports',\n",
       "    'description': 'Endpoint for network communication',\n",
       "    'category': 'network,application'},\n",
       "   {'entity': 'Postgres-netpolicy',\n",
       "    'description': 'NetworkPolicy resource example',\n",
       "    'category': 'network,application'},\n",
       "   {'entity': 'Microservice',\n",
       "    'description': 'Small, independent application component',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Shopping cart microservice',\n",
       "    'description': 'Example microservice component',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'Containerization platform',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"NetworkPolicy\", \"description\": \"specifies the pods that can access a microservice\", \"destination_entity\": \"Microservice\"}, \\n {\"source_entity\": \"Kubernetes\", \"description\": \"manages multiple namespaces for different tenants\", \"destination_entity\": \"Namespace\"}, \\n {\"source_entity\": \"Tenant\", \"description\": \"uses Kubernetes cluster with multiple namespaces labeled with tenant ID\", \"destination_entity\": \"Namespace\"}, \\n {\"source_entity\": \"PodSelector\", \"description\": \"selects pods running in a specific namespace\", \"destination_entity\": \"Namespace\"}, \\n {\"source_entity\": \"Ingress\", \"description\": \"allows incoming traffic from specific pods to access microservice\", \"destination_entity\": \"Microservice\"}, \\n {\"source_entity\": \"Ports\", \"description\": \"specifies the ports through which microservice can be accessed\", \"destination_entity\": \"Microservice\"}, \\n {\"source_entity\": \"Shopping cart microservice\", \"description\": \"needs to be available to all pods running in any of their namespaces\", \"destination_entity\": \"Pod\"}, \\n {\"source_entity\": \"Shopping cart microservice\", \"description\": \"accessed by only pods running in namespaces labeled as tenant=manning\", \"destination_entity\": \"Tenant\"}, \\n {\"source_entity\": \"NetworkPolicy\", \"description\": \"applies to pods labeled as microservice=shopping-cart\", \"destination_entity\": \"Pod\"}, \\n {\"source_entity\": \"Postgres-netpolicy\", \"description\": \"allows only some pods to access other pods and only on a specific port\", \"destination_entity\": \"Pod network\"}, \\n {\"source_entity\": \"Docker\", \"description\": \"used for managing containers running in Kubernetes cluster\", \"destination_entity\": \"Kubernetes\"}, \\n {\"source_entity\": \"Namespace\", \"description\": \"labeled with tenant ID to identify different tenants\", \"destination_entity\": \"Tenant\"}]'},\n",
       " {'page': 434,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '402\\nCHAPTER 13\\nSecuring cluster nodes and the network\\nThis NetworkPolicy ensures only pods running in namespaces labeled as tenant:\\nmanning can access their Shopping Cart microservice, as shown in figure 13.5.\\nIf the shopping cart provider also wants to give access to other tenants (perhaps to\\none of their partner companies), they can either create an additional NetworkPolicy\\nresource or add an additional ingress rule to their existing NetworkPolicy.\\nNOTE\\nIn a multi-tenant Kubernetes cluster, tenants usually can’t add labels\\n(or annotations) to their namespaces themselves. If they could, they’d be able\\nto circumvent the namespaceSelector-based ingress rules.\\n13.4.4 Isolating using CIDR notation\\nInstead of specifying a pod- or namespace selector to define who can access the pods\\ntargeted in the NetworkPolicy, you can also specify an IP block in CIDR notation. For\\nexample, to allow the shopping-cart pods from the previous section to only be acces-\\nsible from IPs in the 192.168.1.1 to .255 range, you’d specify the ingress rule in the\\nnext listing.\\n  ingress:\\n  - from:\\n    - ipBlock:                    \\n        cidr: 192.168.1.0/24      \\nListing 13.24\\nSpecifying an IP block in an ingress rule: network-policy-cidr.yaml\\napp: shopping-cart\\nPod:\\nshopping-cart\\nPort\\n80\\nNamespace selector:\\ntenant=manning\\nPod selector:\\napp=shopping-cart\\nOther pods\\nPods\\nNetworkPolicy:\\nshoppingcart-netpolicy\\nNamespace: manningA\\nNamespace: ecommerce-ltd\\nOther namespaces\\ntenant: manning\\nPods\\nNamespace: manningB\\ntenant: manning\\nFigure 13.5\\nA NetworkPolicy only allowing pods in namespaces matching a namespaceSelector to access a \\nspecific pod.\\nThis ingress rule only allows traffic from \\nclients in the 192.168.1.0/24 IP block. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'NetworkPolicy',\n",
       "    'description': 'a Kubernetes resource that defines network connectivity rules for pods within a cluster',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'NamespaceSelector',\n",
       "    'description': 'a way to select namespaces based on labels or annotations',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'PodSelector',\n",
       "    'description': 'a way to select pods based on labels or annotations',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'CIDR notation',\n",
       "    'description': 'a way to specify an IP block in a network policy',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'IPBlock',\n",
       "    'description': 'a way to specify an IP block in a network policy',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'ingress rule',\n",
       "    'description': 'a rule that defines incoming traffic for a NetworkPolicy',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'shopping-cart',\n",
       "    'description': 'a microservice or application running in the cluster',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'an instance of a container running in the cluster',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'cluster node',\n",
       "    'description': 'a machine or virtual machine running in the Kubernetes cluster',\n",
       "    'category': 'hardware/computer'},\n",
       "   {'entity': 'network',\n",
       "    'description': 'the network infrastructure that connects the cluster nodes',\n",
       "    'category': 'hardware/network'}],\n",
       "  'relationships': '[\\n    {\\n        \"source_entity\": \"Shopping cart provider\",\\n        \"description\": \"create an additional NetworkPolicy resource or add an additional ingress rule to their existing NetworkPolicy\",\\n        \"destination_entity\": \"other tenants\"\\n    },\\n    {\\n        \"source_entity\": \"tenants\",\\n        \"description\": \"can’t add labels (or annotations) to their namespaces themselves\",\\n        \"destination_entity\": \"namespaceSelector-based ingress rules\"\\n    },\\n    {\\n        \"source_entity\": \"shopping-cart pods\",\\n        \"description\": \"only be accessible from IPs in the 192.168.1.0/24 range\",\\n        \"destination_entity\": \"IPs in the 192.168.1.0/24 range\"\\n    },\\n    {\\n        \"source_entity\": \"NetworkPolicy\",\\n        \"description\": \"ensure only pods running in namespaces labeled as tenant: manning can access their Shopping Cart microservice\",\\n        \"destination_entity\": \"pods running in namespaces labeled as tenant: manning\"\\n    },\\n    {\\n        \"source_entity\": \"PodSelector\",\\n        \"description\": \"select specific pods to be accessed by a NetworkPolicy\",\\n        \"destination_entity\": \"specific pods\"\\n    },\\n    {\\n        \"source_entity\": \"NamespaceSelector\",\\n        \"description\": \"select specific namespaces to be accessed by a NetworkPolicy\",\\n        \"destination_entity\": \"specific namespaces\"\\n    },\\n    {\\n        \"source_entity\": \"ingress rule\",\\n        \"description\": \"allow traffic from specific clients to access a pod\",\\n        \"destination_entity\": \"clients in the 192.168.1.0/24 IP block\"\\n    },\\n    {\\n        \"source_entity\": \"CIDR notation\",\\n        \"description\": \"specify an IP block in CIDR notation to allow access to a pod\",\\n        \"destination_entity\": \"IPs in the 192.168.1.0/24 range\"\\n    },\\n    {\\n        \"source_entity\": \"Cluster node\",\\n        \"description\": \"be secured by a NetworkPolicy to prevent unauthorized access\",\\n        \"destination_entity\": \"unauthorized users or systems\"\\n    },\\n    {\\n        \"source_entity\": \"NetworkPolicy\",\\n        \"description\": \"isolate a pod from other pods and networks using CIDR notation\",\\n        \"destination_entity\": \"other pods and networks\"\\n    }\\n]\\n```\\n\\nNote: I\\'ve assumed that the shopping cart provider is an entity that can perform actions, whereas the shopping-cart pods and clients are entities that are acted upon. If this is not correct, please let me know and I\\'ll adjust the relations accordingly.'},\n",
       " {'page': 435,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '403\\nSummary\\n13.4.5 Limiting the outbound traffic of a set of pods\\nIn all previous examples, you’ve been limiting the inbound traffic to the pods that\\nmatch the NetworkPolicy’s pod selector using ingress rules, but you can also limit\\ntheir outbound traffic through egress rules. An example is shown in the next listing.\\nspec:\\n  podSelector:               \\n    matchLabels:             \\n      app: webserver         \\n  egress:               \\n  - to:                       \\n    - podSelector:            \\n        matchLabels:          \\n          app: database       \\nThe NetworkPolicy in the previous listing allows pods that have the app=webserver\\nlabel to only access pods that have the app=database label and nothing else (neither\\nother pods, nor any other IP, regardless of whether it’s internal or external to the\\ncluster).\\n13.5\\nSummary\\nIn this chapter, you learned about securing cluster nodes from pods and pods from\\nother pods. You learned that\\n\\uf0a1Pods can use the node’s Linux namespaces instead of using their own.\\n\\uf0a1Containers can be configured to run as a different user and/or group than the\\none defined in the container image.\\n\\uf0a1Containers can also run in privileged mode, allowing them to access the node’s\\ndevices that are otherwise not exposed to pods.\\n\\uf0a1Containers can be run as read-only, preventing processes from writing to the\\ncontainer’s filesystem (and only allowing them to write to mounted volumes).\\n\\uf0a1Cluster-level PodSecurityPolicy resources can be created to prevent users from\\ncreating pods that could compromise a node.\\n\\uf0a1PodSecurityPolicy resources can be associated with specific users using RBAC’s\\nClusterRoles and ClusterRoleBindings.\\n\\uf0a1NetworkPolicy resources are used to limit a pod’s inbound and/or outbound\\ntraffic.\\nIn the next chapter, you’ll learn how computational resources available to pods can be\\nconstrained and how a pod’s quality of service is configured.\\nListing 13.25\\nUsing egress rules in a NetworkPolicy: network-policy-egress.yaml\\nThis policy applies to pods with \\nthe app=webserver label.\\nIt limits\\nthe pods’\\noutbound\\ntraffic.\\nWebserver pods may only \\nconnect to pods with the \\napp=database label.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'NetworkPolicy',\n",
       "    'description': \"a resource used to limit a pod's inbound and/or outbound traffic\",\n",
       "    'category': 'application'},\n",
       "   {'entity': 'podSelector',\n",
       "    'description': 'a selector used to match pods in a NetworkPolicy',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'egress rules',\n",
       "    'description': \"rules that allow or deny pods' outbound traffic\",\n",
       "    'category': 'networking'},\n",
       "   {'entity': 'to',\n",
       "    'description': 'a directive in an egress rule that specifies the target of the traffic',\n",
       "    'category': 'networking'},\n",
       "   {'entity': 'podSelector (in egress rule)',\n",
       "    'description': 'a selector used to match pods in an egress rule',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'matchLabels',\n",
       "    'description': 'a directive in a podSelector that specifies the labels to match',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'app=webserver',\n",
       "    'description': 'a label applied to pods that are part of the web server application',\n",
       "    'category': 'application'},\n",
       "   {'entity': \"node's Linux namespaces\",\n",
       "    'description': \"a feature that allows pods to use the node's Linux namespaces instead of their own\",\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'container run as a different user and/or group',\n",
       "    'description': 'a configuration option that allows containers to run as a different user and/or group than the one defined in the container image',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'privileged mode',\n",
       "    'description': \"a configuration option that allows containers to access the node's devices that are otherwise not exposed to pods\",\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'read-only container',\n",
       "    'description': \"a feature that prevents processes from writing to the container's filesystem (and only allowing them to write to mounted volumes)\",\n",
       "    'category': 'application'},\n",
       "   {'entity': 'PodSecurityPolicy resources',\n",
       "    'description': 'resources used to prevent users from creating pods that could compromise a node',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ClusterRoles and ClusterRoleBindings',\n",
       "    'description': 'RBAC resources used to associate PodSecurityPolicy resources with specific users',\n",
       "    'category': 'networking'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"NetworkPolicy\",\\n    \"description\": \"limits outbound traffic to pods with matching labels\",\\n    \"destination_entity\": \"pods with app=webserver label\"\\n  },\\n  {\\n    \"source_entity\": \"egress rules\",\\n    \"description\": \"allow only access to specific pods\",\\n    \"destination_entity\": \"pods with app=database label\"\\n  },\\n  {\\n    \"source_entity\": \"node\\'s Linux namespaces\",\\n    \"description\": \"use instead of pod\\'s own namespace\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"privileged mode\",\\n    \"description\": \"access node\\'s devices not exposed to pods\",\\n    \"destination_entity\": \"containers\"\\n  },\\n  {\\n    \"source_entity\": \"matchLabels\",\\n    \"description\": \"select pods based on labels\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"NetworkPolicy\",\\n    \"description\": \"limit inbound and/or outbound traffic of a pod\",\\n    \"destination_entity\": \"a pod\"\\n  },\\n  {\\n    \"source_entity\": \"container run as a different user and/or group\",\\n    \"description\": \"configure container to run with a different user/group\",\\n    \"destination_entity\": \"containers\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterRoles and ClusterRoleBindings\",\\n    \"description\": \"associate PodSecurityPolicy resources with specific users\",\\n    \"destination_entity\": \"users\"\\n  },\\n  {\\n    \"source_entity\": \"PodSecurityPolicy resources\",\\n    \"description\": \"prevent users from creating compromising pods\",\\n    \"destination_entity\": \"users\"\\n  },\\n  {\\n    \"source_entity\": \"podSelector (in egress rule)\",\\n    \"description\": \"select specific pods to limit outbound traffic\",\\n    \"destination_entity\": \"egress rules\"\\n  },\\n  {\\n    \"source_entity\": \"read-only container\",\\n    \"description\": \"prevent processes from writing to the container\\'s filesystem\",\\n    \"destination_entity\": \"containers\"\\n  }\\n]'},\n",
       " {'page': 436,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '404\\nManaging pods’\\n computational resources\\nUp to now you’ve created pods without caring about how much CPU and memory\\nthey’re allowed to consume. But as you’ll see in this chapter, setting both how\\nmuch a pod is expected to consume and the maximum amount it’s allowed to con-\\nsume is a vital part of any pod definition. Setting these two sets of parameters\\nmakes sure that a pod takes only its fair share of the resources provided by the\\nKubernetes cluster and also affects how pods are scheduled across the cluster.\\nThis chapter covers\\n\\uf0a1Requesting CPU, memory, and other \\ncomputational resources for containers\\n\\uf0a1Setting a hard limit for CPU and memory\\n\\uf0a1Understanding Quality of Service guarantees for \\npods\\n\\uf0a1Setting default, min, and max resources for pods \\nin a namespace\\n\\uf0a1Limiting the total amount of resources available \\nin a namespace\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pods',\n",
       "    'description': 'a vital part of any pod definition',\n",
       "    'category': 'Container'},\n",
       "   {'entity': 'CPU',\n",
       "    'description': 'computational resource for containers',\n",
       "    'category': 'Resource'},\n",
       "   {'entity': 'Memory',\n",
       "    'description': 'computational resource for containers',\n",
       "    'category': 'Resource'},\n",
       "   {'entity': 'Kubernetes cluster',\n",
       "    'description': 'a group of machines that run containerized applications',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': 'a logical grouping of resources within a Kubernetes cluster',\n",
       "    'category': 'Resource Management'},\n",
       "   {'entity': 'Resources',\n",
       "    'description': 'CPU, memory, and other computational resources for containers',\n",
       "    'category': 'Resource'},\n",
       "   {'entity': 'Containers',\n",
       "    'description': 'the smallest deployable unit in a Kubernetes application',\n",
       "    'category': 'Application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes cluster\",\\n    \"description\": \"provides computational resources to\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"consume computational resources from\",\\n    \"destination_entity\": \"Kubernetes cluster\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"are scheduled across\",\\n    \"destination_entity\": \"Kubernetes cluster\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"request CPU and memory resources from\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"set a hard limit for CPU and memory usage of\",\\n    \"destination_entity\": \"containers\"\\n  },\\n  {\\n    \"source_entity\": \"Namespace\",\\n    \"description\": \"sets default, min, and max resources for\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"Namespace\",\\n    \"description\": \"limits the total amount of resources available to\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"take only their fair share of computational resources from\",\\n    \"destination_entity\": \"Kubernetes cluster\"\\n  }\\n]'},\n",
       " {'page': 437,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '405\\nRequesting resources for a pod’s containers\\n14.1\\nRequesting resources for a pod’s containers\\nWhen creating a pod, you can specify the amount of CPU and memory that a con-\\ntainer needs (these are called requests) and a hard limit on what it may consume\\n(known as limits). They’re specified for each container individually, not for the pod as\\na whole. The pod’s resource requests and limits are the sum of the requests and lim-\\nits of all its containers. \\n14.1.1 Creating pods with resource requests\\nLet’s look at an example pod manifest, which has the CPU and memory requests spec-\\nified for its single container, as shown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: requests-pod\\nspec:\\n  containers:\\n  - image: busybox\\n    command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"]\\n    name: main              \\n    resources:              \\n      requests:             \\n        cpu: 200m          \\n        memory: 10Mi    \\nIn the pod manifest, your single container requires one-fifth of a CPU core (200 mil-\\nlicores) to run properly. Five such pods/containers can run sufficiently fast on a single\\nCPU core. \\n When you don’t specify a request for CPU, you’re saying you don’t care how much\\nCPU time the process running in your container is allotted. In the worst case, it may\\nnot get any CPU time at all (this happens when a heavy demand by other processes\\nexists on the CPU). Although this may be fine for low-priority batch jobs, which aren’t\\ntime-critical, it obviously isn’t appropriate for containers handling user requests.\\n In the pod spec, you’re also requesting 10 mebibytes of memory for the container.\\nBy doing that, you’re saying that you expect the processes running inside the con-\\ntainer to use at most 10 mebibytes of RAM. They might use less, but you’re not expect-\\ning them to use more than that in normal circumstances. Later in this chapter you’ll\\nsee what happens if they do.\\n Now you’ll run the pod. When the pod starts, you can take a quick look at the pro-\\ncess’ CPU consumption by running the top command inside the container, as shown\\nin the following listing.\\nListing 14.1\\nA pod with resource requests: requests-pod.yaml\\nYou’re specifying resource \\nrequests for the main container.\\nThe container requests 200 \\nmillicores (that is, 1/5 of a \\nsingle CPU core’s time).\\nThe container also\\nrequests 10 mebibytes\\nof memory.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'CPU',\n",
       "    'description': 'Central Processing Unit, responsible for executing instructions and performing calculations.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'memory',\n",
       "    'description': 'Random Access Memory, a type of computer storage that temporarily holds data and applications.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A collection of one or more containers running as a single unit in Kubernetes.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A lightweight and standalone executable package that bundles an application and its dependencies.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'requests-pod',\n",
       "    'description': 'An example pod manifest with CPU and memory requests specified for its single container.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'busybox',\n",
       "    'description': 'A small Linux distribution that provides a minimal set of tools for running containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'dd',\n",
       "    'description': 'A command-line utility for creating and copying disk images.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'if=/dev/zero',\n",
       "    'description': 'A shell syntax specifying the input file for a command, in this case /dev/zero.',\n",
       "    'category': 'syntax'},\n",
       "   {'entity': 'of=/dev/null',\n",
       "    'description': 'A shell syntax specifying the output file for a command, in this case /dev/null.',\n",
       "    'category': 'syntax'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'A field in the pod manifest that specifies the API version of the Kubernetes object.',\n",
       "    'category': 'config'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'A field in the pod manifest that specifies the type of Kubernetes object being created.',\n",
       "    'category': 'config'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'A field in the pod manifest that provides metadata about the pod, such as its name and labels.',\n",
       "    'category': 'config'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'A field in the pod manifest that specifies the desired state of the pod, including its containers and resources.',\n",
       "    'category': 'config'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'A field in the pod manifest that lists the containers running within the pod.',\n",
       "    'category': 'config'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'A field in the container specification that specifies the Docker image to use for the container.',\n",
       "    'category': 'config'},\n",
       "   {'entity': 'command',\n",
       "    'description': 'A field in the container specification that specifies the command to run within the container.',\n",
       "    'category': 'config'},\n",
       "   {'entity': 'resources',\n",
       "    'description': 'A field in the container specification that specifies the resources requested by the container, such as CPU and memory.',\n",
       "    'category': 'config'},\n",
       "   {'entity': 'requests',\n",
       "    'description': 'A field in the container specification that specifies the amount of resources to request for the container.',\n",
       "    'category': 'config'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"container\",\\n    \"description\": \"requires CPU and memory for proper functioning\",\\n    \"destination_entity\": \"CPU\"\\n  },\\n  {\\n    \"source_entity\": \"container\",\\n    \"description\": \"requests a specific amount of CPU time\",\\n    \"destination_entity\": \"requests-pod\"\\n  },\\n  {\\n    \"source_entity\": \"container\",\\n    \"description\": \"requires a certain amount of memory for normal functioning\",\\n    \"destination_entity\": \"memory\"\\n  },\\n  {\\n    \"source_entity\": \"command\",\\n    \"description\": \"runs the process inside the container\",\\n    \"destination_entity\": \"process\"\\n  },\\n  {\\n    \"source_entity\": \"busybox\",\\n    \"description\": \"is used as the image for the container\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"if=/dev/zero\",\\n    \"description\": \"provides data to the process running in the container\",\\n    \"destination_entity\": \"process\"\\n  },\\n  {\\n    \"source_entity\": \"of=/dev/null\",\\n    \"description\": \"is the output destination for the process running in the container\",\\n    \"destination_entity\": \"process\"\\n  },\\n  {\\n    \"source_entity\": \"spec\",\\n    \"description\": \"specifies the resources required by the pod\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"requests-pod\",\\n    \"description\": \"is a specific instance of a pod with resource requests specified\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"resources\",\\n    \"description\": \"specifies the amount of CPU and memory requested by the container\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"requests-pod.yaml\",\\n    \"description\": \"is a YAML file specifying resource requests for the pod\",\\n    \"destination_entity\": \"pod\"\\n  }\\n]\\n```'},\n",
       " {'page': 438,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '406\\nCHAPTER 14\\nManaging pods’ computational resources\\n$ kubectl exec -it requests-pod top\\nMem: 1288116K used, 760368K free, 9196K shrd, 25748K buff, 814840K cached\\nCPU:  9.1% usr 42.1% sys  0.0% nic 48.4% idle  0.0% io  0.0% irq  0.2% sirq\\nLoad average: 0.79 0.52 0.29 2/481 10\\n  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\\n    1     0 root     R     1192  0.0   1 50.2 dd if /dev/zero of /dev/null\\n    7     0 root     R     1200  0.0   0  0.0 top\\nThe dd command you’re running in the container consumes as much CPU as it can,\\nbut it only runs a single thread so it can only use a single core. The Minikube VM,\\nwhich is where this example is running, has two CPU cores allotted to it. That’s why\\nthe process is shown consuming 50% of the whole CPU. \\n Fifty percent of two cores is obviously one whole core, which means the container\\nis using more than the 200 millicores you requested in the pod specification. This is\\nexpected, because requests don’t limit the amount of CPU a container can use. You’d\\nneed to specify a CPU limit to do that. You’ll try that later, but first, let’s see how spec-\\nifying resource requests in a pod affects the scheduling of the pod.\\n14.1.2 Understanding how resource requests affect scheduling\\nBy specifying resource requests, you’re specifying the minimum amount of resources\\nyour pod needs. This information is what the Scheduler uses when scheduling the pod\\nto a node. Each node has a certain amount of CPU and memory it can allocate to\\npods. When scheduling a pod, the Scheduler will only consider nodes with enough\\nunallocated resources to meet the pod’s resource requirements. If the amount of\\nunallocated CPU or memory is less than what the pod requests, Kubernetes will not\\nschedule the pod to that node, because the node can’t provide the minimum amount\\nrequired by the pod.\\nUNDERSTANDING HOW THE SCHEDULER DETERMINES IF A POD CAN FIT ON A NODE\\nWhat’s important and somewhat surprising here is that the Scheduler doesn’t look at\\nhow much of each individual resource is being used at the exact time of scheduling\\nbut at the sum of resources requested by the existing pods deployed on the node.\\nEven though existing pods may be using less than what they’ve requested, scheduling\\nanother pod based on actual resource consumption would break the guarantee given\\nto the already deployed pods.\\n This is visualized in figure 14.1. Three pods are deployed on the node. Together,\\nthey’ve requested 80% of the node’s CPU and 60% of the node’s memory. Pod D,\\nshown at the bottom right of the figure, cannot be scheduled onto the node because it\\nrequests 25% of the CPU, which is more than the 20% of unallocated CPU. The fact\\nthat the three pods are currently using only 70% of the CPU makes no difference.\\nListing 14.2\\nExamining CPU and memory usage from within a container\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 439,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '407\\nRequesting resources for a pod’s containers\\nUNDERSTANDING HOW THE SCHEDULER USES PODS’ REQUESTS WHEN SELECTING THE BEST NODE \\nFOR A POD\\nYou may remember from chapter 11 that the Scheduler first filters the list of nodes to\\nexclude those that the pod can’t fit on and then prioritizes the remaining nodes per the\\nconfigured prioritization functions. Among others, two prioritization functions rank\\nnodes based on the amount of resources requested: LeastRequestedPriority and\\nMostRequestedPriority. The first one prefers nodes with fewer requested resources\\n(with a greater amount of unallocated resources), whereas the second one is the exact\\nopposite—it prefers nodes that have the most requested resources (a smaller amount of\\nunallocated CPU and memory). But, as we’ve discussed, they both consider the amount\\nof requested resources, not the amount of resources actually consumed.\\n The Scheduler is configured to use only one of those functions. You may wonder\\nwhy anyone would want to use the MostRequestedPriority function. After all, if you\\nhave a set of nodes, you usually want to spread CPU load evenly across them. However,\\nthat’s not the case when running on cloud infrastructure, where you can add and\\nremove nodes whenever necessary. By configuring the Scheduler to use the Most-\\nRequestedPriority function, you guarantee that Kubernetes will use the smallest pos-\\nsible number of nodes while still providing each pod with the amount of CPU/memory\\nit requests. By keeping pods tightly packed, certain nodes are left vacant and can be\\nremoved. Because you’re paying for individual nodes, this saves you money.\\nINSPECTING A NODE’S CAPACITY\\nLet’s see the Scheduler in action. You’ll deploy another pod with four times the\\namount of requested resources as before. But before you do that, let’s see your node’s\\ncapacity. Because the Scheduler needs to know how much CPU and memory each\\nnode has, the Kubelet reports this data to the API server, making it available through\\nPod C\\nNode\\nPod A\\nUnallocated\\nCPU requests\\nPod B\\nPod A\\nCurrently unused\\nCPU usage\\nPod B\\nPod C\\n0%\\n100%\\nPod A\\nMemory requests\\nPod B\\nPod C\\nPod A\\nMemory usage\\nPod B\\nPod C\\nCPU requests\\nMemory requests\\nUnallocated\\nCurrently unused\\nPod D\\nPod D cannot be scheduled; its CPU\\nrequests exceed unallocated CPU\\nFigure 14.1\\nThe Scheduler only cares about requests, not actual usage.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [   Pod D cannot be scheduled; its CPU\\nrequests exceed unallocated CPU   Col1  \\\n",
       "   0   Node\\nCPU requests Pod A Pod B Pod C Unallocat...                    None   \n",
       "   1                                                None                   Pod A   \n",
       "   2                                                None                    None   \n",
       "   3                                                None                   Pod A   \n",
       "   4                                                None                    None   \n",
       "   5                                                None                   Pod A   \n",
       "   6                                                None                    None   \n",
       "   7                                                None                   Pod A   \n",
       "   8                                                None                    None   \n",
       "   9                                               Pod D                    None   \n",
       "   10                                               None                    None   \n",
       "   11                                               None                    None   \n",
       "   \n",
       "        Col2   Col3   Col4   Col5   Col6  Col7  Col8             Col9 Col10  \\\n",
       "   0    None   None   None   None   None  None  None             None  None   \n",
       "   1    None   None  Pod B   None  Pod C  None  None             None  None   \n",
       "   2    None   None   None   None   None  None  None             None  None   \n",
       "   3    None  Pod B   None  Pod C   None  None  None             None         \n",
       "   4    None   None   None   None   None  None  None             None  None   \n",
       "   5    None  Pod B   None   None  Pod C  None            Unallocated  None   \n",
       "   6    None   None   None   None   None  None  None                   None   \n",
       "   7   Pod B   None   None  Pod C   None        None         Currentl  None   \n",
       "   8    None   None   None   None   None  None  None                   None   \n",
       "   9    None   None   None   None   None  None  None                   None   \n",
       "   10   None   None   None   None   None  None  None             None  None   \n",
       "   11   None   None   None   None   None  None  None  Memory requests  None   \n",
       "   \n",
       "                  Col11        Col12 Col13  \n",
       "   0               None         None  None  \n",
       "   1                     Unallocated  None  \n",
       "   2                            None  None  \n",
       "   3   Currently unused         None  None  \n",
       "   4                            None  None  \n",
       "   5               None         None  None  \n",
       "   6                            None  None  \n",
       "   7           y unused         None  None  \n",
       "   8               None         None  None  \n",
       "   9                            None        \n",
       "   10      CPU requests         None  None  \n",
       "   11              None         None  None  ]},\n",
       " {'page': 440,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '408\\nCHAPTER 14\\nManaging pods’ computational resources\\nthe Node resource. You can see it by using the kubectl describe command as in the\\nfollowing listing.\\n$ kubectl describe nodes\\nName:       minikube\\n...\\nCapacity:                       \\n  cpu:           2               \\n  memory:        2048484Ki       \\n  pods:          110             \\nAllocatable:                       \\n  cpu:           2                  \\n  memory:        1946084Ki          \\n  pods:          110                \\n...\\nThe output shows two sets of amounts related to the available resources on the node:\\nthe node’s capacity and allocatable resources. The capacity represents the total resources\\nof a node, which may not all be available to pods. Certain resources may be reserved\\nfor Kubernetes and/or system components. The Scheduler bases its decisions only on\\nthe allocatable resource amounts.\\n In the previous example, the node called minikube runs in a VM with two cores\\nand has no CPU reserved, making the whole CPU allocatable to pods. Therefore,\\nthe Scheduler should have no problem scheduling another pod requesting 800\\nmillicores. \\n Run the pod now. You can use the YAML file in the code archive, or run the pod\\nwith the kubectl run command like this:\\n$ kubectl run requests-pod-2 --image=busybox --restart Never\\n➥ --requests=\\'cpu=800m,memory=20Mi\\' -- dd if=/dev/zero of=/dev/null\\npod \"requests-pod-2\" created\\nLet’s see if it was scheduled:\\n$ kubectl get po requests-pod-2\\nNAME             READY     STATUS    RESTARTS   AGE\\nrequests-pod-2   1/1       Running   0          3m\\nOkay, the pod has been scheduled and is running. \\nCREATING A POD THAT DOESN’T FIT ON ANY NODE\\nYou now have two pods deployed, which together have requested a total of 1,000 mil-\\nlicores or exactly 1 core. You should therefore have another 1,000 millicores available\\nfor additional pods, right? You can deploy another pod with a resource request of\\n1,000 millicores. Use a similar command as before:\\n$ kubectl run requests-pod-3 --image=busybox --restart Never\\n➥ --requests=\\'cpu=1,memory=20Mi\\' -- dd if=/dev/zero of=/dev/null\\npod \"requests-pod-2\" created\\nListing 14.3\\nA node’s capacity and allocatable resources\\nThe overall capacity \\nof the node\\nThe resources \\nallocatable to pods\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 441,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '409\\nRequesting resources for a pod’s containers\\nNOTE\\nThis time you’re specifying the CPU request in whole cores (cpu=1)\\ninstead of millicores (cpu=1000m).\\nSo far, so good. The pod has been accepted by the API server (you’ll remember from\\nthe previous chapter that the API server can reject pods if they’re invalid in any way).\\nNow, check if the pod is running:\\n$ kubectl get po requests-pod-3\\nNAME             READY     STATUS    RESTARTS   AGE\\nrequests-pod-3   0/1       Pending   0          4m\\nEven if you wait a while, the pod is still stuck at Pending. You can see more informa-\\ntion on why that’s the case by using the kubectl describe command, as shown in\\nthe following listing.\\n$ kubectl describe po requests-pod-3\\nName:       requests-pod-3\\nNamespace:  default\\nNode:       /                    \\n...\\nConditions:\\n  Type           Status\\n  PodScheduled   False           \\n...\\nEvents:\\n... Warning  FailedScheduling    No nodes are available      \\n                                 that match all of the       \\n                                 following predicates::      \\n                                 Insufficient cpu (1).       \\nThe output shows that the pod hasn’t been scheduled because it can’t fit on any node\\ndue to insufficient CPU on your single node. But why is that? The sum of the CPU\\nrequests of all three pods equals 2,000 millicores or exactly two cores, which is exactly\\nwhat your node can provide. What’s wrong?\\nDETERMINING WHY A POD ISN’T BEING SCHEDULED\\nYou can figure out why the pod isn’t being scheduled by inspecting the node resource.\\nUse the kubectl describe node command again and examine the output more\\nclosely in the following listing.\\n$ kubectl describe node\\nName:                   minikube\\n...\\nNon-terminated Pods:    (7 in total)\\n  Namespace    Name            CPU Requ.   CPU Lim.  Mem Req.    Mem Lim.\\n  ---------    ----            ----------  --------  ---------   --------\\n  default      requests-pod    200m (10%)  0 (0%)    10Mi (0%)   0 (0%)\\nListing 14.4\\nExamining why a pod is stuck at Pending with kubectl describe pod\\nListing 14.5\\nInspecting allocated resources on a node with kubectl describe node\\nNo node is \\nassociated \\nwith the pod.\\nThe pod hasn’t \\nbeen scheduled.\\nScheduling has \\nfailed because of \\ninsufficient CPU.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 442,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '410\\nCHAPTER 14\\nManaging pods’ computational resources\\n  default      requests-pod-2  800m (40%)  0 (0%)    20Mi (1%)   0 (0%)\\n  kube-system  dflt-http-b...  10m (0%)    10m (0%)  20Mi (1%)   20Mi (1%)\\n  kube-system  kube-addon-...  5m (0%)     0 (0%)    50Mi (2%)   0 (0%)\\n  kube-system  kube-dns-26...  260m (13%)  0 (0%)    110Mi (5%)  170Mi (8%)\\n  kube-system  kubernetes-...  0 (0%)      0 (0%)    0 (0%)      0 (0%)\\n  kube-system  nginx-ingre...  0 (0%)      0 (0%)    0 (0%)      0 (0%)\\nAllocated resources:\\n  (Total limits may be over 100 percent, i.e., overcommitted.)\\n  CPU Requests  CPU Limits      Memory Requests Memory Limits\\n  ------------  ----------      --------------- -------------\\n  1275m (63%)   10m (0%)        210Mi (11%)     190Mi (9%)\\nIf you look at the bottom left of the listing, you’ll see a total of 1,275 millicores have\\nbeen requested by the running pods, which is 275 millicores more than what you\\nrequested for the first two pods you deployed. Something is eating up additional\\nCPU resources. \\n You can find the culprit in the list of pods in the previous listing. Three pods in the\\nkube-system namespace have explicitly requested CPU resources. Those pods plus\\nyour two pods leave only 725 millicores available for additional pods. Because your\\nthird pod requested 1,000 millicores, the Scheduler won’t schedule it to this node, as\\nthat would make the node overcommitted. \\nFREEING RESOURCES TO GET THE POD SCHEDULED\\nThe pod will only be scheduled when an adequate amount of CPU is freed (when one\\nof the first two pods is deleted, for example). If you delete your second pod, the\\nScheduler will be notified of the deletion (through the watch mechanism described in\\nchapter 11) and will schedule your third pod as soon as the second pod terminates.\\nThis is shown in the following listing.\\n$ kubectl delete po requests-pod-2\\npod \"requests-pod-2\" deleted \\n$ kubectl get po\\nNAME             READY     STATUS        RESTARTS   AGE\\nrequests-pod     1/1       Running       0          2h\\nrequests-pod-2   1/1       Terminating   0          1h\\nrequests-pod-3   0/1       Pending       0          1h\\n$ kubectl get po\\nNAME             READY     STATUS    RESTARTS   AGE\\nrequests-pod     1/1       Running   0          2h\\nrequests-pod-3   1/1       Running   0          1h\\nIn all these examples, you’ve specified a request for memory, but it hasn’t played any\\nrole in the scheduling because your node has more than enough allocatable memory to\\naccommodate all your pods’ requests. Both CPU and memory requests are treated the\\nsame way by the Scheduler, but in contrast to memory requests, a pod’s CPU requests\\nalso play a role elsewhere—while the pod is running. You’ll learn about this next.\\nListing 14.6\\nPod is scheduled after deleting another pod\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 443,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '411\\nRequesting resources for a pod’s containers\\n14.1.3 Understanding how CPU requests affect CPU time sharing\\nYou now have two pods running in your cluster (you can disregard the system pods\\nright now, because they’re mostly idle). One has requested 200 millicores and the\\nother one five times as much. At the beginning of the chapter, we said Kubernetes dis-\\ntinguishes between resource requests and limits. You haven’t defined any limits yet, so\\nthe two pods are in no way limited when it comes to how much CPU they can each\\nconsume. If the process inside each pod consumes as much CPU time as it can, how\\nmuch CPU time does each pod get? \\n The CPU requests don’t only affect scheduling—they also determine how the\\nremaining (unused) CPU time is distributed between pods. Because your first pod\\nrequested 200 millicores of CPU and the other one 1,000 millicores, any unused CPU\\nwill be split among the two pods in a 1 to 5 ratio, as shown in figure 14.2. If both pods\\nconsume as much CPU as they can, the first pod will get one sixth or 16.7% of the\\nCPU time and the other one the remaining five sixths or 83.3%.\\nBut if one container wants to use up as much CPU as it can, while the other one is sit-\\nting idle at a given moment, the first container will be allowed to use the whole CPU\\ntime (minus the small amount of time used by the second container, if any). After all,\\nit makes sense to use all the available CPU if no one else is using it, right? As soon as\\nthe second container needs CPU time, it will get it and the first container will be throt-\\ntled back.\\n14.1.4 Defining and requesting custom resources\\nKubernetes also allows you to add your own custom resources to a node and request\\nthem in the pod’s resource requests. Initially these were known as Opaque Integer\\nResources, but were replaced with Extended Resources in Kubernetes version 1.8.\\nPod A:\\n200 m\\nCPU\\nrequests\\nPod B: 1000 m\\n800 m available\\nCPU\\nusage\\n2000 m\\n1000 m\\n0 m\\nPod A and B requests\\nare in 1:5 ratio.\\nAvailable CPU time is\\ndistributed in same ratio.\\nPod B: 1667 m\\n133 m\\n(1/6)\\n667 m\\n(5/6)\\nPod A:\\n333 m\\nFigure 14.2\\nUnused CPU time is distributed to containers based on their CPU requests.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [            Col0      Col1           Col2  Col3  Col4           Col5  \\\n",
       "   0  Pod A:\\n200 m      None  Pod B: 1000 m  None  None                  \n",
       "   1                     None           None  None  None           None   \n",
       "   2           None      None                 None  None           None   \n",
       "   3                 Pod\\n333          A:\\nm              Pod B: 1667 m   \n",
       "   \n",
       "     133 m (1/6)      667 m (5/6)  \n",
       "   0              800 m available  \n",
       "   1                               \n",
       "   2        None                   \n",
       "   3        None                   ]},\n",
       " {'page': 444,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '412\\nCHAPTER 14\\nManaging pods’ computational resources\\n First, you obviously need to make Kubernetes aware of your custom resource by\\nadding it to the Node object’s capacity field. This can be done by performing a\\nPATCH HTTP request. The resource name can be anything, such as example.org/my-\\nresource, as long as it doesn’t start with the kubernetes.io domain. The quantity\\nmust be an integer (for example, you can’t set it to 100 millis, because 0.1 isn’t an inte-\\nger; but you can set it to 1000m or 2000m or, simply, 1 or 2). The value will be copied\\nfrom the capacity to the allocatable field automatically.\\n Then, when creating pods, you specify the same resource name and the requested\\nquantity under the resources.requests field in the container spec or with --requests\\nwhen using kubectl run like you did in previous examples. The Scheduler will make\\nsure the pod is only deployed to a node that has the requested amount of the custom\\nresource available. Every deployed pod obviously reduces the number of allocatable\\nunits of the resource.\\n An example of a custom resource could be the number of GPU units available on the\\nnode. Pods requiring the use of a GPU specify that in their requests. The Scheduler then\\nmakes sure the pod is only scheduled to nodes with at least one GPU still unallocated.\\n14.2\\nLimiting resources available to a container\\nSetting resource requests for containers in a pod ensures each container gets the min-\\nimum amount of resources it needs. Now let’s see the other side of the coin—the\\nmaximum amount the container will be allowed to consume. \\n14.2.1 Setting a hard limit for the amount of resources a container can use\\nWe’ve seen how containers are allowed to use up all the CPU if all the other processes\\nare sitting idle. But you may want to prevent certain containers from using up more\\nthan a specific amount of CPU. And you’ll always want to limit the amount of memory\\na container can consume. \\n CPU is a compressible resource, which means the amount used by a container can\\nbe throttled without affecting the process running in the container in an adverse way.\\nMemory is obviously different—it’s incompressible. Once a process is given a chunk of\\nmemory, that memory can’t be taken away from it until it’s released by the process\\nitself. That’s why you need to limit the maximum amount of memory a container can\\nbe given. \\n Without limiting memory, a container (or a pod) running on a worker node may\\neat up all the available memory and affect all other pods on the node and any new\\npods scheduled to the node (remember that new pods are scheduled to the node\\nbased on the memory requests and not actual memory usage). A single malfunction-\\ning or malicious pod can practically make the whole node unusable.\\nCREATING A POD WITH RESOURCE LIMITS\\nTo prevent this from happening, Kubernetes allows you to specify resource limits for\\nevery container (along with, and virtually in the same way as, resource requests). The\\nfollowing listing shows an example pod manifest with resource limits.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 445,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '413\\nLimiting resources available to a container\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: limited-pod\\nspec:\\n  containers:\\n  - image: busybox\\n    command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"]\\n    name: main\\n    resources:            \\n      limits:             \\n        cpu: 1             \\n        memory: 20Mi       \\nThis pod’s container has resource limits configured for both CPU and memory. The\\nprocess or processes running inside the container will not be allowed to consume\\nmore than 1 CPU core and 20 mebibytes of memory. \\nNOTE\\nBecause you haven’t specified any resource requests, they’ll be set to\\nthe same values as the resource limits.\\nOVERCOMMITTING LIMITS\\nUnlike resource requests, resource limits aren’t constrained by the node’s allocatable\\nresource amounts. The sum of all limits of all the pods on a node is allowed to exceed\\n100% of the node’s capacity (figure 14.3). Restated, resource limits can be overcom-\\nmitted. This has an important consequence—when 100% of the node’s resources are\\nused up, certain containers will need to be killed.\\nYou’ll see how Kubernetes decides which containers to kill in section 14.3, but individ-\\nual containers can be killed even if they try to use more than their resource limits\\nspecify. You’ll learn more about this next.\\nListing 14.7\\nA pod with a hard limit on CPU and memory: limited-pod.yaml\\nSpecifying resource \\nlimits for the container\\nThis container will be \\nallowed to use at \\nmost 1 CPU core.\\nThe container will be\\nallowed to use up to 20\\nmebibytes of memory.\\nNode\\n0%\\n136%\\n100%\\nPod A\\nMemory requests\\nPod B\\nPod C\\nPod A\\nMemory limits\\nPod B\\nUnallocated\\nPod C\\nFigure 14.3\\nThe sum of resource limits of all pods on a node can exceed 100% of the node’s \\ncapacity.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Pod A, Pod B, Pod C, Unallocated]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [Pod A, Pod B, Pod C, Col3]\n",
       "   Index: []]},\n",
       " {'page': 446,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '414\\nCHAPTER 14\\nManaging pods’ computational resources\\n14.2.2 Exceeding the limits\\nWhat happens when a process running in a container tries to use a greater amount of\\nresources than it’s allowed to? \\n You’ve already learned that CPU is a compressible resource, and it’s only natural\\nfor a process to want to consume all of the CPU time when not waiting for an I/O\\noperation. As you’ve learned, a process’ CPU usage is throttled, so when a CPU\\nlimit is set for a container, the process isn’t given more CPU time than the config-\\nured limit. \\n With memory, it’s different. When a process tries to allocate memory over its\\nlimit, the process is killed (it’s said the container is OOMKilled, where OOM stands\\nfor Out Of Memory). If the pod’s restart policy is set to Always or OnFailure, the\\nprocess is restarted immediately, so you may not even notice it getting killed. But if it\\nkeeps going over the memory limit and getting killed, Kubernetes will begin restart-\\ning it with increasing delays between restarts. You’ll see a CrashLoopBackOff status\\nin that case:\\n$ kubectl get po\\nNAME        READY     STATUS             RESTARTS   AGE\\nmemoryhog   0/1       CrashLoopBackOff   3          1m\\nThe CrashLoopBackOff status doesn’t mean the Kubelet has given up. It means that\\nafter each crash, the Kubelet is increasing the time period before restarting the con-\\ntainer. After the first crash, it restarts the container immediately and then, if it crashes\\nagain, waits for 10 seconds before restarting it again. On subsequent crashes, this\\ndelay is then increased exponentially to 20, 40, 80, and 160 seconds, and finally lim-\\nited to 300 seconds. Once the interval hits the 300-second limit, the Kubelet keeps\\nrestarting the container indefinitely every five minutes until the pod either stops\\ncrashing or is deleted. \\n To examine why the container crashed, you can check the pod’s log and/or use\\nthe kubectl describe pod command, as shown in the following listing.\\n$ kubectl describe pod\\nName:       memoryhog\\n...\\nContainers:\\n  main:\\n    ...\\n    State:          Terminated          \\n      Reason:       OOMKilled           \\n      Exit Code:    137\\n      Started:      Tue, 27 Dec 2016 14:55:53 +0100\\n      Finished:     Tue, 27 Dec 2016 14:55:58 +0100\\n    Last State:     Terminated            \\n      Reason:       OOMKilled             \\n      Exit Code:    137\\nListing 14.8\\nInspecting why a container terminated with kubectl describe pod\\nThe current container was \\nkilled because it was out \\nof memory (OOM).\\nThe previous container \\nwas also killed because \\nit was  OOM\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 447,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '415\\nLimiting resources available to a container\\n      Started:      Tue, 27 Dec 2016 14:55:37 +0100\\n      Finished:     Tue, 27 Dec 2016 14:55:50 +0100\\n    Ready:          False\\n...\\nThe OOMKilled status tells you that the container was killed because it was out of mem-\\nory. In the previous listing, the container went over its memory limit and was killed\\nimmediately. \\n It’s important not to set memory limits too low if you don’t want your container to\\nbe killed. But containers can get OOMKilled even if they aren’t over their limit. You’ll\\nsee why in section 14.3.2, but first, let’s discuss something that catches most users off-\\nguard the first time they start specifying limits for their containers.\\n14.2.3 Understanding how apps in containers see limits\\nIf you haven’t deployed the pod from listing 14.7, deploy it now:\\n$ kubectl create -f limited-pod.yaml\\npod \"limited-pod\" created\\nNow, run the top command in the container, the way you did at the beginning of the\\nchapter. The command’s output is shown in the following listing.\\n$ kubectl exec -it limited-pod top\\nMem: 1450980K used, 597504K free, 22012K shrd, 65876K buff, 857552K cached\\nCPU: 10.0% usr 40.0% sys  0.0% nic 50.0% idle  0.0% io  0.0% irq  0.0% sirq\\nLoad average: 0.17 1.19 2.47 4/503 10\\n  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\\n    1     0 root     R     1192  0.0   1 49.9 dd if /dev/zero of /dev/null\\n    5     0 root     R     1196  0.0   0  0.0 top\\nFirst, let me remind you that the pod’s CPU limit is set to 1 core and its memory limit\\nis set to 20 MiB. Now, examine the output of the top command closely. Is there any-\\nthing that strikes you as odd?\\n Look at the amount of used and free memory. Those numbers are nowhere near\\nthe 20 MiB you set as the limit for the container. Similarly, you set the CPU limit to\\none core and it seems like the main process is using only 50% of the available CPU\\ntime, even though the dd command, when used like you’re using it, usually uses all the\\nCPU it has available. What’s going on?\\nUNDERSTANDING THAT CONTAINERS ALWAYS SEE THE NODE’S MEMORY, NOT THE CONTAINER’S\\nThe top command shows the memory amounts of the whole node the container is\\nrunning on. Even though you set a limit on how much memory is available to a con-\\ntainer, the container will not be aware of this limit. \\nListing 14.9\\nRunning the top command in a CPU- and memory-limited container\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 448,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '416\\nCHAPTER 14\\nManaging pods’ computational resources\\n This has an unfortunate effect on any application that looks up the amount of\\nmemory available on the system and uses that information to decide how much mem-\\nory it wants to reserve. \\n The problem is visible when running Java apps, especially if you don’t specify the\\nmaximum heap size for the Java Virtual Machine with the -Xmx option. In that case,\\nthe JVM will set the maximum heap size based on the host’s total memory instead of\\nthe memory available to the container. When you run your containerized Java apps in\\na Kubernetes cluster on your laptop, the problem doesn’t manifest itself, because the\\ndifference between the memory limits you set for the pod and the total memory avail-\\nable on your laptop is not that great. \\n But when you deploy your pod onto a production system, where nodes have much\\nmore physical memory, the JVM may go over the container’s memory limit you config-\\nured and will be OOMKilled. \\n And if you think setting the -Xmx option properly solves the issue, you’re wrong,\\nunfortunately. The -Xmx option only constrains the heap size, but does nothing about\\nthe JVM’s off-heap memory. Luckily, new versions of Java alleviate that problem by tak-\\ning the configured container limits into account.\\nUNDERSTANDING THAT CONTAINERS ALSO SEE ALL THE NODE’S CPU CORES\\nExactly like with memory, containers will also see all the node’s CPUs, regardless of\\nthe CPU limits configured for the container. Setting a CPU limit to one core doesn’t\\nmagically only expose only one CPU core to the container. All the CPU limit does is\\nconstrain the amount of CPU time the container can use. \\n A container with a one-core CPU limit running on a 64-core CPU will get 1/64th\\nof the overall CPU time. And even though its limit is set to one core, the container’s\\nprocesses will not run on only one core. At different points in time, its code may be\\nexecuted on different cores.\\n Nothing is wrong with this, right? While that’s generally the case, at least one sce-\\nnario exists where this situation is catastrophic.\\n Certain applications look up the number of CPUs on the system to decide how\\nmany worker threads they should run. Again, such an app will run fine on a develop-\\nment laptop, but when deployed on a node with a much bigger number of cores, it’s\\ngoing to spin up too many threads, all competing for the (possibly) limited CPU time.\\nAlso, each thread requires additional memory, causing the apps memory usage to sky-\\nrocket. \\n You may want to use the Downward API to pass the CPU limit to the container and\\nuse it instead of relying on the number of CPUs your app can see on the system. You\\ncan also tap into the cgroups system directly to get the configured CPU limit by read-\\ning the following files:\\n\\uf0a1/sys/fs/cgroup/cpu/cpu.cfs_quota_us\\n\\uf0a1/sys/fs/cgroup/cpu/cpu.cfs_period_us\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 449,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '417\\nUnderstanding pod QoS classes\\n14.3\\nUnderstanding pod QoS classes\\nWe’ve already mentioned that resource limits can be overcommitted and that a\\nnode can’t necessarily provide all its pods the amount of resources specified in their\\nresource limits. \\n Imagine having two pods, where pod A is using, let’s say, 90% of the node’s mem-\\nory and then pod B suddenly requires more memory than what it had been using up\\nto that point and the node can’t provide the required amount of memory. Which\\ncontainer should be killed? Should it be pod B, because its request for memory can’t\\nbe satisfied, or should pod A be killed to free up memory, so it can be provided to\\npod B? \\n Obviously, it depends. Kubernetes can’t make a proper decision on its own. You\\nneed a way to specify which pods have priority in such cases. Kubernetes does this by\\ncategorizing pods into three Quality of Service (QoS) classes:\\n\\uf0a1\\nBestEffort (the lowest priority)\\n\\uf0a1\\nBurstable\\n\\uf0a1\\nGuaranteed (the highest)\\n14.3.1 Defining the QoS class for a pod\\nYou might expect these classes to be assignable to pods through a separate field in the\\nmanifest, but they aren’t. The QoS class is derived from the combination of resource\\nrequests and limits for the pod’s containers. Here’s how.\\nASSIGNING A POD TO THE BESTEFFORT CLASS\\nThe lowest priority QoS class is the BestEffort class. It’s assigned to pods that don’t\\nhave any requests or limits set at all (in any of their containers). This is the QoS class\\nthat has been assigned to all the pods you created in previous chapters. Containers\\nrunning in these pods have had no resource guarantees whatsoever. In the worst\\ncase, they may get almost no CPU time at all and will be the first ones killed when\\nmemory needs to be freed for other pods. But because a BestEffort pod has no\\nmemory limits set, its containers may use as much memory as they want, if enough\\nmemory is available.\\nASSIGNING A POD TO THE GUARANTEED CLASS\\nOn the other end of the spectrum is the Guaranteed QoS class. This class is given to\\npods whose containers’ requests are equal to the limits for all resources. For a pod’s\\nclass to be Guaranteed, three things need to be true:\\n\\uf0a1Requests and limits need to be set for both CPU and memory.\\n\\uf0a1They need to be set for each container.\\n\\uf0a1They need to be equal (the limit needs to match the request for each resource\\nin each container).\\nBecause a container’s resource requests, if not set explicitly, default to the limits,\\nspecifying the limits for all resources (for each container in the pod) is enough for\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'pod',\n",
       "    'description': 'A group of one or more containers that share the same network space and can communicate with each other.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'QoS',\n",
       "    'description': 'Quality of Service, a way to specify which pods have priority in cases where resources are limited.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'BestEffort',\n",
       "    'description': 'The lowest priority QoS class, assigned to pods with no resource requests or limits set.',\n",
       "    'category': 'QoS_class'},\n",
       "   {'entity': 'Burstable',\n",
       "    'description': \"A QoS class that allows a pod's containers to burst above their requested resources for short periods of time.\",\n",
       "    'category': 'QoS_class'},\n",
       "   {'entity': 'Guaranteed',\n",
       "    'description': 'The highest priority QoS class, assigned to pods with resource requests equal to their limits.',\n",
       "    'category': 'QoS_class'},\n",
       "   {'entity': 'resource limit',\n",
       "    'description': 'A maximum amount of a particular resource (such as memory or CPU) that a pod can use.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'resource request',\n",
       "    'description': \"The minimum amount of a particular resource (such as memory or CPU) that a pod's containers must have available.\",\n",
       "    'category': 'process'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A lightweight and standalone process that bundles code, libraries, and dependencies together.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'manifest',\n",
       "    'description': 'A file that defines the configuration of a pod, including its containers, resource requests, and limits.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \\n   \"description\": \"determines which pods to kill when resources are overcommitted\", \\n   \"destination_entity\": \"pod\"},\\n  \\n  {\"source_entity\": \"node\", \\n   \"description\": \"can\\'t provide all its pods the amount of resources specified in their resource limits\", \\n   \"destination_entity\": \"pod\"},\\n  \\n  {\"source_entity\": \"pod A\", \\n   \"description\": \"uses 90% of the node\\'s memory\", \\n   \"destination_entity\": \"node\"},\\n  \\n  {\"source_entity\": \"pod B\", \\n   \"description\": \"requires more memory than it had been using up to that point\", \\n   \"destination_entity\": \"node\"},\\n  \\n  {\"source_entity\": \"Kubernetes\", \\n   \"description\": \"can\\'t make a proper decision on its own when resources are overcommitted\", \\n   \"destination_entity\": null},\\n  \\n  {\"source_entity\": \"pod B\", \\n   \"description\": \"request for memory can\\'t be satisfied\", \\n   \"destination_entity\": \"node\"},\\n  \\n  {\"source_entity\": \"Kubernetes\", \\n   \"description\": \"categorizes pods into three Quality of Service (QoS) classes\", \\n   \"destination_entity\": null},\\n  \\n  {\"source_entity\": \"pod B\", \\n   \"description\": \"should be killed to free up memory for pod A\", \\n   \"destination_entity\": \"pod A\"},\\n  \\n  {\"source_entity\": \"pod A\", \\n   \"description\": \"should not be killed if its QoS class is higher than pod B\\'s\", \\n   \"destination_entity\": \"QoS\"},\\n  \\n  {\"source_entity\": \"BestEffort class\", \\n   \"description\": \"is assigned to pods that don\\'t have any requests or limits set at all\", \\n   \"destination_entity\": null},\\n  \\n  {\"source_entity\": \"Burstable class\", \\n   \"description\": \"is assigned when a pod\\'s containers\\' requests are less than the limits for some resources\", \\n   \"destination_entity\": null},\\n  \\n  {\"source_entity\": \"Guaranteed class\", \\n   \"description\": \"is given to pods whose containers\\' requests are equal to the limits for all resources\", \\n   \"destination_entity\": null},\\n  \\n  {\"source_entity\": \"QoS class\", \\n   \"description\": \"is derived from the combination of resource requests and limits for a pod\\'s containers\", \\n   \"destination_entity\": \"pod\"},\\n  \\n  {\"source_entity\": \"manifest\", \\n   \"description\": \"doesn\\'t assign QoS classes to pods directly\", \\n   \"destination_entity\": null},\\n  \\n  {\"source_entity\": \"container\", \\n   \"description\": \"has resource requests that default to the limits if not set explicitly\", \\n   \"destination_entity\": null},\\n  \\n  {\"source_entity\": \"resource request\", \\n   \"description\": \"needs to be equal to the resource limit for a pod\\'s class to be Guaranteed\", \\n   \"destination_entity\": \"resource limit\"},\\n  \\n  {\"source_entity\": \"BestEffort class\", \\n   \"description\": \"has the lowest priority QoS class\", \\n   \"destination_entity\": null},\\n  \\n  {\"source_entity\": \"Guaranteed class\", \\n   \"description\": \"has the highest priority QoS class\", \\n   \"destination_entity\": null}\\n]'},\n",
       " {'page': 450,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '418\\nCHAPTER 14\\nManaging pods’ computational resources\\nthe pod to be Guaranteed. Containers in those pods get the requested amount of\\nresources, but cannot consume additional ones (because their limits are no higher\\nthan their requests). \\nASSIGNING THE BURSTABLE QOS CLASS TO A POD\\nIn between BestEffort and Guaranteed is the Burstable QoS class. All other pods\\nfall into this class. This includes single-container pods where the container’s limits\\ndon’t match its requests and all pods where at least one container has a resource\\nrequest specified, but not the limit. It also includes pods where one container’s\\nrequests match their limits, but another container has no requests or limits specified.\\nBurstable pods get the amount of resources they request, but are allowed to use addi-\\ntional resources (up to the limit) if needed.\\nUNDERSTANDING HOW THE RELATIONSHIP BETWEEN REQUESTS AND LIMITS DEFINES THE QOS CLASS\\nAll three QoS classes and their relationships with requests and limits are shown in fig-\\nure 14.4.\\nThinking about what QoS class a pod has can make your head spin, because it involves\\nmultiple containers, multiple resources, and all the possible relationships between\\nrequests and limits. It’s easier if you start by thinking about QoS at the container level\\n(although QoS classes are a property of pods, not containers) and then derive the\\npod’s QoS class from the QoS classes of containers. \\nFIGURING OUT A CONTAINER’S QOS CLASS\\nTable 14.1 shows the QoS class based on how resource requests and limits are\\ndefined on a single container. For single-container pods, the QoS class applies to\\nthe pod as well.\\n \\nBestEffort\\nQoS\\nRequests\\nLimits\\nBurstable\\nQoS\\nRequests\\nLimits\\nGuaranteed\\nQoS\\nRequests\\nLimits\\nRequests and\\nlimits are not set\\nRequests are\\nbelow limits\\nRequests\\nequal limits\\nFigure 14.4\\nResource requests, limits and QoS classes\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Requests, Limits]\n",
       "   Index: [],\n",
       "          Col0    Col1\n",
       "   0  Requests  Limits,\n",
       "   Empty DataFrame\n",
       "   Columns: [Requests, Limits]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A group of one or more containers that share the same namespace.',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Pod\", \"description\": \"get the requested amount of resources\", \"destination_entity\": \"Containers\"},\\n  {\"source_entity\": \"Pods in Burstable QoS class\", \"description\": \"allowed to use additional resources up to the limit\", \"destination_entity\": \"Additional Resources\"},\\n  {\"source_entity\": \"Burstable pods\", \"description\": \"get the amount of resources they request\", \"destination_entity\": \"Requested Resources\"},\\n  {\"source_entity\": \"QoS classes are a property of pods, not containers\", \"description\": \"derive the pod\\'s QoS class from the QoS classes of containers\", \"destination_entity\": \"Containers\"},\\n  {\"source_entity\": \"Single-container pods\", \"description\": \"the QoS class applies to the pod as well\", \"destination_entity\": \"Pod\"}\\n]\\n```\\n\\nThese relations were extracted based on the following actions performed by the entity \\'Pod\\' and their effects:\\n\\n* A Pod gets the requested amount of resources for its Containers.\\n* Burstable pods are allowed to use additional resources up to the limit, making them have access to Additional Resources.\\n* Burstable pods get the amount of resources they request, which is equivalent to Requested Resources.\\n* QoS classes are a property of Pods and not Containers, but the Pod\\'s QoS class can be derived from the QoS classes of its Containers.\\n* The QoS class for Single-container pods applies to the Pod as well.'},\n",
       " {'page': 451,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '419\\nUnderstanding pod QoS classes\\nNOTE\\nIf only requests are set, but not limits, refer to the table rows where\\nrequests are less than the limits. If only limits are set, requests default to the\\nlimits, so refer to the rows where requests equal limits.\\nFIGURING OUT THE QOS CLASS OF A POD WITH MULTIPLE CONTAINERS\\nFor multi-container pods, if all the containers have the same QoS class, that’s also the\\npod’s QoS class. If at least one container has a different class, the pod’s QoS class is\\nBurstable, regardless of what the container classes are. Table 14.2 shows how a two-\\ncontainer pod’s QoS class relates to the classes of its two containers. You can easily\\nextend this to pods with more than two containers.\\nNOTE\\nA pod’s QoS class is shown when running kubectl describe pod and\\nin the pod’s YAML/JSON manifest in the status.qosClass field.\\nWe’ve explained how QoS classes are determined, but we still need to look at how they\\ndetermine which container gets killed in an overcommitted system.\\nTable 14.1\\nThe QoS class of a single-container pod based on resource requests and limits\\nCPU requests vs. limits\\nMemory requests vs. limits\\nContainer QoS class\\nNone set\\nNone set\\nBestEffort\\nNone set\\nRequests < Limits\\nBurstable\\nNone set\\nRequests = Limits\\nBurstable\\nRequests < Limits\\nNone set\\nBurstable\\nRequests < Limits\\nRequests < Limits\\nBurstable\\nRequests < Limits\\nRequests = Limits\\nBurstable\\nRequests = Limits\\nRequests = Limits\\nGuaranteed\\nTable 14.2\\nA Pod’s QoS class derived from the classes of its containers\\nContainer 1 QoS class\\nContainer 2 QoS class\\nPod’s QoS class\\nBestEffort\\nBestEffort\\nBestEffort\\nBestEffort\\nBurstable\\nBurstable\\nBestEffort\\nGuaranteed\\nBurstable\\nBurstable\\nBurstable\\nBurstable\\nBurstable\\nGuaranteed\\nBurstable\\nGuaranteed\\nGuaranteed\\nGuaranteed\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [                             CPU requests vs. limits  \\\n",
       "   0  None set\\nNone set\\nNone set\\nRequests < Limit...   \n",
       "   \n",
       "                             Memory requests vs. limits  \\\n",
       "   0  None set\\nRequests < Limits\\nRequests = Limits...   \n",
       "   \n",
       "                                    Container QoS class  \n",
       "   0  BestEffort\\nBurstable\\nBurstable\\nBurstable\\nB...  ,\n",
       "                                  Container 1 QoS class  \\\n",
       "   0  BestEffort\\nBestEffort\\nBestEffort\\nBurstable\\...   \n",
       "   \n",
       "                                  Container 2 QoS class  \\\n",
       "   0  BestEffort\\nBurstable\\nGuaranteed\\nBurstable\\n...   \n",
       "   \n",
       "                                        Pod’s QoS class  \n",
       "   0  BestEffort\\nBurstable\\nBurstable\\nBurstable\\nB...  ],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A pod is the basic execution unit in Kubernetes.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'QoS Class',\n",
       "    'description': \"Quality of Service class determines how a pod's resources are allocated.\",\n",
       "    'category': 'application'},\n",
       "   {'entity': 'requests',\n",
       "    'description': 'The amount of resource requested by a container or pod.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'limits',\n",
       "    'description': 'The maximum amount of resource that can be used by a container or pod.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for managing Kubernetes resources.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'describe',\n",
       "    'description': 'A command in kubectl to display detailed information about a resource.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'YAML/JSON manifest',\n",
       "    'description': 'A file format for describing Kubernetes resources.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'status.qosClass field',\n",
       "    'description': \"A field in a pod's YAML/JSON manifest that shows its QoS class.\",\n",
       "    'category': 'database'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'The smallest unit of execution in Kubernetes, running as a process within a pod.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'overcommitted system',\n",
       "    'description': 'A situation where the total resource requests exceed the available resources.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'BestEffort QoS class',\n",
       "    'description': \"The lowest level of QoS, where a pod's resources are not guaranteed.\",\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Burstable QoS class',\n",
       "    'description': \"A higher level of QoS, where a pod's resources can burst above its requests but have a maximum limit.\",\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Guaranteed QoS class',\n",
       "    'description': \"The highest level of QoS, where a pod's resources are guaranteed up to its limits.\",\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"requests\", \"description\": \"default to limits when not set\", \"destination_entity\": \"limits\"},\\n  {\"source_entity\": \"limits\", \"description\": \"determine QoS class based on requests and limits\", \"destination_entity\": \"QoS Class\"},\\n  {\"source_entity\": \"overcommitted system\", \"description\": \"determine which container gets killed\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"status.qosClass field\", \"description\": \"show pod\\'s QoS class in YAML/JSON manifest\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"kubectl describe pod\", \"description\": \"show pod\\'s QoS class when running command\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"container\", \"description\": \"determine container\\'s QoS class based on requests and limits\", \"destination_entity\": \"QoS Class\"},\\n  {\"source_entity\": \"requests\", \"description\": \"set default to guarantees if equal to limits\", \"destination_entity\": \"Guaranteed QoS class\"},\\n  {\"source_entity\": \"limits\", \"description\": \"determine Burstable QoS class based on requests and limits\", \"destination_entity\": \"Burstable QoS class\"},\\n  {\"source_entity\": \"requests\", \"description\": \"set default to BestEffort if not set or less than limits\", \"destination_entity\": \"BestEffort QoS class\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"run command to show pod\\'s QoS class\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"YAML/JSON manifest\", \"description\": \"show pod\\'s QoS class in status.qosClass field\", \"destination_entity\": \"Pod\"}\\n]'},\n",
       " {'page': 452,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '420\\nCHAPTER 14\\nManaging pods’ computational resources\\n14.3.2 Understanding which process gets killed when memory is low\\nWhen the system is overcommitted, the QoS classes determine which container gets\\nkilled first so the freed resources can be given to higher priority pods. First in line to\\nget killed are pods in the BestEffort class, followed by Burstable pods, and finally\\nGuaranteed pods, which only get killed if system processes need memory.\\nUNDERSTANDING HOW QOS CLASSES LINE UP\\nLet’s look at the example shown in figure 14.5. Imagine having two single-container\\npods, where the first one has the BestEffort QoS class, and the second one’s is\\nBurstable. When the node’s whole memory is already maxed out and one of the pro-\\ncesses on the node tries to allocate more memory, the system will need to kill one of\\nthe processes (perhaps even the process trying to allocate additional memory) to\\nhonor the allocation request. In this case, the process running in the BestEffort pod\\nwill always be killed before the one in the Burstable pod.\\nObviously, a BestEffort pod’s process will also be killed before any Guaranteed pods’\\nprocesses are killed. Likewise, a Burstable pod’s process will also be killed before that\\nof a Guaranteed pod. But what happens if there are only two Burstable pods? Clearly,\\nthe selection process needs to prefer one over the other.\\nUNDERSTANDING HOW CONTAINERS WITH THE SAME QOS CLASS ARE HANDLED\\nEach running process has an OutOfMemory (OOM) score. The system selects the\\nprocess to kill by comparing OOM scores of all the running processes. When memory\\nneeds to be freed, the process with the highest score gets killed.\\n OOM scores are calculated from two things: the percentage of the available mem-\\nory the process is consuming and a fixed OOM score adjustment, which is based on the\\npod’s QoS class and the container’s requested memory. When two single-container pods\\nexist, both in the Burstable class, the system will kill the one using more of its requested\\nBestEffort\\nQoS pod\\nPod A\\nFirst in line\\nto be killed\\nActual usage\\nRequests\\nLimits\\nBurstable\\nQoS pod\\nPod B\\nSecond in line\\nto be killed\\n90% used\\nRequests\\nLimits\\nBurstable\\nQoS pod\\nPod C\\nThird in line\\nto be killed\\n70% used\\nRequests\\nLimits\\nGuaranteed\\nQoS pod\\nPod D\\nLast to\\nbe killed\\n99% used\\nRequests\\nLimits\\nFigure 14.5\\nWhich pods get killed first\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'pods',\n",
       "    'description': 'a group of one or more containers that share a common IP address and port space',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'QoS classes',\n",
       "    'description': 'Quality of Service classes used to determine which process gets killed when memory is low',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'BestEffort class',\n",
       "    'description': 'a QoS class that gets killed first when memory is low',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Burstable class',\n",
       "    'description': 'a QoS class that gets killed second when memory is low',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Guaranteed class',\n",
       "    'description': 'a QoS class that gets killed last when memory is low',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'the running process in a pod',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'OOM scores',\n",
       "    'description': 'scores used to determine which process to kill when memory needs to be freed',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'OutOfMemory (OOM) score',\n",
       "    'description': 'a score calculated from available memory consumption and QoS class adjustment',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'memory',\n",
       "    'description': 'the amount of RAM used by a process',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a group of one or more containers that share a common IP address and port space',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"system\",\\n    \"description\": \"determines which container gets killed first so the freed resources can be given to higher priority pods.\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"QoS classes\",\\n    \"description\": \"determine which container gets killed first when the system is overcommitted.\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"BestEffort class\",\\n    \"description\": \"gets killed first when the system is overcommitted.\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"Burstable pods\",\\n    \"description\": \"get killed second in line when the system is overcommitted.\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"Guaranteed pods\",\\n    \"description\": \"only get killed if system processes need memory when the system is overcommitted.\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"system\",\\n    \"description\": \"will kill a process (perhaps even the process trying to allocate additional memory) to honor the allocation request.\",\\n    \"destination_entity\": \"process\"\\n  },\\n  {\\n    \"source_entity\": \"BestEffort pod\\'s process\",\\n    \"description\": \"will always be killed before the one in the Burstable pod when the node\\'s whole memory is already maxed out.\",\\n    \"destination_entity\": \"Burstable pod\\'s process\"\\n  },\\n  {\\n    \"source_entity\": \"BestEffort pod\\'s process\",\\n    \"description\": \"will also be killed before any Guaranteed pods\\' processes are killed.\",\\n    \"destination_entity\": \"Guaranteed pods\\' process\"\\n  },\\n  {\\n    \"source_entity\": \"Burstable pod\\'s process\",\\n    \"description\": \"will also be killed before that of a Guaranteed pod when the system is overcommitted.\",\\n    \"destination_entity\": \"Guaranteed pod\\'s process\"\\n  },\\n  {\\n    \"source_entity\": \"process running in the BestEffort pod\",\\n    \"description\": \"has an OutOfMemory (OOM) score, which determines the priority of getting killed.\",\\n    \"destination_entity\": \"OOM scores\"\\n  },\\n  {\\n    \"source_entity\": \"OOM scores\",\\n    \"description\": \"are calculated from two things: the percentage of the available memory the process is consuming and a fixed OOM score adjustment.\",\\n    \"destination_entity\": \"process\"\\n  },\\n  {\\n    \"source_entity\": \"system\",\\n    \"description\": \"selects the process to kill by comparing OOM scores of all the running processes.\",\\n    \"destination_entity\": \"OOM scores\"\\n  },\\n  {\\n    \"source_entity\": \"process with the highest score\",\\n    \"description\": \"gets killed when memory needs to be freed.\",\\n    \"destination_entity\": \"system\"\\n  }\\n]\\n\\nNote that some entities appear multiple times as source or destination, but I have kept each relation separate for clarity.'},\n",
       " {'page': 453,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '421\\nSetting default requests and limits for pods per namespace\\nmemory than the other, percentage-wise. That’s why in figure 14.5, pod B, using 90%\\nof its requested memory, gets killed before pod C, which is only using 70%, even\\nthough it’s using more megabytes of memory than pod B. \\n This shows you need to be mindful of not only the relationship between requests\\nand limits, but also of requests and the expected actual memory consumption. \\n14.4\\nSetting default requests and limits for pods per \\nnamespace\\nWe’ve looked at how resource requests and limits can be set for each individual con-\\ntainer. If you don’t set them, the container is at the mercy of all other containers that\\ndo specify resource requests and limits. It’s a good idea to set requests and limits on\\nevery container.\\n14.4.1 Introducing the LimitRange resource\\nInstead of having to do this for every container, you can also do it by creating a Limit-\\nRange resource. It allows you to specify (for each namespace) not only the minimum\\nand maximum limit you can set on a container for each resource, but also the default\\nresource requests for containers that don’t specify requests explicitly, as depicted in\\nfigure 14.6.\\nAPI server\\nValidation\\nPod A\\nmanifest\\n- Requests\\n- Limits\\nPod A\\nmanifest\\n- Requests\\n- Limits\\nPod B\\nmanifest\\n- No\\nrequests\\nor limits\\nPod B\\nmanifest\\n- No\\nrequests\\nor limits\\nDefaulting\\nRejected because\\nrequests and limits are\\noutside min/max values\\nDefaults\\napplied\\nNamespace XYZ\\nLimitRange\\nPod B\\nmanifest\\n- Default\\nrequests\\n- Default\\nlimits\\nPod B\\n- Default requests\\n- Default limits\\n- Min/max CPU\\n- Min/max memory\\n- Default requests\\n- Default limits\\nFigure 14.6\\nA LimitRange is used for validation and defaulting pods.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'requests',\n",
       "    'description': 'Resource request for a container or pod',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'limits',\n",
       "    'description': 'Maximum resource limit for a container or pod',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Lightweight and ephemeral worker running an application',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'Logical partition of resources in a cluster',\n",
       "    'category': 'software/application'},\n",
       "   {'entity': 'LimitRange',\n",
       "    'description': 'Resource configuration for a namespace',\n",
       "    'category': 'software/resource'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Component of Kubernetes that provides the API to interact with clusters',\n",
       "    'category': 'software/infrastructure'},\n",
       "   {'entity': 'Validation',\n",
       "    'description': 'Process of checking whether a pod or container conforms to certain rules',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'manifest',\n",
       "    'description': 'Definition of a pod or container, including its configuration and resources',\n",
       "    'category': 'software/configuration'},\n",
       "   {'entity': 'CPU',\n",
       "    'description': 'Central Processing Unit resource',\n",
       "    'category': 'hardware/resource'},\n",
       "   {'entity': 'memory',\n",
       "    'description': 'Random Access Memory resource',\n",
       "    'category': 'hardware/resource'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"requests\", \"description\": \"can be set for each individual container\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"limits\", \"description\": \"can be set for each individual container\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"LimitRange\", \"description\": \"allows specifying default resource requests and limits for containers in a namespace\", \"destination_entity\": \"namespace\"},\\n  {\"source_entity\": \"API server\", \"description\": \"validation is performed by the API server\", \"destination_entity\": \"requests\"},\\n  {\"source_entity\": \"API server\", \"description\": \"validation is performed by the API server\", \"destination_entity\": \"limits\"},\\n  {\"source_entity\": \"Validation\", \"description\": \"is performed on requests and limits\", \"destination_entity\": \"requests\"},\\n  {\"source_entity\": \"Validation\", \"description\": \"is performed on requests and limits\", \"destination_entity\": \"limits\"},\\n  {\"source_entity\": \"manifest\", \"description\": \"contains information about requests and limits\", \"destination_entity\": \"requests\"},\\n  {\"source_entity\": \"manifest\", \"description\": \"contains information about requests and limits\", \"destination_entity\": \"limits\"},\\n  {\"source_entity\": \"requests\", \"description\": \"are compared to min/max values when setting defaulting\", \"destination_entity\": \"min/max values\"},\\n  {\"source_entity\": \"limits\", \"description\": \"are compared to min/max values when setting defaulting\", \"destination_entity\": \"min/max values\"},\\n  {\"source_entity\": \"pod\", \"description\": \"has requests and limits that can be set or defaulted by a LimitRange\", \"destination_entity\": \"requests\"},\\n  {\"source_entity\": \"pod\", \"description\": \"has requests and limits that can be set or defaulted by a LimitRange\", \"destination_entity\": \"limits\"},\\n  {\"source_entity\": \"memory\", \"description\": \"can be used more than requested if the container has not specified limits\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"namespace\", \"description\": \"has default requests and limits set by a LimitRange\", \"destination_entity\": \"requests\"},\\n  {\"source_entity\": \"namespace\", \"description\": \"has default requests and limits set by a LimitRange\", \"destination_entity\": \"limits\"}\\n]\\n```'},\n",
       " {'page': 454,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '422\\nCHAPTER 14\\nManaging pods’ computational resources\\nLimitRange resources are used by the LimitRanger Admission Control plugin (we\\nexplained what those plugins are in chapter 11). When a pod manifest is posted to the\\nAPI server, the LimitRanger plugin validates the pod spec. If validation fails, the mani-\\nfest is rejected immediately. Because of this, a great use-case for LimitRange objects is\\nto prevent users from creating pods that are bigger than any node in the cluster. With-\\nout such a LimitRange, the API server will gladly accept the pod, but then never\\nschedule it. \\n The limits specified in a LimitRange resource apply to each individual pod/con-\\ntainer or other kind of object created in the same namespace as the LimitRange\\nobject. They don’t limit the total amount of resources available across all the pods in\\nthe namespace. This is specified through ResourceQuota objects, which are explained\\nin section 14.5. \\n14.4.2 Creating a LimitRange object\\nLet’s look at a full example of a LimitRange and see what the individual properties do.\\nThe following listing shows the full definition of a LimitRange resource.\\napiVersion: v1\\nkind: LimitRange\\nmetadata:\\n  name: example\\nspec:\\n  limits:\\n  - type: Pod           \\n    min:                         \\n      cpu: 50m                   \\n      memory: 5Mi                \\n    max:                          \\n      cpu: 1                      \\n      memory: 1Gi                 \\n  - type: Container             \\n    defaultRequest:             \\n      cpu: 100m                 \\n      memory: 10Mi              \\n    default:                      \\n      cpu: 200m                   \\n      memory: 100Mi               \\n    min:                         \\n      cpu: 50m                   \\n      memory: 5Mi                \\n    max:                         \\n      cpu: 1                     \\n      memory: 1Gi                \\n    maxLimitRequestRatio:         \\n      cpu: 4                      \\n      memory: 10                  \\nListing 14.10\\nA LimitRange resource: limits.yaml\\nSpecifies the \\nlimits for a pod \\nas a whole\\nMinimum CPU and memory all the \\npod’s containers can request in total\\nMaximum CPU and memory all the pod’s \\ncontainers can request (and limit)\\nThe\\ncontainer\\nlimits are\\nspecified\\nbelow this\\nline.\\nDefault requests for CPU and memory \\nthat will be applied to containers that \\ndon’t specify them explicitly\\nDefault limits for containers \\nthat don’t specify them\\nMinimum and maximum \\nrequests/limits that a \\ncontainer can have\\nMaximum ratio between \\nthe limit and request \\nfor each resource\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'LimitRange',\n",
       "    'description': \"An object used to manage pods' computational resources\",\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Admission Control plugin',\n",
       "    'description': 'A plugin that validates pod specs against LimitRange objects',\n",
       "    'category': 'plugin'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The server that accepts and validates pod manifests',\n",
       "    'category': 'server'},\n",
       "   {'entity': 'LimitRanger Admission Control plugin',\n",
       "    'description': 'A specific plugin used to validate pod specs against LimitRange objects',\n",
       "    'category': 'plugin'},\n",
       "   {'entity': 'ResourceQuota',\n",
       "    'description': 'An object used to specify resource limits across all pods in a namespace',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A unit of computation that can be scheduled on a node',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'A lightweight and standalone process that runs within a pod',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'cpu',\n",
       "    'description': 'A computational resource measured in milli-cores',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'memory',\n",
       "    'description': 'A storage resource measured in bytes',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': 'A scope for grouping resources together',\n",
       "    'category': 'scope'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'A physical or virtual machine that can run pods',\n",
       "    'category': 'machine'},\n",
       "   {'entity': 'cluster',\n",
       "    'description': 'A collection of nodes that can schedule and manage pods',\n",
       "    'category': 'group'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"LimitRanger Admission Control plugin\", \"description\": \"validates pod spec\", \"destination_entity\": \"pod manifest\"},\\n  {\"source_entity\": \"LimitRange object\", \"description\": \"applies to each individual pod/container or other kind of object created in the same namespace\", \"destination_entity\": \"namespace\"},\\n  {\"source_entity\": \"API server\", \"description\": \"gently accepts pod, but never schedule it without LimitRange\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"ResourceQuota objects\", \"description\": \"specify the total amount of resources available across all pods in the namespace\", \"destination_entity\": \"namespace\"},\\n  {\"source_entity\": \"LimitRanger Admission Control plugin\", \"description\": \"rejects manifest immediately if validation fails\", \"destination_entity\": \"pod manifest\"},\\n  {\"source_entity\": \"user\", \"description\": \"tries to create pod that is bigger than any node in the cluster\", \"destination_entity\": \"node\"},\\n  {\"source_entity\": \"LimitRanger Admission Control plugin\", \"description\": \"prevents users from creating pods that are bigger than any node in the cluster\", \"destination_entity\": \"cluster\"},\\n  {\"source_entity\": \"LimitRange resource\", \"description\": \"specifies limits for a pod as a whole\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"LimitRange object\", \"description\": \"applies minimum CPU and memory all the pod\\'s containers can request in total\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Container\", \"description\": \"limits are specified below this line\", \"destination_entity\": \"Line of code\"},\\n  {\"source_entity\": \"Namespace\", \"description\": \"contains resources for a LimitRange object\", \"destination_entity\": \"LimitRanger Admission Control plugin\"},\\n  {\"source_entity\": \"node\", \"description\": \"is bigger than the pod that user tries to create\", \"destination_entity\": \"user\"},\\n  {\"source_entity\": \"cluster\", \"description\": \"has multiple nodes, one of which is bigger than the pod that user tries to create\", \"destination_entity\": \"user\"},\\n  {\"source_entity\": \"memory\", \"description\": \"is limited by a LimitRange object\", \"destination_entity\": \"Container\"},\\n  {\"source_entity\": \"ResourceQuota objects\", \"description\": \"are used in conjunction with LimitRange objects\", \"destination_entity\": \"LimitRanger Admission Control plugin\"},\\n  {\"source_entity\": \"Pod\", \"description\": \"has multiple containers, each of which has limits specified by a LimitRange object\", \"destination_entity\": \"Container\"}\\n]\\n```\\n\\nNote: I\\'ve only extracted the relations mentioned in the document page and matched them with the entities provided. Let me know if you need any further clarification!'},\n",
       " {'page': 455,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '423\\nSetting default requests and limits for pods per namespace\\n  - type: PersistentVolumeClaim      \\n    min:                             \\n      storage: 1Gi                   \\n    max:                             \\n      storage: 10Gi                  \\nAs you can see from the previous example, the minimum and maximum limits for a\\nwhole pod can be configured. They apply to the sum of all the pod’s containers’\\nrequests and limits. \\n Lower down, at the container level, you can set not only the minimum and maxi-\\nmum, but also default resource requests (defaultRequest) and default limits\\n(default) that will be applied to each container that doesn’t specify them explicitly. \\n Beside the min, max, and default values, you can even set the maximum ratio of\\nlimits vs. requests. The previous listing sets the CPU maxLimitRequestRatio to 4,\\nwhich means a container’s CPU limits will not be allowed to be more than four times\\ngreater than its CPU requests. A container requesting 200 millicores will not be\\naccepted if its CPU limit is set to 801 millicores or higher. For memory, the maximum\\nratio is set to 10.\\n In chapter 6 we looked at PersistentVolumeClaims (PVC), which allow you to claim\\na certain amount of persistent storage similarly to how a pod’s containers claim CPU\\nand memory. In the same way you’re limiting the minimum and maximum amount of\\nCPU a container can request, you should also limit the amount of storage a single\\nPVC can request. A LimitRange object allows you to do that as well, as you can see at\\nthe bottom of the example.\\n The example shows a single LimitRange object containing limits for everything,\\nbut you could also split them into multiple objects if you prefer to have them orga-\\nnized per type (one for pod limits, another for container limits, and yet another for\\nPVCs, for example). Limits from multiple LimitRange objects are all consolidated\\nwhen validating a pod or PVC.\\n Because the validation (and defaults) configured in a LimitRange object is per-\\nformed by the API server when it receives a new pod or PVC manifest, if you modify\\nthe limits afterwards, existing pods and PVCs will not be revalidated—the new limits\\nwill only apply to pods and PVCs created afterward. \\n14.4.3 Enforcing the limits\\nWith your limits in place, you can now try creating a pod that requests more CPU than\\nallowed by the LimitRange. You’ll find the YAML for the pod in the code archive. The\\nnext listing only shows the part relevant to the discussion.\\n    resources:\\n      requests:\\n        cpu: 2\\nListing 14.11\\nA pod with CPU requests greater than the limit: limits-pod-too-big.yaml\\nA LimitRange can also set \\nthe minimum and maximum \\namount of storage a PVC \\ncan request.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'a claim for persistent storage',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'min', 'description': 'minimum value', 'category': 'process'},\n",
       "   {'entity': 'max', 'description': 'maximum value', 'category': 'process'},\n",
       "   {'entity': 'storage',\n",
       "    'description': 'amount of storage',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'defaultRequest',\n",
       "    'description': 'default resource request',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'default',\n",
       "    'description': 'default limit',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'maxLimitRequestRatio',\n",
       "    'description': 'maximum ratio of limits vs. requests',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'PersistentVolumeClaims',\n",
       "    'description': 'allowing to claim persistent storage',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a group of containers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'individual units within a pod',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'limits-pod-too-big.yaml',\n",
       "    'description': 'a YAML file for a pod with excessive CPU requests',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"max\",\\n    \"description\": \"sets maximum value for a resource\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"has minimum and maximum limits configured\",\\n    \"destination_entity\": \"limits-pod-too-big.yaml\"\\n  },\\n  {\\n    \"source_entity\": \"maxLimitRequestRatio\",\\n    \"description\": \"sets the CPU max limit request ratio\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"PersistentVolumeClaim\",\\n    \"description\": \"allows you to claim a certain amount of persistent storage\",\\n    \"destination_entity\": \"storage\"\\n  },\\n  {\\n    \"source_entity\": \"min\",\\n    \"description\": \"sets minimum value for a resource\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"defaultRequest\",\\n    \"description\": \"sets default request value for a container\",\\n    \"destination_entity\": \"containers\"\\n  },\\n  {\\n    \"source_entity\": \"default\",\\n    \"description\": \"sets default value for a resource\",\\n    \"destination_entity\": \"PersistentVolumeClaim\"\\n  },\\n  {\\n    \"source_entity\": \"min\",\\n    \"description\": \"sets minimum storage amount for a PVC\",\\n    \"destination_entity\": \"PVC\"\\n  },\\n  {\\n    \"source_entity\": \"max\",\\n    \"description\": \"sets maximum storage amount for a PVC\",\\n    \"destination_entity\": \"PVC\"\\n  },\\n  {\\n    \"source_entity\": \"limits-pod-too-big.yaml\",\\n    \"description\": \"tries to create a pod with CPU requests greater than the limit\",\\n    \"destination_entity\": \"LimitRange\"\\n  },\\n  {\\n    \"source_entity\": \"maxLimitRequestRatio\",\\n    \"description\": \"sets maximum ratio of limits vs. requests for a container\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"PersistentVolumeClaims\",\\n    \"description\": \"has minimum and maximum limits configured\",\\n    \"destination_entity\": \"storage\"\\n  },\\n  {\\n    \"source_entity\": \"defaultRequest\",\\n    \"description\": \"sets default request value for a container\",\\n    \"destination_entity\": \"containers\"\\n  }\\n]'},\n",
       " {'page': 456,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '424\\nCHAPTER 14\\nManaging pods’ computational resources\\nThe pod’s single container is requesting two CPUs, which is more than the maximum\\nyou set in the LimitRange earlier. Creating the pod yields the following result:\\n$ kubectl create -f limits-pod-too-big.yaml \\nError from server (Forbidden): error when creating \"limits-pod-too-big.yaml\": \\npods \"too-big\" is forbidden: [\\n  maximum cpu usage per Pod is 1, but request is 2., \\n  maximum cpu usage per Container is 1, but request is 2.]\\nI’ve modified the output slightly to make it more legible. The nice thing about the\\nerror message from the server is that it lists all the reasons why the pod was rejected,\\nnot only the first one it encountered. As you can see, the pod was rejected for two rea-\\nsons: you requested two CPUs for the container, but the maximum CPU limit for a\\ncontainer is one. Likewise, the pod as a whole requested two CPUs, but the maximum\\nis one CPU (if this was a multi-container pod, even if each individual container\\nrequested less than the maximum amount of CPU, together they’d still need to\\nrequest less than two CPUs to pass the maximum CPU for pods). \\n14.4.4 Applying default resource requests and limits\\nNow let’s also see how default resource requests and limits are set on containers that\\ndon’t specify them. Deploy the kubia-manual pod from chapter 3 again:\\n$ kubectl create -f ../Chapter03/kubia-manual.yaml\\npod \"kubia-manual\" created\\nBefore you set up your LimitRange object, all your pods were created without any\\nresource requests or limits, but now the defaults are applied automatically when creat-\\ning the pod. You can confirm this by describing the kubia-manual pod, as shown in\\nthe following listing.\\n$ kubectl describe po kubia-manual\\nName:           kubia-manual\\n...\\nContainers:\\n  kubia:\\n    Limits:\\n      cpu:      200m\\n      memory:   100Mi\\n    Requests:\\n      cpu:      100m\\n      memory:   10Mi\\nThe container’s requests and limits match the ones you specified in the LimitRange\\nobject. If you used a different LimitRange specification in another namespace, pods\\ncreated in that namespace would obviously have different requests and limits. This\\nallows admins to configure default, min, and max resources for pods per namespace.\\nListing 14.12\\nInspecting limits that were applied to a pod automatically\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for managing Kubernetes resources',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'create',\n",
       "    'description': 'Verb used to create a resource (e.g., pod, deployment)',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'f',\n",
       "    'description': 'Flag used with the create command to specify a file or YAML configuration',\n",
       "    'category': 'flag'},\n",
       "   {'entity': 'limits-pod-too-big.yaml',\n",
       "    'description': 'YAML file containing pod configuration with CPU limit request exceeding maximum allowed value',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'LimitRange',\n",
       "    'description': 'Kubernetes resource object used to set default, min, and max limits for pods per namespace',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'Resource type in Kubernetes that represents a logical host or container',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'CPU',\n",
       "    'description': 'Computational resource measured in CPU units (e.g., cores, threads)',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'memory',\n",
       "    'description': 'Computational resource measured in memory units (e.g., bytes, kilobytes)',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kubia-manual.yaml',\n",
       "    'description': 'YAML file containing pod configuration with default requests and limits set by LimitRange',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'kubia-manual',\n",
       "    'description': 'Pod name and reference to the YAML file containing its configuration',\n",
       "    'category': 'resource'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"CPU\",\\n    \"description\": \"Maximum CPU limit exceeded for a container\",\\n    \"destination_entity\": \"limits-pod-too-big.yaml\"\\n  },\\n  {\\n    \"source_entity\": \"CPU\",\\n    \"description\": \"Maximum CPU limit exceeded for a pod\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"LimitRange\",\\n    \"description\": \"Resource requests and limits set on containers without specification\",\\n    \"destination_entity\": \"kubia-manual.yaml\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"Create a pod from a YAML file\",\\n    \"destination_entity\": \"limits-pod-too-big.yaml\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"Create a pod from a YAML file\",\\n    \"destination_entity\": \"kubia-manual.yaml\"\\n  },\\n  {\\n    \"source_entity\": \"memory\",\\n    \"description\": \"Default resource requests and limits set on containers without specification\",\\n    \"destination_entity\": \"LimitRange\"\\n  },\\n  {\\n    \"source_entity\": \"f\",\\n    \"description\": \"Forbidden error message due to exceeding maximum CPU limit\",\\n    \"destination_entity\": \"pods\"\\n  }\\n]'},\n",
       " {'page': 457,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '425\\nLimiting the total resources available in a namespace\\nIf namespaces are used to separate different teams or to separate development, QA,\\nstaging, and production pods running in the same Kubernetes cluster, using a differ-\\nent LimitRange in each namespace ensures large pods can only be created in certain\\nnamespaces, whereas others are constrained to smaller pods.\\n But remember, the limits configured in a LimitRange only apply to each individual\\npod/container. It’s still possible to create many pods and eat up all the resources avail-\\nable in the cluster. LimitRanges don’t provide any protection from that. A Resource-\\nQuota object, on the other hand, does. You’ll learn about them next.\\n14.5\\nLimiting the total resources available in a namespace\\nAs you’ve seen, LimitRanges only apply to individual pods, but cluster admins also\\nneed a way to limit the total amount of resources available in a namespace. This is\\nachieved by creating a ResourceQuota object. \\n14.5.1 Introducing the ResourceQuota object\\nIn chapter 10 we said that several Admission Control plugins running inside the API\\nserver verify whether the pod may be created or not. In the previous section, I said\\nthat the LimitRanger plugin enforces the policies configured in LimitRange resources.\\nSimilarly, the ResourceQuota Admission Control plugin checks whether the pod\\nbeing created would cause the configured ResourceQuota to be exceeded. If that’s\\nthe case, the pod’s creation is rejected. Because resource quotas are enforced at pod\\ncreation time, a ResourceQuota object only affects pods created after the Resource-\\nQuota object is created—creating it has no effect on existing pods.\\n A ResourceQuota limits the amount of computational resources the pods and the\\namount of storage PersistentVolumeClaims in a namespace can consume. It can also\\nlimit the number of pods, claims, and other API objects users are allowed to create\\ninside the namespace. Because you’ve mostly dealt with CPU and memory so far, let’s\\nstart by looking at how to specify quotas for them.\\nCREATING A RESOURCEQUOTA FOR CPU AND MEMORY\\nThe overall CPU and memory all the pods in a namespace are allowed to consume is\\ndefined by creating a ResourceQuota object as shown in the following listing.\\napiVersion: v1\\nkind: ResourceQuota\\nmetadata:\\n  name: cpu-and-mem\\nspec:\\n  hard:\\n    requests.cpu: 400m\\n    requests.memory: 200Mi\\n    limits.cpu: 600m\\n    limits.memory: 500Mi\\nListing 14.13\\nA ResourceQuota resource for CPU and memory: quota-cpu-memory.yaml\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Namespace',\n",
       "    'description': 'An object in Kubernetes that represents a logical isolation environment.',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'LimitRange',\n",
       "    'description': 'A Kubernetes resource that specifies the limits for resources such as CPU and memory.',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'ResourceQuota',\n",
       "    'description': 'A Kubernetes object that sets limits on the total amount of resources available in a namespace.',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes, similar to a Docker container.',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': 'The central component of the Kubernetes control plane that exposes the Kubernetes API.',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'Admission Control',\n",
       "    'description': 'A mechanism in Kubernetes that checks incoming requests to ensure they meet certain criteria.',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'PersistentVolumeClaims',\n",
       "    'description': 'An object in Kubernetes that represents a request for storage resources.',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'CPU',\n",
       "    'description': 'A type of computational resource that can be allocated to pods.',\n",
       "    'category': 'Hardware'},\n",
       "   {'entity': 'Memory',\n",
       "    'description': 'A type of storage resource that can be allocated to pods.',\n",
       "    'category': 'Hardware'},\n",
       "   {'entity': 'Plugin',\n",
       "    'description': 'A component in Kubernetes that provides a specific functionality, such as Admission Control or the LimitRanger plugin.',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[{\"source_entity\": \"Kubernetes Cluster Admin\", \"description\": \"limits the total resources available\", \"destination_entity\": \"Namespace\"},\\n\\n {\"source_entity\": \"LimitRange\", \"description\": \"only applies to individual pods\", \"destination_entity\": \"Pod\"},\\n\\n {\"source_entity\": \"Admission Control Plugin\", \"description\": \"checks if pod creation exceeds ResourceQuota\", \"destination_entity\": \"ResourceQuota\"},\\n\\n {\"source_entity\": \"ResourceQuota Admission Control Plugin\", \"description\": \"rejects pod creation if exceeds ResourceQuota\", \"destination_entity\": \"Pod\"},\\n\\n {\"source_entity\": \"ResourceQuota\", \"description\": \"limits computational resources for pods and PersistentVolumeClaims\", \"destination_entity\": \"Namespace\"},\\n\\n {\"source_entity\": \"Kubernetes Cluster Admin\", \"description\": \"creates a ResourceQuota object\", \"destination_entity\": \"ResourceQuota\"},\\n\\n {\"source_entity\": \"API Server\", \"description\": \"runs Admission Control plugins\", \"destination_entity\": \"Admission Control Plugin\"},\\n\\n {\"source_entity\": \"Namespace\", \"description\": \"can consume computational resources\", \"destination_entity\": \"Pods in Namespace\"},\\n\\n {\"source_entity\": \"Plugin (LimitRanger)\", \"description\": \"enforces policies configured in LimitRange resources\", \"destination_entity\": \"LimitRange\"},\\n\\n {\"source_entity\": \"API Server\", \"description\": \"runs Admission Control plugins\", \"destination_entity\": \"Admission Control Plugin\"},\\n\\n {\"source_entity\": \"ResourceQuota\", \"description\": \"defines CPU and memory quotas for pods\", \"destination_entity\": \"Pods in Namespace\"},\\n\\n {\"source_entity\": \"Kubernetes Cluster Admin\", \"description\": \"can create multiple ResourceQuota objects\", \"destination_entity\": \"Namespace\"}]\\n\\nNote: Some entities like \\'CPU\\', \\'Memory\\', \\'PersistentVolumeClaims\\' are not included as source or destination entity in the above JSON list since they are more of attributes rather than entities.'},\n",
       " {'page': 458,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '426\\nCHAPTER 14\\nManaging pods’ computational resources\\nInstead of defining a single total for each resource, you define separate totals for\\nrequests and limits for both CPU and memory. You’ll notice the structure is a bit dif-\\nferent, compared to that of a LimitRange. Here, both the requests and the limits for\\nall resources are defined in a single place. \\n This ResourceQuota sets the maximum amount of CPU pods in the namespace\\ncan request to 400 millicores. The maximum total CPU limits in the namespace are\\nset to 600 millicores. For memory, the maximum total requests are set to 200 MiB,\\nwhereas the limits are set to 500 MiB.\\n A ResourceQuota object applies to the namespace it’s created in, like a Limit-\\nRange, but it applies to all the pods’ resource requests and limits in total and not to\\neach individual pod or container separately, as shown in figure 14.7.\\nINSPECTING THE QUOTA AND QUOTA USAGE\\nAfter you post the ResourceQuota object to the API server, you can use the kubectl\\ndescribe command to see how much of the quota is already used up, as shown in\\nthe following listing.\\n$ kubectl describe quota\\nName:           cpu-and-mem\\nNamespace:      default\\nResource        Used   Hard\\n--------        ----   ----\\nlimits.cpu      200m   600m\\nlimits.memory   100Mi  500Mi\\nrequests.cpu    100m   400m\\nrequests.memory 10Mi   200Mi\\nI only have the kubia-manual pod running, so the Used column matches its resource\\nrequests and limits. When I run additional pods, their requests and limits are added to\\nthe used amounts.\\nListing 14.14\\nInspecting the ResourceQuota with kubectl describe quota\\nLimitRange\\nResourceQuota\\nNamespace: FOO\\nPod A\\nPod B\\nPod C\\nLimitRange\\nResourceQuota\\nNamespace: BAR\\nPod D\\nPod E\\nPod F\\nFigure 14.7\\nLimitRanges apply to individual pods; ResourceQuotas apply to all pods in the \\nnamespace.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Namespace: FOO\n",
       "   LimitRange ResourceQuota\n",
       "   Pod A Pod B Pod C, Col1, Namespace: BAR\n",
       "   LimitRange ResourceQuota\n",
       "   Pod D Pod E Pod F]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'CPU',\n",
       "    'description': 'Computational resource',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'memory',\n",
       "    'description': 'Storage resource',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'requests',\n",
       "    'description': 'Maximum amount of CPU or memory a pod can request',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'limits',\n",
       "    'description': 'Maximum total CPU or memory limits in the namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ResourceQuota',\n",
       "    'description': 'Object that sets maximum amounts of CPU and memory for all pods in a namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'Logical grouping of resources, such as pods, services, etc.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'Instances of containerized applications running on the cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'Lightweight and standalone execution environment for an application',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool to interact with a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'describe',\n",
       "    'description': 'Command to display detailed information about a resource',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'api server',\n",
       "    'description': 'Component that manages communication between the control plane and other components of the cluster',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"defines separate totals for requests and limits for both CPU and memory\",\\n    \"destination_entity\": \"limits\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"sets the maximum amount of CPU pods can request\",\\n    \"destination_entity\": \"CPU\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"sets the maximum total CPU limits in the namespace\",\\n    \"destination_entity\": \"limits.cpu\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"sets the maximum total memory requests in the namespace\",\\n    \"destination_entity\": \"requests.memory\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"sets the maximum total memory limits in the namespace\",\\n    \"destination_entity\": \"limits.memory\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"used to see how much of the quota is already used up\",\\n    \"destination_entity\": \"quota usage\"\\n  },\\n  {\\n    \"source_entity\": \"api server\",\\n    \"description\": \"posts the ResourceQuota object to the API server\",\\n    \"destination_entity\": \"ResourceQuota\"\\n  },\\n  {\\n    \"source_entity\": \"pods\",\\n    \"description\": \"their requests and limits are added to the used amounts\",\\n    \"destination_entity\": \"quota usage\"\\n  },\\n  {\\n    \"source_entity\": \"requests.cpu\",\\n    \"description\": \"sets the maximum amount of CPU pods can request\",\\n    \"destination_entity\": \"CPU\"\\n  },\\n  {\\n    \"source_entity\": \"limits.cpu\",\\n    \"description\": \"sets the maximum total CPU limits in the namespace\",\\n    \"destination_entity\": \"CPU\"\\n  },\\n  {\\n    \"source_entity\": \"requests.memory\",\\n    \"description\": \"sets the maximum total memory requests in the namespace\",\\n    \"destination_entity\": \"memory\"\\n  },\\n  {\\n    \"source_entity\": \"limits.memory\",\\n    \"description\": \"sets the maximum total memory limits in the namespace\",\\n    \"destination_entity\": \"memory\"\\n  }\\n]'},\n",
       " {'page': 459,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '427\\nLimiting the total resources available in a namespace\\nCREATING A LIMITRANGE ALONG WITH A RESOURCEQUOTA\\nOne caveat when creating a ResourceQuota is that you will also want to create a Limit-\\nRange object alongside it. In your case, you have a LimitRange configured from the\\nprevious section, but if you didn’t have one, you couldn’t run the kubia-manual pod,\\nbecause it doesn’t specify any resource requests or limits. Here’s what would happen\\nin that case:\\n$ kubectl create -f ../Chapter03/kubia-manual.yaml\\nError from server (Forbidden): error when creating \"../Chapter03/kubia-\\nmanual.yaml\": pods \"kubia-manual\" is forbidden: failed quota: cpu-and-\\nmem: must specify limits.cpu,limits.memory,requests.cpu,requests.memory\\nWhen a quota for a specific resource (CPU or memory) is configured (request or\\nlimit), pods need to have the request or limit (respectively) set for that same resource;\\notherwise the API server will not accept the pod. That’s why having a LimitRange with\\ndefaults for those resources can make life a bit easier for people creating pods.\\n14.5.2 Specifying a quota for persistent storage\\nA ResourceQuota object can also limit the amount of persistent storage that can be\\nclaimed in the namespace, as shown in the following listing.\\napiVersion: v1\\nkind: ResourceQuota\\nmetadata:\\n  name: storage\\nspec:\\n  hard:\\n    requests.storage: 500Gi                               \\n    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi     \\n    standard.storageclass.storage.k8s.io/requests.storage: 1Ti\\nIn this example, the amount of storage all PersistentVolumeClaims in a namespace\\ncan request is limited to 500 GiB (by the requests.storage entry in the Resource-\\nQuota object). But as you’ll remember from chapter 6, PersistentVolumeClaims can\\nrequest a dynamically provisioned PersistentVolume of a specific StorageClass. That’s\\nwhy Kubernetes also makes it possible to define storage quotas for each StorageClass\\nindividually. The previous example limits the total amount of claimable SSD storage\\n(designated by the ssd StorageClass) to 300 GiB. The less-performant HDD storage\\n(StorageClass standard) is limited to 1 TiB.\\n14.5.3 Limiting the number of objects that can be created\\nA ResourceQuota can also be configured to limit the number of Pods, Replication-\\nControllers, Services, and other objects inside a single namespace. This allows the\\ncluster admin to limit the number of objects users can create based on their payment\\nListing 14.15\\nA ResourceQuota for storage: quota-storage.yaml\\nThe amount of \\nstorage claimable \\noverall\\nThe amount \\nof claimable \\nstorage in \\nStorageClass ssd\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ResourceQuota',\n",
       "    'description': 'An object that limits the total resources available in a namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'LimitRange',\n",
       "    'description': 'An object that sets default resource requests and limits for pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CPU',\n",
       "    'description': 'A computing resource that measures the execution time of a processor',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'memory',\n",
       "    'description': 'A type of computer storage that temporarily holds data being processed by the CPU',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'PersistentVolumeClaims',\n",
       "    'description': 'Objects that request access to persistent storage',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PersistentVolumes',\n",
       "    'description': 'Objects that provide persistent storage',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'StorageClass',\n",
       "    'description': 'A way to provision and manage persistent storage',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Entities that run containers in the cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicationControllers',\n",
       "    'description': 'Objects that manage the replication of pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'Objects that provide network connectivity to pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubia-manual.yaml',\n",
       "    'description': 'A configuration file for the kubia-manual pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool for interacting with Kubernetes clusters',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"limits the total resources available in a namespace\",\\n    \"destination_entity\": \"namespace\"\\n  },\\n  {\\n    \"source_entity\": \"LimitRange\",\\n    \"description\": \"sets defaults for resource requests and limits\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"limits the amount of persistent storage that can be claimed\",\\n    \"destination_entity\": \"PersistentVolumeClaims\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"limits the number of objects that can be created inside a namespace\",\\n    \"destination_entity\": \"ReplicationControllers\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"limits the number of objects that can be created inside a namespace\",\\n    \"destination_entity\": \"Services\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"limits the number of objects that can be created inside a namespace\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"creates a pod without specifying resource requests or limits\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"limits the amount of storage claimable for a specific StorageClass\",\\n    \"destination_entity\": \"StorageClass\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"limits the amount of SSD storage that can be claimed\",\\n    \"destination_entity\": \"PersistentVolumeClaims\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"limits the amount of HDD storage that can be claimed\",\\n    \"destination_entity\": \"PersistentVolumeClaims\"\\n  }\\n]\\n```\\n\\nNote: I\\'ve extracted relations based on the entities provided and the context of the document page.'},\n",
       " {'page': 460,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '428\\nCHAPTER 14\\nManaging pods’ computational resources\\nplan, for example, and can also limit the number of public IPs or node ports Ser-\\nvices can use. \\n The following listing shows what a ResourceQuota object that limits the number of\\nobjects may look like.\\napiVersion: v1\\nkind: ResourceQuota\\nmetadata:\\n  name: objects\\nspec:\\n  hard:\\n    pods: 10                        \\n    replicationcontrollers: 5       \\n    secrets: 10                     \\n    configmaps: 10                  \\n    persistentvolumeclaims: 4       \\n    services: 5                      \\n    services.loadbalancers: 1        \\n    services.nodeports: 2            \\n    ssd.storageclass.storage.k8s.io/persistentvolumeclaims: 2   \\nThe ResourceQuota in this listing allows users to create at most 10 Pods in the name-\\nspace, regardless if they’re created manually or by a ReplicationController, Replica-\\nSet, DaemonSet, Job, and so on. It also limits the number of ReplicationControllers to\\nfive. A maximum of five Services can be created, of which only one can be a LoadBal-\\nancer-type Service, and only two can be NodePort Services. Similar to how the maxi-\\nmum amount of requested storage can be specified per StorageClass, the number of\\nPersistentVolumeClaims can also be limited per StorageClass.\\n Object count quotas can currently be set for the following objects: \\n\\uf0a1Pods\\n\\uf0a1ReplicationControllers \\n\\uf0a1Secrets\\n\\uf0a1ConfigMaps\\n\\uf0a1PersistentVolumeClaims\\n\\uf0a1Services (in general), and for two specific types of Services, such as Load-\\nBalancer Services (services.loadbalancers) and NodePort Services (ser-\\nvices.nodeports) \\nFinally, you can even set an object count quota for ResourceQuota objects themselves.\\nThe number of other objects, such as ReplicaSets, Jobs, Deployments, Ingresses, and\\nso on, cannot be limited yet (but this may have changed since the book was published,\\nso please check the documentation for up-to-date information).\\nListing 14.16\\nA ResourceQuota for max number of resources: quota-object-count.yaml\\nOnly 10 Pods, 5 ReplicationControllers, \\n10 Secrets, 10 ConfigMaps, and \\n4 PersistentVolumeClaims can be \\ncreated in the namespace.\\nFive Services overall can be created, \\nof which at most one can be a \\nLoadBalancer Service and at most \\ntwo can be NodePort Services.\\nOnly two PVCs can claim storage\\nwith the ssd StorageClass.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ResourceQuota',\n",
       "    'description': 'an object that limits the number of resources in a namespace',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'compute resources managed by Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'a controller that manages replicas of a pod',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'secrets',\n",
       "    'description': 'objects that store sensitive information',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'configmaps',\n",
       "    'description': 'key-value pairs stored as objects',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'persistentvolumeclaims',\n",
       "    'description': 'objects that request storage resources',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'services',\n",
       "    'description': 'abstracted access to network resources',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'loadbalancers',\n",
       "    'description': 'controllers that distribute traffic across multiple instances',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'nodeports',\n",
       "    'description': 'a service type that exposes a port on each node in the cluster',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'ResourceQuota objects themselves',\n",
       "    'description': 'the ability to set object count quotas for ResourceQuota objects',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ResourceQuota\", \"description\": \"limits the number of pods\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"ResourceQuota\", \"description\": \"limits the number of ReplicationControllers\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"ResourceQuota\", \"description\": \"limits the number of secrets\", \"destination_entity\": \"secrets\"},\\n  {\"source_entity\": \"ResourceQuota\", \"description\": \"limits the number of configmaps\", \"destination_entity\": \"configmaps\"},\\n  {\"source_entity\": \"ResourceQuota\", \"description\": \"limits the number of persistentvolumeclaims\", \"destination_entity\": \"persistentvolumeclaims\"},\\n  {\"source_entity\": \"ResourceQuota\", \"description\": \"limits the number of services\", \"destination_entity\": \"services\"},\\n  {\"source_entity\": \"ResourceQuota\", \"description\": \"limits the number of LoadBalancer Services\", \"destination_entity\": \"loadbalancers\"},\\n  {\"source_entity\": \"ResourceQuota\", \"description\": \"limits the number of NodePort Services\", \"destination_entity\": \"nodeports\"},\\n  {\"source_entity\": \"ResourceQuota\", \"description\": \"limits the count of ResourceQuota objects themselves\", \"destination_entity\": \"ResourceQuota objects themselves\"}\\n]\\n```\\n\\nThese relations were extracted by analyzing the text and identifying the entities mentioned in it. The source entity is the one performing an action (in this case, limiting or counting), while the destination entity is the object on which that action is performed.'},\n",
       " {'page': 461,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '429\\nLimiting the total resources available in a namespace\\n14.5.4 Specifying quotas for specific pod states and/or QoS classes\\nThe quotas you’ve created so far have applied to all pods, regardless of their current\\nstate and QoS class. But quotas can also be limited to a set of quota scopes. Four scopes are\\ncurrently available: BestEffort, NotBestEffort, Terminating, and NotTerminating. \\n The BestEffort and NotBestEffort scopes determine whether the quota applies\\nto pods with the BestEffort QoS class or with one of the other two classes (that is,\\nBurstable and Guaranteed). \\n The other two scopes (Terminating and NotTerminating) don’t apply to pods\\nthat are (or aren’t) in the process of shutting down, as the name might lead you to\\nbelieve. We haven’t talked about this, but you can specify how long each pod is\\nallowed to run before it’s terminated and marked as Failed. This is done by setting\\nthe activeDeadlineSeconds field in the pod spec. This property defines the number\\nof seconds a pod is allowed to be active on the node relative to its start time before it’s\\nmarked as Failed and then terminated. The Terminating quota scope applies to pods\\nthat have the activeDeadlineSeconds set, whereas the NotTerminating applies to\\nthose that don’t. \\n When creating a ResourceQuota, you can specify the scopes that it applies to. A\\npod must match all the specified scopes for the quota to apply to it. Additionally, what\\na quota can limit depends on the quota’s scope. BestEffort scope can only limit the\\nnumber of pods, whereas the other three scopes can limit the number of pods,\\nCPU/memory requests, and CPU/memory limits. \\n If, for example, you want the quota to apply only to BestEffort, NotTerminating\\npods, you can create the ResourceQuota object shown in the following listing.\\napiVersion: v1\\nkind: ResourceQuota\\nmetadata:\\n  name: besteffort-notterminating-pods\\nspec:\\n  scopes:                 \\n  - BestEffort            \\n  - NotTerminating        \\n  hard: \\n    pods: 4          \\nThis quota ensures that at most four pods exist with the BestEffort QoS class,\\nwhich don’t have an active deadline. If the quota was targeting NotBestEffort pods\\ninstead, you could also specify requests.cpu, requests.memory, limits.cpu, and\\nlimits.memory.\\nNOTE\\nBefore you move on to the next section of this chapter, please delete\\nall the ResourceQuota and LimitRange resources you created. You won’t\\nListing 14.17\\nResourceQuota for BestEffort/NotTerminating pods: \\nquota-scoped.yaml\\nThis quota only applies to pods \\nthat have the BestEffort QoS and \\ndon’t have an active deadline set.\\nOnly four such \\npods can exist.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ResourceQuota',\n",
       "    'description': 'A Kubernetes resource that specifies quotas for specific pod states and/or QoS classes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'quota scopes',\n",
       "    'description': 'Four quota scopes available in Kubernetes: BestEffort, NotBestEffort, Terminating, and NotTerminating.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'Kubernetes pods that can be limited by quotas.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'QoS classes',\n",
       "    'description': 'Classes of resource quality of service in Kubernetes: BestEffort, Burstable, and Guaranteed.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'activeDeadlineSeconds',\n",
       "    'description': \"A pod spec property that defines the number of seconds a pod is allowed to be active on a node before it's marked as Failed and terminated.\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ResourceQuota object',\n",
       "    'description': 'A Kubernetes resource that specifies quotas for specific pod states and/or QoS classes.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'LimitRange',\n",
       "    'description': 'A Kubernetes resource that limits the resources available in a namespace.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'BestEffort scope',\n",
       "    'description': 'A quota scope that applies to pods with the BestEffort QoS class or with one of the other two classes (Burstable and Guaranteed).',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'NotBestEffort scope',\n",
       "    'description': 'A quota scope that determines whether the quota applies to pods with one of the other three QoS classes.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Terminating scope',\n",
       "    'description': 'A quota scope that applies to pods that have an active deadline set.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'NotTerminating scope',\n",
       "    'description': \"A quota scope that applies to pods that don't have an active deadline set.\",\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"pods\",\\n    \"description\": \"can exist with BestEffort QoS class without active deadline\",\\n    \"destination_entity\": \"BestEffort scope\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota object\",\\n    \"description\": \"can be created to limit resources for pods\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"QoS classes\",\\n    \"description\": \"determine whether quota applies to pods\",\\n    \"destination_entity\": \"quotas\"\\n  },\\n  {\\n    \"source_entity\": \"activeDeadlineSeconds\",\\n    \"description\": \"defines number of seconds a pod is allowed to run before termination\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"Terminating scope\",\\n    \"description\": \"applies to pods that have active deadline set\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"NotTerminating scope\",\\n    \"description\": \"applies to pods that don\\'t have active deadline set\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"can be created to limit resources for specific pod states and QoS classes\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"LimitRange\",\\n    \"description\": \"resource that can be used to limit resources for pods\",\\n    \"destination_entity\": \"ResourceQuota\"\\n  },\\n  {\\n    \"source_entity\": \"BestEffort scope\",\\n    \"description\": \"can only limit number of pods\",\\n    \"destination_entity\": \"quotas\"\\n  },\\n  {\\n    \"source_entity\": \"NotBestEffort scope\",\\n    \"description\": \"can limit CPU/memory requests and limits for NotTerminating pods\",\\n    \"destination_entity\": \"NotTerminating scope\"\\n  },\\n  {\\n    \"source_entity\": \"Terminating scope\",\\n    \"description\": \"can limit number of pods, CPU/memory requests, and limits for Terminating pods\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"NotTerminating scope\",\\n    \"description\": \"can only limit number of NotTerminating pods\",\\n    \"destination_entity\": \"quotas\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota object\",\\n    \"description\": \"can be created to target specific quota scopes\",\\n    \"destination_entity\": \"quota scopes\"\\n  }\\n]'},\n",
       " {'page': 462,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '430\\nCHAPTER 14\\nManaging pods’ computational resources\\nneed them anymore and they may interfere with examples in the following\\nchapters.\\n14.6\\nMonitoring pod resource usage\\nProperly setting resource requests and limits is crucial for getting the most out of your\\nKubernetes cluster. If requests are set too high, your cluster nodes will be underuti-\\nlized and you’ll be throwing money away. If you set them too low, your apps will be\\nCPU-starved or even killed by the OOM Killer. How do you find the sweet spot for\\nrequests and limits?\\n You find it by monitoring the actual resource usage of your containers under the\\nexpected load levels. Once the application is exposed to the public, you should keep\\nmonitoring it and adjust the resource requests and limits if required.\\n14.6.1 Collecting and retrieving actual resource usages\\nHow does one monitor apps running in Kubernetes? Luckily, the Kubelet itself\\nalready contains an agent called cAdvisor, which performs the basic collection of\\nresource consumption data for both individual containers running on the node and\\nthe node as a whole. Gathering those statistics centrally for the whole cluster requires\\nyou to run an additional component called Heapster. \\n Heapster runs as a pod on one of the nodes and is exposed through a regular\\nKubernetes Service, making it accessible at a stable IP address. It collects the data\\nfrom all cAdvisors in the cluster and exposes it in a single location. Figure 14.8\\nshows the flow of the metrics data from the pods, through cAdvisor and finally into\\nHeapster.\\nKubelet\\ncAdvisor\\nNode 1\\nPod\\nPod\\nKubelet\\ncAdvisor\\nNode 2\\nPod\\nKubelet\\ncAdvisor\\nNode X\\nPod\\nHeapster\\nEach cAdvisor collects metrics from\\ncontainers running on its node.\\nHeapster runs on one of the nodes as a\\npod and collects metrics from all nodes.\\nFigure 14.8\\nThe flow of metrics data into Heapster\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Lightweight and portable containerization',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'cAdvisor',\n",
       "    'description': 'Agent that collects resource consumption data for containers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Heapster',\n",
       "    'description': 'Component that gathers statistics centrally for the whole cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'Agent that runs on each node and provides a way to run pods on it',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'Abstraction layer provided by Kubernetes to access an application',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Resource requests',\n",
       "    'description': 'Amount of resources requested by an app to run efficiently',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Resource limits',\n",
       "    'description': 'Upper bound on the amount of resources allowed to be consumed by an app',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'OOM Killer',\n",
       "    'description': 'Component that kills processes that consume too much memory',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"manages pods\\' computational resources\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Users\",\\n    \"description\": \"set resource requests and limits to get the most out of Kubernetes cluster\",\\n    \"destination_entity\": \"Resource requests\"\\n  },\\n  {\\n    \"source_entity\": \"Users\",\\n    \"description\": \"find the sweet spot for requests and limits by monitoring actual resource usage\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Heapster\",\\n    \"description\": \"collects metrics from all cAdvisors in the cluster\",\\n    \"destination_entity\": \"cAdvisor\"\\n  },\\n  {\\n    \"source_entity\": \"Heapster\",\\n    \"description\": \"exposes collected metrics in a single location\",\\n    \"destination_entity\": \"Users\"\\n  },\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"contains an agent called cAdvisor to collect resource consumption data\",\\n    \"destination_entity\": \"cAdvisor\"\\n  },\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"runs as a process on each node\",\\n    \"destination_entity\": \"Node\"\\n  },\\n  {\\n    \"source_entity\": \"Heapster\",\\n    \"description\": \"runs as a pod on one of the nodes and collects metrics from all cAdvisors\",\\n    \"destination_entity\": \"cAdvisor\"\\n  },\\n  {\\n    \"source_entity\": \"Heapster\",\\n    \"description\": \"exposes collected metrics through a regular Kubernetes Service\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"OOM Killer\",\\n    \"description\": \"kills apps when they are CPU-starved or run out of memory\",\\n    \"destination_entity\": \"Apps\"\\n  },\\n  {\\n    \"source_entity\": \"Users\",\\n    \"description\": \"monitor actual resource usage of containers under expected load levels\",\\n    \"destination_entity\": \"Resource limits\"\\n  },\\n  {\\n    \"source_entity\": \"Heapster\",\\n    \"description\": \"collects metrics from all nodes in the cluster\",\\n    \"destination_entity\": \"Node\"\\n  }\\n]\\n```\\n\\nNote: I\\'ve assumed that \"Users\" is an entity, as they are mentioned in the context of setting resource requests and limits. If you\\'d like to replace it with a more specific entity (e.g., \"Cluster Administrators\"), feel free to do so!'},\n",
       " {'page': 463,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '431\\nMonitoring pod resource usage\\nThe arrows in the figure show how the metrics data flows. They don’t show which com-\\nponent connects to which to get the data. The pods (or the containers running\\ntherein) don’t know anything about cAdvisor, and cAdvisor doesn’t know anything\\nabout Heapster. It’s Heapster that connects to all the cAdvisors, and it’s the cAdvisors\\nthat collect the container and node usage data without having to talk to the processes\\nrunning inside the pods’ containers.\\nENABLING HEAPSTER\\nIf you’re running a cluster in Google Kubernetes Engine, Heapster is enabled by\\ndefault. If you’re using Minikube, it’s available as an add-on and can be enabled with\\nthe following command:\\n$ minikube addons enable heapster\\nheapster was successfully enabled\\nTo run Heapster manually in other types of Kubernetes clusters, you can refer to\\ninstructions located at https:/\\n/github.com/kubernetes/heapster. \\n After enabling Heapster, you’ll need to wait a few minutes for it to collect metrics\\nbefore you can see resource usage statistics for your cluster, so be patient. \\nDISPLAYING CPU AND MEMORY USAGE FOR CLUSTER NODES\\nRunning Heapster in your cluster makes it possible to obtain resource usages for\\nnodes and individual pods through the kubectl top command. To see how much\\nCPU and memory is being used on your nodes, you can run the command shown in\\nthe following listing.\\n$ kubectl top node\\nNAME       CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%\\nminikube   170m         8%        556Mi           27%\\nThis shows the actual, current CPU and memory usage of all the pods running on the\\nnode, unlike the kubectl describe node command, which shows the amount of CPU\\nand memory requests and limits instead of actual runtime usage data. \\nDISPLAYING CPU AND MEMORY USAGE FOR INDIVIDUAL PODS\\nTo see how much each individual pod is using, you can use the kubectl top pod com-\\nmand, as shown in the following listing.\\n$ kubectl top pod --all-namespaces\\nNAMESPACE      NAME                             CPU(cores)   MEMORY(bytes)\\nkube-system    influxdb-grafana-2r2w9           1m           32Mi\\nkube-system    heapster-40j6d                   0m           18Mi\\nListing 14.18\\nActual CPU and memory usage of nodes\\nListing 14.19\\nActual CPU and memory usages of pods\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Heapster',\n",
       "    'description': 'A component that collects metrics data from cAdvisors to display resource usage statistics for cluster nodes and individual pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'cAdvisor',\n",
       "    'description': \"A tool that collects container and node usage data without having to talk to the processes running inside the pods' containers.\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl top command',\n",
       "    'description': 'A command used to display actual CPU and memory usage of cluster nodes and individual pods.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Heapster',\n",
       "    'description': 'A component that collects metrics data from cAdvisors to display resource usage statistics for cluster nodes and individual pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'cAdvisor',\n",
       "    'description': \"A tool that collects container and node usage data without having to talk to the processes running inside the pods' containers.\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl top node command',\n",
       "    'description': 'A command used to display actual CPU and memory usage of cluster nodes.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubectl top pod command',\n",
       "    'description': 'A command used to display actual CPU and memory usage of individual pods.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'minikube addons enable heapster command',\n",
       "    'description': 'A command used to enable Heapster on a Minikube cluster.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Heapster',\n",
       "    'description': 'A component that collects metrics data from cAdvisors to display resource usage statistics for cluster nodes and individual pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes Engine',\n",
       "    'description': 'A cloud-based container orchestration system provided by Google.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Heapster',\n",
       "    'description': 'A component that collects metrics data from cAdvisors to display resource usage statistics for cluster nodes and individual pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod resources',\n",
       "    'description': 'The actual CPU and memory usage of pods in a Kubernetes cluster.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Heapster\",\\n    \"description\": \"collects metrics data from cAdvisors\",\\n    \"destination_entity\": \"cAdvisor\"\\n  },\\n  {\\n    \"source_entity\": \"Heapster\",\\n    \"description\": \"connects to all the cAdvisors to get data\",\\n    \"destination_entity\": \"all the cAdvisors\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl top command\",\\n    \"description\": \"displays actual CPU and memory usage for nodes\",\\n    \"destination_entity\": \"pod resources\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl top node command\",\\n    \"description\": \"shows actual, current CPU and memory usage of all pods on the node\",\\n    \"destination_entity\": \"node resources\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl top pod command\",\\n    \"description\": \"displays actual CPU and memory usage for individual pods\",\\n    \"destination_entity\": \"pod resources\"\\n  },\\n  {\\n    \"source_entity\": \"minikube addons enable heapster command\",\\n    \"description\": \"enables Heapster in Minikube cluster\",\\n    \"destination_entity\": \"Heapster\"\\n  },\\n  {\\n    \"source_entity\": \"cAdvisor\",\\n    \"description\": \"collects container and node usage data without talking to processes running inside the pods\\' containers\",\\n    \"destination_entity\": \"pod resources\"\\n  }\\n]\\n```\\n\\nNote that I have only included relations where an entity performs some action on another entity, as per your instructions. Let me know if you\\'d like me to clarify any of these relations!'},\n",
       " {'page': 464,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '432\\nCHAPTER 14\\nManaging pods’ computational resources\\ndefault        kubia-3773182134-63bmb           0m           9Mi\\nkube-system    kube-dns-v20-z0hq6               1m           11Mi\\nkube-system    kubernetes-dashboard-r53mc       0m           14Mi\\nkube-system    kube-addon-manager-minikube      7m           33Mi\\nThe outputs of both these commands are fairly simple, so you probably don’t need me\\nto explain them, but I do need to warn you about one thing. Sometimes the top pod\\ncommand will refuse to show any metrics and instead print out an error like this:\\n$ kubectl top pod\\nW0312 22:12:58.021885   15126 top_pod.go:186] Metrics not available for pod \\ndefault/kubia-3773182134-63bmb, age: 1h24m19.021873823s\\nerror: Metrics not available for pod default/kubia-3773182134-63bmb, age: \\n1h24m19.021873823s\\nIf this happens, don’t start looking for the cause of the error yet. Relax, wait a while,\\nand rerun the command—it may take a few minutes, but the metrics should appear\\neventually. The kubectl top command gets the metrics from Heapster, which aggre-\\ngates the data over a few minutes and doesn’t expose it immediately. \\nTIP\\nTo see resource usages across individual containers instead of pods, you\\ncan use the --containers option. \\n14.6.2 Storing and analyzing historical resource consumption statistics\\nThe top command only shows current resource usages—it doesn’t show you how\\nmuch CPU or memory your pods consumed throughout the last hour, yesterday, or a\\nweek ago, for example. In fact, both cAdvisor and Heapster only hold resource usage\\ndata for a short window of time. If you want to analyze your pods’ resource consump-\\ntion over longer time periods, you’ll need to run additional tools.\\n When using Google Kubernetes Engine, you can monitor your cluster with Google\\nCloud Monitoring, but when you’re running your own local Kubernetes cluster\\n(either through Minikube or other means), people usually use InfluxDB for storing\\nstatistics data and Grafana for visualizing and analyzing them. \\nINTRODUCING INFLUXDB AND GRAFANA\\nInfluxDB is an open source time-series database ideal for storing application metrics\\nand other monitoring data. Grafana, also open source, is an analytics and visualization\\nsuite with a nice-looking web console that allows you to visualize the data stored in\\nInfluxDB and discover how your application’s resource usage behaves over time (an\\nexample showing three Grafana charts is shown in figure 14.9).\\n \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'A command-line tool to interact with Kubernetes clusters.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'top_pod.go',\n",
       "    'description': 'A Go file containing code for the `kubectl top` command.',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'Heapster',\n",
       "    'description': 'An aggregator that collects resource usage metrics from Kubernetes clusters.',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'cAdvisor',\n",
       "    'description': 'A tool for monitoring and analyzing container resources.',\n",
       "    'category': 'tool'},\n",
       "   {'entity': 'InfluxDB',\n",
       "    'description': 'An open-source time-series database for storing application metrics.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Grafana',\n",
       "    'description': 'An analytics and visualization suite for visualizing data stored in InfluxDB.',\n",
       "    'category': 'tool'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'A tool for running a local Kubernetes cluster on a single machine.',\n",
       "    'category': 'tool'},\n",
       "   {'entity': 'Google Cloud Monitoring',\n",
       "    'description': 'A service for monitoring Google Kubernetes Engine clusters.',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'The basic execution unit in a Kubernetes cluster.',\n",
       "    'category': 'object'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A lightweight and standalone process in a pod.',\n",
       "    'category': 'object'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Heapster\", \"description\": \"aggregates data over a few minutes and doesn\\'t expose it immediately.\", \"destination_entity\": \"kubectl\"},\\n  {\"source_entity\": \"cAdvisor\", \"description\": \"holds resource usage data for a short window of time\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"InfluxDB\", \"description\": \"stores application metrics and other monitoring data\", \"destination_entity\": \"Grafana\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"gets the metrics from Heapster\", \"destination_entity\": \"Heapster\"},\\n  {\"source_entity\": \"Minikube\", \"description\": \"a local Kubernetes cluster\", \"destination_entity\": \"InfluxDB\"},\\n  {\"source_entity\": \"Grafana\", \"description\": \"visualizes and analyzes data stored in InfluxDB\", \"destination_entity\": \"InfluxDB\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"uses the --containers option to see resource usages across individual containers\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"Heapster\", \"description\": \"stores resource usage data for a short window of time\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Grafana\", \"description\": \"displays charts showing how application\\'s resource usage behaves over time\", \"destination_entity\": \"application metrics\"}\\n]\\n```\\n\\nNote that some entities may have multiple relations, but I\\'ve only extracted each relation once. Let me know if you\\'d like me to clarify or expand on any of these!'},\n",
       " {'page': 465,\n",
       "  'img_cnt': 1,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '433\\nMonitoring pod resource usage\\nRUNNING INFLUXDB AND GRAFANA IN YOUR CLUSTER\\nBoth InfluxDB and Grafana can run as pods. Deploying them is straightforward. All\\nthe necessary manifests are available in the Heapster Git repository at http:/\\n/github\\n.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb.\\n When using Minikube, you don’t even need to deploy them manually, because\\nthey’re deployed along with Heapster when you enable the Heapster add-on.\\nANALYZING RESOURCE USAGE WITH GRAFANA\\nTo discover how much of each resource your pod requires over time, open the\\nGrafana web console and explore the predefined dashboards. Generally, you can find\\nout the URL of Grafana’s web console with kubectl cluster-info:\\n$ kubectl cluster-info\\n...\\nmonitoring-grafana is running at \\nhttps://192.168.99.100:8443/api/v1/proxy/namespaces/kube-\\nsystem/services/monitoring-grafana\\nFigure 14.9\\nGrafana dashboard showing CPU usage across the cluster\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'InfluxDB',\n",
       "    'description': 'A time-series database for storing monitoring data',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Grafana',\n",
       "    'description': 'An open-source platform for building dashboards and visualizing data',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Heapster',\n",
       "    'description': 'A monitoring add-on for Kubernetes clusters, providing features like resource usage tracking and anomaly detection',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'A tool for running a single-node Kubernetes cluster on a local machine',\n",
       "    'category': 'container management tool'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line interface for interacting with a Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'cluster-info',\n",
       "    'description': 'A kubectl command for displaying information about the current Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'monitoring-grafana',\n",
       "    'description': \"A service running in the kube-system namespace, providing access to Grafana's web console\",\n",
       "    'category': 'service'}],\n",
       "  'relationships': '[\\n    {\\n        \"source_entity\": \"kubectl\",\\n        \"description\": \"run cluster-info command to get Grafana URL\",\\n        \"destination_entity\": \"cluster-info\"\\n    },\\n    {\\n        \"source_entity\": \"Grafana\",\\n        \"description\": \"provide dashboards for monitoring pod resource usage\",\\n        \"destination_entity\": \"monitoring-grafana\"\\n    },\\n    {\\n        \"source_entity\": \"Heapster\",\\n        \"description\": \"deploy InfluxDB and Grafana as pods\",\\n        \"destination_entity\": \"InfluxDB\"\\n    },\\n    {\\n        \"source_entity\": \"Heapster\",\\n        \"description\": \"deploy InfluxDB and Grafana as pods\",\\n        \"destination_entity\": \"Grafana\"\\n    },\\n    {\\n        \"source_entity\": \"Minikube\",\\n        \"description\": \"enable Heapster add-on to deploy InfluxDB and Grafana\",\\n        \"destination_entity\": \"Heapster\"\\n    },\\n    {\\n        \"source_entity\": \"InfluxDB\",\\n        \"description\": \"run as a pod in the cluster\",\\n        \"destination_entity\": \"cluster\"\\n    },\\n    {\\n        \"source_entity\": \"Grafana\",\\n        \"description\": \"run as a pod in the cluster\",\\n        \"destination_entity\": \"cluster\"\\n    },\\n    {\\n        \"source_entity\": \"kubectl\",\\n        \"description\": \"use to run commands on the cluster\",\\n        \"destination_entity\": \"cluster\"\\n    }\\n]\\n```'},\n",
       " {'page': 466,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '434\\nCHAPTER 14\\nManaging pods’ computational resources\\nWhen using Minikube, Grafana’s web console is exposed through a NodePort Service,\\nso you can open it in your browser with the following command:\\n$ minikube service monitoring-grafana -n kube-system\\nOpening kubernetes service kube-system/monitoring-grafana in default \\nbrowser...\\nA new browser window or tab will open and show the Grafana Home screen. On the\\nright-hand side, you’ll see a list of dashboards containing two entries:\\n\\uf0a1Cluster\\n\\uf0a1Pods\\nTo see the resource usage statistics of the nodes, open the Cluster dashboard. There\\nyou’ll see several charts showing the overall cluster usage, usage by node, and the\\nindividual usage for CPU, memory, network, and filesystem. The charts will not only\\nshow the actual usage, but also the requests and limits for those resources (where\\nthey apply).\\n If you then switch over to the Pods dashboard, you can examine the resource\\nusages for each individual pod, again with both requests and limits shown alongside\\nthe actual usage. \\n Initially, the charts show the statistics for the last 30 minutes, but you can zoom out\\nand see the data for much longer time periods: days, months, or even years.\\nUSING THE INFORMATION SHOWN IN THE CHARTS\\nBy looking at the charts, you can quickly see if the resource requests or limits you’ve\\nset for your pods need to be raised or whether they can be lowered to allow more pods\\nto fit on your nodes. Let’s look at an example. Figure 14.10 shows the CPU and mem-\\nory charts for a pod.\\n At the far right of the top chart, you can see the pod is using more CPU than was\\nrequested in the pod’s manifest. Although this isn’t problematic when this is the only\\npod running on the node, you should keep in mind that a pod is only guaranteed as\\nmuch of a resource as it requests through resource requests. Your pod may be running\\nfine now, but when other pods are deployed to the same node and start using the\\nCPU, your pod’s CPU time may be throttled. Because of this, to ensure the pod can\\nuse as much CPU as it needs to at any time, you should raise the CPU resource request\\nfor the pod’s container.\\n The bottom chart shows the pod’s memory usage and request. Here the situation is\\nthe exact opposite. The amount of memory the pod is using is well below what was\\nrequested in the pod’s spec. The requested memory is reserved for the pod and won’t\\nbe available to other pods. The unused memory is therefore wasted. You should\\ndecrease the pod’s memory request to make the memory available to other pods run-\\nning on the node. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Minikube',\n",
       "    'description': 'A tool for running Kubernetes locally on a machine',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Grafana',\n",
       "    'description': 'An open-source platform for building dashboards and visualizations',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'NodePort Service',\n",
       "    'description': 'A type of service in Kubernetes that exposes a port on the node level',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line interface for running commands against a Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A single instance of a container in a Kubernetes cluster',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'CPU requests and limits',\n",
       "    'description': 'The amount of CPU resources requested or limited for a pod',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'memory requests and limits',\n",
       "    'description': 'The amount of memory resources requested or limited for a pod',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Cluster dashboard',\n",
       "    'description': 'A view in Grafana that shows resource usage statistics for the entire cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pods dashboard',\n",
       "    'description': 'A view in Grafana that shows resource usage statistics for individual pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'requests and limits',\n",
       "    'description': 'The amount of resources requested or limited for a pod',\n",
       "    'category': 'resource'}],\n",
       "  'relationships': '[{\"source_entity\": \"Cluster\", \"description\": \"displaying resource usage statistics of the nodes\", \"destination_entity\": \"charts\"}, \\n{\"source_entity\": \"Charts\", \"description\": \"showing actual, request and limit usage for resources\", \"destination_entity\": \"resources\"}, \\n{\"source_entity\": \"Pods dashboard\", \"description\": \"examining resource usages for each individual pod\", \"destination_entity\": \"pods\"}, \\n{\"source_entity\": \"pod\", \"description\": \"using more CPU than requested in the manifest\", \"destination_entity\": \"CPU requests and limits\"}, \\n{\"source_entity\": \"pod\", \"description\": \"using less memory than requested in the spec\", \"destination_entity\": \"memory requests and limits\"}, \\n{\"source_entity\": \"Grafana\", \"description\": \"exposing web console through NodePort Service\", \"destination_entity\": \"NodePort Service\"}, \\n{\"source_entity\": \"Minikube\", \"description\": \"providing service to access Grafana web console\", \"destination_entity\": \"kubectl\"}, \\n{\"source_entity\": \"requests and limits\", \"description\": \"raising or lowering based on resource usage statistics\", \"destination_entity\": \"pods\"}, \\n{\"source_entity\": \"charts\", \"description\": \"showing resource usage for CPU, memory, network and filesystem\", \"destination_entity\": \"resources\"}]'},\n",
       " {'page': 467,\n",
       "  'img_cnt': 1,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '435\\nSummary\\n14.7\\nSummary\\nThis chapter has shown you that you need to consider your pod’s resource usage and\\nconfigure both the resource requests and the limits for your pod to keep everything\\nrunning smoothly. The key takeaways from this chapter are\\n\\uf0a1Specifying resource requests helps Kubernetes schedule pods across the cluster.\\n\\uf0a1Specifying resource limits keeps pods from starving other pods of resources.\\n\\uf0a1Unused CPU time is allocated based on containers’ CPU requests.\\n\\uf0a1Containers never get killed if they try to use too much CPU, but they are killed\\nif they try to use too much memory.\\n\\uf0a1In an overcommitted system, containers also get killed to free memory for more\\nimportant pods, based on the pods’ QoS classes and actual memory usage.\\nActual CPU usage is higher\\nthan what was requested.\\nThe application’s CPU time\\nwill be throttled when other\\napps demand more CPU.\\nYou should increase the\\nCPU request.\\nActual memory usage is well\\nbelow requested memory.\\nYou’ve reserved too much\\nmemory for this app. You’re\\nwasting memory, because it\\nwon’t ever be used by this\\napp and also can’t be used\\nby other apps. You should\\ndecrease the memory\\nrequest.\\nCPU request\\nCPU usage\\nMemory request\\nMemory usage\\nFigure 14.10\\nCPU and memory usage chart for a pod\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A pod is a logical host for one or more containers.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Resource requests',\n",
       "    'description': 'The amount of resources (CPU and memory) that a pod requires to run.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Resource limits',\n",
       "    'description': 'The maximum amount of resources (CPU and memory) that a pod is allowed to use.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'QoS classes',\n",
       "    'description': 'A way to prioritize pods based on their resource requirements.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Containers',\n",
       "    'description': 'Lightweight and standalone packages of software that contain everything the application needs to run.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'CPU requests',\n",
       "    'description': 'The amount of CPU time that a container requires to run.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Memory requests',\n",
       "    'description': 'The amount of memory that a container requires to run.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'CPU usage',\n",
       "    'description': 'The actual amount of CPU time being used by a container.',\n",
       "    'category': 'metric'},\n",
       "   {'entity': 'Memory usage',\n",
       "    'description': 'The actual amount of memory being used by a container.',\n",
       "    'category': 'metric'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"schedules pods across the cluster based on resource requests\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"keeps pods from starving other pods of resources by enforcing resource limits\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"never get killed if they try to use too much CPU, but are killed if they try to use too much memory\",\\n    \"destination_entity\": \"Memory\"\\n  },\\n  {\\n    \"source_entity\": \"Containers\",\\n    \"description\": \"get killed in an overcommitted system to free memory for more important pods based on QoS classes and actual memory usage\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"CPU requests\",\\n    \"description\": \"helps Kubernetes schedule pods across the cluster\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Resource requests\",\\n    \"description\": \"helps Kubernetes schedule pods across the cluster\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"CPU request\",\\n    \"description\": \"should be increased when actual CPU usage is higher than requested CPU usage\",\\n    \"destination_entity\": \"CPU usage\"\\n  },\\n  {\\n    \"source_entity\": \"Memory request\",\\n    \"description\": \"should be decreased when actual memory usage is well below requested memory usage\",\\n    \"destination_entity\": \"Memory usage\"\\n  },\\n  {\\n    \"source_entity\": \"Resource limits\",\\n    \"description\": \"keeps pods from starving other pods of resources\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"QoS classes\",\\n    \"description\": \"determines which pods to kill in an overcommitted system\",\\n    \"destination_entity\": \"Pod\"\\n  }\\n]\\n```\\n\\nNote that I\\'ve used the exact entity names from the input list, even if they appear multiple times as different entities (e.g. \"CPU request\" and \"CPU requests\").'},\n",
       " {'page': 468,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '436\\nCHAPTER 14\\nManaging pods’ computational resources\\n\\uf0a1You can use LimitRange objects to define the minimum, maximum, and default\\nresource requests and limits for individual pods.\\n\\uf0a1You can use ResourceQuota objects to limit the amount of resources available\\nto all the pods in a namespace.\\n\\uf0a1To know how high to set a pod’s resource requests and limits, you need to mon-\\nitor how the pod uses resources over a long-enough time period.\\nIn the next chapter, you’ll see how these metrics can be used by Kubernetes to auto-\\nmatically scale your pods.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'LimitRange',\n",
       "    'description': 'object for defining resource requests and limits for individual pods',\n",
       "    'category': 'kubernetes_resource_management'},\n",
       "   {'entity': 'ResourceQuota',\n",
       "    'description': 'object for limiting resources available to all pods in a namespace',\n",
       "    'category': 'kubernetes_resource_management'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'computational unit that can contain one or more containers',\n",
       "    'category': 'kubernetes_component'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"use to define resource requests and limits for individual pods.\",\\n    \"destination_entity\": \"LimitRange\"\\n  },\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"use to limit the amount of resources available to all pods in a namespace.\",\\n    \"destination_entity\": \"ResourceQuota\"\\n  },\\n  {\\n    \"source_entity\": \"LimitRange\",\\n    \"description\": \"defines the minimum, maximum, and default resource requests and limits for individual pods.\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"ResourceQuota\",\\n    \"description\": \"limits the amount of resources available to all pods in a namespace.\",\\n    \"destination_entity\": \"namespace\"\\n  },\\n  {\\n    \"source_entity\": \"You\",\\n    \"description\": \"need to monitor how the pod uses resources over a long-enough time period.\",\\n    \"destination_entity\": \"pod\"\\n  }\\n]\\n```'},\n",
       " {'page': 469,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '437\\nAutomatic scaling\\nof pods and cluster nodes\\nApplications running in pods can be scaled out manually by increasing the\\nreplicas field in the ReplicationController, ReplicaSet, Deployment, or other\\nscalable resource. Pods can also be scaled vertically by increasing their container’s\\nresource requests and limits (though this can currently only be done at pod cre-\\nation time, not while the pod is running). Although manual scaling is okay for\\ntimes when you can anticipate load spikes in advance or when the load changes\\ngradually over longer periods of time, requiring manual intervention to handle\\nsudden, unpredictable traffic increases isn’t ideal. \\nThis chapter covers\\n\\uf0a1Configuring automatic horizontal scaling of pods \\nbased on CPU utilization\\n\\uf0a1Configuring automatic horizontal scaling of pods \\nbased on custom metrics\\n\\uf0a1Understanding why vertical scaling of pods isn’t \\npossible yet\\n\\uf0a1Understanding automatic horizontal scaling of \\ncluster nodes\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'ReplicationController',\n",
       "    'description': 'A resource that manages the replication of a pod in Kubernetes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A resource that manages the replication of a pod in Kubernetes, similar to ReplicationController but with more features.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A resource that manages the deployment and scaling of an application in Kubernetes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Replicas field',\n",
       "    'description': 'A configuration option in a scalable resource that controls the number of replicas of a pod.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'The basic execution unit in Kubernetes that contains one or more containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'A lightweight and standalone process in which an application runs within a pod.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CPU utilization',\n",
       "    'description': 'A metric that measures the percentage of CPU resources used by pods or containers.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Custom metrics',\n",
       "    'description': 'User-defined metrics that can be used to trigger scaling actions in Kubernetes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Horizontal scaling',\n",
       "    'description': 'The process of increasing or decreasing the number of replicas of a pod based on resource utilization or custom metrics.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Vertical scaling',\n",
       "    'description': 'The process of increasing or decreasing the resources allocated to a single pod, such as CPU or memory.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Pods\", \"description\": \"can be scaled out manually by increasing the replicas field in the ReplicationController, ReplicaSet, Deployment, or other scalable resource.\", \"destination_entity\": \"Replicas field\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"can be scaled vertically by increasing their container’s resource requests and limits\", \"destination_entity\": \"Container\"},\\n  {\"source_entity\": \"CPU utilization\", \"description\": \"is used to configure automatic horizontal scaling of pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Custom metrics\", \"description\": \"are used to configure automatic horizontal scaling of pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"manages the number of replicas of a pod, which can be scaled manually\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"is another scalable resource that can manage the number of replicas of a pod\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"ReplicaSet\", \"description\": \"manages the number of replicas of a pod, which can be scaled manually\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Horizontal scaling\", \"description\": \"can be configured automatically based on CPU utilization or custom metrics\", \"destination_entity\": \"CPU utilization\"},\\n  {\"source_entity\": \"Horizontal scaling\", \"description\": \"can be configured automatically based on custom metrics\", \"destination_entity\": \"Custom metrics\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"cannot be scaled vertically yet due to technical limitations\", \"destination_entity\": \"Vertical scaling\"},\\n  {\"source_entity\": \"Cluster nodes\", \"description\": \"can also be horizontally scaled automatically\", \"destination_entity\": \"Horizontal scaling\"}\\n]\\n```\\n\\nThese relations capture the connections between the entities in the document page, such as how pods can be scaled out or vertically, and how horizontal scaling of pods or cluster nodes can be configured based on CPU utilization or custom metrics.'},\n",
       " {'page': 470,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '438\\nCHAPTER 15\\nAutomatic scaling of pods and cluster nodes\\n Luckily, Kubernetes can monitor your pods and scale them up automatically as\\nsoon as it detects an increase in the CPU usage or some other metric. If running on a\\ncloud infrastructure, it can even spin up additional nodes if the existing ones can’t\\naccept any more pods. This chapter will explain how to get Kubernetes to do both pod\\nand node autoscaling.\\n The autoscaling feature in Kubernetes was completely rewritten between the 1.6\\nand the 1.7 version, so be aware you may find outdated information on this subject\\nonline.\\n15.1\\nHorizontal pod autoscaling\\nHorizontal pod autoscaling is the automatic scaling of the number of pod replicas man-\\naged by a controller. It’s performed by the Horizontal controller, which is enabled and\\nconfigured by creating a HorizontalPodAutoscaler (HPA) resource. The controller\\nperiodically checks pod metrics, calculates the number of replicas required to meet\\nthe target metric value configured in the HorizontalPodAutoscaler resource, and\\nadjusts the replicas field on the target resource (Deployment, ReplicaSet, Replication-\\nController, or StatefulSet). \\n15.1.1 Understanding the autoscaling process\\nThe autoscaling process can be split into three steps:\\n\\uf0a1Obtain metrics of all the pods managed by the scaled resource object.\\n\\uf0a1Calculate the number of pods required to bring the metrics to (or close to) the\\nspecified target value.\\n\\uf0a1Update the replicas field of the scaled resource.\\nLet’s examine all three steps next.\\nOBTAINING POD METRICS\\nThe Autoscaler doesn’t perform the gathering of the pod metrics itself. It gets the\\nmetrics from a different source. As we saw in the previous chapter, pod and node met-\\nrics are collected by an agent called cAdvisor, which runs in the Kubelet on each node,\\nand then aggregated by the cluster-wide component called Heapster. The horizontal\\npod autoscaler controller gets the metrics of all the pods by querying Heapster\\nthrough REST calls. The flow of metrics data is shown in figure 15.1 (although all the\\nconnections are initiated in the opposite direction).\\nThis implies that Heapster must be running in the cluster for autoscaling to work. If\\nyou’re using Minikube and were following along in the previous chapter, Heapster\\nPod(s)\\ncAdvisor(s)\\nHorizontal Pod Autoscaler(s)\\nHeapster\\nFigure 15.1\\nFlow of metrics from the pod(s) to the HorizontalPodAutoscaler(s)\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Pod(s), cAdvisor(s) Heapster, Horizontal Pod Autoscaler(s)]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [cAdvisor(s), Col1]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'units of computation in Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'autoscaling',\n",
       "    'description': 'automatic scaling of pods and cluster nodes',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'HorizontalPodAutoscaler (HPA)',\n",
       "    'description': 'resource for enabling autoscaling',\n",
       "    'category': 'framework'},\n",
       "   {'entity': 'cAdvisor',\n",
       "    'description': 'agent that collects pod and node metrics',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Heapster',\n",
       "    'description': 'cluster-wide component that aggregates pod and node metrics',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'component that runs on each node and communicates with cAdvisor',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'resource that manages a set of replicas',\n",
       "    'category': 'framework'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'resource that manages a set of replicas',\n",
       "    'category': 'framework'},\n",
       "   {'entity': 'Replication-Controller',\n",
       "    'description': 'resource that manages a set of replicas',\n",
       "    'category': 'framework'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'resource that manages a set of replicas',\n",
       "    'category': 'framework'}],\n",
       "  'relationships': '[\\n    {\\n        \"source_entity\": \"Kubernetes\",\\n        \"description\": \"can monitor pods and scale them up automatically as soon as it detects an increase in CPU usage or some other metric\",\\n        \"destination_entity\": \"pods\"\\n    },\\n    {\\n        \"source_entity\": \"Horizontal Pod Autoscaler (HPA)\",\\n        \"description\": \"performs automatic scaling of the number of pod replicas managed by a controller\",\\n        \"destination_entity\": \"pods\"\\n    },\\n    {\\n        \"source_entity\": \"Kubernetes\",\\n        \"description\": \"can even spin up additional nodes if the existing ones can\\'t accept any more pods\",\\n        \"destination_entity\": \"cluster nodes\"\\n    },\\n    {\\n        \"source_entity\": \"HorizontalPodAutoscaler (HPA)\",\\n        \"description\": \"periodically checks pod metrics, calculates the number of replicas required to meet the target metric value configured in the HorizontalPodAutoscaler resource\",\\n        \"destination_entity\": \"pods\"\\n    },\\n    {\\n        \"source_entity\": \"cAdvisor\",\\n        \"description\": \"collects pod and node metrics on each node\",\\n        \"destination_entity\": \"Heapster\"\\n    },\\n    {\\n        \"source_entity\": \"Heapster\",\\n        \"description\": \"aggregates the collected metrics and makes them available for querying by the Horizontal Pod Autoscaler controller\",\\n        \"destination_entity\": \"Horizontal Pod Autoscaler (HPA)\"\\n    },\\n    {\\n        \"source_entity\": \"Kubelet\",\\n        \"description\": \"runs cAdvisor on each node\",\\n        \"destination_entity\": \"cAdvisor\"\\n    },\\n    {\\n        \"source_entity\": \"Minikube\",\\n        \"description\": \"must be running in the cluster for autoscaling to work\",\\n        \"destination_entity\": \"Heapster\"\\n    },\\n    {\\n        \"source_entity\": \"Kubernetes\",\\n        \"description\": \"can scale pods up automatically as soon as it detects an increase in CPU usage or some other metric\",\\n        \"destination_entity\": \"pods\"\\n    }\\n]\\n```\\n\\nNote that there are multiple relations between the same entities, but each relation is unique and described accurately.'},\n",
       " {'page': 471,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '439\\nHorizontal pod autoscaling\\nshould already be enabled in your cluster. If not, make sure to enable the Heapster\\nadd-on before trying out any autoscaling examples.\\n Although you don’t need to query Heapster directly, if you’re interested in doing\\nso, you’ll find both the Heapster Pod and the Service it’s exposed through in the\\nkube-system namespace. \\nCALCULATING THE REQUIRED NUMBER OF PODS\\nOnce the Autoscaler has metrics for all the pods belonging to the resource the Auto-\\nscaler is scaling (the Deployment, ReplicaSet, ReplicationController, or StatefulSet\\nresource), it can use those metrics to figure out the required number of replicas. It\\nneeds to find the number that will bring the average value of the metric across all\\nthose replicas as close to the configured target value as possible. The input to this cal-\\nculation is a set of pod metrics (possibly multiple metrics per pod) and the output is a\\nsingle integer (the number of pod replicas). \\n When the Autoscaler is configured to consider only a single metric, calculating the\\nrequired replica count is simple. All it takes is summing up the metrics values of all\\nthe pods, dividing that by the target value set on the HorizontalPodAutoscaler\\nresource, and then rounding it up to the next-larger integer. The actual calculation is\\na bit more involved than this, because it also makes sure the Autoscaler doesn’t thrash\\naround when the metric value is unstable and changes rapidly. \\n When autoscaling is based on multiple pod metrics (for example, both CPU usage\\nand Queries-Per-Second [QPS]), the calculation isn’t that much more complicated.\\nThe Autoscaler calculates the replica count for each metric individually and then\\ntakes the highest value (for example, if four pods are required to achieve the target\\nCPU usage, and three pods are required to achieve the target QPS, the Autoscaler will\\nscale to four pods). Figure 15.2 shows this example.\\nA look at changes related to how the Autoscaler obtains metrics\\nPrior to Kubernetes version 1.6, the HorizontalPodAutoscaler obtained the metrics\\nfrom Heapster directly. In version 1.8, the Autoscaler can get the metrics through an\\naggregated version of the resource metrics API by starting the Controller Manager\\nwith the --horizontal-pod-autoscaler-use-rest-clients=true flag. From ver-\\nsion 1.9, this behavior will be enabled by default.\\nThe core API server will not expose the metrics itself. From version 1.7, Kubernetes\\nallows registering multiple API servers and making them appear as a single API\\nserver. This allows it to expose metrics through one of those underlying API servers.\\nWe’ll explain API server aggregation in the last chapter. \\nSelecting what metrics collector to use in their clusters will be up to cluster adminis-\\ntrators. A simple translation layer is usually required to expose the metrics in the\\nappropriate API paths and in the appropriate format.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Heapster',\n",
       "    'description': 'add-on for enabling Horizontal pod autoscaling',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Horizontal pod autoscaling',\n",
       "    'description': 'should already be enabled in your cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Heapster Pod',\n",
       "    'description': 'pod containing the Heapster service',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'exposed through the kube-system namespace',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'kube-system namespace',\n",
       "    'description': 'namespace where the Service is exposed',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Autoscaler',\n",
       "    'description': 'calculates the required number of replicas based on metrics',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'resource being scaled by the Autoscaler',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'resource being scaled by the Autoscaler',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': 'resource being scaled by the Autoscaler',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'resource being scaled by the Autoscaler',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metric',\n",
       "    'description': 'input to the calculation for required replica count',\n",
       "    'category': 'data'},\n",
       "   {'entity': 'target value',\n",
       "    'description': 'configured target value for scaling',\n",
       "    'category': 'data'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'obtains metrics through an aggregated version of the resource metrics API',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'exposes metrics through one of the underlying API servers',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'API server aggregation',\n",
       "    'description': 'allows registering multiple API servers and making them appear as a single API server',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Horizontal pod autoscaling\", \"description\": \"should already be enabled in your cluster\", \"destination_entity\": null},\\n  {\"source_entity\": \"cluster administators\", \"description\": \"Selecting what metrics collector to use will be up to them\", \"destination_entity\": \"metrics collector\"},\\n  {\"source_entity\": \"Heapster\", \"description\": \"is required to enable Heapster add-on before autoscaling\", \"destination_entity\": null},\\n  {\"source_entity\": \"Heapster Pod\", \"description\": \"can be found in kube-system namespace along with Service it\\'s exposed through\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"needs to find the required number of replicas based on pod metrics\", \"destination_entity\": \"Replicas\"},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"calculates the required replica count by summing up metrics values and dividing it by target value\", \"destination_entity\": null},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"makes sure it doesn\\'t thrash around when metric value is unstable and changes rapidly\", \"destination_entity\": null},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"calculates the replica count for each metric individually and takes the highest value\", \"destination_entity\": null},\\n  {\"source_entity\": \"Heapster\", \"description\": \"obtained metrics from directly before Kubernetes version 1.6\", \"destination_entity\": null},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"can get metrics through an aggregated version of the resource metrics API since Kubernetes version 1.8\", \"destination_entity\": null},\\n  {\"source_entity\": \"Controller Manager\", \"description\": \"needs to be started with --horizontal-pod-autoscaler-use-rest-clients=true flag to use REST clients\", \"destination_entity\": null},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"obtains metrics through one of the underlying API servers since Kubernetes version 1.7\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"cluster administators\", \"description\": \"will be selecting what metrics collector to use in their clusters\", \"destination_entity\": null},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"calculates the required replica count based on multiple pod metrics\", \"destination_entity\": null},\\n  {\"source_entity\": \"ReplicationController\", \"description\": \"is one of the resources that Autoscaler can scale\", \"destination_entity\": null},\\n  {\"source_entity\": \"Service\", \"description\": \"exposes the Heapster Pod along with Heapster Pod in kube-system namespace\", \"destination_entity\": null},\\n  {\"source_entity\": \"Deployment\", \"description\": \"is one of the resources that Autoscaler can scale\", \"destination_entity\": null},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"is one of the resources that Autoscaler can scale\", \"destination_entity\": null}\\n]\\n\\nNote: The relations extracted are based on the context and entities provided in the document page. If there\\'s any confusion or incorrect extraction, please let me know!'},\n",
       " {'page': 472,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '440\\nCHAPTER 15\\nAutomatic scaling of pods and cluster nodes\\nUPDATING THE DESIRED REPLICA COUNT ON THE SCALED RESOURCE\\nThe final step of an autoscaling operation is updating the desired replica count field\\non the scaled resource object (a ReplicaSet, for example) and then letting the Replica-\\nSet controller take care of spinning up additional pods or deleting excess ones.\\n The Autoscaler controller modifies the replicas field of the scaled resource\\nthrough the Scale sub-resource. It enables the Autoscaler to do its work without know-\\ning any details of the resource it’s scaling, except for what’s exposed through the Scale\\nsub-resource (see figure 15.3).\\nThis allows the Autoscaler to operate on any scalable resource, as long as the API\\nserver exposes the Scale sub-resource for it. Currently, it’s exposed for\\n\\uf0a1Deployments\\n\\uf0a1ReplicaSets\\n\\uf0a1ReplicationControllers\\n\\uf0a1StatefulSets\\nThese are currently the only objects you can attach an Autoscaler to.\\nPod 1\\nCPU\\nutilization\\nQPS\\nPod 2\\nPod 3\\nTarget\\nCPU utilization\\nTarget QPS\\nReplicas: 4\\nReplicas: 3\\nReplicas: 4\\n30\\n12\\n15\\n20\\n(15 + 30 + 12) / 20 = 57 / 20\\n(60 + 90 + 50) / 50 = 200 / 50\\nMax(4, 3)\\n50%\\n60%\\n90%\\n50%\\nFigure 15.2\\nCalculating the number of replicas from two metrics\\nAutoscaler adjusts replicas (++ or --)\\nHorizontal Pod Autoscaler\\nDeployment, ReplicaSet,\\nStatefulSet, or\\nReplicationController\\nScale\\nsub-resource\\nFigure 15.3\\nThe Horizontal Pod Autoscaler modifies only on the Scale sub-resource.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [    60% Col1\n",
       "   0  None     ,\n",
       "   Empty DataFrame\n",
       "   Columns: [90%, Col1]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [50%, Col1]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [50%, Col1]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [Autoscaler adjusts replicas (++ or --)\n",
       "   Horizontal Pod Autoscaler, Deployment, ReplicaSet,\n",
       "   StatefulSet, or\n",
       "   ReplicationController\n",
       "   Scale\n",
       "   sub-resource]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Autoscaler',\n",
       "    'description': 'A controller that adjusts replicas by modifying the desired replica count field on the scaled resource object (e.g. ReplicaSet) through the Scale sub-resource.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A scaled resource object that can be modified by the Autoscaler to spin up or delete pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Scale sub-resource',\n",
       "    'description': 'An API endpoint that exposes the replicas field of a scaled resource, allowing the Autoscaler to operate on it.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Deployments',\n",
       "    'description': 'A type of scalable resource that can be attached an Autoscaler to.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSets',\n",
       "    'description': 'A type of scalable resource that can be attached an Autoscaler to.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicationControllers',\n",
       "    'description': 'A type of scalable resource that can be attached an Autoscaler to.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'StatefulSets',\n",
       "    'description': 'A type of scalable resource that can be attached an Autoscaler to.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Containers running in a Kubernetes cluster that are adjusted by the Autoscaler based on metrics such as CPU utilization and QPS.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'CPU utilization',\n",
       "    'description': 'A metric used by the Autoscaler to adjust replicas of pods.',\n",
       "    'category': 'metric'},\n",
       "   {'entity': 'QPS',\n",
       "    'description': 'A metric used by the Autoscaler to adjust replicas of pods.',\n",
       "    'category': 'metric'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"modifies\", \"destination_entity\": \"Scale sub-resource\"},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"operates on\", \"destination_entity\": \"Deployments\"},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"operates on\", \"destination_entity\": \"ReplicaSets\"},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"operates on\", \"destination_entity\": \"StatefulSets\"},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"modifies\", \"destination_entity\": \"Replicas field of the scaled resource\"},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"spins up\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"deletes excess\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"ReplicaSet controller\", \"description\": \"takes care of spinning up additional\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"ReplicaSet controller\", \"description\": \"takes care of deleting excess\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"CPU utilization\", \"description\": \"is monitored by\", \"destination_entity\": \"Autoscaler\"},\\n  {\"source_entity\": \"QPS\", \"description\": \"is monitored by\", \"destination_entity\": \"Autoscaler\"},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"adjusts replicas based on CPU utilization and QPS\", \"destination_entity\": \"Replicas field of the scaled resource\"}\\n]\\n```\\n\\nNote: I have only included the relations that are directly mentioned in the document page. Let me know if you\\'d like me to include any implicit or inferred relations as well!'},\n",
       " {'page': 473,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '441\\nHorizontal pod autoscaling\\nUNDERSTANDING THE WHOLE AUTOSCALING PROCESS\\nYou now understand the three steps involved in autoscaling, so let’s visualize all the\\ncomponents involved in the autoscaling process. They’re shown in figure 15.4.\\nThe arrows leading from the pods to the cAdvisors, which continue on to Heapster\\nand finally to the Horizontal Pod Autoscaler, indicate the direction of the flow of met-\\nrics data. It’s important to be aware that each component gets the metrics from the\\nother components periodically (that is, cAdvisor gets the metrics from the pods in a\\ncontinuous loop; the same is also true for Heapster and for the HPA controller). The\\nend effect is that it takes quite a while for the metrics data to be propagated and a res-\\ncaling action to be performed. It isn’t immediate. Keep this in mind when you observe\\nthe Autoscaler in action next.\\n15.1.2 Scaling based on CPU utilization\\nPerhaps the most important metric you’ll want to base autoscaling on is the amount of\\nCPU consumed by the processes running inside your pods. Imagine having a few pods\\nproviding a service. When their CPU usage reaches 100% it’s obvious they can’t cope\\nwith the demand anymore and need to be scaled either up (vertical scaling—increas-\\ning the amount of CPU the pods can use) or out (horizontal scaling—increasing the\\nnumber of pods). Because we’re talking about the horizontal pod autoscaler here,\\nAutoscaler adjusts\\nreplicas (++ or --)\\nHeapster collects\\nmetrics from all nodes\\ncAdvisor collects metrics\\nfrom all containers on a node\\nDeployment\\nReplicaSet\\nAutoscaler collects\\nmetrics from Heapster\\nKubelet\\ncAdvisor\\nNode 1\\nPod\\nPod\\nKubelet\\ncAdvisor\\nNode 2\\nPod\\nNode X\\nHeapster\\nHorizontal Pod\\nAutoscaler\\nFigure 15.4\\nHow the autoscaler obtains metrics and rescales the target deployment \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Horizontal pod autoscaling',\n",
       "    'description': '',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'cAdvisor',\n",
       "    'description': 'collects metrics from all containers on a node',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'Heapster',\n",
       "    'description': 'collects metrics from all nodes',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'Horizontal Pod Autoscaler',\n",
       "    'description': '',\n",
       "    'category': 'controller'},\n",
       "   {'entity': 'Deployment', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'ReplicaSet', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'Autoscaler',\n",
       "    'description': 'adjusts replicas (++ or --)',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'metrics',\n",
       "    'description': 'data collected by cAdvisor and Heapster',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'CPU utilization', 'description': '', 'category': 'metric'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'provide a service',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Kubelet', 'description': '', 'category': 'component'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'physical or virtual machine',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'running inside a node',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Horizontal pod autoscaling\", \"description\": \"adjusts replicas\", \"destination_entity\": \"ReplicaSet\"},\\n  {\"source_entity\": \"Heapster\", \"description\": \"collects metrics\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"cAdvisor\", \"description\": \"collects metrics\", \"destination_entity\": \"containers on a node\"},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"adjusts replicas\", \"destination_entity\": \"ReplicaSet\"},\\n  {\"source_entity\": \"Autoscaler\", \"description\": \"collects metrics from Heapster\", \"destination_entity\": \"Heapster\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"provides metrics to cAdvisor\", \"destination_entity\": \"cAdvisor\"},\\n  {\"source_entity\": \"cAdvisor\", \"description\": \"provides metrics to Autoscaler\", \"destination_entity\": \"Autoscaler\"},\\n  {\"source_entity\": \"Heapster\", \"description\": \"collects metrics from all nodes\", \"destination_entity\": \"all nodes\"},\\n  {\"source_entity\": \"CPU utilization\", \"description\": \"indicates need for scaling\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"Horizontal pod autoscaling\", \"description\": \"scales based on CPU utilization\", \"destination_entity\": \"pods\"}\\n]\\n```\\n\\nThese relations describe the interactions and dependencies between various entities involved in horizontal pod autoscaling, such as:\\n\\n* Horizontal Pod Autoscaling adjusting ReplicaSets\\n* Heapster collecting metrics from pods\\n* cAdvisor collecting metrics from containers on a node\\n* Autoscaler collecting metrics from Heapster and adjusting replicas based on CPU utilization'},\n",
       " {'page': 474,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '442\\nCHAPTER 15\\nAutomatic scaling of pods and cluster nodes\\nwe’re only focusing on scaling out (increasing the number of pods). By doing that,\\nthe average CPU usage should come down. \\n Because CPU usage is usually unstable, it makes sense to scale out even before the\\nCPU is completely swamped—perhaps when the average CPU load across the pods\\nreaches or exceeds 80%. But 80% of what, exactly?\\nTIP\\nAlways set the target CPU usage well below 100% (and definitely never\\nabove 90%) to leave enough room for handling sudden load spikes.\\nAs you may remember from the previous chapter, the process running inside a con-\\ntainer is guaranteed the amount of CPU requested through the resource requests\\nspecified for the container. But at times when no other processes need CPU, the pro-\\ncess may use all the available CPU on the node. When someone says a pod is consum-\\ning 80% of the CPU, it’s not clear if they mean 80% of the node’s CPU, 80% of the\\npod’s guaranteed CPU (the resource request), or 80% of the hard limit configured\\nfor the pod through resource limits. \\n As far as the Autoscaler is concerned, only the pod’s guaranteed CPU amount (the\\nCPU requests) is important when determining the CPU utilization of a pod. The Auto-\\nscaler compares the pod’s actual CPU consumption and its CPU requests, which\\nmeans the pods you’re autoscaling need to have CPU requests set (either directly or\\nindirectly through a LimitRange object) for the Autoscaler to determine the CPU uti-\\nlization percentage.\\nCREATING A HORIZONTALPODAUTOSCALER BASED ON CPU USAGE\\nLet’s see how to create a HorizontalPodAutoscaler now and configure it to scale pods\\nbased on their CPU utilization. You’ll create a Deployment similar to the one in chap-\\nter 9, but as we’ve discussed, you’ll need to make sure the pods created by the Deploy-\\nment all have the CPU resource requests specified in order to make autoscaling\\npossible. You’ll have to add a CPU resource request to the Deployment’s pod tem-\\nplate, as shown in the following listing.\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: kubia\\nspec:\\n  replicas: 3                \\n  template:\\n    metadata:\\n      name: kubia\\n      labels:\\n        app: kubia\\n    spec:\\n      containers:\\n      - image: luksa/kubia:v1     \\n        name: nodejs\\nListing 15.1\\nDeployment with CPU requests set: deployment.yaml\\nManually setting the \\n(initial) desired number \\nof replicas to three\\nRunning the \\nkubia:v1 image\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'CPU',\n",
       "    'description': 'A processing unit that executes instructions and performs calculations.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system for automating the deployment, scaling, and management of containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'The basic execution unit in Kubernetes, consisting of one or more containers.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A Kubernetes resource that manages a set of replicas of an application.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Horizontal Pod Autoscaler (HPA)',\n",
       "    'description': 'A Kubernetes component that automatically scales the number of replicas based on CPU utilization or other metrics.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Autoscaler',\n",
       "    'description': 'A mechanism for scaling resources such as pods, nodes, or services in response to changing demand or conditions.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'LimitRange',\n",
       "    'description': 'A Kubernetes resource that defines the minimum and maximum values for CPU, memory, and other resource requests.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'APIVersion',\n",
       "    'description': 'The version of the API used to create or update a resource.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Kind',\n",
       "    'description': 'The type of Kubernetes resource being created or updated.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Replicas',\n",
       "    'description': 'The number of replicas desired for a Deployment or other resource.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'A lightweight and standalone execution environment for an application.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Image',\n",
       "    'description': \"A snapshot of a container's file system at a specific point in time.\",\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"Horizontal Pod Autoscaler (HPA)\", \"description\": \"compares pod\\'s actual CPU consumption and its CPU requests\", \"destination_entity\": \"Pods\"},\\n\\n {\"source_entity\": \"Horizontal Pod Autoscaler (HPA)\", \"description\": \"determines the CPU utilization percentage\", \"destination_entity\": \"CPU\"},\\n\\n {\"source_entity\": \"Autoscaler\", \"description\": \"uses pod\\'s guaranteed CPU amount to determine CPU utilization\", \"destination_entity\": \"Pods\"},\\n\\n {\"source_entity\": \"Kubernetes\", \"description\": \"guarantees process running inside a container the requested CPU amount\", \"destination_entity\": \"Container\"},\\n\\n {\"source_entity\": \"Kubernetes\", \"description\": \"allows setting target CPU usage well below 100%\", \"destination_entity\": \"CPU\"},\\n\\n {\"source_entity\": \"Kubernetes\", \"description\": \"comprises pod\\'s guaranteed CPU amount, CPU requests and hard limit configured for the pod\", \"destination_entity\": \"Pods\"},\\n\\n {\"source_entity\": \"Horizontal Pod Autoscaler (HPA)\", \"description\": \"requires pods to have CPU resource requests specified\", \"destination_entity\": \"Pods\"},\\n\\n {\"source_entity\": \"Deployment\", \"description\": \"needs to add a CPU resource request to the pod template\", \"destination_entity\": \"Container\"},\\n\\n {\"source_entity\": \"LimitRange\", \"description\": \"indirectly sets CPU requests for pods through an object\", \"destination_entity\": \"Pods\"},\\n\\n {\"source_entity\": \"APIVersion\", \"description\": \"specifies the API version for creating a HorizontalPodAutoscaler\", \"destination_entity\": \"Horizontal Pod Autoscaler (HPA)\"}, \\n\\n {\"source_entity\": \"Kind\", \"description\": \"defines the type of resource being created, in this case, a Deployment\", \"destination_entity\": \"Deployment\"},\\n\\n {\"source_entity\": \"Image\", \"description\": \"specifies the container image to be used in the pod\", \"destination_entity\": \"Container\"}]'},\n",
       " {'page': 475,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '443\\nHorizontal pod autoscaling\\n        resources:              \\n          requests:             \\n            cpu: 100m           \\nThis is a regular Deployment object—it doesn’t use autoscaling yet. It will run three\\ninstances of the kubia NodeJS app, with each instance requesting 100 millicores\\nof CPU. \\n After creating the Deployment, to enable horizontal autoscaling of its pods, you\\nneed to create a HorizontalPodAutoscaler (HPA) object and point it to the Deploy-\\nment. You could prepare and post the YAML manifest for the HPA, but an easier way\\nexists—using the kubectl autoscale command:\\n$ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5\\ndeployment \"kubia\" autoscaled\\nThis creates the HPA object for you and sets the Deployment called kubia as the scal-\\ning target. You’re setting the target CPU utilization of the pods to 30% and specifying\\nthe minimum and maximum number of replicas. The Autoscaler will constantly keep\\nadjusting the number of replicas to keep their CPU utilization around 30%, but it will\\nnever scale down to less than one or scale up to more than five replicas. \\nTIP\\nAlways make sure to autoscale Deployments instead of the underlying\\nReplicaSets. This way, you ensure the desired replica count is preserved across\\napplication updates (remember that a Deployment creates a new ReplicaSet\\nfor each version). The same rule applies to manual scaling, as well.\\nLet’s look at the definition of the HorizontalPodAutoscaler resource to gain a better\\nunderstanding of it. It’s shown in the following listing.\\n$ kubectl get hpa.v2beta1.autoscaling kubia -o yaml\\napiVersion: autoscaling/v2beta1            \\nkind: HorizontalPodAutoscaler              \\nmetadata:\\n  name: kubia               \\n  ...\\nspec:\\n  maxReplicas: 5                   \\n  metrics:                              \\n  - resource:                           \\n      name: cpu                         \\n      targetAverageUtilization: 30      \\n    type: Resource                      \\n  minReplicas: 1                   \\n  scaleTargetRef:                          \\n    apiVersion: extensions/v1beta1         \\n    kind: Deployment                       \\n    name: kubia                            \\nListing 15.2\\nA HorizontalPodAutoscaler YAML definition\\nRequesting 100 millicores \\nof CPU per pod\\nHPA resources are in the \\nautoscaling API group.\\nEach HPA has a name (it doesn’t \\nneed to match the name of the \\nDeployment as in this case).\\nThe\\nminimum\\nand\\nmaximum\\nnumber of\\nreplicas\\nyou\\nspecified\\nYou’d like the Autoscaler to \\nadjust the number of pods \\nso they each utilize 30% of \\nrequested CPU.\\nThe target resource \\nwhich this Autoscaler \\nwill act upon\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Deployment',\n",
       "    'description': \"A regular Deployment object that doesn't use autoscaling yet.\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Horizontal pod autoscaling',\n",
       "    'description': 'A feature to enable horizontal autoscaling of its pods.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'cpu',\n",
       "    'description': '100 millicores of CPU per pod requested by the deployment.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command used to create a HorizontalPodAutoscaler (HPA) object and point it to the Deployment.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'HorizontalPodAutoscaler (HPA)',\n",
       "    'description': 'An object that enables horizontal autoscaling of its pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSets',\n",
       "    'description': 'A ReplicaSet is a deployment resource in Kubernetes.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'autoscaling API group',\n",
       "    'description': 'A group of APIs related to scaling and autoscaling.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Deployment object',\n",
       "    'description': 'An object that represents the deployment of an application.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubia NodeJS app',\n",
       "    'description': 'An application deployed using a Deployment object.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'minReplicas',\n",
       "    'description': 'The minimum number of replicas specified in the HPA resource.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'maxReplicas',\n",
       "    'description': 'The maximum number of replicas specified in the HPA resource.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'targetAverageUtilization',\n",
       "    'description': 'The target average utilization for CPU usage in the HPA resource.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'scaleTargetRef',\n",
       "    'description': 'A reference to the Deployment object that this HPA will scale.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Horizontal pod autoscaling\",\\n    \"description\": \"Enables horizontal scaling of Deployment object pods to keep their CPU utilization around a target percentage.\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"Creates an HPA object for a Deployment and sets it as the scaling target, specifying the minimum and maximum number of replicas.\",\\n    \"destination_entity\": \"HorizontalPodAutoscaler (HPA)\"\\n  },\\n  {\\n    \"source_entity\": \"Deployment\",\\n    \"description\": \"Can be autoscaled to adjust the number of replicas based on CPU utilization.\",\\n    \"destination_entity\": \"ReplicaSets\"\\n  },\\n  {\\n    \"source_entity\": \"autoscaling API group\",\\n    \"description\": \"Provides the resources for HPA, including metrics and targetAverageUtilization.\",\\n    \"destination_entity\": \"HorizontalPodAutoscaler (HPA)\"\\n  },\\n  {\\n    \"source_entity\": \"scaleTargetRef\",\\n    \"description\": \"Specifies the Deployment object that the Autoscaler will scale based on CPU utilization.\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"targetAverageUtilization\",\\n    \"description\": \"Sets the target CPU utilization percentage for the Autoscaler to keep at.\",\\n    \"destination_entity\": \"HorizontalPodAutoscaler (HPA)\"\\n  },\\n  {\\n    \"source_entity\": \"maxReplicas\",\\n    \"description\": \"Specifies the maximum number of replicas that the Autoscaler will scale up to.\",\\n    \"destination_entity\": \"HorizontalPodAutoscaler (HPA)\"\\n  },\\n  {\\n    \"source_entity\": \"minReplicas\",\\n    \"description\": \"Specifies the minimum number of replicas that the Autoscaler will scale down to.\",\\n    \"destination_entity\": \"HorizontalPodAutoscaler (HPA)\"\\n  },\\n  {\\n    \"source_entity\": \"cpu\",\\n    \"description\": \"Requests 100 millicores per pod, which is utilized by the Autoscaler.\",\\n    \"destination_entity\": \"kubia NodeJS app\"\\n  }\\n]\\n```'},\n",
       " {'page': 476,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '444\\nCHAPTER 15\\nAutomatic scaling of pods and cluster nodes\\nstatus:\\n  currentMetrics: []        \\n  currentReplicas: 3        \\n  desiredReplicas: 0        \\nNOTE\\nMultiple versions of HPA resources exist: the new autoscaling/v2beta1\\nand the old autoscaling/v1. You’re requesting the new version here.\\nSEEING THE FIRST AUTOMATIC RESCALE EVENT\\nIt takes a while for cAdvisor to get the CPU metrics and for Heapster to collect them\\nbefore the Autoscaler can take action. During that time, if you display the HPA resource\\nwith kubectl get, the TARGETS column will show <unknown>:\\n$ kubectl get hpa\\nNAME      REFERENCE          TARGETS           MINPODS   MAXPODS   REPLICAS\\nkubia     Deployment/kubia   <unknown> / 30%   1         5         0       \\nBecause you’re running three pods that are currently receiving no requests, which\\nmeans their CPU usage should be close to zero, you should expect the Autoscaler to\\nscale them down to a single pod, because even with a single pod, the CPU utilization\\nwill still be below the 30% target. \\n And sure enough, the autoscaler does exactly that. It soon scales the Deployment\\ndown to a single replica:\\n$ kubectl get deployment\\nNAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\\nkubia     1         1         1            1           23m\\nRemember, the autoscaler only adjusts the desired replica count on the Deployment.\\nThe Deployment controller then takes care of updating the desired replica count on\\nthe ReplicaSet object, which then causes the ReplicaSet controller to delete two excess\\npods, leaving one pod running.\\n You can use kubectl describe to see more information on the HorizontalPod-\\nAutoscaler and the operation of the underlying controller, as the following listing shows.\\n$ kubectl describe hpa\\nName:                             kubia\\nNamespace:                        default\\nLabels:                           <none>\\nAnnotations:                      <none>\\nCreationTimestamp:                Sat, 03 Jun 2017 12:59:57 +0200\\nReference:                        Deployment/kubia\\nMetrics:                          ( current / target )\\n  resource cpu on pods  \\n  (as a percentage of request):   0% (0) / 30%\\nMin replicas:                     1\\nMax replicas:                     5\\nListing 15.3\\nInspecting a HorizontalPodAutoscaler with kubectl describe\\nThe current status \\nof the Autoscaler\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'HPA',\n",
       "    'description': 'Horizontal Pod Autoscaler',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'Kubernetes resource for managing pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'Kubernetes resource for managing replica count',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Lightweight and ephemeral containers in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'cAdvisor',\n",
       "    'description': 'Container metrics collection agent',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Heapster',\n",
       "    'description': 'Metrics collection system for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Autoscaler',\n",
       "    'description': 'Controller that scales pods based on CPU utilization',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment controller',\n",
       "    'description': 'Component responsible for updating replica count in ReplicaSet',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSet controller',\n",
       "    'description': 'Component responsible for deleting excess pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line interface for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'hpa',\n",
       "    'description': 'Short name for Horizontal Pod Autoscaler resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'autoscaling/v2beta1',\n",
       "    'description': 'Kubernetes API version for HPA resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'autoscaling/v1',\n",
       "    'description': 'Older Kuberentes API version for HPA resources',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"Autoscaler\", \"description\": \"adjusts desired replica count on Deployment\", \"destination_entity\": \"Deployment\"},\\n {\"source_entity\": \"Deployment controller\", \"description\": \"updates desired replica count on ReplicaSet\", \"destination_entity\": \"ReplicaSet\"},\\n {\"source_entity\": \"Autoscaler\", \"description\": \"monitors CPU usage of Pods\", \"destination_entity\": \"Pods\"},\\n {\"source_entity\": \"cAdvisor\", \"description\": \"collects CPU metrics\", \"destination_entity\": \"Heapster\"},\\n {\"source_entity\": \"Heapster\", \"description\": \"collects CPU metrics\", \"destination_entity\": \"Autoscaler\"},\\n {\"source_entity\": \"HPA\", \"description\": \"requests new autoscaling version (v2beta1)\", \"destination_entity\": \"autoscaling/v2beta1\"},\\n {\"source_entity\": \"Autoscaler\", \"description\": \"takes action based on CPU metrics\", \"destination_entity\": \"ReplicaSet\"},\\n {\"source_entity\": \"Deployment controller\", \"description\": \"deletes excess pods\", \"destination_entity\": \"Pods\"},\\n {\"source_entity\": \"kubectl\", \"description\": \"displays HPA resource with kubectl get\", \"destination_entity\": \"HPA\"},\\n {\"source_entity\": \"Heapster\", \"description\": \"collects CPU metrics\", \"destination_entity\": \"Autoscaler\"}]\\n\\nNote: I\\'ve removed any extra words or characters from the JSON objects as per your request. Let me know if this is correct!'},\n",
       " {'page': 477,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '445\\nHorizontal pod autoscaling\\nEvents:\\nFrom                        Reason              Message\\n----                        ------              ---\\nhorizontal-pod-autoscaler   SuccessfulRescale   New size: 1; reason: All \\n                                                metrics below target\\nNOTE\\nThe output has been modified to make it more readable.\\nTurn your focus to the table of events at the bottom of the listing. You see the horizon-\\ntal pod autoscaler has successfully rescaled to one replica, because all metrics were\\nbelow target. \\nTRIGGERING A SCALE-UP\\nYou’ve already witnessed your first automatic rescale event (a scale-down). Now, you’ll\\nstart sending requests to your pod, thereby increasing its CPU usage, and you should\\nsee the autoscaler detect this and start up additional pods.\\n You’ll need to expose the pods through a Service, so you can hit all of them through\\na single URL. You may remember that the easiest way to do that is with kubectl expose:\\n$ kubectl expose deployment kubia --port=80 --target-port=8080\\nservice \"kubia\" exposed\\nBefore you start hitting your pod(s) with requests, you may want to run the follow-\\ning command in a separate terminal to keep an eye on what’s happening with the\\nHorizontalPodAutoscaler and the Deployment, as shown in the following listing.\\n$ watch -n 1 kubectl get hpa,deployment\\nEvery \\n1.0s: \\nkubectl \\nget \\nhpa,deployment \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNAME        REFERENCE          TARGETS    MINPODS   MAXPODS   REPLICAS  AGE\\nhpa/kubia   Deployment/kubia   0% / 30%   1         5         1         45m\\nNAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\\ndeploy/kubia   1         1         1            1           56m\\nTIP\\nList multiple resource types with kubectl get by delimiting them with\\na comma. \\nIf you’re using OSX, you’ll have to replace the watch command with a loop, manually\\nrun kubectl get periodically, or use kubectl’s --watch option. But although a plain\\nkubectl get can show multiple types of resources at once, that’s not the case when\\nusing the aforementioned --watch option, so you’ll need to use two terminals if you\\nwant to watch both the HPA and the Deployment objects. \\n Keep an eye on the state of those two objects while you run a load-generating pod.\\nYou’ll run the following command in another terminal:\\n$ kubectl run -it --rm --restart=Never loadgenerator --image=busybox \\n➥ -- sh -c \"while true; do wget -O - -q http://kubia.default; done\"\\nListing 15.4\\nWatching multiple resources in parallel\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command to expose the pods through a Service ',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'deployment',\n",
       "    'description': 'resource type that manages the rollout of new versions of an application ',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'horizontal pod autoscaler',\n",
       "    'description': 'controller that automatically scales the number of replicas based on CPU usage ',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'resource that allows access to a deployment through a single URL ',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'hpa',\n",
       "    'description': 'short for HorizontalPodAutoscaler, controller that automatically scales the number of replicas based on CPU usage ',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment/kubia',\n",
       "    'description': 'resource type that manages the rollout of new versions of an application ',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubia',\n",
       "    'description': 'name of the deployment and service ',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'busybox',\n",
       "    'description': 'image used for load-generating pod ',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'wget',\n",
       "    'description': 'command used in load-generating pod to continuously make requests to the kubia service ',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'http://kubia.default',\n",
       "    'description': 'URL of the kubia service that is being hit by the load-generating pod ',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'kubectl expose',\n",
       "    'description': 'command used to expose the pods through a Service ',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'watch -n 1 kubectl get hpa,deployment',\n",
       "    'description': 'command used to periodically check the state of the HPA and deployment objects ',\n",
       "    'category': 'software'},\n",
       "   {'entity': '-it --rm --restart=Never loadgenerator',\n",
       "    'description': 'options used when running the load-generating pod ',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"horizontal pod autoscaler\", \\n   \"description\": \"successfully rescaled to one replica because all metrics were below target\",\\n   \"destination_entity\": \"Deployment/kubia\"},\\n\\n  {\"source_entity\": \"kubectl expose\", \\n   \"description\": \"exposed the pods through a Service so they can be hit from a single URL\",\\n   \"destination_entity\": \"Service\"},\\n\\n  {\"source_entity\": \"horizontal pod autoscaler\", \\n   \"description\": \"detected increased CPU usage and started scaling up additional pods\",\\n   \"destination_entity\": \"kubectl run\"},\\n\\n  {\"source_entity\": \"kubectl get\", \\n   \"description\": \"showed the state of HPA and Deployment objects in real-time\",\\n   \"destination_entity\": \"hpa/kubia\"},\\n\\n  {\"source_entity\": \"kubectl get\", \\n   \"description\": \"showed the state of Deployment object in real-time\",\\n   \"destination_entity\": \"Deployment/kubia\"},\\n\\n  {\"source_entity\": \"watch -n 1 kubectl get hpa,deployment\", \\n   \"description\": \"monitored changes to HPA and Deployment objects every second\",\\n   \"destination_entity\": \"hpa/kubia\"},\\n\\n  {\"source_entity\": \"kubectl run\", \\n   \"description\": \"ran a load-generating pod that sent requests to the kubia Service\",\\n   \"destination_entity\": \"http://kubia.default\"},\\n\\n  {\"source_entity\": \"busybox\", \\n   \"description\": \"used as the image for the load-generating pod\",\\n   \"destination_entity\": \"kubectl run\"},\\n\\n  {\"source_entity\": \"loadgenerator\", \\n   \"description\": \"ran a continuous loop of wget requests to the kubia Service\",\\n   \"destination_entity\": \"http://kubia.default\"}\\n]\\n```'},\n",
       " {'page': 478,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '446\\nCHAPTER 15\\nAutomatic scaling of pods and cluster nodes\\nThis will run a pod which repeatedly hits the kubia Service. You’ve seen the -it\\noption a few times when running the kubectl exec command. As you can see, it can\\nalso be used with kubectl run. It allows you to attach the console to the process,\\nwhich will not only show you the process’ output directly, but will also terminate the\\nprocess as soon as you press CTRL+C. The --rm option causes the pod to be deleted\\nafterward, and the --restart=Never option causes kubectl run to create an unman-\\naged pod directly instead of through a Deployment object, which you don’t need.\\nThis combination of options is useful for running commands inside the cluster with-\\nout having to piggyback on an existing pod. It not only behaves the same as if you\\nwere running the command locally, it even cleans up everything when the command\\nterminates. \\nSEEING THE AUTOSCALER SCALE UP THE DEPLOYMENT\\nAs the load-generator pod runs, you’ll see it initially hitting the single pod. As before,\\nit takes time for the metrics to be updated, but when they are, you’ll see the autoscaler\\nincrease the number of replicas. In my case, the pod’s CPU utilization initially jumped\\nto 108%, which caused the autoscaler to increase the number of pods to four. The\\nutilization on the individual pods then decreased to 74% and then stabilized at\\naround 26%. \\nNOTE\\nIf the CPU load in your case doesn’t exceed 30%, try running addi-\\ntional load-generators.\\nAgain, you can inspect autoscaler events with kubectl describe to see what the\\nautoscaler has done (only the most important information is shown in the following\\nlisting).\\nFrom    Reason              Message\\n----    ------              -------\\nh-p-a   SuccessfulRescale   New size: 1; reason: All metrics below target\\nh-p-a   SuccessfulRescale   New size: 4; reason: cpu resource utilization \\n                            (percentage of request) above target\\nDoes it strike you as odd that the initial average CPU utilization in my case, when I\\nonly had one pod, was 108%, which is more than 100%? Remember, a container’s\\nCPU utilization is the container’s actual CPU usage divided by its requested CPU. The\\nrequested CPU defines the minimum, not maximum amount of CPU available to the\\ncontainer, so a container may consume more than the requested CPU, bringing the\\npercentage over 100. \\n Before we go on, let’s do a little math and see how the autoscaler concluded that\\nfour replicas are needed. Initially, there was one replica handling requests and its\\nCPU usage spiked to 108%. Dividing 108 by 30 (the target CPU utilization percent-\\nage) gives 3.6, which the autoscaler then rounded up to 4. If you divide 108 by 4, you\\nListing 15.5\\nEvents of a HorizontalPodAutoscaler\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for interacting with Kubernetes clusters.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A logical host within a Kubernetes cluster, running one or more containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': '-it option',\n",
       "    'description': 'An option for attaching the console to a process when running kubectl exec command.',\n",
       "    'category': 'software'},\n",
       "   {'entity': '--rm option',\n",
       "    'description': 'An option that causes the pod to be deleted after execution is complete.',\n",
       "    'category': 'software'},\n",
       "   {'entity': '--restart=Never option',\n",
       "    'description': 'An option that creates an unmanaged pod directly instead of through a Deployment object.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A Kubernetes concept for managing multiple replicas of a pod.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'autoscaler',\n",
       "    'description': 'A component that automatically scales the number of pods based on CPU utilization.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'HorizontalPodAutoscaler',\n",
       "    'description': 'A Kubernetes feature for scaling pods horizontally based on CPU utilization.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubia Service',\n",
       "    'description': 'A Kubernetes service that provides a load balancer for kubia applications.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'load-generator pod',\n",
       "    'description': 'A pod that repeatedly hits the kubia Service, simulating traffic.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'CPU utilization',\n",
       "    'description': 'A metric that measures the actual CPU usage of a container divided by its requested CPU.',\n",
       "    'category': 'metric'},\n",
       "   {'entity': 'target CPU utilization percentage',\n",
       "    'description': 'The desired average CPU utilization percentage for a pod.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"load-generator pod\", \"description\": \"hitting the kubia Service\", \"destination_entity\": \"kubia Service\"},\\n  {\"source_entity\": \"autoscaler\", \"description\": \"increasing the number of replicas\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"pod\", \"description\": \"CPU utilization spiked to 108%\", \"destination_entity\": \"CPU utilization\"},\\n  {\"source_entity\": \"-it option\", \"description\": \"allowing console attachment and termination\", \"destination_entity\": \"kubectl exec command\"},\\n  {\"source_entity\": \"--restart=Never option\", \"description\": \"creating an unmanaged pod directly\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"--rm option\", \"description\": \"causing the pod to be deleted afterward\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"HorizontalPodAutoscaler\", \"description\": \"scaling up the Deployment\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"autoscaler\", \"description\": \"concluding four replicas are needed\", \"destination_entity\": \"HorizontalPodAutoscaler\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"inspecting autoscaler events\", \"destination_entity\": \"HorizontalPodAutoscaler\"}\\n]\\n```\\n\\nI have extracted the following relations:\\n\\n1. The load-generator pod is hitting the kubia Service.\\n2. The autoscaler is increasing the number of replicas for the Deployment.\\n3. A pod\\'s CPU utilization spiked to 108%.\\n4. The -it option allows console attachment and termination for the kubectl exec command.\\n5. The --restart=Never option creates an unmanaged pod directly for the Deployment.\\n6. The --rm option causes the pod to be deleted afterward.\\n7. The HorizontalPodAutoscaler is scaling up the Deployment.\\n8. The autoscaler concludes that four replicas are needed for the HorizontalPodAutoscaler.\\n9. Kubectl is inspecting autoscaler events for the HorizontalPodAutoscaler.'},\n",
       " {'page': 479,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '447\\nHorizontal pod autoscaling\\nget 27%. If the autoscaler scales up to four pods, their average CPU utilization is\\nexpected to be somewhere in the neighborhood of 27%, which is close to the target\\nvalue of 30% and almost exactly what the observed CPU utilization was.\\nUNDERSTANDING THE MAXIMUM RATE OF SCALING\\nIn my case, the CPU usage shot up to 108%, but in general, the initial CPU usage\\ncould spike even higher. Even if the initial average CPU utilization was higher (say\\n150%), requiring five replicas to achieve the 30% target, the autoscaler would still\\nonly scale up to four pods in the first step, because it has a limit on how many repli-\\ncas can be added in a single scale-up operation. The autoscaler will at most double\\nthe number of replicas in a single operation, if more than two current replicas\\nexist. If only one or two exist, it will scale up to a maximum of four replicas in a sin-\\ngle step. \\n Additionally, it has a limit on how soon a subsequent autoscale operation can\\noccur after the previous one. Currently, a scale-up will occur only if no rescaling\\nevent occurred in the last three minutes. A scale-down event is performed even less\\nfrequently—every five minutes. Keep this in mind so you don’t wonder why the\\nautoscaler refuses to perform a rescale operation even if the metrics clearly show\\nthat it should.\\nMODIFYING THE TARGET METRIC VALUE ON AN EXISTING HPA OBJECT\\nTo wrap up this section, let’s do one last exercise. Maybe your initial CPU utilization\\ntarget of 30% was a bit too low, so increase it to 60%. You do this by editing the HPA\\nresource with the kubectl edit command. When the text editor opens, change the\\ntargetAverageUtilization field to 60, as shown in the following listing.\\n...\\nspec:\\n  maxReplicas: 5\\n  metrics:\\n  - resource:\\n      name: cpu\\n      targetAverageUtilization: 60    \\n    type: Resource\\n...\\nAs with most other resources, after you modify the resource, your changes will be\\ndetected by the autoscaler controller and acted upon. You could also delete the\\nresource and recreate it with different target values, because by deleting the HPA\\nresource, you only disable autoscaling of the target resource (a Deployment in this\\ncase) and leave it at the scale it is at that time. The automatic scaling will resume after\\nyou create a new HPA resource for the Deployment.\\nListing 15.6\\nIncreasing the target CPU utilization by editing the HPA resource\\nChange this \\nfrom 30 to 60.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Horizontal pod autoscaling',\n",
       "    'description': 'A feature in Kubernetes that automatically scales the number of replicas based on CPU usage.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'CPU utilization',\n",
       "    'description': 'The average usage of CPU resources by pods, measured as a percentage.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Autoscaler',\n",
       "    'description': 'A controller in Kubernetes that automatically scales the number of replicas based on CPU usage.',\n",
       "    'category': 'controller'},\n",
       "   {'entity': 'Replicas',\n",
       "    'description': 'The number of identical copies of a pod that are running at any given time.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl edit command',\n",
       "    'description': 'A command in Kubernetes that allows editing resources directly from the terminal.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Resource',\n",
       "    'description': 'An abstraction in Kubernetes that represents a single unit of work, such as a pod or service.',\n",
       "    'category': 'object'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A resource in Kubernetes that manages the rollout of new versions of an application.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Metrics',\n",
       "    'description': 'Data collected by the autoscaler to determine whether to scale up or down.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n    {\\n        \"source_entity\": \"Horizontal pod autoscaling\",\\n        \"description\": \"Scales up to four pods when CPU utilization is expected to be around 27%\",\\n        \"destination_entity\": \"CPU utilization\"\\n    },\\n    {\\n        \"source_entity\": \"Autoscaler\",\\n        \"description\": \"Has a limit on how many replicas can be added in a single scale-up operation, doubling the number of replicas if more than two exist\",\\n        \"destination_entity\": \"Replicas\"\\n    },\\n    {\\n        \"source_entity\": \"Autoscaler\",\\n        \"description\": \"Will at most double the number of replicas in a single operation\",\\n        \"destination_entity\": \"Replicas\"\\n    },\\n    {\\n        \"source_entity\": \"Autoscaler\",\\n        \"description\": \"Has a limit on how soon a subsequent autoscale operation can occur after the previous one, every three minutes for scale-up and five minutes for scale-down\",\\n        \"destination_entity\": \"Time\"\\n    },\\n    {\\n        \"source_entity\": \"Horizontal pod autoscaling\",\\n        \"description\": \"Will be refused to perform a rescale operation if no rescaling event occurred in the last three minutes\",\\n        \"destination_entity\": \"Time\"\\n    },\\n    {\\n        \"source_entity\": \"kubectl edit command\",\\n        \"description\": \"Is used to modify the HPA resource targetAverageUtilization field\",\\n        \"destination_entity\": \"HPA resource\"\\n    },\\n    {\\n        \"source_entity\": \"Metrics\",\\n        \"description\": \"Are monitored by the autoscaler controller and acted upon when modified\",\\n        \"destination_entity\": \"Autoscaler controller\"\\n    },\\n    {\\n        \"source_entity\": \"Deployment\",\\n        \"description\": \"Is left at the scale it is at that time when the HPA resource is deleted\",\\n        \"destination_entity\": \"HPA resource\"\\n    },\\n    {\\n        \"source_entity\": \"Horizontal pod autoscaling\",\\n        \"description\": \"Will resume after a new HPA resource is created for the Deployment\",\\n        \"destination_entity\": \"Deployment\"\\n    },\\n    {\\n        \"source_entity\": \"CPU utilization\",\\n        \"description\": \"Is increased from 30 to 60 by modifying the targetAverageUtilization field in the HPA resource\",\\n        \"destination_entity\": \"HPA resource\"\\n    }\\n]'},\n",
       " {'page': 480,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '448\\nCHAPTER 15\\nAutomatic scaling of pods and cluster nodes\\n15.1.3 Scaling based on memory consumption\\nYou’ve seen how easily the horizontal Autoscaler can be configured to keep CPU uti-\\nlization at the target level. But what about autoscaling based on the pods’ memory\\nusage? \\n Memory-based autoscaling is much more problematic than CPU-based autoscal-\\ning. The main reason is because after scaling up, the old pods would somehow need to\\nbe forced to release memory. This needs to be done by the app itself—it can’t be done\\nby the system. All the system could do is kill and restart the app, hoping it would use\\nless memory than before. But if the app then uses the same amount as before, the\\nAutoscaler would scale it up again. And again, and again, until it reaches the maxi-\\nmum number of pods configured on the HPA resource. Obviously, this isn’t what any-\\none wants. Memory-based autoscaling was introduced in Kubernetes version 1.8, and\\nis configured exactly like CPU-based autoscaling. Exploring it is left up to the reader.\\n15.1.4 Scaling based on other and custom metrics\\nYou’ve seen how easy it is to scale pods based on their CPU usage. Initially, this was the\\nonly autoscaling option that was usable in practice. To have the autoscaler use custom,\\napp-defined metrics to drive its autoscaling decisions was fairly complicated. The ini-\\ntial design of the autoscaler didn’t make it easy to move beyond simple CPU-based\\nscaling. This prompted the Kubernetes Autoscaling Special Interest Group (SIG) to\\nredesign the autoscaler completely. \\n If you’re interested in learning how complicated it was to use the initial autoscaler\\nwith custom metrics, I invite you to read my blog post entitled “Kubernetes autoscal-\\ning based on custom metrics without using a host port,” which you’ll find online at\\nhttp:/\\n/medium.com/@marko.luksa. You’ll learn about all the other problems I\\nencountered when trying to set up autoscaling based on custom metrics. Luckily,\\nnewer versions of Kubernetes don’t have those problems. I’ll cover the subject in a\\nnew blog post. \\n Instead of going through a complete example here, let’s quickly go over how to\\nconfigure the autoscaler to use different metrics sources. We’ll start by examining how\\nwe defined what metric to use in our previous example. The following listing shows\\nhow your previous HPA object was configured to use the CPU usage metric.\\n...\\nspec:\\n  maxReplicas: 5\\n  metrics:\\n  - type: Resource      \\n    resource:\\n      name: cpu                      \\n      targetAverageUtilization: 30    \\n...\\nListing 15.7\\nHorizontalPodAutoscaler definition for CPU-based autoscaling\\nDefines the type \\nof metric\\nThe resource, whose \\nutilization will be monitored\\nThe target utilization \\nof this resource\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Autoscaler',\n",
       "    'description': 'System to automatically scale pods and cluster nodes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'HPA',\n",
       "    'description': 'Horizontal Pod Autoscaler, a resource for autoscaling pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CPU',\n",
       "    'description': 'Central Processing Unit, a hardware component',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Memory',\n",
       "    'description': 'Random Access Memory, a type of computer storage',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Lightweight and portable containers in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Cluster nodes',\n",
       "    'description': 'Nodes in a Kubernetes cluster',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Resource',\n",
       "    'description': 'A component of the autoscaler that monitors CPU or memory utilization',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Type',\n",
       "    'description': 'The type of metric used by the autoscaler, such as CPU or Memory',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Utilization',\n",
       "    'description': 'The amount of CPU or memory being used by a resource',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Target Average Utilization',\n",
       "    'description': 'The desired average utilization of a resource',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"CPU\",\\n    \"description\": \"utilization is monitored by the Autoscaler\",\\n    \"destination_entity\": \"Autoscaler\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"introduced memory-based autoscaling in version 1.8\",\\n    \"destination_entity\": \"Memory\"\\n  },\\n  {\\n    \"source_entity\": \"Target Average Utilization\",\\n    \"description\": \"is set to 30% for CPU usage metric\",\\n    \"destination_entity\": \"CPU\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"can be scaled up or down based on memory consumption\",\\n    \"destination_entity\": \"Memory\"\\n  },\\n  {\\n    \"source_entity\": \"Autoscaler\",\\n    \"description\": \"is designed to scale pods based on CPU usage, memory usage, or custom metrics\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster nodes\",\\n    \"description\": \"can be scaled up or down based on CPU consumption\",\\n    \"destination_entity\": \"CPU\"\\n  },\\n  {\\n    \"source_entity\": \"Type\",\\n    \"description\": \"defines the type of metric used by the Autoscaler\",\\n    \"destination_entity\": \"Autoscaler\"\\n  },\\n  {\\n    \"source_entity\": \"HPA\",\\n    \"description\": \"resource is configured to use CPU usage metric\",\\n    \"destination_entity\": \"CPU\"\\n  },\\n  {\\n    \"source_entity\": \"Resource\",\\n    \"description\": \"whose utilization will be monitored by the Autoscaler\",\\n    \"destination_entity\": \"Autoscaler\"\\n  },\\n  {\\n    \"source_entity\": \"Utilization\",\\n    \"description\": \"is monitored by the Autoscaler for CPU, memory, and custom metrics\",\\n    \"destination_entity\": \"Autoscaler\"\\n  }\\n]\\n```'},\n",
       " {'page': 481,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '449\\nHorizontal pod autoscaling\\nAs you can see, the metrics field allows you to define more than one metric to use.\\nIn the listing, you’re using a single metric. Each entry defines the type of metric—\\nin this case, a Resource metric. You have three types of metrics you can use in an\\nHPA object:\\n\\uf0a1\\nResource\\n\\uf0a1\\nPods\\n\\uf0a1\\nObject\\nUNDERSTANDING THE RESOURCE METRIC TYPE\\nThe Resource type makes the autoscaler base its autoscaling decisions on a resource\\nmetric, like the ones specified in a container’s resource requests. We’ve already seen\\nhow to do that, so let’s focus on the other two types.\\nUNDERSTANDING THE PODS METRIC TYPE\\nThe Pods type is used to refer to any other (including custom) metric related to the\\npod directly. An example of such a metric could be the already mentioned Queries-\\nPer-Second (QPS) or the number of messages in a message broker’s queue (when the\\nmessage broker is running as a pod). To configure the autoscaler to use the pod’s QPS\\nmetric, the HPA object would need to include the entry shown in the following listing\\nunder its metrics field.\\n...\\nspec:\\n  metrics:\\n  - type: Pods              \\n    resource:\\n      metricName: qps             \\n      targetAverageValue: 100    \\n...\\nThe example in the listing configures the autoscaler to keep the average QPS of all\\nthe pods managed by the ReplicaSet (or other) controller targeted by this HPA\\nresource at 100. \\nUNDERSTANDING THE OBJECT METRIC TYPE\\nThe Object metric type is used when you want to make the autoscaler scale pods\\nbased on a metric that doesn’t pertain directly to those pods. For example, you may\\nwant to scale pods according to a metric of another cluster object, such as an Ingress\\nobject. The metric could be QPS as in listing 15.8, the average request latency, or\\nsomething else completely. \\n Unlike in the previous case, where the autoscaler needed to obtain the metric for\\nall targeted pods and then use the average of those values, when you use an Object\\nmetric type, the autoscaler obtains a single metric from the single object. In the HPA\\nListing 15.8\\nReferring to a custom pod metric in the HPA\\nDefines a pod metric\\nThe name of \\nthe metric\\nThe target average value \\nacross all targeted pods\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Horizontal Pod Autoscaling',\n",
       "    'description': 'A mechanism to automatically scale the number of replicas of a deployment based on CPU or memory utilization.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'metrics field',\n",
       "    'description': 'A field in the HPA object that defines the metrics to use for autoscaling decisions.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Resource metric type',\n",
       "    'description': \"A type of metric used in HPA objects based on a resource metric, like the ones specified in a container's resource requests.\",\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Pods metric type',\n",
       "    'description': 'A type of metric used in HPA objects to refer to any other (including custom) metric related to the pod directly.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Object metric type',\n",
       "    'description': \"A type of metric used in HPA objects when you want to make the autoscaler scale pods based on a metric that doesn't pertain directly to those pods.\",\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Refers to any other (including custom) metric related to the pod directly.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Object',\n",
       "    'description': \"Used when you want to make the autoscaler scale pods based on a metric that doesn't pertain directly to those pods.\",\n",
       "    'category': 'container'},\n",
       "   {'entity': 'HPA object',\n",
       "    'description': 'An object that defines the configuration for Horizontal Pod Autoscaling.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'targetAverageValue',\n",
       "    'description': 'The target average value of a metric across all targeted pods.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'metricName',\n",
       "    'description': 'The name of the metric used in an HPA object.',\n",
       "    'category': 'command'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Horizontal Pod Autoscaling\", \\n   \"description\": \"defines more than one metric to use\",\\n   \"destination_entity\": \"metrics field\"},\\n\\n  {\"source_entity\": \"metrics field\", \\n   \"description\": \"using a single metric\",\\n   \"destination_entity\": \"Pods metric type\"},\\n\\n  {\"source_entity\": \"Pods metric type\", \\n   \"description\": \"refers to any other (including custom) metric related to the pod directly\",\\n   \"destination_entity\": \"pods managed by the ReplicaSet controller\"},\\n\\n  {\"source_entity\": \"metrics field\", \\n   \"description\": \"configures the autoscaler to use the pod\\'s QPS metric\",\\n   \"destination_entity\": \"Pods metric type\"},\\n\\n  {\"source_entity\": \"HPA object\", \\n   \"description\": \"includes the entry shown in the listing under its metrics field\",\\n   \"destination_entity\": \"metrics field\"},\\n\\n  {\"source_entity\": \"Resource metric type\", \\n   \"description\": \"makes the autoscaler base its autoscaling decisions on a resource metric\",\\n   \"destination_entity\": \"container\\'s resource requests\"},\\n\\n  {\"source_entity\": \"Object metric type\", \\n   \"description\": \"makes the autoscaler scale pods based on a metric that doesn’t pertain directly to those pods\",\\n   \"destination_entity\": \"another cluster object, such as an Ingress object\"},\\n\\n  {\"source_entity\": \"metrics field\", \\n   \"description\": \"defines the name of the metric and the target average value across all targeted pods\",\\n   \"destination_entity\": \"Pods metric type\"},\\n\\n  {\"source_entity\": \"HPA object\", \\n   \"description\": \"configures the autoscaler to keep the average QPS of all the pods managed by the ReplicaSet controller targeted by this HPA resource at 100\",\\n   \"destination_entity\": \"Pods metric type\"},\\n  \\n  {\"source_entity\": \"metrics field\", \\n   \"description\": \"defines a pod metric\",\\n   \"destination_entity\": \"Object metric type\"},\\n\\n  {\"source_entity\": \"Horizontal Pod Autoscaling\", \\n   \"description\": \"allows you to define more than one metric to use\",\\n   \"destination_entity\": \"metrics field\"},\\n  \\n  {\"source_entity\": \"HPA object\", \\n   \"description\": \"uses a single metric\",\\n   \"destination_entity\": \"metrics field\"},\\n\\n  {\"source_entity\": \"Object metric type\", \\n   \"description\": \"obtains a single metric from the single object\",\\n   \"destination_entity\": \"cluster object, such as an Ingress object\"}\\n]\\n```'},\n",
       " {'page': 482,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '450\\nCHAPTER 15\\nAutomatic scaling of pods and cluster nodes\\ndefinition, you need to specify the target object and the target value. The following\\nlisting shows an example.\\n...\\nspec:\\n  metrics:\\n  - type: Object                   \\n    resource:\\n      metricName: latencyMillis           \\n      target: \\n        apiVersion: extensions/v1beta1     \\n        kind: Ingress                      \\n        name: frontend                     \\n      targetValue: 20                   \\n  scaleTargetRef:                          \\n    apiVersion: extensions/v1beta1         \\n    kind: Deployment                       \\n    name: kubia                            \\n...\\nIn this example, the HPA is configured to use the latencyMillis metric of the\\nfrontend Ingress object. The target value for the metric is 20. The horizontal pod\\nautoscaler will monitor the Ingress’ metric and if it rises too far above the target value,\\nthe autoscaler will scale the kubia Deployment resource. \\n15.1.5 Determining which metrics are appropriate for autoscaling\\nYou need to understand that not all metrics are appropriate for use as the basis of\\nautoscaling. As mentioned previously, the pods’ containers’ memory consumption isn’t\\na good metric for autoscaling. The autoscaler won’t function properly if increasing\\nthe number of replicas doesn’t result in a linear decrease of the average value of the\\nobserved metric (or at least close to linear). \\n For example, if you have only a single pod instance and the value of the metric is X\\nand the autoscaler scales up to two replicas, the metric needs to fall to somewhere\\nclose to X/2. An example of such a custom metric is Queries per Second (QPS),\\nwhich in the case of web applications reports the number of requests the application\\nis receiving per second. Increasing the number of replicas will always result in a pro-\\nportionate decrease of QPS, because a greater number of pods will be handling the\\nsame total number of requests. \\n Before you decide to base the autoscaler on your app’s own custom metric, be sure\\nto think about how its value will behave when the number of pods increases or\\ndecreases.\\n15.1.6 Scaling down to zero replicas\\nThe horizontal pod autoscaler currently doesn’t allow setting the minReplicas field\\nto 0, so the autoscaler will never scale down to zero, even if the pods aren’t doing\\nListing 15.9\\nReferring to a metric of a different object in the HPA\\nUse metric of a \\nspecific object\\nThe name of \\nthe metric\\nThe specific object whose metric \\nthe autoscaler should obtain\\nThe\\nAutoscaler\\nshould\\nscale so\\nthe value\\nof the\\nmetric\\nstays close\\nto this.\\nThe scalable resource the \\nautoscaler will scale\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Object',\n",
       "    'description': 'A target object for horizontal pod autoscaling',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'metricName',\n",
       "    'description': 'The name of a metric to be used for scaling',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'targetValue',\n",
       "    'description': 'The target value for the metric',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'The API version for a Kubernetes resource',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'The type of a Kubernetes resource',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'The name of a Kubernetes resource',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A Kubernetes deployment resource',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Ingress',\n",
       "    'description': 'A Kubernetes ingress resource',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'metrics',\n",
       "    'description': 'A list of metrics used for scaling',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'horizontal pod autoscaler (HPA)',\n",
       "    'description': 'A mechanism to automatically scale pods based on metrics',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'queries per second (QPS)',\n",
       "    'description': 'A custom metric for autoscaling web applications',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"HORIZONTAL POD AUTOSCALER (HPA)\", \\n   \"description\": \"monitoring latencyMillis metric of frontend Ingress object\", \\n   \"destination_entity\": \"frontend Ingress object\"},\\n\\n  {\"source_entity\": \"HORIZONTAL POD AUTOSCALER (HPA)\", \\n   \"description\": \"autoscaling kubia Deployment resource based on target value\", \\n   \"destination_entity\": \"kubia Deployment resource\"},\\n\\n  {\"source_entity\": \"AUTOSCALER\", \\n   \"description\": \"checking if metric is appropriate for autoscaling\", \\n   \"destination_entity\": \"pods\\' containers\\' memory consumption\"},\\n\\n  {\"source_entity\": \"AUTOSCALER\", \\n   \"description\": \"expecting proportional decrease of Queries per Second (QPS) with more replicas\", \\n   \"destination_entity\": \"Queries per Second (QPS)\"},\\n\\n  {\"source_entity\": \"AUTOSCALER\", \\n   \"description\": \"not scaling down to zero replicas because minReplicas field is not allowed to be set to 0\", \\n   \"destination_entity\": \"pods\"},\\n\\n  {\"source_entity\": \"HORIZONTAL POD AUTOSCALER (HPA)\", \\n   \"description\": \"referencing specific object metric in autoscaler configuration\", \\n   \"destination_entity\": \"metric of a different object\"},\\n\\n  {\"source_entity\": \"AUTOSCALER\", \\n   \"description\": \"scaling resource based on metric value\", \\n   \"destination_entity\": \"scalable resource\"}\\n]\\n```\\n\\nNote that I\\'ve used the entities provided in the input list, and tried to match them with relevant phrases or concepts from the document page. Let me know if you\\'d like me to clarify any of these relations!'},\n",
       " {'page': 483,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '451\\nVertical pod autoscaling\\nanything. Allowing the number of pods to be scaled down to zero can dramatically\\nincrease the utilization of your hardware. When you run services that get requests only\\nonce every few hours or even days, it doesn’t make sense to have them running all the\\ntime, eating up resources that could be used by other pods. But you still want to have\\nthose services available immediately when a client request comes in. \\n This is known as idling and un-idling. It allows pods that provide a certain service\\nto be scaled down to zero. When a new request comes in, the request is blocked until\\nthe pod is brought up and then the request is finally forwarded to the pod. \\n Kubernetes currently doesn’t provide this feature yet, but it will eventually. Check\\nthe documentation to see if idling has been implemented yet. \\n15.2\\nVertical pod autoscaling\\nHorizontal scaling is great, but not every application can be scaled horizontally. For\\nsuch applications, the only option is to scale them vertically—give them more CPU\\nand/or memory. Because a node usually has more resources than a single pod\\nrequests, it should almost always be possible to scale a pod vertically, right? \\n Because a pod’s resource requests are configured through fields in the pod\\nmanifest, vertically scaling a pod would be performed by changing those fields. I\\nsay “would” because it’s currently not possible to change either resource requests\\nor limits of existing pods. Before I started writing the book (well over a year ago), I\\nwas sure that by the time I wrote this chapter, Kubernetes would already support\\nproper vertical pod autoscaling, so I included it in my proposal for the table of con-\\ntents. Sadly, what seems like a lifetime later, vertical pod autoscaling is still not\\navailable yet. \\n15.2.1 Automatically configuring resource requests\\nAn experimental feature sets the CPU and memory requests on newly created pods, if\\ntheir containers don’t have them set explicitly. The feature is provided by an Admission\\nControl plugin called InitialResources. When a new pod without resource requests is\\ncreated, the plugin looks at historical resource usage data of the pod’s containers (per\\nthe underlying container image and tag) and sets the requests accordingly. \\n You can deploy pods without specifying resource requests and rely on Kubernetes\\nto eventually figure out what each container’s resource needs are. Effectively, Kuber-\\nnetes is vertically scaling the pod. For example, if a container keeps running out of\\nmemory, the next time a pod with that container image is created, its resource request\\nfor memory will be set higher automatically.\\n15.2.2 Modifying resource requests while a pod is running\\nEventually, the same mechanism will be used to modify an existing pod’s resource\\nrequests, which means it will vertically scale the pod while it’s running. As I’m writing\\nthis, a new vertical pod autoscaling proposal is being finalized. Please refer to the\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Lightweight and portable container runtimes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'CPU',\n",
       "    'description': 'Central Processing Unit resource',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Memory',\n",
       "    'description': 'Random Access Memory resource',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Resource requests',\n",
       "    'description': 'Configure the amount of resources a pod can use',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Resource limits',\n",
       "    'description': 'Limit the amount of resources a pod can use',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Admission Control',\n",
       "    'description': 'Security feature that enforces access to a Kubernetes cluster',\n",
       "    'category': 'security'},\n",
       "   {'entity': 'InitialResources',\n",
       "    'description': 'Admission control plugin for automatically configuring resource requests',\n",
       "    'category': 'plugin'},\n",
       "   {'entity': 'Vertical pod autoscaling',\n",
       "    'description': 'Scalability feature that allows pods to be scaled up or down based on CPU and memory usage',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Horizontal scaling',\n",
       "    'description': 'Scalability feature that allows multiple replicas of a pod to run simultaneously',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Container image',\n",
       "    'description': 'Template for creating a new container instance',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Tag',\n",
       "    'description': 'Identifier for a specific version of a container image',\n",
       "    'category': 'metadata'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"allows pods to be scaled down to zero for idling and un-idling\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"can be scaled horizontally, but not every application can be scaled horizontally\",\\n    \"destination_entity\": \"Horizontal scaling\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"does not currently provide vertical pod autoscaling, but will eventually\",\\n    \"destination_entity\": \"Vertical pod autoscaling\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"can be scaled vertically by giving them more CPU and/or memory\",\\n    \"destination_entity\": \"Memory\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"can be scaled vertically by giving them more CPU and/or memory\",\\n    \"destination_entity\": \"CPU\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"sets CPU and memory requests on newly created pods automatically using InitialResources plugin\",\\n    \"destination_entity\": \"InitialResources\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"can have resource requests set by Kubernetes automatically using historical resource usage data\",\\n    \"destination_entity\": \"Resource requests\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"will eventually modify an existing pod\\'s resource requests while it\\'s running\",\\n    \"destination_entity\": \"Resource limits\"\\n  },\\n  {\\n    \"source_entity\": \"Admission Control\",\\n    \"description\": \"provides the InitialResources plugin to set CPU and memory requests on newly created pods automatically\",\\n    \"destination_entity\": \"InitialResources\"\\n  },\\n  {\\n    \"source_entity\": \"Container image\",\\n    \"description\": \"historical resource usage data is used by Kubernetes to determine resource needs for each container\",\\n    \"destination_entity\": \"Resource requests\"\\n  },\\n  {\\n    \"source_entity\": \"Vertical pod autoscaling\",\\n    \"description\": \"is still not available in Kubernetes, but a proposal is being finalized\",\\n    \"destination_entity\": \"Kubernetes\"\\n  }\\n]\\n```\\n\\nThese relations capture the connections between the entities mentioned in the document page, such as how Kubernetes interacts with pods and resource requests, and how admission control plugins like InitialResources are used to set CPU and memory requests automatically.'},\n",
       " {'page': 484,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '452\\nCHAPTER 15\\nAutomatic scaling of pods and cluster nodes\\nKubernetes documentation to find out whether vertical pod autoscaling is already\\nimplemented or not.\\n15.3\\nHorizontal scaling of cluster nodes\\nThe Horizontal Pod Autoscaler creates additional pod instances when the need for\\nthem arises. But what about when all your nodes are at capacity and can’t run any\\nmore pods? Obviously, this problem isn’t limited only to when new pod instances are\\ncreated by the Autoscaler. Even when creating pods manually, you may encounter the\\nproblem where none of the nodes can accept the new pods, because the node’s\\nresources are used up by existing pods. \\n In that case, you’d need to delete several of those existing pods, scale them down\\nvertically, or add additional nodes to your cluster. If your Kubernetes cluster is run-\\nning on premises, you’d need to physically add a new machine and make it part of the\\nKubernetes cluster. But if your cluster is running on a cloud infrastructure, adding\\nadditional nodes is usually a matter of a few clicks or an API call to the cloud infra-\\nstructure. This can be done automatically, right?\\n Kubernetes includes the feature to automatically request additional nodes from\\nthe cloud provider as soon as it detects additional nodes are needed. This is per-\\nformed by the Cluster Autoscaler.\\n15.3.1 Introducing the Cluster Autoscaler\\nThe Cluster Autoscaler takes care of automatically provisioning additional nodes\\nwhen it notices a pod that can’t be scheduled to existing nodes because of a lack of\\nresources on those nodes. It also de-provisions nodes when they’re underutilized for\\nlonger periods of time. \\nREQUESTING ADDITIONAL NODES FROM THE CLOUD INFRASTRUCTURE\\nA new node will be provisioned if, after a new pod is created, the Scheduler can’t\\nschedule it to any of the existing nodes. The Cluster Autoscaler looks out for such\\npods and asks the cloud provider to start up an additional node. But before doing\\nthat, it checks whether the new node can even accommodate the pod. After all, if\\nthat’s not the case, it makes no sense to start up such a node.\\n Cloud providers usually group nodes into groups (or pools) of same-sized nodes\\n(or nodes having the same features). The Cluster Autoscaler thus can’t simply say\\n“Give me an additional node.” It needs to also specify the node type.\\n The Cluster Autoscaler does this by examining the available node groups to see if\\nat least one node type would be able to fit the unscheduled pod. If exactly one such\\nnode group exists, the Autoscaler can increase the size of the node group to have the\\ncloud provider add another node to the group. If more than one option is available,\\nthe Autoscaler must pick the best one. The exact meaning of “best” will obviously\\nneed to be configurable. In the worst case, it selects a random one. A simple overview\\nof how the cluster Autoscaler reacts to an unschedulable pod is shown in figure 15.5.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod Autoscaler',\n",
       "    'description': 'Automatically scales pod instances based on resource usage',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Cluster Autoscaler',\n",
       "    'description': 'Automatically provisions additional nodes when resources are needed',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'Physical or virtual machine that runs pods',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Cluster',\n",
       "    'description': 'Group of nodes that run pods and services',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'Component that assigns pods to nodes based on resource availability',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Cloud Provider',\n",
       "    'description': 'Infrastructure provider (e.g. Amazon, Google) that offers scalable resources',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'API Call',\n",
       "    'description': 'Request to cloud provider to provision additional nodes',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Node Group',\n",
       "    'description': 'Group of nodes with the same features or characteristics',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Vertical Pod Autoscaling',\n",
       "    'description': 'Automatically scales resources for a single pod',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"implements feature to automatically request additional nodes from cloud provider\",\\n    \"destination_entity\": \"Cloud Provider\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster Autoscaler\",\\n    \"description\": \"requests additional node from cloud provider when it detects pod that can\\'t be scheduled to existing nodes\",\\n    \"destination_entity\": \"Cloud Provider\"\\n  },\\n  {\\n    \"source_entity\": \"Scheduler\",\\n    \"description\": \"can\\'t schedule new pod to any of the existing nodes, Cluster Autoscaler is triggered\",\\n    \"destination_entity\": \"Cluster Autoscaler\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster Autoscaler\",\\n    \"description\": \"de-provisions nodes when they\\'re underutilized for longer periods of time\",\\n    \"destination_entity\": \"Node\"\\n  },\\n  {\\n    \"source_entity\": \"Pod Autoscaler\",\\n    \"description\": \"creates additional pod instances when the need for them arises\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster Autoscaler\",\\n    \"description\": \"picks best node group to fit unscheduled pod, if exactly one option exists, increases size of node group\",\\n    \"destination_entity\": \"Node Group\"\\n  },\\n  {\\n    \"source_entity\": \"API Call\",\\n    \"description\": \"can be used to add additional nodes to cluster when running on cloud infrastructure\",\\n    \"destination_entity\": \"Cluster\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes Documentation\",\\n    \"description\": \"available to find out whether vertical pod autoscaling is already implemented or not\",\\n    \"destination_entity\": \"Vertical Pod Autoscaling\"\\n  }\\n]\\n```\\n\\nI\\'ve extracted the following relations:\\n\\n1. Kubernetes implements a feature to automatically request additional nodes from cloud providers.\\n2. Cluster Autoscaler requests additional nodes from cloud providers when it detects pods that can\\'t be scheduled to existing nodes.\\n3. Scheduler\\'s inability to schedule new pods triggers Cluster Autoscaler.\\n4. Cluster Autoscaler de-provisions underutilized nodes.\\n5. Pod Autoscaler creates additional pod instances.\\n6. Cluster Autoscaler picks the best node group to fit unscheduled pods and increases the size of the node group if necessary.\\n7. API Call can be used to add additional nodes to clusters when running on cloud infrastructure.\\n8. Kubernetes Documentation is available to find out whether vertical pod autoscaling is already implemented or not.\\n\\nNote that some relations might seem implicit, but I\\'ve tried to follow the rules provided and extract the most direct connections between entities mentioned in the document page.'},\n",
       " {'page': 485,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '453\\nHorizontal scaling of cluster nodes\\nWhen the new node starts up, the Kubelet on that node contacts the API server and\\nregisters the node by creating a Node resource. From then on, the node is part of the\\nKubernetes cluster and pods can be scheduled to it.\\n Simple, right? What about scaling down?\\nRELINQUISHING NODES\\nThe Cluster Autoscaler also needs to scale down the number of nodes when they\\naren’t being utilized enough. The Autoscaler does this by monitoring the requested\\nCPU and memory on all the nodes. If the CPU and memory requests of all the pods\\nrunning on a given node are below 50%, the node is considered unnecessary. \\n That’s not the only determining factor in deciding whether to bring a node down.\\nThe Autoscaler also checks to see if any system pods are running (only) on that node\\n(apart from those that are run on every node, because they’re deployed by a Daemon-\\nSet, for example). If a system pod is running on a node, the node won’t be relinquished.\\nThe same is also true if an unmanaged pod or a pod with local storage is running on the\\nnode, because that would cause disruption to the service the pod is providing. In other\\nwords, a node will only be returned to the cloud provider if the Cluster Autoscaler\\nknows the pods running on the node will be rescheduled to other nodes.\\n When a node is selected to be shut down, the node is first marked as unschedula-\\nble and then all the pods running on the node are evicted. Because all those pods\\nbelong to ReplicaSets or other controllers, their replacements are created and sched-\\nuled to the remaining nodes (that’s why the node that’s being shut down is first\\nmarked as unschedulable).\\nNode group X\\nNode X1\\n1. Autoscaler notices a\\nPod can’t be scheduled\\nto existing nodes\\n3. Autoscaler scales up the\\nnode group selected in\\nprevious step\\n2. Autoscaler determines which node\\ntype (if any) would be able to ﬁt the\\npod. If multiple types could ﬁt the\\npod, it selects one of them.\\nCluster\\nAutoscaler\\nPods\\nNode X2\\nPods\\nNode group Y\\nNode Y1\\nPods\\nUnschedulable\\npod\\nFigure 15.5\\nThe Cluster Autoscaler scales up when it finds a pod that can’t be scheduled to \\nexisting nodes.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Centralized control plane for Kubernetes cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'Physical or virtual machine running Kubelet and hosting pods',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'Agent that runs on each node, managing containers and communicating with API server',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Cluster Autoscaler',\n",
       "    'description': 'Component that scales up or down the number of nodes in a cluster based on utilization',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Daemon-Set',\n",
       "    'description': 'Controller that runs a pod on every node in a cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicaSets',\n",
       "    'description': 'Controller that ensures a specified number of replicas (identical pods) are running at any given time',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Lightweight and portable container runtime environment',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"contacts and registers a new node with the API server\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster Autoscaler\",\\n    \"description\": \"monitors CPU and memory requests of all pods on a node\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster Autoscaler\",\\n    \"description\": \"checks for system pods running on a node\",\\n    \"destination_entity\": \"System pods\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster Autoscaler\",\\n    \"description\": \"evicts all pods from an unschedulable node\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Autoscaler\",\\n    \"description\": \"notices a pod that can\\'t be scheduled to existing nodes\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Autoscaler\",\\n    \"description\": \"determines which node type would fit the pod\",\\n    \"destination_entity\": \"Node\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster Autoscaler\",\\n    \"description\": \"scales up a node group selected in previous step\",\\n    \"destination_entity\": \"Node group\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSets\",\\n    \"description\": \"controllers create replacements for pods evicted from an unschedulable node\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes cluster\",\\n    \"description\": \"pods can be scheduled to any available node\",\\n    \"destination_entity\": \"Node\"\\n  }\\n]\\n```'},\n",
       " {'page': 486,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '454\\nCHAPTER 15\\nAutomatic scaling of pods and cluster nodes\\n15.3.2 Enabling the Cluster Autoscaler\\nCluster autoscaling is currently available on\\n\\uf0a1Google Kubernetes Engine (GKE)\\n\\uf0a1Google Compute Engine (GCE)\\n\\uf0a1Amazon Web Services (AWS)\\n\\uf0a1Microsoft Azure\\nHow you start the Autoscaler depends on where your Kubernetes cluster is running.\\nFor your kubia cluster running on GKE, you can enable the Cluster Autoscaler like\\nthis:\\n$ gcloud container clusters update kubia --enable-autoscaling \\\\\\n  --min-nodes=3 --max-nodes=5\\nIf your cluster is running on GCE, you need to set three environment variables before\\nrunning kube-up.sh: \\n\\uf0a1\\nKUBE_ENABLE_CLUSTER_AUTOSCALER=true\\n\\uf0a1\\nKUBE_AUTOSCALER_MIN_NODES=3\\n\\uf0a1\\nKUBE_AUTOSCALER_MAX_NODES=5\\nRefer to the Cluster Autoscaler GitHub repo at https:/\\n/github.com/kubernetes/auto-\\nscaler/tree/master/cluster-autoscaler for information on how to enable it on other\\nplatforms. \\nNOTE\\nThe Cluster Autoscaler publishes its status to the cluster-autoscaler-\\nstatus ConfigMap in the kube-system namespace.\\n15.3.3 Limiting service disruption during cluster scale-down\\nWhen a node fails unexpectedly, nothing you can do will prevent its pods from becom-\\ning unavailable. But when a node is shut down voluntarily, either by the Cluster Auto-\\nscaler or by a human operator, you can make sure the operation doesn’t disrupt the\\nservice provided by the pods running on that node through an additional feature.\\nManually cordoning and draining nodes\\nA node can also be marked as unschedulable and drained manually. Without going\\ninto specifics, this is done with the following kubectl commands:\\n\\uf0a1\\nkubectl cordon <node> marks the node as unschedulable (but doesn’t do\\nanything with pods running on that node).\\n\\uf0a1\\nkubectl drain <node> marks the node as unschedulable and then evicts all\\nthe pods from the node.\\nIn both cases, no new pods are scheduled to the node until you uncordon it again\\nwith kubectl uncordon <node>.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Cluster Autoscaler',\n",
       "    'description': 'A feature that automatically scales a Kubernetes cluster based on resource utilization.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Google Kubernetes Engine (GKE)',\n",
       "    'description': 'A managed container environment for deploying and managing containerized applications.',\n",
       "    'category': 'Cloud Service'},\n",
       "   {'entity': 'Google Compute Engine (GCE)',\n",
       "    'description': 'A cloud-based computing platform that provides virtual machines, persistent disks, and other resources.',\n",
       "    'category': 'Cloud Service'},\n",
       "   {'entity': 'Amazon Web Services (AWS)',\n",
       "    'description': 'A comprehensive cloud services platform offering a broad range of global compute, storage, database, analytics, networking, and security solutions.',\n",
       "    'category': 'Cloud Service'},\n",
       "   {'entity': 'Microsoft Azure',\n",
       "    'description': 'A cloud computing service that provides a scalable and secure environment for deploying and managing applications.',\n",
       "    'category': 'Cloud Service'},\n",
       "   {'entity': 'kubectl cordon',\n",
       "    'description': 'A command used to mark a node as unschedulable, preventing new pods from being scheduled on it.',\n",
       "    'category': 'Command'},\n",
       "   {'entity': 'kubectl drain',\n",
       "    'description': 'A command used to mark a node as unschedulable and evict all pods from it.',\n",
       "    'category': 'Command'},\n",
       "   {'entity': 'kubectl uncordon',\n",
       "    'description': 'A command used to unmark a node as unschedulable, allowing new pods to be scheduled on it.',\n",
       "    'category': 'Command'},\n",
       "   {'entity': 'Cluster Autoscaler status',\n",
       "    'description': \"The publication of Cluster Autoscaler's status to the cluster-autoscaler-status ConfigMap in the kube-system namespace.\",\n",
       "    'category': 'Application'}],\n",
       "  'relationships': '[{\"source_entity\": \"Google Kubernetes Engine (GKE)\", \"description\": \"enable Cluster Autoscaler\", \"destination_entity\": \"Cluster Autoscaler\"},{\"source_entity\": \"gcloud container clusters update kubia\", \"description\": \"update cluster settings to enable autoscaling\", \"destination_entity\": \"Cluster Autoscaler\"},{\"source_entity\": \"KUBE_ENABLE_CLUSTER_AUTOSCALER\", \"description\": \"enable Cluster Autoscaler feature\", \"destination_entity\": \"Cluster Autoscaler\"},{\"source_entity\": \"KUBE_AUTOSCALER_MIN_NODES\", \"description\": \"set minimum number of nodes for autoscaling\", \"destination_entity\": \"Cluster Autoscaler\"},{\"source_entity\": \"KUBE_AUTOSCALER_MAX_NODES\", \"description\": \"set maximum number of nodes for autoscaling\", \"destination_entity\": \"Cluster Autoscaler\"},{\"source_entity\": \"kubectl cordon\", \"description\": \"mark node as unschedulable and prevent new pods from being scheduled\", \"destination_entity\": \"node\"},{\"source_entity\": \"kubectl drain\", \"description\": \"evict all pods from a marked unschedulable node\", \"destination_entity\": \"node\"},{\"source_entity\": \"kubectl uncordon\", \"description\": \"remove mark on node to allow scheduling of new pods\", \"destination_entity\": \"node\"},{\"source_entity\": \"Cluster Autoscaler status\", \"description\": \"publish status updates to ConfigMap in kube-system namespace\", \"destination_entity\": \"ConfigMap\"},{\"source_entity\": \"human operator\", \"description\": \"shut down node voluntarily using Cluster Autoscaler or manual commands\", \"destination_entity\": \"node\"},{\"source_entity\": \"Amazon Web Services (AWS)\", \"description\": \"support platform for enabling Cluster Autoscaler\", \"destination_entity\": \"Cluster Autoscaler\"},{\"source_entity\": \"Google Compute Engine (GCE)\", \"description\": \"support platform for enabling Cluster Autoscaler\", \"destination_entity\": \"Cluster Autoscaler\"},{\"source_entity\": \"Microsoft Azure\", \"description\": \"support platform for enabling Cluster Autoscaler\", \"destination_entity\": \"Cluster Autoscaler\"}]'},\n",
       " {'page': 487,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '455\\nHorizontal scaling of cluster nodes\\n Certain services require that a minimum number of pods always keeps running;\\nthis is especially true for quorum-based clustered applications. For this reason, Kuber-\\nnetes provides a way of specifying the minimum number of pods that need to keep\\nrunning while performing these types of operations. This is done by creating a Pod-\\nDisruptionBudget resource.\\n Even though the name of the resource sounds complex, it’s one of the simplest\\nKubernetes resources available. It contains only a pod label selector and a number\\nspecifying the minimum number of pods that must always be available or, starting\\nfrom Kubernetes version 1.7, the maximum number of pods that can be unavailable.\\nWe’ll look at what a PodDisruptionBudget (PDB) resource manifest looks like, but\\ninstead of creating it from a YAML file, you’ll create it with kubectl create pod-\\ndisruptionbudget and then obtain and examine the YAML later.\\n If you want to ensure three instances of your kubia pod are always running (they\\nhave the label app=kubia), create the PodDisruptionBudget resource like this:\\n$ kubectl create pdb kubia-pdb --selector=app=kubia --min-available=3\\npoddisruptionbudget \"kubia-pdb\" created\\nSimple, right? Now, retrieve the PDB’s YAML. It’s shown in the next listing.\\n$ kubectl get pdb kubia-pdb -o yaml\\napiVersion: policy/v1beta1\\nkind: PodDisruptionBudget\\nmetadata:\\n  name: kubia-pdb\\nspec:\\n  minAvailable: 3         \\n  selector:                \\n    matchLabels:           \\n      app: kubia           \\nstatus:\\n  ...\\nYou can also use a percentage instead of an absolute number in the minAvailable\\nfield. For example, you could state that 60% of all pods with the app=kubia label need\\nto be running at all times.\\nNOTE\\nStarting with Kubernetes 1.7, the PodDisruptionBudget resource also\\nsupports the maxUnavailable field, which you can use instead of min-\\nAvailable if you want to block evictions when more than that many pods are\\nunavailable. \\nWe don’t have much more to say about this resource. As long as it exists, both the\\nCluster Autoscaler and the kubectl drain command will adhere to it and will never\\nevict a pod with the app=kubia label if that would bring the number of such pods\\nbelow three. \\nListing 15.10\\nA PodDisruptionBudget definition\\nHow many pods should \\nalways be available\\nThe label selector that \\ndetermines which pods \\nthis budget applies to\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Horizontal scaling',\n",
       "    'description': 'the process of increasing or decreasing the number of nodes in a cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pod-DisruptionBudget',\n",
       "    'description': 'a resource that specifies the minimum number of pods that must always be available',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'an open-source container orchestration system',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Cluster Autoscaler',\n",
       "    'description': 'a component that automatically scales the cluster based on resource utilization',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl drain command',\n",
       "    'description': 'a command used to evict pods from a node without deleting them',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'the basic execution unit in Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'label selector',\n",
       "    'description': 'a way to select pods based on their labels',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'minAvailable',\n",
       "    'description': 'the minimum number of pods that must always be available',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'maxUnavailable',\n",
       "    'description': 'the maximum number of pods that can be unavailable',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'percentage',\n",
       "    'description': 'a way to specify the minimum or maximum number of pods as a percentage',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kubia pod',\n",
       "    'description': 'a specific type of pod with the label app=kubia',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'PDB YAML',\n",
       "    'description': 'the YAML representation of a PodDisruptionBudget resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'a field in the YAML that specifies the API version',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'a field in the YAML that specifies the type of resource',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'a field in the YAML that contains metadata about the resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'a field in the YAML that specifies the configuration of the resource',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'minAvailable field',\n",
       "    'description': 'a field in the PDB YAML that specifies the minimum number of pods',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"provides a way to specify the minimum number of pods that need to keep running while performing certain operations.\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"supports the creation of Pod-DisruptionBudget resources\",\\n    \"destination_entity\": \"Pod-DisruptionBudget\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"adheres to the PodDisruptionBudget resource and will not evict a pod if it would bring the number of pods below a certain threshold\",\\n    \"destination_entity\": \"Cluster Autoscaler\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"will adhere to the PodDisruptionBudget resource and will not evict a pod with a certain label if it would bring the number of such pods below a certain threshold\",\\n    \"destination_entity\": \"kubectl drain command\"\\n  },\\n  {\\n    \"source_entity\": \"Pod-DisruptionBudget\",\\n    \"description\": \"specifies the minimum number of pods that need to keep running while performing certain operations\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Pod-DisruptionBudget\",\\n    \"description\": \"contains a pod label selector and a number specifying the minimum number of pods that must always be available\",\\n    \"destination_entity\": \"metadata\"\\n  },\\n  {\\n    \"source_entity\": \"minAvailable field\",\\n    \"description\": \"specifies the minimum number of pods that need to keep running while performing certain operations\",\\n    \"destination_entity\": \"Pod-DisruptionBudget\"\\n  },\\n  {\\n    \"source_entity\": \"maxUnavailable\",\\n    \"description\": \"supports the creation of Pod-DisruptionBudget resources and specifies the maximum number of pods that can be unavailable\",\\n    \"destination_entity\": \"Pod-DisruptionBudget\"\\n  },\\n  {\\n    \"source_entity\": \"minAvailable\",\\n    \"description\": \"specifies the minimum number of pods that need to keep running while performing certain operations\",\\n    \"destination_entity\": \"Pod-DisruptionBudget\"\\n  },\\n  {\\n    \"source_entity\": \"percentage\",\\n    \"description\": \"can be used instead of an absolute number in the minAvailable field\",\\n    \"destination_entity\": \"minAvailable field\"\\n  },\\n  {\\n    \"source_entity\": \"kubia pod\",\\n    \"description\": \"must always have three instances running while performing certain operations\",\\n    \"destination_entity\": \"Pod-DisruptionBudget\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl create\",\\n    \"description\": \"creates a Pod-DisruptionBudget resource with a specified label selector and minimum number of pods\",\\n    \"destination_entity\": \"Pod-DisruptionBudget\"\\n  },\\n  {\\n    \"source_entity\": \"PDB YAML\",\\n    \"description\": \"specifies the details of a Pod-DisruptionBudget resource, including its metadata and spec\",\\n    \"destination_entity\": \"metadata\"\\n  }\\n]'},\n",
       " {'page': 488,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '456\\nCHAPTER 15\\nAutomatic scaling of pods and cluster nodes\\n For example, if there were four pods altogether and minAvailable was set to three\\nas in the example, the pod eviction process would evict pods one by one, waiting for\\nthe evicted pod to be replaced with a new one by the ReplicaSet controller, before\\nevicting another pod. \\n15.4\\nSummary\\nThis chapter has shown you how Kubernetes can scale not only your pods, but also\\nyour nodes. You’ve learned that\\n\\uf0a1Configuring the automatic horizontal scaling of pods is as easy as creating a\\nHorizontalPodAutoscaler object and pointing it to a Deployment, ReplicaSet,\\nor ReplicationController and specifying the target CPU utilization for the pods.\\n\\uf0a1Besides having the Horizontal Pod Autoscaler perform scaling operations based\\non the pods’ CPU utilization, you can also configure it to scale based on your\\nown application-provided custom metrics or metrics related to other objects\\ndeployed in the cluster.\\n\\uf0a1Vertical pod autoscaling isn’t possible yet.\\n\\uf0a1Even cluster nodes can be scaled automatically if your Kubernetes cluster runs\\non a supported cloud provider.\\n\\uf0a1You can run one-off processes in a pod and have the pod stopped and deleted\\nautomatically as soon you press CTRL+C by using kubectl run with the -it and\\n--rm options.\\nIn the next chapter, you’ll explore advanced scheduling features, such as how to keep\\ncertain pods away from certain nodes and how to schedule pods either close together\\nor apart.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'HorizontalPodAutoscaler',\n",
       "    'description': '',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'a Kubernetes concept for managing a set of identical replicas.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'a Kubernetes concept for maintaining a specified number of identical replicas.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'ReplicationController',\n",
       "    'description': '',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'Pods', 'description': '', 'category': 'container'},\n",
       "   {'entity': 'CPU utilization', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'Metrics', 'description': '', 'category': 'data,metrics'},\n",
       "   {'entity': 'kubectl run',\n",
       "    'description': 'a command for running one-off processes in a pod.',\n",
       "    'category': 'software,command'},\n",
       "   {'entity': 'Horizontal scaling',\n",
       "    'description': '',\n",
       "    'category': 'process,application'},\n",
       "   {'entity': 'Vertical scaling',\n",
       "    'description': '',\n",
       "    'category': 'process,application'},\n",
       "   {'entity': 'Cluster nodes',\n",
       "    'description': '',\n",
       "    'category': 'hardware,node'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"can scale not only your pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Horizontal Pod Autoscaler\", \"description\": \"perform scaling operations based on CPU utilization\", \"destination_entity\": \"CPU utilization\"},\\n  {\"source_entity\": \"Horizontal Pod Autoscaler\", \"description\": \"scale based on custom metrics\", \"destination_entity\": \"Metrics\"},\\n  {\"source_entity\": \"ReplicaSet controller\", \"description\": \"replace evicted pod with a new one\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Config\", \"description\": \"point Horizontal Pod Autoscaler to Deployment, ReplicaSet or ReplicationController\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"Config\", \"description\": \"point Horizontal Pod Autoscaler to Deployment, ReplicaSet or ReplicationController\", \"destination_entity\": \"ReplicaSet\"},\\n  {\"source_entity\": \"Config\", \"description\": \"point Horizontal Pod Autoscaler to Deployment, ReplicaSet or ReplicationController\", \"destination_entity\": \"ReplicationController\"},\\n  {\"source_entity\": \"kubectl run\", \"description\": \"run one-off processes in a pod and delete it automatically\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Horizontal scaling\", \"description\": \"scale not possible yet\", \"destination_entity\": \"Vertical scaling\"},\\n  {\"source_entity\": \"Cluster nodes\", \"description\": \"can be scaled automatically\", \"destination_entity\": \"Kubernetes\"},\\n  {\"source_entity\": \"HorizontalPodAutoscaler\", \"description\": \"configured to scale based on own metrics\", \"destination_entity\": \"Metrics\"},\\n  {\"source_entity\": \"ReplicaSet controller\", \"description\": \"replace evicted pod with a new one, before evicting another pod\", \"destination_entity\": \"Pods\"}\\n]\\n```'},\n",
       " {'page': 489,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '457\\nAdvanced scheduling\\nKubernetes allows you to affect where pods are scheduled. Initially, this was only\\ndone by specifying a node selector in the pod specification, but additional mech-\\nanisms were later added that expanded this functionality. They’re covered in this\\nchapter.\\n16.1\\nUsing taints and tolerations to repel pods from \\ncertain nodes\\nThe first two features related to advanced scheduling that we’ll explore here are\\nthe node taints and pods’ tolerations of those taints. They’re used for restricting\\nThis chapter covers\\n\\uf0a1Using node taints and pod tolerations to keep \\npods away from certain nodes\\n\\uf0a1Defining node affinity rules as an alternative to \\nnode selectors\\n\\uf0a1Co-locating pods using pod affinity \\n\\uf0a1Keeping pods away from each other using pod \\nanti-affinity\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'Lightweight and ephemeral containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'node selector',\n",
       "    'description': 'Mechanism to specify node for pod scheduling',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'taints',\n",
       "    'description': 'Feature to restrict pods from certain nodes',\n",
       "    'category': 'feature'},\n",
       "   {'entity': 'tolerations',\n",
       "    'description': \"Pod's ability to tolerate taints on a node\",\n",
       "    'category': 'feature'},\n",
       "   {'entity': 'node affinity rules',\n",
       "    'description': 'Alternative to node selectors for pod scheduling',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pod affinity',\n",
       "    'description': 'Mechanism to co-locate pods',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pod anti-affinity',\n",
       "    'description': 'Mechanism to keep pods away from each other',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"allows you to affect where pods are scheduled\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"node selector\",\\n    \"description\": \"was initially used for specifying a node selector in the pod specification\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"expanded functionality through additional mechanisms\",\\n    \"destination_entity\": \"taints and tolerations\"\\n  },\\n  {\\n    \"source_entity\": \"node taints\",\\n    \"description\": \"are used for restricting pods from certain nodes\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"pod tolerations\",\\n    \"description\": \"tolerate node taints and keep pods away from certain nodes\",\\n    \"destination_entity\": \"node taints\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"covers using node affinity rules as an alternative to node selectors\",\\n    \"destination_entity\": \"node affinity rules\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"covers co-locating pods using pod affinity\",\\n    \"destination_entity\": \"pod affinity\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"covers keeping pods away from each other using pod anti-affinity\",\\n    \"destination_entity\": \"pod anti-affinity\"\\n  },\\n  {\\n    \"source_entity\": \"taints and tolerations\",\\n    \"description\": \"are the first two features related to advanced scheduling\",\\n    \"destination_entity\": \"advanced scheduling\"\\n  },\\n  {\\n    \"source_entity\": \"node selector\",\\n    \"description\": \"is an alternative to node affinity rules\",\\n    \"destination_entity\": \"node affinity rules\"\\n  },\\n  {\\n    \"source_entity\": \"pod tolerations\",\\n    \"description\": \"tolerate node taints and keep pods away from certain nodes\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"node taints\",\\n    \"description\": \"keep pods away from certain nodes\",\\n    \"destination_entity\": \"pods\"\\n  }\\n]\\n```'},\n",
       " {'page': 490,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '458\\nCHAPTER 16\\nAdvanced scheduling\\nwhich pods can use a certain node. A pod can only be scheduled to a node if it toler-\\nates the node’s taints.\\n This is somewhat different from using node selectors and node affinity, which\\nyou’ll learn about later in this chapter. Node selectors and node affinity rules make\\nit possible to select which nodes a pod can or can’t be scheduled to by specifically\\nadding that information to the pod, whereas taints allow rejecting deployment of\\npods to certain nodes by only adding taints to the node without having to modify\\nexisting pods. Pods that you want deployed on a tainted node need to opt in to use\\nthe node, whereas with node selectors, pods explicitly specify which node(s) they\\nwant to be deployed to.\\n16.1.1 Introducing taints and tolerations\\nThe best path to learn about node taints is to see an existing taint. Appendix B shows\\nhow to set up a multi-node cluster with the kubeadm tool. By default, the master node\\nin such a cluster is tainted, so only Control Plane pods can be deployed on it. \\nDISPLAYING A NODE’S TAINTS\\nYou can see the node’s taints using kubectl describe node, as shown in the follow-\\ning listing.\\n$ kubectl describe node master.k8s\\nName:         master.k8s\\nRole:\\nLabels:       beta.kubernetes.io/arch=amd64\\n              beta.kubernetes.io/os=linux\\n              kubernetes.io/hostname=master.k8s\\n              node-role.kubernetes.io/master=\\nAnnotations:  node.alpha.kubernetes.io/ttl=0\\n              volumes.kubernetes.io/controller-managed-attach-detach=true\\nTaints:       node-role.kubernetes.io/master:NoSchedule      \\n...\\nThe master node has a single taint. Taints have a key, value, and an effect, and are repre-\\nsented as <key>=<value>:<effect>. The master node’s taint shown in the previous\\nlisting has the key node-role.kubernetes.io/master, a null value (not shown in the\\ntaint), and the effect of NoSchedule. \\n This taint prevents pods from being scheduled to the master node, unless those pods\\ntolerate this taint. The pods that tolerate it are usually system pods (see figure 16.1).\\n \\n \\n \\n \\nListing 16.1\\nDescribing the master node in a cluster created with kubeadm\\nThe master node \\nhas one taint.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubernetes',\n",
       "    'description': 'a container orchestration system for automating the deployment, scaling, and management of containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'the basic execution unit in a Kubernetes cluster',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'a physical or virtual machine that is part of a Kubernetes cluster',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'taints',\n",
       "    'description': 'a way to mark a node as undesirable for scheduling pods, with the option for pods to tolerate them',\n",
       "    'category': 'node configuration'},\n",
       "   {'entity': 'tolerations',\n",
       "    'description': 'a way for pods to opt in and use a tainted node',\n",
       "    'category': 'pod configuration'},\n",
       "   {'entity': 'kubeadm',\n",
       "    'description': 'a tool for setting up a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the command-line interface to interact with a Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'describe node',\n",
       "    'description': 'a command to display information about a specific node in the cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'node selectors',\n",
       "    'description': 'a way to select which nodes a pod can or cannot be scheduled to, by adding that information to the pod',\n",
       "    'category': 'node configuration'},\n",
       "   {'entity': 'node affinity',\n",
       "    'description': 'a way to select which nodes a pod can or cannot be scheduled to, with more fine-grained control than node selectors',\n",
       "    'category': 'node configuration'},\n",
       "   {'entity': 'Control Plane pods',\n",
       "    'description': 'specialized system pods that manage the Kubernetes cluster',\n",
       "    'category': 'pod type'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"node\",\\n    \"description\": \"has a taint that prevents pods from being scheduled unless they tolerate it.\",\\n    \"destination_entity\": \"Control Plane pods\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"can be used to describe a node and see its taints.\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"kubeadm\",\\n    \"description\": \"is a tool that can be used to set up a multi-node cluster with a tainted master node.\",\\n    \"destination_entity\": \"master node\"\\n  },\\n  {\\n    \"source_entity\": \"node selectors\",\\n    \"description\": \"allow selecting which nodes a pod can or can\\'t be scheduled to by adding that information to the pod.\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"tolerations\",\\n    \"description\": \"are used by pods to opt in to using a tainted node.\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl describe node\",\\n    \"description\": \"can be used to see the taints of a node.\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"Control Plane pods\",\\n    \"description\": \"are typically system pods that tolerate the master node\\'s taint.\",\\n    \"destination_entity\": \"master node\"\\n  },\\n  {\\n    \"source_entity\": \"node affinity\",\\n    \"description\": \"makes it possible to select which nodes a pod can or can\\'t be scheduled to by adding that information to the pod.\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"taints\",\\n    \"description\": \"are used to reject deployment of pods to certain nodes by adding taints to the node without modifying existing pods.\",\\n    \"destination_entity\": \"node\"\\n  }\\n]\\n```'},\n",
       " {'page': 491,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '459\\nUsing taints and tolerations to repel pods from certain nodes\\nDISPLAYING A POD’S TOLERATIONS\\nIn a cluster installed with kubeadm, the kube-proxy cluster component runs as a pod\\non every node, including the master node, because master components that run as\\npods may also need to access Kubernetes Services. To make sure the kube-proxy pod\\nalso runs on the master node, it includes the appropriate toleration. In total, the pod\\nhas three tolerations, which are shown in the following listing.\\n$ kubectl describe po kube-proxy-80wqm -n kube-system\\n...\\nTolerations:    node-role.kubernetes.io/master=:NoSchedule\\n                node.alpha.kubernetes.io/notReady=:Exists:NoExecute\\n                node.alpha.kubernetes.io/unreachable=:Exists:NoExecute\\n...\\nAs you can see, the first toleration matches the master node’s taint, allowing this kube-\\nproxy pod to be scheduled to the master node. \\nNOTE\\nDisregard the equal sign, which is shown in the pod’s tolerations, but\\nnot in the node’s taints. Kubectl apparently displays taints and tolerations dif-\\nferently when the taint’s/toleration’s value is null.\\nUNDERSTANDING TAINT EFFECTS\\nThe two other tolerations on the kube-proxy pod define how long the pod is allowed\\nto run on nodes that aren’t ready or are unreachable (the time in seconds isn’t shown,\\nListing 16.2\\nA pod’s tolerations\\nSystem pod may be\\nscheduled to master\\nnode because its\\ntoleration matches\\nthe node’s taint.\\nSystem pod\\nMaster node\\nTaint:\\nnode-role.kubernetes.io\\n/master:NoSchedule\\nToleration:\\nnode-role.kubernetes.io\\n/master:NoSchedule\\nRegular pod\\nRegular node\\nNo taints\\nNo tolerations\\nPods with no tolerations\\nmay only be scheduled\\nto nodes without taints.\\nFigure 16.1\\nA pod is only scheduled to a node if it tolerates the node’s taints.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'taint',\n",
       "    'description': 'a mark attached to a node to indicate that it should not be used for scheduling pods',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'toleration',\n",
       "    'description': 'a condition that allows a pod to be scheduled on a node despite its taint',\n",
       "    'category': 'kubernetes'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a container that can contain one or more application containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'a machine in the cluster where pods are run',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'kube-proxy',\n",
       "    'description': 'the Kubernetes service proxy, which runs as a pod on each node to provide network connectivity between services',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubeadm',\n",
       "    'description': 'a tool for setting up and configuring a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the command-line tool for interacting with the Kubernetes API server',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kube-system',\n",
       "    'description': 'a namespace in which system-level components, such as the kube-proxy pod, run',\n",
       "    'category': 'kubernetes'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"kube-proxy pod\",\\n    \"description\": \"has three tolerations to run on master node\",\\n    \"destination_entity\": \"master node\"\\n  },\\n  {\\n    \"source_entity\": \"kube-proxy pod\",\\n    \"description\": \"includes appropriate toleration to run on master node\",\\n    \"destination_entity\": \"kube-system\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"displays taints and tolerations differently\",\\n    \"destination_entity\": \"kubeadm\"\\n  },\\n  {\\n    \"source_entity\": \"kube-proxy pod\",\\n    \"description\": \"matches master node\\'s taint with toleration\",\\n    \"destination_entity\": \"master node\"\\n  },\\n  {\\n    \"source_entity\": \"tolerations\",\\n    \"description\": \"define how long the pod is allowed to run on nodes that aren\\'t ready or are unreachable\",\\n    \"destination_entity\": \"node\"\\n  },\\n  {\\n    \"source_entity\": \"kube-proxy pod\",\\n    \"description\": \"scheduled to master node because its toleration matches the node\\'s taint\",\\n    \"destination_entity\": \"master node\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"displays equal sign in pod\\'s tolerations but not in node\\'s taints\",\\n    \"destination_entity\": \"kubeadm\"\\n  },\\n  {\\n    \"source_entity\": \"regular pod\",\\n    \"description\": \"may only be scheduled to nodes without taints\",\\n    \"destination_entity\": \"node\"\\n  }\\n]'},\n",
       " {'page': 492,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '460\\nCHAPTER 16\\nAdvanced scheduling\\nbut can be seen in the pod’s YAML). Those two tolerations refer to the NoExecute\\ninstead of the NoSchedule effect. \\n Each taint has an effect associated with it. Three possible effects exist:\\n\\uf0a1\\nNoSchedule, which means pods won’t be scheduled to the node if they don’t tol-\\nerate the taint.\\n\\uf0a1\\nPreferNoSchedule is a soft version of NoSchedule, meaning the scheduler will\\ntry to avoid scheduling the pod to the node, but will schedule it to the node if it\\ncan’t schedule it somewhere else. \\n\\uf0a1\\nNoExecute, unlike NoSchedule and PreferNoSchedule that only affect schedul-\\ning, also affects pods already running on the node. If you add a NoExecute taint\\nto a node, pods that are already running on that node and don’t tolerate the\\nNoExecute taint will be evicted from the node. \\n16.1.2 Adding custom taints to a node\\nImagine having a single Kubernetes cluster where you run both production and non-\\nproduction workloads. It’s of the utmost importance that non-production pods never\\nrun on the production nodes. This can be achieved by adding a taint to your produc-\\ntion nodes. To add a taint, you use the kubectl taint command:\\n$ kubectl taint node node1.k8s node-type=production:NoSchedule\\nnode \"node1.k8s\" tainted\\nThis adds a taint with key node-type, value production and the NoSchedule effect. If\\nyou now deploy multiple replicas of a regular pod, you’ll see none of them are sched-\\nuled to the node you tainted, as shown in the following listing.\\n$ kubectl run test --image busybox --replicas 5 -- sleep 99999\\ndeployment \"test\" created\\n$ kubectl get po -o wide\\nNAME                READY  STATUS    RESTARTS   AGE   IP          NODE\\ntest-196686-46ngl   1/1    Running   0          12s   10.47.0.1   node2.k8s\\ntest-196686-73p89   1/1    Running   0          12s   10.47.0.7   node2.k8s\\ntest-196686-77280   1/1    Running   0          12s   10.47.0.6   node2.k8s\\ntest-196686-h9m8f   1/1    Running   0          12s   10.47.0.5   node2.k8s\\ntest-196686-p85ll   1/1    Running   0          12s   10.47.0.4   node2.k8s\\nNow, no one can inadvertently deploy pods onto the production nodes. \\n16.1.3 Adding tolerations to pods\\nTo deploy production pods to the production nodes, they need to tolerate the taint\\nyou added to the nodes. The manifests of your production pods need to include the\\nYAML snippet shown in the following listing.\\n \\nListing 16.3\\nDeploying pods without a toleration\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command for interacting with Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'taint',\n",
       "    'description': 'a mark on a node that affects scheduling and eviction of pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'node-type',\n",
       "    'description': 'key in the taint command to specify the type of nodes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'production',\n",
       "    'description': 'value associated with the node-type key, indicating production nodes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'NoSchedule',\n",
       "    'description': 'effect of a taint that prevents pods from being scheduled on tainted nodes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'PreferNoSchedule',\n",
       "    'description': 'soft version of NoSchedule effect, allowing scheduler to schedule pod elsewhere if possible',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'NoExecute',\n",
       "    'description': 'effect of a taint that evicts running pods without toleration from the node',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl taint',\n",
       "    'description': 'command for adding custom taints to nodes in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'node1.k8s',\n",
       "    'description': 'name of a production node',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'yaml',\n",
       "    'description': 'file format for defining configuration and metadata in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'lightweight and portable container in Kubernetes that can be scheduled on nodes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'replica',\n",
       "    'description': 'multiple instances of the same pod running simultaneously',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'template used to create a new container, in this case busybox for a test pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'busybox',\n",
       "    'description': 'lightweight and minimalist Linux distribution image used as a test container',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'deployment',\n",
       "    'description': \"mechanism for managing multiple replicas of the same pod, creating a deployment resource called 'test'\",\n",
       "    'category': 'application'},\n",
       "   {'entity': 'get po -o wide',\n",
       "    'description': 'command to retrieve information about pods in Kubernetes, including their status and node assignments',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'node2.k8s',\n",
       "    'description': 'name of another production node',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'taint command',\n",
       "    'description': 'command for adding or removing taints from nodes in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'tolerations',\n",
       "    'description': 'key-value pairs that allow pods to coexist with tainted nodes, requiring them to tolerate the specific effects and keys',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"add a taint to a node with key node-type, value production and the NoSchedule effect.\", \"destination_entity\": \"node1.k8s\"},\\n  {\"source_entity\": \"pod\", \"description\": \"won\\'t be scheduled to the node if they don\\'t tolerate the taint.\", \"destination_entity\": \"NoSchedule\"},\\n  {\"source_entity\": \"kubectl taint command\", \"description\": \"adds a taint with key node-type, value production and the NoSchedule effect.\", \"destination_entity\": \"node1.k8s\"},\\n  {\"source_entity\": \"busybox\", \"description\": \"can\\'t schedule pods to run on the tainted node.\", \"destination_entity\": \"NoSchedule\"},\\n  {\"source_entity\": \"kubectl taint command\", \"description\": \"adds a taint with key node-type, value production and the NoSchedule effect.\", \"destination_entity\": \"node1.k8s\"},\\n  {\"source_entity\": \"pod\", \"description\": \"won\\'t be scheduled to the node if they don\\'t tolerate the taint.\", \"destination_entity\": \"NoSchedule\"},\\n  {\"source_entity\": \"kubectl run test command\", \"description\": \"deploys multiple replicas of a regular pod.\", \"destination_entity\": \"test deployment\"},\\n  {\"source_entity\": \"test deployment\", \"description\": \"schedules pods to node2.k8s instead of tainted node1.k8s.\", \"destination_entity\": \"node2.k8s\"},\\n  {\"source_entity\": \"kubectl get po -o wide command\", \"description\": \"displays the list of running pods on node2.k8s.\", \"destination_entity\": \"node2.k8s\"},\\n  {\"source_entity\": \"taint command\", \"description\": \"adds a taint with key NoSchedule effect to a node.\", \"destination_entity\": \"NoSchedule\"},\\n  {\"source_entity\": \"kubectl get po -o wide command\", \"description\": \"displays the list of running pods on node2.k8s without any tolerations.\", \"destination_entity\": \"node2.k8s\"},\\n  {\"source_entity\": \"production nodes\", \"description\": \"have a taint with key NoSchedule effect added to them.\", \"destination_entity\": \"NoSchedule\"},\\n  {\"source_entity\": \"taint command\", \"description\": \"adds a taint with key PreferNoSchedule effect to a node.\", \"destination_entity\": \"PreferNoSchedule\"},\\n  {\"source_entity\": \"kubectl get po -o wide command\", \"description\": \"displays the list of running pods on node2.k8s without any tolerations.\", \"destination_entity\": \"node2.k8s\"},\\n  {\"source_entity\": \"taint command\", \"description\": \"adds a taint with key NoExecute effect to a node.\", \"destination_entity\": \"NoExecute\"}\\n]\\n```\\n\\nNote that I\\'ve extracted relations between various entities such as commands, nodes, pods, and effects.'},\n",
       " {'page': 493,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '461\\nUsing taints and tolerations to repel pods from certain nodes\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: prod\\nspec:\\n  replicas: 5\\n  template:\\n    spec:\\n      ...\\n      tolerations:\\n      - key: node-type         \\n        Operator: Equal        \\n        value: production      \\n        effect: NoSchedule     \\nIf you deploy this Deployment, you’ll see its pods get deployed to the production\\nnode, as shown in the next listing.\\n$ kubectl get po -o wide\\nNAME                READY  STATUS    RESTARTS   AGE   IP          NODE\\nprod-350605-1ph5h   0/1    Running   0          16s   10.44.0.3   node1.k8s\\nprod-350605-ctqcr   1/1    Running   0          16s   10.47.0.4   node2.k8s\\nprod-350605-f7pcc   0/1    Running   0          17s   10.44.0.6   node1.k8s\\nprod-350605-k7c8g   1/1    Running   0          17s   10.47.0.9   node2.k8s\\nprod-350605-rp1nv   0/1    Running   0          17s   10.44.0.4   node1.k8s\\nAs you can see in the listing, production pods were also deployed to node2, which isn’t\\na production node. To prevent that from happening, you’d also need to taint the non-\\nproduction nodes with a taint such as node-type=non-production:NoSchedule. Then\\nyou’d also need to add the matching toleration to all your non-production pods.\\n16.1.4 Understanding what taints and tolerations can be used for\\nNodes can have more than one taint and pods can have more than one toleration. As\\nyou’ve seen, taints can only have a key and an effect and don’t require a value. Tolera-\\ntions can tolerate a specific value by specifying the Equal operator (that’s also the\\ndefault operator if you don’t specify one), or they can tolerate any value for a specific\\ntaint key if you use the Exists operator.\\nUSING TAINTS AND TOLERATIONS DURING SCHEDULING\\nTaints can be used to prevent scheduling of new pods (NoSchedule effect) and to\\ndefine unpreferred nodes (PreferNoSchedule effect) and even evict existing pods\\nfrom a node (NoExecute).\\n You can set up taints and tolerations any way you see fit. For example, you could\\npartition your cluster into multiple partitions, allowing your development teams to\\nschedule pods only to their respective nodes. You can also use taints and tolerations\\nListing 16.4\\nA production Deployment with a toleration: production-deployment.yaml\\nListing 16.5\\nPods with the toleration are deployed on production node1\\nThis toleration allows the \\npod to be scheduled to \\nproduction nodes.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'taint',\n",
       "    'description': 'a key-value pair attached to a node that can be used to prevent scheduling of new pods or evict existing pods',\n",
       "    'category': 'node'},\n",
       "   {'entity': 'toleration',\n",
       "    'description': 'a configuration in a pod that allows it to be scheduled on nodes with specific taints',\n",
       "    'category': 'pod'},\n",
       "   {'entity': 'node-type',\n",
       "    'description': 'a key used in taints and tolerations to specify the type of node',\n",
       "    'category': 'node'},\n",
       "   {'entity': 'production node',\n",
       "    'description': 'a node labeled as production',\n",
       "    'category': 'node'},\n",
       "   {'entity': 'non-production node',\n",
       "    'description': 'a node labeled as non-production',\n",
       "    'category': 'node'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'a Kubernetes object that represents an application and manages its replicas',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'replicas',\n",
       "    'description': 'the number of copies of a deployment to be run simultaneously',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl get po -o wide',\n",
       "    'description': 'a command used to view information about pods in the cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'a running instance of an application in a Kubernetes cluster',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'node1.k8s',\n",
       "    'description': 'a node with name node1.k8s',\n",
       "    'category': 'node'},\n",
       "   {'entity': 'node2.k8s',\n",
       "    'description': 'a node with name node2.k8s',\n",
       "    'category': 'node'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Taint\", \"description\": \"prevent scheduling of new pods on a node\", \"destination_entity\": \"Non-production Node\"},\\n  {\"source_entity\": \"Tolerations\", \"description\": \"allow pods to be scheduled on production nodes\", \"destination_entity\": \"Production Pod\"},\\n  {\"source_entity\": \"kubectl get po -o wide\", \"description\": \"display information about running pods\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Node2.k8s\", \"description\": \"run multiple replicas of a Deployment\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"Taint\", \"description\": \"define unpreferred nodes for scheduling\", \"destination_entity\": \"Non-production Node\"},\\n  {\"source_entity\": \"NoSchedule effect\", \"description\": \"prevent new pods from being scheduled on a node\", \"destination_entity\": \"Node\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"create replicas of a pod\", \"destination_entity\": \"Replicas\"},\\n  {\"source_entity\": \"Tolerations\", \"description\": \"tolerate specific taints and allow scheduling\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"kubectl get po -o wide\", \"description\": \"display information about running pods on a non-production node\", \"destination_entity\": \"Non-production Node\"},\\n  {\"source_entity\": \"Node1.k8s\", \"description\": \"run multiple replicas of a Deployment\", \"destination_entity\": \"Deployment\"},\\n  {\"source_entity\": \"Taint\", \"description\": \"evict existing pods from a node with NoExecute effect\", \"destination_entity\": \"Node\"},\\n  {\"source_entity\": \"Replicas\", \"description\": \"specify the number of replicas to create for a Deployment\", \"destination_entity\": \"Deployment\"}\\n]\\n```\\n\\nNote: I\\'ve only considered the entities provided in the list and ignored any other text or information not related to those entities.'},\n",
       " {'page': 494,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '462\\nCHAPTER 16\\nAdvanced scheduling\\nwhen several of your nodes provide special hardware and only part of your pods need\\nto use it.\\nCONFIGURING HOW LONG AFTER A NODE FAILURE A POD IS RESCHEDULED\\nYou can also use a toleration to specify how long Kubernetes should wait before\\nrescheduling a pod to another node if the node the pod is running on becomes\\nunready or unreachable. If you look at the tolerations of one of your pods, you’ll see\\ntwo tolerations, which are shown in the following listing.\\n$ kubectl get po prod-350605-1ph5h -o yaml\\n...\\n  tolerations:\\n  - effect: NoExecute                            \\n    key: node.alpha.kubernetes.io/notReady       \\n    operator: Exists                             \\n    tolerationSeconds: 300                       \\n  - effect: NoExecute                              \\n    key: node.alpha.kubernetes.io/unreachable      \\n    operator: Exists                               \\n    tolerationSeconds: 300                         \\nThese two tolerations say that this pod tolerates a node being notReady or unreach-\\nable for 300 seconds. The Kubernetes Control Plane, when it detects that a node is no\\nlonger ready or no longer reachable, will wait for 300 seconds before it deletes the\\npod and reschedules it to another node.\\n These two tolerations are automatically added to pods that don’t define them. If\\nthat five-minute delay is too long for your pods, you can make the delay shorter by\\nadding those two tolerations to the pod’s spec.\\nNOTE\\nThis is currently an alpha feature, so it may change in future versions\\nof Kubernetes. Taint-based evictions also aren’t enabled by default. You enable\\nthem by running the Controller Manager with the --feature-gates=Taint-\\nBasedEvictions=true option.\\n16.2\\nUsing node affinity to attract pods to certain nodes\\nAs you’ve learned, taints are used to keep pods away from certain nodes. Now you’ll\\nlearn about a newer mechanism called node affinity, which allows you to tell Kuberne-\\ntes to schedule pods only to specific subsets of nodes.\\nCOMPARING NODE AFFINITY TO NODE SELECTORS\\nThe initial node affinity mechanism in early versions of Kubernetes was the node-\\nSelector field in the pod specification. The node had to include all the labels speci-\\nfied in that field to be eligible to become the target for the pod. \\n Node selectors get the job done and are simple, but they don’t offer everything\\nthat you may need. Because of that, a more powerful mechanism was introduced.\\nListing 16.6\\nPod with default tolerations\\nThe pod tolerates the node being \\nnotReady for 300 seconds, before \\nit needs to be rescheduled.\\nThe same applies to the \\nnode being unreachable.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Tolerations',\n",
       "    'description': 'Specification of how long Kubernetes should wait before rescheduling a pod to another node if the node the pod is running on becomes unready or unreachable',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'A physical or virtual machine in a cluster that provides resources for pods',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A logical entity that represents an application running on the cluster',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'Component of Kubernetes responsible for managing nodes and handling evictions',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Taint-BasedEvictions',\n",
       "    'description': 'Mechanism to enable taint-based evictions in Kubernetes',\n",
       "    'category': 'feature'},\n",
       "   {'entity': 'Node Affinity',\n",
       "    'description': 'Mechanism to tell Kubernetes to schedule pods only to specific subsets of nodes',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Node Selectors',\n",
       "    'description': 'Initial mechanism in early versions of Kubernetes to schedule pods to specific nodes based on labels',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Labels',\n",
       "    'description': 'Key-value pairs attached to a node or pod that can be used for scheduling and selection',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'Taints',\n",
       "    'description': 'Mechanism to mark nodes as unavailability for pods based on specific conditions',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[{\"source_entity\": \"Kubernetes\", \"description\": \"allows to tell Kubernetes to schedule pods only to specific subsets of nodes\", \"destination_entity\": \"Node Affinity\"},\\n\\n {\"source_entity\": \"Node\", \"description\": \"has to include all the labels specified in the node-Selector field to be eligible to become the target for the pod\", \"destination_entity\": \"Node Selectors\"},\\n\\n {\"source_entity\": \"Kubernetes\", \"description\": \"will wait for 300 seconds before it deletes the pod and reschedules it to another node if a node is no longer ready or unreachable\", \"destination_entity\": \"Pod\"},\\n\\n {\"source_entity\": \"Controller Manager\", \"description\": \"needs to be run with the --feature-gates=Taint-BasedEvictions=true option to enable Taint-based evictions\", \"destination_entity\": \"Taint-BasedEvictions\"},\\n\\n {\"source_entity\": \"Kubernetes\", \"description\": \"adds two tolerations automatically to pods that don’t define them\", \"destination_entity\": \"Pod\"},\\n\\n {\"source_entity\": \"Kubernetes\", \"description\": \"tolerates a node being notReady or unreachable for 300 seconds\", \"destination_entity\": \"Tolerations\"},\\n\\n {\"source_entity\": \"Node\", \"description\": \"can be marked with taints to keep pods away from certain nodes\", \"destination_entity\": \"Taints\"},\\n\\n {\"source_entity\": \"Kubernetes\", \"description\": \"uses node affinity mechanism to schedule pods only to specific subsets of nodes\", \"destination_entity\": \"Node Affinity\"},\\n\\n {\"source_entity\": \"Controller Manager\", \"description\": \"manages the scheduling of pods based on tolerations and taints\", \"destination_entity\": \"Pod\"}]\\n\\nNote: These relations are extracted from the document page provided, but may not be an exhaustive list.'},\n",
       " {'page': 495,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '463\\nUsing node affinity to attract pods to certain nodes\\nNode selectors will eventually be deprecated, so it’s important you understand the\\nnew node affinity rules.\\n Similar to node selectors, each pod can define its own node affinity rules. These\\nallow you to specify either hard requirements or preferences. By specifying a prefer-\\nence, you tell Kubernetes which nodes you prefer for a specific pod, and Kubernetes\\nwill try to schedule the pod to one of those nodes. If that’s not possible, it will choose\\none of the other nodes. \\nEXAMINING THE DEFAULT NODE LABELS\\nNode affinity selects nodes based on their labels, the same way node selectors do.\\nBefore you see how to use node affinity, let’s examine the labels of one of the nodes in\\na Google Kubernetes Engine cluster (GKE) to see what the default node labels are.\\nThey’re shown in the following listing.\\n$ kubectl describe node gke-kubia-default-pool-db274c5a-mjnf\\nName:     gke-kubia-default-pool-db274c5a-mjnf\\nRole:\\nLabels:   beta.kubernetes.io/arch=amd64\\n          beta.kubernetes.io/fluentd-ds-ready=true\\n          beta.kubernetes.io/instance-type=f1-micro\\n          beta.kubernetes.io/os=linux\\n          cloud.google.com/gke-nodepool=default-pool\\n          failure-domain.beta.kubernetes.io/region=europe-west1         \\n          failure-domain.beta.kubernetes.io/zone=europe-west1-d         \\n          kubernetes.io/hostname=gke-kubia-default-pool-db274c5a-mjnf   \\nThe node has many labels, but the last three are the most important when it comes to\\nnode affinity and pod affinity, which you’ll learn about later. The meaning of those\\nthree labels is as follows:\\n\\uf0a1\\nfailure-domain.beta.kubernetes.io/region specifies the geographical region\\nthe node is located in.\\n\\uf0a1\\nfailure-domain.beta.kubernetes.io/zone specifies the availability zone the\\nnode is in.\\n\\uf0a1\\nkubernetes.io/hostname is obviously the node’s hostname.\\nThese and other labels can be used in pod affinity rules. In chapter 3, you already\\nlearned how you can add a custom label to nodes and use it in a pod’s node selector.\\nYou used the custom label to deploy pods only to nodes with that label by adding a node\\nselector to the pods. Now, you’ll see how to do the same using node affinity rules.\\n16.2.1 Specifying hard node affinity rules\\nIn the example in chapter 3, you used the node selector to deploy a pod that requires\\na GPU only to nodes that have a GPU. The pod spec included the nodeSelector field\\nshown in the following listing.\\nListing 16.7\\nDefault labels of a node in GKE\\nThese three\\nlabels are the\\nmost important\\nones related to\\nnode affinity.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Node Affinity',\n",
       "    'description': 'A feature that allows you to specify hard requirements or preferences for a pod to be scheduled on a specific node.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Node Selectors',\n",
       "    'description': 'A feature that will eventually be deprecated, but currently used to deploy pods only to nodes with certain labels.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Entities that can define their own node affinity rules and specify preferences for a specific node.',\n",
       "    'category': 'Container'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'GKE (Google Kubernetes Engine)',\n",
       "    'description': 'A managed environment for running containerized applications.',\n",
       "    'category': 'Cloud Service'},\n",
       "   {'entity': 'Labels',\n",
       "    'description': 'Key-value pairs that can be used to select nodes or pods based on their characteristics.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': '$ kubectl describe node',\n",
       "    'description': 'A command used to examine the details of a node in a Kubernetes cluster.',\n",
       "    'category': 'Command'},\n",
       "   {'entity': 'Node Labels',\n",
       "    'description': 'Labels that are automatically added to nodes by Kubernetes, such as beta.kubernetes.io/arch and cloud.google.com/gke-nodepool.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'failure-domain.beta.kubernetes.io/region',\n",
       "    'description': 'A label that specifies the geographical region where a node is located.',\n",
       "    'category': 'Label'},\n",
       "   {'entity': 'failure-domain.beta.kubernetes.io/zone',\n",
       "    'description': 'A label that specifies the availability zone where a node is located.',\n",
       "    'category': 'Label'},\n",
       "   {'entity': 'kubernetes.io/hostname',\n",
       "    'description': 'A label that specifies the hostname of a node.',\n",
       "    'category': 'Label'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"selects nodes based on their labels\", \"destination_entity\": \"Node Affinity\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"will try to schedule the pod to one of the preferred nodes\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"choose one of the other nodes if not possible\", \"destination_entity\": \"Nodes\"},\\n  {\"source_entity\": \"Node Affinity\", \"description\": \"selects nodes based on their labels\", \"destination_entity\": \"Labels\"},\\n  {\"source_entity\": \"GKE (Google Kubernetes Engine)\", \"description\": \"has default node labels\", \"destination_entity\": \"Node Labels\"},\\n  {\"source_entity\": \"$ kubectl describe node\", \"description\": \"shows the default node labels\", \"destination_entity\": \"Node Labels\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"prefer to schedule pods on certain nodes\", \"destination_entity\": \"Nodes\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"can define their own node affinity rules\", \"destination_entity\": \"Node Affinity\"},\\n  {\"source_entity\": \"Node Selectors\", \"description\": \"are being deprecated by Node Affinity\", \"destination_entity\": \"Kubernetes\"}\\n]\\n```\\n\\nThese relations were extracted based on the actions described in the document page, such as:\\n\\n* Kubernetes selecting nodes based on their labels (relation between Kubernetes and Node Affinity)\\n* Kubernetes trying to schedule pods to preferred nodes (relation between Kubernetes and Pods)\\n* Kubernetes choosing other nodes if not possible (relation between Kubernetes and Nodes)\\n* Node Affinity selecting nodes based on labels (relation between Node Affinity and Labels)\\n* GKE having default node labels (relation between GKE and Node Labels)\\n* `$ kubectl describe node` showing default node labels (relation between `$ kubectl describe node` and Node Labels)\\n* Kubernetes preferring to schedule pods on certain nodes (relation between Kubernetes and Nodes)\\n* Pods defining their own node affinity rules (relation between Pods and Node Affinity)\\n* Node Selectors being deprecated by Node Affinity (relation between Node Selectors and Kubernetes)'},\n",
       " {'page': 496,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '464\\nCHAPTER 16\\nAdvanced scheduling\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: kubia-gpu\\nspec:\\n  nodeSelector:          \\n    gpu: \"true\"          \\n  ...\\nThe nodeSelector field specifies that the pod should only be deployed on nodes that\\ninclude the gpu=true label. If you replace the node selector with a node affinity rule,\\nthe pod definition will look like the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: kubia-gpu\\nspec:\\n  affinity:\\n    nodeAffinity:\\n      requiredDuringSchedulingIgnoredDuringExecution:\\n        nodeSelectorTerms:\\n        - matchExpressions:\\n          - key: gpu\\n            operator: In\\n            values:\\n            - \"true\"\\nThe first thing you’ll notice is that this is much more complicated than a simple node\\nselector. But that’s because it’s much more expressive. Let’s examine the rule in detail. \\nMAKING SENSE OF THE LONG NODEAFFINITY ATTRIBUTE NAME\\nAs you can see, the pod’s spec section contains an affinity field that contains a node-\\nAffinity field, which contains a field with an extremely long name, so let’s focus on\\nthat first.\\n Let’s break it down into two parts and examine what they mean:\\n\\uf0a1\\nrequiredDuringScheduling... means the rules defined under this field spec-\\nify the labels the node must have for the pod to be scheduled to the node.\\n\\uf0a1\\n...IgnoredDuringExecution means the rules defined under the field don’t\\naffect pods already executing on the node. \\nAt this point, let me make things easier for you by letting you know that affinity cur-\\nrently only affects pod scheduling and never causes a pod to be evicted from a node.\\nThat’s why all the rules right now always end with IgnoredDuringExecution. Eventu-\\nally, Kubernetes will also support RequiredDuringExecution, which means that if you\\nListing 16.8\\nA pod using a node selector: kubia-gpu-nodeselector.yaml\\nListing 16.9\\nA pod using a nodeAffinity rule: kubia-gpu-nodeaffinity.yaml\\nThis pod is only scheduled \\nto nodes that have the \\ngpu=true label.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'nodeSelector',\n",
       "    'description': 'field in pod spec to select nodes with gpu=true label',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'nodeAffinity',\n",
       "    'description': 'field in pod spec for node affinity rule',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'lightweight and ephemeral worker running one or more containers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'GPU',\n",
       "    'description': 'Graphics Processing Unit label for node selection',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'label',\n",
       "    'description': 'key-value pair to identify nodes with gpu=true',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'affinity',\n",
       "    'description': 'field in pod spec for node affinity rule',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'requiredDuringSchedulingIgnoredDuringExecution',\n",
       "    'description': 'nodeAffinity field to specify labels for scheduling',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'IgnoredDuringExecution',\n",
       "    'description': 'nodeAffinity field to ignore rules during execution',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'scheduler',\n",
       "    'description': 'component responsible for scheduling pods on nodes',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"Kubernetes\", \"description\": \"specifies labels node must have for pod to be scheduled\", \"destination_entity\": \"Pod\"}, \\n{\"source_entity\": \"requiredDuringSchedulingIgnoredDuringExecution\", \"description\": \"defines rules that specify labels node must have for pod to be scheduled\", \"destination_entity\": \"Pod\"}, \\n{\"source_entity\": \"scheduler\", \"description\": \"decides which node to schedule pod on based on affinity rules\", \"destination_entity\": \"Pod\"}, \\n{\"source_entity\": \"GPU\", \"description\": \"is a resource required by some pods and specified in nodeAffinity rules\", \"destination_entity\": \"nodeAffinity\"}, \\n{\"source_entity\": \"nodeAffinity\", \"description\": \"specifies which nodes to schedule pod on based on labels\", \"destination_entity\": \"Pod\"}, \\n{\"source_entity\": \"IgnoredDuringExecution\", \"description\": \"means affinity rules do not affect pods already executing on node\", \"destination_entity\": \"nodeAffinity\"}, \\n{\"source_entity\": \"nodeSelector\", \"description\": \"specifies which nodes to schedule pod on based on labels, but is less expressive than nodeAffinity\", \"destination_entity\": \"Pod\"}, \\n{\"source_entity\": \"affinity\", \"description\": \"is a field in Pod spec that specifies rules for scheduling pod on node\", \"destination_entity\": \"Pod\"}, \\n{\"source_entity\": \"label\", \"description\": \"is a key-value pair attached to a node or pod, used to specify affinity rules\", \"destination_entity\": \"nodeAffinity\"}]'},\n",
       " {'page': 497,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '465\\nUsing node affinity to attract pods to certain nodes\\nremove a label from a node, pods that require the node to have that label will be\\nevicted from such a node. As I’ve said, that’s not yet supported in Kubernetes, so let’s\\nnot concern ourselves with the second part of that long field any longer.\\nUNDERSTANDING NODESELECTORTERMS\\nBy keeping what was explained in the previous section in mind, it’s easy to understand\\nthat the nodeSelectorTerms field and the matchExpressions field define which\\nexpressions the node’s labels must match for the pod to be scheduled to the node.\\nThe single expression in the example is simple to understand. The node must have a\\ngpu label whose value is set to true. \\n This pod will therefore only be scheduled to nodes that have the gpu=true label, as\\nshown in figure 16.2.\\nNow comes the more interesting part. Node also affinity allows you to prioritize nodes\\nduring scheduling. We’ll look at that next.\\n16.2.2 Prioritizing nodes when scheduling a pod\\nThe biggest benefit of the newly introduced node affinity feature is the ability to spec-\\nify which nodes the Scheduler should prefer when scheduling a specific pod. This is\\ndone through the preferredDuringSchedulingIgnoredDuringExecution field.\\n Imagine having multiple datacenters across different countries. Each datacenter\\nrepresents a separate availability zone. In each zone, you have certain machines meant\\nonly for your own use and others that your partner companies can use. You now want\\nto deploy a few pods and you’d prefer them to be scheduled to zone1 and to the\\nNode with a GPU\\nPod\\nNode afﬁnity\\nRequired label:\\ngpu=true\\nPod\\nNo node afﬁnity\\ngpu: true\\nNode with a GPU\\nNode without a GPU\\nNode without a GPU\\ngpu: true\\nThis pod may be scheduled only\\nto nodes with gpu=true label\\nThis pod may be\\nscheduled to any node\\nFigure 16.2\\nA pod’s node affinity specifies which labels a node must have for the pod to be \\nscheduled to it.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'node affinity',\n",
       "    'description': 'a feature in Kubernetes that allows prioritizing nodes during scheduling',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'metadata assigned to nodes or pods, used for scheduling and other purposes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'gpu label',\n",
       "    'description': 'a specific label indicating a node has a GPU, required by some pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'nodeSelectorTerms',\n",
       "    'description': 'a field in the Kubernetes API that defines expressions for node labels',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'matchExpressions',\n",
       "    'description': 'a field in the Kubernetes API that specifies match conditions for node labels',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'preferredDuringSchedulingIgnoredDuringExecution',\n",
       "    'description': 'a field in the Kubernetes API that allows prioritizing nodes during scheduling',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'datacenters',\n",
       "    'description': 'physical locations hosting machines and storage, used for high availability and scalability',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'availability zones',\n",
       "    'description': ' logical groups of datacenters within a region, used for high availability and scalability',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"uses node affinity to attract pods to certain nodes\", \"destination_entity\": \"pods\"},\\n  \\n  {\"source_entity\": \"node affinity\", \"description\": \"allows prioritizing nodes during scheduling\", \"destination_entity\": \"nodes\"},\\n  \\n  {\"source_entity\": \"labels\", \"description\": \"define which expressions the node\\'s labels must match for the pod to be scheduled\", \"destination_entity\": \"nodeSelectorTerms\"},\\n  \\n  {\"source_entity\": \"matchExpressions\", \"description\": \"define which expressions the node\\'s labels must match for the pod to be scheduled\", \"destination_entity\": \"nodeSelectorTerms\"},\\n  \\n  {\"source_entity\": \"preferredDuringSchedulingIgnoredDuringExecution\", \"description\": \"prioritizes nodes during scheduling\", \"destination_entity\": \"nodes\"},\\n  \\n  {\"source_entity\": \"availability zones\", \"description\": \"represent separate availability zones across different countries\", \"destination_entity\": \"datacenters\"},\\n  \\n  {\"source_entity\": \"nodeSelectorTerms\", \"description\": \"define which expressions the node\\'s labels must match for the pod to be scheduled\", \"destination_entity\": \"labels\"},\\n  \\n  {\"source_entity\": \"gpu label\", \"description\": \"specifies which nodes the Scheduler should prefer when scheduling a specific pod\", \"destination_entity\": \"preferredDuringSchedulingIgnoredDuringExecution\"},\\n  \\n  {\"source_entity\": \"datacenters\", \"description\": \"represent separate availability zones across different countries\", \"destination_entity\": \"availability zones\"}\\n]\\n```\\n\\nNote that I have only included the entities provided in the list as input, and excluded any other entities mentioned in the document page.'},\n",
       " {'page': 498,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '466\\nCHAPTER 16\\nAdvanced scheduling\\nmachines reserved for your company’s deployments. If those machines don’t have\\nenough room for the pods or if other important reasons exist that prevent them from\\nbeing scheduled there, you’re okay with them being scheduled to the machines your\\npartners use and to the other zones. Node affinity allows you to do that.\\nLABELING NODES\\nFirst, the nodes need to be labeled appropriately. Each node needs to have a label that\\ndesignates the availability zone the node belongs to and a label marking it as either a\\ndedicated or a shared node.\\n Appendix B explains how to set up a three-node cluster (one master and two\\nworker nodes) in VMs running locally. In the following examples, I’ll use the two worker\\nnodes in that cluster, but you can also use Google Kubernetes Engine or any other\\nmulti-node cluster. \\nNOTE\\nMinikube isn’t the best choice for running these examples, because it\\nruns only one node.\\nFirst, label the nodes, as shown in the next listing.\\n$ kubectl label node node1.k8s availability-zone=zone1\\nnode \"node1.k8s\" labeled\\n$ kubectl label node node1.k8s share-type=dedicated\\nnode \"node1.k8s\" labeled\\n$ kubectl label node node2.k8s availability-zone=zone2\\nnode \"node2.k8s\" labeled\\n$ kubectl label node node2.k8s share-type=shared\\nnode \"node2.k8s\" labeled\\n$ kubectl get node -L availability-zone -L share-type\\nNAME         STATUS    AGE       VERSION   AVAILABILITY-ZONE   SHARE-TYPE\\nmaster.k8s   Ready     4d        v1.6.4    <none>              <none>\\nnode1.k8s    Ready     4d        v1.6.4    zone1               dedicated\\nnode2.k8s    Ready     4d        v1.6.4    zone2               shared\\nSPECIFYING PREFERENTIAL NODE AFFINITY RULES\\nWith the node labels set up, you can now create a Deployment that prefers dedicated\\nnodes in zone1. The following listing shows the Deployment manifest.\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: pref\\nspec:\\n  template:\\n    ...\\n    spec:\\n      affinity:\\n        nodeAffinity:\\nListing 16.10\\nLabeling nodes\\nListing 16.11\\nDeployment with preferred node affinity: preferred-deployment.yaml\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Node',\n",
       "    'description': 'A machine in a Kubernetes cluster',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'A containerization platform for software applications',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'A tool for running Kubernetes locally on a single machine',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line interface to interact with a Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'nodeAffinity',\n",
       "    'description': 'A feature in Kubernetes that allows specifying preferences for node selection',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'label',\n",
       "    'description': 'A way to attach metadata to nodes or pods in a Kubernetes cluster',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'availability-zone',\n",
       "    'description': 'A label that indicates the availability zone of a node',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'share-type',\n",
       "    'description': 'A label that indicates whether a node is dedicated or shared',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A way to manage and scale containers in Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'A field in the Deployment manifest that specifies the API version',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'A field in the Deployment manifest that specifies the type of resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'A field in the Deployment manifest that contains metadata about the Deployment',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'A field in the Deployment manifest that contains the specification for the Deployment',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"Kubernetes\", \"description\": \"allows to schedule machines to other zones if needed\", \"destination_entity\": \"other zones\"},\\n\\n{\"source_entity\": \"kubectl\", \"description\": \"labels nodes with specific labels\", \"destination_entity\": \"node1.k8s\"},\\n\\n{\"source_entity\": \"kubectl\", \"description\": \"labels node1.k8s as dedicated\", \"destination_entity\": \"share-type\"},\\n\\n{\"source_entity\": \"kubectl\", \"description\": \"labels node2.k8s as shared\", \"destination_entity\": \"share-type\"},\\n\\n{\"source_entity\": \"Kubernetes\", \"description\": \"specifies preferential node affinity rules\", \"destination_entity\": \"nodeAffinity\"},\\n\\n{\"source_entity\": \"Deployment\", \"description\": \"prefers dedicated nodes in zone1\", \"destination_entity\": \"Docker\"},\\n\\n{\"source_entity\": \"kubectl\", \"description\": \"labels node with availability-zone=zone1\", \"destination_entity\": \"availability-zone\"},\\n\\n{\"source_entity\": \"nodeAffinity\", \"description\": \"allows to schedule machines to other zones if needed\", \"destination_entity\": \"other zones\"},\\n\\n{\"source_entity\": \"Kubernetes\", \"description\": \"runs on top of Docker\", \"destination_entity\": \"Docker\"},\\n\\n{\"source_entity\": \"kubectl\", \"description\": \"gets node information with labels\", \"destination_entity\": \"metadata\"},\\n\\n{\"source_entity\": \"node1.k8s\", \"description\": \"labeled as dedicated\", \"destination_entity\": \"share-type\"},\\n\\n{\"source_entity\": \"node2.k8s\", \"description\": \"labeled as shared\", \"destination_entity\": \"share-type\"},\\n\\n{\"source_entity\": \"Kubernetes\", \"description\": \"uses nodeAffinity to schedule machines\", \"destination_entity\": \"Node\"},\\n\\n{\"source_entity\": \"Deployment\", \"description\": \"specifies preferred node affinity rules\", \"destination_entity\": \"nodeAffinity\"}]\\n\\nNote: I\\'ve tried to be as accurate as possible in extracting the relations, but there might be some minor errors or omissions.'},\n",
       " {'page': 499,\n",
       "  'img_cnt': 17,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '467\\nUsing node affinity to attract pods to certain nodes\\n          preferredDuringSchedulingIgnoredDuringExecution:    \\n          - weight: 80                               \\n            preference:                              \\n              matchExpressions:                      \\n              - key: availability-zone               \\n                operator: In                         \\n                values:                              \\n                - zone1                              \\n          - weight: 20                     \\n            preference:                    \\n              matchExpressions:            \\n              - key: share-type            \\n                operator: In               \\n                values:                    \\n                - dedicated                \\n      ...\\nLet’s examine the listing closely. You’re defining a node affinity preference, instead of\\na hard requirement. You want the pods scheduled to nodes that include the labels\\navailability-zone=zone1 and share-type=dedicated. You’re saying that the first\\npreference rule is important by setting its weight to 80, whereas the second one is\\nmuch less important (weight is set to 20).\\nUNDERSTANDING HOW NODE PREFERENCES WORK\\nIf your cluster had many nodes, when scheduling the pods of the Deployment in the\\nprevious listing, the nodes would be split into four groups, as shown in figure 16.3.\\nNodes whose availability-zone and share-type labels match the pod’s node affin-\\nity are ranked the highest. Then, because of how the weights in the pod’s node affinity\\nrules are configured, next come the shared nodes in zone1, then come the dedicated\\nnodes in the other zones, and at the lowest priority are all the other nodes.\\nYou’re\\nspecifying\\npreferences,\\nnot hard\\nrequirements.\\nYou prefer the pod to be \\nscheduled to zone1. This \\nis your most important \\npreference.\\nYou also prefer that your \\npods be scheduled to \\ndedicated nodes, but this is \\nfour times less important \\nthan your zone preference.\\nNode\\nTop priority\\nAvailability zone 1\\nPod\\nPriority: 2\\nPriority: 3\\nPriority: 4\\nNode afﬁnity\\nPreferred labels:\\navail-zone: zone1 (weight 80)\\nshare: dedicated (weight 20)\\navail-zone: zone1\\nshare: dedicated\\nNode\\navail-zone: zone1\\nshare: shared\\nNode\\nAvailability zone 2\\navail-zone: zone2\\nshare: dedicated\\nNode\\navail-zone: zone2\\nshare: shared\\nThis pod may be scheduled to\\nany node, but certain nodes are\\npreferred based on their labels.\\nFigure 16.3\\nPrioritizing nodes based on a pod’s node affinity preferences\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'node_affinity',\n",
       "    'description': 'A feature in Kubernetes that allows you to specify preferred nodes for pods based on node labels.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A lightweight and portable executable container in Kubernetes that contains an application and its dependencies.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'node_labels',\n",
       "    'description': 'Labels attached to a node in a cluster that can be used for scheduling pods.',\n",
       "    'category': 'hardware,software,application'},\n",
       "   {'entity': 'availability_zone',\n",
       "    'description': 'A label that indicates the availability zone of a node.',\n",
       "    'category': 'hardware,software,application'},\n",
       "   {'entity': 'share_type',\n",
       "    'description': 'A label that indicates the share type of a node (e.g. dedicated, shared).',\n",
       "    'category': 'hardware,software,application'},\n",
       "   {'entity': 'matchExpressions',\n",
       "    'description': 'A field in the node affinity preference that specifies the labels to match.',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'key',\n",
       "    'description': 'The label key to match (e.g. availability-zone, share-type).',\n",
       "    'category': 'hardware,software,application'},\n",
       "   {'entity': 'operator',\n",
       "    'description': 'The operator to use when matching node labels (e.g. In, Exists).',\n",
       "    'category': 'hardware,software,application'},\n",
       "   {'entity': 'values',\n",
       "    'description': 'The values of the label to match (e.g. zone1, dedicated).',\n",
       "    'category': 'hardware,software,application'},\n",
       "   {'entity': 'weight',\n",
       "    'description': 'A field in the node affinity preference that specifies the importance of a rule (e.g. 80, 20).',\n",
       "    'category': 'software,application'},\n",
       "   {'entity': 'preference',\n",
       "    'description': 'The preferred labels for a pod.',\n",
       "    'category': 'hardware,software,application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"cluster\", \"description\": \"define node affinity preference instead of hard requirement\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"user\", \"description\": \"want pods scheduled to nodes with labels availability-zone=zone1 and share-type=dedicated\", \"destination_entity\": \"node\"},\\n  {\"source_entity\": \"cluster\", \"description\": \"split nodes into four groups based on node affinity rules\", \"destination_entity\": \"node\"},\\n  {\"source_entity\": \"pod\", \"description\": \"ranked highest with availability-zone and share-type labels matching the pod\\'s node affinity\", \"destination_entity\": \"node\"},\\n  {\"source_entity\": \"user\", \"description\": \"specify preferences, not hard requirements\", \"destination_entity\": \"cluster\"},\\n  {\"source_entity\": \"pod\", \"description\": \"prefer to be scheduled to zone1\", \"destination_entity\": \"node\"},\\n  {\"source_entity\": \"pod\", \"description\": \"prefer to be scheduled to dedicated nodes, but this is four times less important than zone preference\", \"destination_entity\": \"node\"},\\n  {\"source_entity\": \"cluster\", \"description\": \"prioritize nodes based on pod\\'s node affinity preferences\", \"destination_entity\": \"node\"}\\n]\\n```\\n\\nHere are the steps I followed:\\n\\n1. For each entity in the list, I extracted relations from the document page.\\n2. For each relation, I defined a JSON object with three keys: `source_entity`, `description`, and `destination_entity`.\\n\\t* `source_entity` is the entity performing the action (i.e., the subject).\\n\\t* `destination_entity` is the entity on which the action is performed (i.e., the object).\\n\\t* `description` is a brief summary of what action was being carried out by the source entity on the destination entity.\\n3. I repeated this process for each entity in the list, and collected all the relations into a single list of JSON objects.\\n\\nNote that some entities appear multiple times as sources or destinations in different relations, but I have not tried to condense these relationships into more complex representations (e.g., nested JSON objects). Instead, I have kept each relation separate and explicit.'},\n",
       " {'page': 500,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '468\\nCHAPTER 16\\nAdvanced scheduling\\nDEPLOYING THE PODS IN THE TWO-NODE CLUSTER\\nIf you create this Deployment in your two-node cluster, you should see most (if not\\nall) of your pods deployed to node1. Examine the following listing to see if that’s true.\\n$ kubectl get po -o wide\\nNAME                READY   STATUS    RESTARTS  AGE   IP          NODE\\npref-607515-1rnwv   1/1     Running   0         4m    10.47.0.1   node2.k8s\\npref-607515-27wp0   1/1     Running   0         4m    10.44.0.8   node1.k8s\\npref-607515-5xd0z   1/1     Running   0         4m    10.44.0.5   node1.k8s\\npref-607515-jx9wt   1/1     Running   0         4m    10.44.0.4   node1.k8s\\npref-607515-mlgqm   1/1     Running   0         4m    10.44.0.6   node1.k8s\\nOut of the five pods that were created, four of them landed on node1 and only one\\nlanded on node2. Why did one of them land on node2 instead of node1? The reason is\\nthat besides the node affinity prioritization function, the Scheduler also uses other pri-\\noritization functions to decide where to schedule a pod. One of those is the Selector-\\nSpreadPriority function, which makes sure pods belonging to the same ReplicaSet or\\nService are spread around different nodes so a node failure won’t bring the whole ser-\\nvice down. That’s most likely what caused one of the pods to be scheduled to node2.\\n You can try scaling the Deployment up to 20 or more and you’ll see the majority of\\npods will be scheduled to node1. In my test, only two out of the 20 were scheduled to\\nnode2. If you hadn’t defined any node affinity preferences, the pods would have been\\nspread around the two nodes evenly.\\n16.3\\nCo-locating pods with pod affinity and anti-affinity\\nYou’ve seen how node affinity rules are used to influence which node a pod is scheduled\\nto. But these rules only affect the affinity between a pod and a node, whereas sometimes\\nyou’d like to have the ability to specify the affinity between pods themselves. \\n For example, imagine having a frontend and a backend pod. Having those pods\\ndeployed near to each other reduces latency and improves the performance of the\\napp. You could use node affinity rules to ensure both are deployed to the same node,\\nrack, or datacenter, but then you’d have to specify exactly which node, rack, or data-\\ncenter to schedule them to, which is not the best solution. It’s better to let Kubernetes\\ndeploy your pods anywhere it sees fit, while keeping the frontend and backend pods\\nclose together. This can be achieved using pod affinity. Let’s learn more about it with\\nan example.\\n16.3.1 Using inter-pod affinity to deploy pods on the same node\\nYou’ll deploy a backend pod and five frontend pod replicas with pod affinity config-\\nured so that they’re all deployed on the same node as the backend pod.\\n First, deploy the backend pod:\\n$ kubectl run backend -l app=backend --image busybox -- sleep 999999\\ndeployment \"backend\" created\\nListing 16.12\\nSeeing where pods were scheduled\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Deployment',\n",
       "    'description': 'A Kubernetes resource that manages a set of replicas.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'A worker machine in a Kubernetes cluster.',\n",
       "    'category': 'Hardware'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line tool for interacting with Kubernetes clusters.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'get po -o wide',\n",
       "    'description': 'A command to display pod information.',\n",
       "    'category': 'Command'},\n",
       "   {'entity': 'node affinity prioritization function',\n",
       "    'description': 'A Scheduler function that prioritizes nodes based on affinity rules.',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'Selector-SpreadPriority function',\n",
       "    'description': 'A Scheduler function that spreads pods across different nodes to ensure service availability.',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'A Kubernetes resource that ensures a specified number of replicas are running at any given time.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A Kubernetes resource that provides network connectivity to pods.',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'pod affinity',\n",
       "    'description': 'A feature in Kubernetes that allows specifying the affinity between pods.',\n",
       "    'category': 'Software Feature'},\n",
       "   {'entity': 'frontend pod',\n",
       "    'description': 'A type of pod that serves as a frontend for an application.',\n",
       "    'category': 'Software Component'},\n",
       "   {'entity': 'backend pod',\n",
       "    'description': 'A type of pod that serves as a backend for an application.',\n",
       "    'category': 'Software Component'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"node affinity prioritization function\",\\n    \"description\": \"uses other prioritization functions to decide where to schedule a pod\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"Selector-SpreadPriority function\",\\n    \"description\": \"makes sure pods belonging to the same ReplicaSet or Service are spread around different nodes so a node failure won’t bring the whole service down\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"deployment (pref-607515-1rnwv)\",\\n    \"description\": \"scheduled to node2 instead of node1 because of Selector-SpreadPriority function\",\\n    \"destination_entity\": \"node2.k8s\"\\n  },\\n  {\\n    \"source_entity\": \"deployment (pref-607515-1rnwv)\",\\n    \"description\": \"scheduled to node1 along with other deployments (pref-607515-27wp0, pref-607515-5xd0z, pref-607515-jx9wt, pref-607515-mlgqm) because of Selector-SpreadPriority function\",\\n    \"destination_entity\": \"node1.k8s\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl get po -o wide command\",\\n    \"description\": \"used to check the status of pods\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"get po -o wide command\",\\n    \"description\": \"shows information about running and waiting pods\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"frontend pod\",\\n    \"description\": \"uses pod affinity to deploy near the backend pod\",\\n    \"destination_entity\": \"backend pod\"\\n  },\\n  {\\n    \"source_entity\": \"pod affinity\",\\n    \"description\": \"used to specify the affinity between pods themselves, reducing latency and improving performance of app\",\\n    \"destination_entity\": \"frontend and backend pods\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl run command\",\\n    \"description\": \"deployed backend pod with specified image and label\",\\n    \"destination_entity\": \"backend pod\"\\n  },\\n  {\\n    \"source_entity\": \"pod (pref-607515-1rnwv)\",\\n    \"description\": \"served as an example of deployment in a two-node cluster\",\\n    \"destination_entity\": \"two-node cluster\"\\n  },\\n  {\\n    \"source_entity\": \"Node\",\\n    \"description\": \"used by Scheduler to decide where to schedule a pod\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"Deployment\",\\n    \"description\": \"used to deploy backend and frontend pods with specified configuration\",\\n    \"destination_entity\": \"backend and frontend pods\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"used by Selector-SpreadPriority function to spread pods around different nodes so a node failure won’t bring the whole service down\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicaSet\",\\n    \"description\": \"used by Selector-SpreadPriority function to spread pods around different nodes so a node failure won’t bring the whole service down\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"command used to interact with Kubernetes cluster and check pod status\",\\n    \"destination_entity\": \"pod\"\\n  }\\n]\\n```'},\n",
       " {'page': 501,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '469\\nCo-locating pods with pod affinity and anti-affinity\\nThis Deployment is not special in any way. The only thing you need to note is the\\napp=backend label you added to the pod using the -l option. This label is what you’ll\\nuse in the frontend pod’s podAffinity configuration. \\nSPECIFYING POD AFFINITY IN A POD DEFINITION\\nThe frontend pod’s definition is shown in the following listing.\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: frontend\\nspec:\\n  replicas: 5\\n  template:\\n    ...\\n    spec:\\n      affinity:\\n        podAffinity:                                 \\n          requiredDuringSchedulingIgnoredDuringExecution:   \\n          - topologyKey: kubernetes.io/hostname           \\n            labelSelector:                                \\n              matchLabels:                                \\n                app: backend                              \\n      ...\\nThe listing shows that this Deployment will create pods that have a hard requirement\\nto be deployed on the same node (specified by the topologyKey field) as pods that\\nhave the app=backend label (see figure 16.4).\\nListing 16.13\\nPod using podAffinity: frontend-podaffinity-host.yaml\\nDefining \\npodAffinity rules\\nDefining a hard \\nrequirement, not \\na preference\\nThe pods of this Deployment \\nmust be deployed on the \\nsame node as the pods that \\nmatch the selector.\\nAll frontend pods will\\nbe scheduled only to\\nthe node the backend\\npod was scheduled to.\\nSome node\\nOther nodes\\nFrontend pods\\nBackend\\npod\\nPod afﬁnity\\nLabel selector: app=backend\\nTopology key: hostname\\napp: backend\\nFigure 16.4\\nPod affinity allows scheduling pods to the node where other pods \\nwith a specific label are.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Deployment',\n",
       "    'description': 'A Kubernetes resource that manages a set of replicable, identical Pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes, equivalent to a container.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'labelSelector',\n",
       "    'description': 'A selector used to identify pods with matching labels.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'matchLabels',\n",
       "    'description': 'A set of key-value pairs that match the label selector.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'app=backend',\n",
       "    'description': 'A label that identifies pods as backend instances.',\n",
       "    'category': 'label'},\n",
       "   {'entity': 'podAffinity',\n",
       "    'description': 'A scheduling rule that requires pods to be deployed on the same node as other pods with matching labels.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'requiredDuringSchedulingIgnoredDuringExecution',\n",
       "    'description': 'A field in pod affinity rules that specifies a hard requirement for pod placement.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'topologyKey',\n",
       "    'description': \"A key used to identify the node's topology, such as its hostname or IP address.\",\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'hostname',\n",
       "    'description': 'The unique identifier of a node in Kubernetes.',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[{\"source_entity\": \"Deployment\", \"description\": \"must be deployed on the same node\", \"destination_entity\": \"hostname\"},{\"source_entity\": \"labelSelector\", \"description\": \"match with the label app=backend\", \"destination_entity\": \"app=backend\"},{\"source_entity\": \"Pod\", \"description\": \"scheduling to the node where other pods match the selector\", \"destination_entity\": \"labelSelector\"},{\"source_entity\": \"Deployment\", \"description\": \"create pods that have a hard requirement\", \"destination_entity\": \"Pod\"},{\"source_entity\": \"podAffinity\", \"description\": \"specify a hard requirement for deployment\", \"destination_entity\": \"Deployment\"},{\"source_entity\": \"topologyKey\", \"description\": \"specify the key for hostname\", \"destination_entity\": \"hostname\"},{\"source_entity\": \"requiredDuringSchedulingIgnoredDuringExecution\", \"description\": \"specify the condition for podAffinity\", \"destination_entity\": \"podAffinity\"}]\\n\\nNote that I\\'ve only included relations where the source entity is one of the entities in the input list, and the destination entity is also in the input list. Let me know if you\\'d like me to include any additional information!'},\n",
       " {'page': 502,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '470\\nCHAPTER 16\\nAdvanced scheduling\\nNOTE\\nInstead of the simpler matchLabels field, you could also use the more\\nexpressive matchExpressions field.\\nDEPLOYING A POD WITH POD AFFINITY\\nBefore you create this Deployment, let’s see which node the backend pod was sched-\\nuled to earlier:\\n$ kubectl get po -o wide\\nNAME                   READY  STATUS   RESTARTS  AGE  IP         NODE\\nbackend-257820-qhqj6   1/1    Running  0         8m   10.47.0.1  node2.k8s\\nWhen you create the frontend pods, they should be deployed to node2 as well. You’re\\ngoing to create the Deployment and see where the pods are deployed. This is shown\\nin the next listing.\\n$ kubectl create -f frontend-podaffinity-host.yaml\\ndeployment \"frontend\" created\\n$ kubectl get po -o wide\\nNAME                   READY  STATUS    RESTARTS  AGE  IP         NODE\\nbackend-257820-qhqj6   1/1    Running   0         8m   10.47.0.1  node2.k8s\\nfrontend-121895-2c1ts  1/1    Running   0         13s  10.47.0.6  node2.k8s\\nfrontend-121895-776m7  1/1    Running   0         13s  10.47.0.4  node2.k8s\\nfrontend-121895-7ffsm  1/1    Running   0         13s  10.47.0.8  node2.k8s\\nfrontend-121895-fpgm6  1/1    Running   0         13s  10.47.0.7  node2.k8s\\nfrontend-121895-vb9ll  1/1    Running   0         13s  10.47.0.5  node2.k8s\\nAll the frontend pods were indeed scheduled to the same node as the backend pod.\\nWhen scheduling the frontend pod, the Scheduler first found all the pods that match\\nthe labelSelector defined in the frontend pod’s podAffinity configuration and\\nthen scheduled the frontend pod to the same node.\\nUNDERSTANDING HOW THE SCHEDULER USES POD AFFINITY RULES\\nWhat’s interesting is that if you now delete the backend pod, the Scheduler will sched-\\nule the pod to node2 even though it doesn’t define any pod affinity rules itself (the\\nrules are only on the frontend pods). This makes sense, because otherwise if the back-\\nend pod were to be deleted by accident and rescheduled to a different node, the fron-\\ntend pods’ affinity rules would be broken. \\n You can confirm the Scheduler takes other pods’ pod affinity rules into account, if\\nyou increase the Scheduler’s logging level and then check its log. The following listing\\nshows the relevant log lines.\\n... Attempting to schedule pod: default/backend-257820-qhqj6\\n... ...\\n... backend-qhqj6 -> node2.k8s: Taint Toleration Priority, Score: (10)\\nListing 16.14\\nDeploying frontend pods and seeing which node they’re scheduled to\\nListing 16.15\\nScheduler log showing why the backend pod is scheduled to node2\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'Command-line interface for interacting with Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'matchLabels',\n",
       "    'description': 'Field used in Kubernetes scheduling',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'matchExpressions',\n",
       "    'description': 'More expressive field used in Kubernetes scheduling',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PodAffinity',\n",
       "    'description': 'Kubernetes feature that schedules pods to the same node as another pod',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'Kubernetes resource for managing a set of replicas of an application',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'backend-257820-qhqj6',\n",
       "    'description': 'Name of a Kubernetes pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'frontend-podaffinity-host.yaml',\n",
       "    'description': 'YAML file used to create a Kubernetes deployment',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'frontend-121895-2c1ts',\n",
       "    'description': 'Name of a Kubernetes pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'node2.k8s',\n",
       "    'description': 'Name of a Kubernetes node',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'Kubernetes component responsible for scheduling pods to nodes',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Scheduler\", \"description\": \"uses pod affinity rules to schedule pods\", \"destination_entity\": \"pod affinity rules\"},\\n  {\"source_entity\": \"Scheduler\", \"description\": \"takes other pods\\' pod affinity rules into account\", \"destination_entity\": \"other pods\\' pod affinity rules\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"creates a deployment from a YAML file\", \"destination_entity\": \"frontend-podaffinity-host.yaml\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"schedules pods to nodes based on pod affinity rules\", \"destination_entity\": \"pod affinity rules\"},\\n  {\"source_entity\": \"PodAffinity\", \"description\": \"defines the node where a pod should be scheduled\", \"destination_entity\": \"node2.k8s\"},\\n  {\"source_entity\": \"backend-257820-qhqj6\", \"description\": \"scheduled to node2 due to pod affinity rule\", \"destination_entity\": \"node2.k8s\"},\\n  {\"source_entity\": \"frontend-121895-2c1ts\", \"description\": \"scheduled to node2 due to pod affinity rule\", \"destination_entity\": \"node2.k8s\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"gets the status of pods\", \"destination_entity\": \"backend-257820-qhqj6\"},\\n  {\"source_entity\": \"matchLabels\", \"description\": \"defines labels for a pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"matchExpressions\", \"description\": \"defines expressions for a pod\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"frontend-podaffinity-host.yaml\", \"description\": \"defines the pod affinity rule for frontend pods\", \"destination_entity\": \"pod affinity rule\"},\\n  {\"source_entity\": \"node2.k8s\", \"description\": \"hosts the backend and frontend pods\", \"destination_entity\": \"backend-257820-qhqj6\" and \"frontend-121895-2c1ts\"}\\n]\\n```\\n\\nNote that I\\'ve assumed that `pod` is a generic term for any pod, and not a specific entity. If you\\'d like me to make any changes or provide further clarification, please let me know!'},\n",
       " {'page': 503,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '471\\nCo-locating pods with pod affinity and anti-affinity\\n... backend-qhqj6 -> node1.k8s: Taint Toleration Priority, Score: (10)\\n... backend-qhqj6 -> node2.k8s: InterPodAffinityPriority, Score: (10)\\n... backend-qhqj6 -> node1.k8s: InterPodAffinityPriority, Score: (0)\\n... backend-qhqj6 -> node2.k8s: SelectorSpreadPriority, Score: (10)\\n... backend-qhqj6 -> node1.k8s: SelectorSpreadPriority, Score: (10)\\n... backend-qhqj6 -> node2.k8s: NodeAffinityPriority, Score: (0)\\n... backend-qhqj6 -> node1.k8s: NodeAffinityPriority, Score: (0)\\n... Host node2.k8s => Score 100030\\n... Host node1.k8s => Score 100022\\n... Attempting to bind backend-257820-qhqj6 to node2.k8s\\nIf you focus on the two lines in bold, you’ll see that during the scheduling of the back-\\nend pod, node2 received a higher score than node1 because of inter-pod affinity. \\n16.3.2 Deploying pods in the same rack, availability zone, or \\ngeographic region\\nIn the previous example, you used podAffinity to deploy frontend pods onto the\\nsame node as the backend pods. You probably don’t want all your frontend pods to\\nrun on the same machine, but you’d still like to keep them close to the backend\\npod—for example, run them in the same availability zone. \\nCO-LOCATING PODS IN THE SAME AVAILABILITY ZONE\\nThe cluster I’m using runs in three VMs on my local machine, so all the nodes are in\\nthe same availability zone, so to speak. But if the nodes were in different zones, all I’d\\nneed to do to run the frontend pods in the same zone as the backend pod would be to\\nchange the topologyKey property to failure-domain.beta.kubernetes.io/zone. \\nCO-LOCATING PODS IN THE SAME GEOGRAPHICAL REGION\\nTo allow the pods to be deployed in the same region instead of the same zone (cloud\\nproviders usually have datacenters located in different geographical regions and split\\ninto multiple availability zones in each region), the topologyKey would be set to\\nfailure-domain.beta.kubernetes.io/region.\\nUNDERSTANDING HOW TOPOLOGYKEY WORKS\\nThe way topologyKey works is simple. The three keys we’ve mentioned so far aren’t\\nspecial. If you want, you can easily use your own topologyKey, such as rack, to have\\nthe pods scheduled to the same server rack. The only prerequisite is to add a rack\\nlabel to your nodes. This scenario is shown in figure 16.5.\\n For example, if you had 20 nodes, with 10 in each rack, you’d label the first ten as\\nrack=rack1 and the others as rack=rack2. Then, when defining a pod’s podAffinity,\\nyou’d set the toplogyKey to rack. \\n When the Scheduler is deciding where to deploy a pod, it checks the pod’s pod-\\nAffinity config, finds the pods that match the label selector, and looks up the nodes\\nthey’re running on. Specifically, it looks up the nodes’ label whose key matches the\\ntopologyKey field specified in podAffinity. Then it selects all the nodes whose label\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod Affinity',\n",
       "    'description': 'A scheduling feature in Kubernetes that allows pods to be co-located on the same node or in the same availability zone.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'NodeAffinityPriority',\n",
       "    'description': 'A priority score given by the Scheduler when determining where to deploy a pod based on node affinity.',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'SelectorSpreadPriority',\n",
       "    'description': 'A priority score given by the Scheduler when determining where to deploy a pod based on selector spread.',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'InterPodAffinityPriority',\n",
       "    'description': 'A priority score given by the Scheduler when determining where to deploy a pod based on inter-pod affinity.',\n",
       "    'category': 'Process'},\n",
       "   {'entity': 'ToplogyKey',\n",
       "    'description': 'A key used in Kubernetes scheduling to determine the topology of nodes and schedule pods accordingly.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Availability Zone',\n",
       "    'description': 'A geographic region or zone within a cloud provider where multiple nodes are located.',\n",
       "    'category': 'Network'},\n",
       "   {'entity': 'Geographic Region',\n",
       "    'description': 'A larger geographic area that encompasses multiple availability zones and datacenters.',\n",
       "    'category': 'Network'},\n",
       "   {'entity': 'Rack',\n",
       "    'description': 'A physical server rack within a datacenter where multiple nodes are located.',\n",
       "    'category': 'Hardware'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'A machine or VM in Kubernetes that runs one or more pods.',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A lightweight and portable container created by the Docker engine.',\n",
       "    'category': 'Container'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'The component in Kubernetes responsible for determining where to deploy pods based on various scheduling factors.',\n",
       "    'category': 'Process'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Pod Affinity\",\\n    \"description\": \"deploy pods onto the same node as other matching pods\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"Geographic Region\",\\n    \"description\": \"schedule pods to run in the same geographical region\",\\n    \"destination_entity\": \"Availability Zone\"\\n  },\\n  {\\n    \"source_entity\": \"Rack\",\\n    \"description\": \"schedule pods to run on nodes with matching rack label\",\\n    \"destination_entity\": \"Node\"\\n  },\\n  {\\n    \"source_entity\": \"Pod Affinity\",\\n    \"description\": \"assign a higher score to nodes that match the affinity rule\",\\n    \"destination_entity\": \"NodeAffinityPriority\"\\n  },\\n  {\\n    \"source_entity\": \"Pod Affinity\",\\n    \"description\": \"assign a higher score to nodes that match the affinity rule\",\\n    \"destination_entity\": \"InterPodAffinityPriority\"\\n  },\\n  {\\n    \"source_entity\": \"SelectorSpreadPriority\",\\n    \"description\": \"spread pods across multiple nodes with matching label selector\",\\n    \"destination_entity\": \"Node\"\\n  },\\n  {\\n    \"source_entity\": \"TopologyKey\",\\n    \"description\": \"schedule pods to run on nodes with matching topology key label\",\\n    \"destination_entity\": \"Node\"\\n  },\\n  {\\n    \"source_entity\": \"Availability Zone\",\\n    \"description\": \"schedule frontend pods to run in the same availability zone as backend pod\",\\n    \"destination_entity\": \"Pod Affinity\"\\n  },\\n  {\\n    \"source_entity\": \"Scheduler\",\\n    \"description\": \"decide where to deploy a pod based on pod affinity config and node labels\",\\n    \"destination_entity\": \"NodeAffinityPriority\"\\n  }\\n]'},\n",
       " {'page': 504,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '472\\nCHAPTER 16\\nAdvanced scheduling\\nmatches the values of the pods it found earlier. In figure 16.5, the label selector\\nmatched the backend pod, which runs on Node 12. The value of the rack label on\\nthat node equals rack2, so when scheduling a frontend pod, the Scheduler will only\\nselect among the nodes that have the rack=rack2 label.\\nNOTE\\nBy default, the label selector only matches pods in the same name-\\nspace as the pod that’s being scheduled. But you can also select pods from\\nother namespaces by adding a namespaces field at the same level as label-\\nSelector.\\n16.3.3 Expressing pod affinity preferences instead of hard requirements\\nEarlier, when we talked about node affinity, you saw that nodeAffinity can be used to\\nexpress a hard requirement, which means a pod is only scheduled to nodes that match\\nthe node affinity rules. It can also be used to specify node preferences, to instruct the\\nScheduler to schedule the pod to certain nodes, while allowing it to schedule it any-\\nwhere else if those nodes can’t fit the pod for any reason.\\n The same also applies to podAffinity. You can tell the Scheduler you’d prefer to\\nhave your frontend pods scheduled onto the same node as your backend pod, but if\\nthat’s not possible, you’re okay with them being scheduled elsewhere. An example of\\na Deployment using the preferredDuringSchedulingIgnoredDuringExecution pod\\naffinity rule is shown in the next listing.\\nFrontend pods will be\\nscheduled to nodes in\\nthe same rack as the\\nbackend pod.\\nNode 1\\nRack 1\\nrack: rack1\\nNode 2\\nrack: rack1\\nNode 3\\n...\\nrack: rack1\\nNode 10\\nrack: rack1\\nNode 11\\nRack 2\\nrack: rack2\\nNode 12\\nrack: rack2\\n...\\nNode 20\\nrack: rack2\\nBackend\\npod\\napp: backend\\nFrontend pods\\nPod afﬁnity (required)\\nLabel selector: app=backend\\nTopology key: rack\\nFigure 16.5\\nThe topologyKey in podAffinity determines the scope of where the pod \\nshould be scheduled to.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Rack 1\n",
       "   rack: rack1\n",
       "   Node 1\n",
       "   rack: rack1\n",
       "   Node 2\n",
       "   rack: rack1\n",
       "   Node 3\n",
       "   ...\n",
       "   rack: rack1\n",
       "   Node 10, Col1, Rack 2\n",
       "   rack: rack2\n",
       "   Node 11\n",
       "   rack: rack2\n",
       "   app: backend\n",
       "   Backend\n",
       "   pod\n",
       "   Node 12\n",
       "   ...\n",
       "   rack: rack2\n",
       "   Node 20, Frontend pods will be\n",
       "   scheduled to nodes in\n",
       "   the same rack as the\n",
       "   backend pod.\n",
       "   Frontend pods\n",
       "   Pod affinity (required)\n",
       "   Label selector: app=backend\n",
       "   Topology key: rack]\n",
       "   Index: [],\n",
       "   Empty DataFrame\n",
       "   Columns: [n, d]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Lightweight and portable containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'Component responsible for scheduling pods on nodes',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Label selector',\n",
       "    'description': 'Mechanism used to select pods based on labels',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Node affinity',\n",
       "    'description': 'Rule that specifies the node on which a pod should be scheduled',\n",
       "    'category': 'resource management'},\n",
       "   {'entity': 'Pod affinity',\n",
       "    'description': 'Rule that specifies the nodes or pods on which a pod should be scheduled',\n",
       "    'category': 'resource management'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'Kubernetes object used to manage and scale applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'Physical or virtual machine running Kubernetes',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': 'Logical partitioning of resources within a cluster',\n",
       "    'category': 'resource management'},\n",
       "   {'entity': 'Rack',\n",
       "    'description': 'Topological key used to specify the node on which a pod should be scheduled',\n",
       "    'category': 'resource management'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Scheduler\",\\n    \"description\": \"schedules pods based on label selector and node affinity rules\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Label selector\",\\n    \"description\": \"matches labels of pods to select nodes for scheduling\",\\n    \"destination_entity\": \"Nodes\"\\n  },\\n  {\\n    \"source_entity\": \"Node affinity\",\\n    \"description\": \"expresses hard requirements or preferences for node selection during pod scheduling\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Deployment\",\\n    \"description\": \"uses preferredDuringSchedulingIgnoredDuringExecution pod affinity rule to schedule frontend pods\",\\n    \"destination_entity\": \"Frontend pods\"\\n  },\\n  {\\n    \"source_entity\": \"Pod affinity\",\\n    \"description\": \"specifies node preferences for scheduling pods based on label selector and topology key\",\\n    \"destination_entity\": \"Nodes\"\\n  },\\n  {\\n    \"source_entity\": \"Namespace\",\\n    \"description\": \"defines the scope of pod scheduling based on label selector\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Rack\",\\n    \"description\": \"determines the topology key for pod affinity and node selection during scheduling\",\\n    \"destination_entity\": \"Nodes\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"are scheduled to nodes based on label selector, node affinity, and pod affinity rules\",\\n    \"destination_entity\": \"Node\"\\n  }\\n]\\n```'},\n",
       " {'page': 505,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '473\\nCo-locating pods with pod affinity and anti-affinity\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: frontend\\nspec:\\n  replicas: 5\\n  template:\\n    ...\\n    spec:\\n      affinity:\\n        podAffinity:\\n          preferredDuringSchedulingIgnoredDuringExecution:  \\n          - weight: 80                                        \\n            podAffinityTerm:                                  \\n              topologyKey: kubernetes.io/hostname             \\n              labelSelector:                                  \\n                matchLabels:                                  \\n                  app: backend                                \\n      containers: ...\\nAs in nodeAffinity preference rules, you need to define a weight for each rule. You\\nalso need to specify the topologyKey and labelSelector, as in the hard-requirement\\npodAffinity rules. Figure 16.6 shows this scenario.\\nDeploying this pod, as with your nodeAffinity example, deploys four pods on the same\\nnode as the backend pod, and one pod on the other node (see the following listing).\\nListing 16.16\\nPod affinity preference\\nPreferred \\ninstead of \\nRequired\\nA weight and a \\npodAffinity term is \\nspecified as in the \\nprevious example\\nThe Scheduler will prefer\\nNode 2 for frontend pods,\\nbut may schedule pods\\nto Node 1 as well.\\nNode 1\\nNode 2\\nBackend\\npod\\napp: backend\\nFrontend pod\\nPod afﬁnity (preferred)\\nLabel selector: app=backend\\nTopology key: hostname\\nhostname: node2\\nhostname: node1\\nFigure 16.6\\nPod affinity can be used to make the Scheduler prefer nodes where \\npods with a certain label are running. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'podAffinityTerm',\n",
       "    'description': 'A term in pod affinity rule that specifies the topology key and label selector.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'weight',\n",
       "    'description': 'A value that defines the preference weight for a pod affinity rule.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'topologyKey',\n",
       "    'description': 'The key used to specify the topology for a pod affinity rule.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'labelSelector',\n",
       "    'description': \"A selector that matches labels in a pod's metadata.\",\n",
       "    'category': 'process'},\n",
       "   {'entity': 'matchLabels',\n",
       "    'description': 'The labels that match the label selector.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'app',\n",
       "    'description': 'A label key for an application.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'nodeAffinity',\n",
       "    'description': 'A rule in pod affinity that specifies a node preference.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'podAffinity',\n",
       "    'description': 'A rule that specifies a preferred or required node for a pod.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Scheduler',\n",
       "    'description': 'The component responsible for scheduling pods on nodes.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'A machine that runs a pod.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'hostname',\n",
       "    'description': 'The unique identifier of a node.',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Topology Key\", \"description\": \"specifies the key to be used for affinity rules\", \"destination_entity\": \"topologyKey\"},\\n  {\"source_entity\": \"Label Selector\", \"description\": \"defines a label selector that matches pods with specific labels\", \"destination_entity\": \"labelSelector\"},\\n  {\"source_entity\": \"Weight\", \"description\": \"assigns a preference value to each rule\", \"destination_entity\": \"weight\"},\\n  {\"source_entity\": \"Node Affinity\", \"description\": \"specifies the node preference rules\", \"destination_entity\": \"nodeAffinity\"},\\n  {\"source_entity\": \"Pod Affinity Term\", \"description\": \"defines a term that specifies the label selector and topology key for pod affinity rules\", \"destination_entity\": \"podAffinityTerm\"},\\n  {\"source_entity\": \"Scheduler\", \"description\": \"prefers nodes where pods with specific labels are running\", \"destination_entity\": \"Scheduler\"},\\n  {\"source_entity\": \"Node\", \"description\": \"hosts a pod or other resources\", \"destination_entity\": \"node\"},\\n  {\"source_entity\": \"Label\", \"description\": \"matches pods with specific labels\", \"destination_entity\": \"matchLabels\"},\\n  {\"source_entity\": \"Pod Affinity\", \"description\": \"makes the Scheduler prefer nodes where pods with specific labels are running\", \"destination_entity\": \"podAffinity\"},\\n  {\"source_entity\": \"App\", \"description\": \"specifies the application name for pod affinity rules\", \"destination_entity\": \"app\"}\\n]\\n```\\n\\nNote: I\\'ve also included some entities that were not in the original list but seemed relevant to the context, such as \"Topology Key\" and \"Label\". If you\\'d like me to remove them, please let me know!'},\n",
       " {'page': 506,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '474\\nCHAPTER 16\\nAdvanced scheduling\\n$ kubectl get po -o wide\\nNAME                   READY  STATUS   RESTARTS  AGE  IP          NODE\\nbackend-257820-ssrgj   1/1    Running  0         1h   10.47.0.9   node2.k8s\\nfrontend-941083-3mff9  1/1    Running  0         8m   10.44.0.4   node1.k8s\\nfrontend-941083-7fp7d  1/1    Running  0         8m   10.47.0.6   node2.k8s\\nfrontend-941083-cq23b  1/1    Running  0         8m   10.47.0.1   node2.k8s\\nfrontend-941083-m70sw  1/1    Running  0         8m   10.47.0.5   node2.k8s\\nfrontend-941083-wsjv8  1/1    Running  0         8m   10.47.0.4   node2.k8s\\n16.3.4 Scheduling pods away from each other with pod anti-affinity\\nYou’ve seen how to tell the Scheduler to co-locate pods, but sometimes you may want\\nthe exact opposite. You may want to keep pods away from each other. This is called\\npod anti-affinity. It’s specified the same way as pod affinity, except that you use the\\npodAntiAffinity property instead of podAffinity, which results in the Scheduler\\nnever choosing nodes where pods matching the podAntiAffinity’s label selector are\\nrunning, as shown in figure 16.7.\\nAn example of why you’d want to use pod anti-affinity is when two sets of pods inter-\\nfere with each other’s performance if they run on the same node. In that case, you\\nwant to tell the Scheduler to never schedule those pods on the same node. Another\\nexample would be to force the Scheduler to spread pods of the same group across dif-\\nferent availability zones or regions, so that a failure of a whole zone (or region) never\\nbrings the service down completely. \\nListing 16.17\\nPods deployed with podAffinity preferences\\nThese pods will NOT be scheduled\\nto the same node(s) where pods\\nwith app=foo label are running.\\nSome node\\nOther nodes\\nPods\\nPod: foo\\nPod\\n(required)\\nanti-afﬁnity\\nLabel selector: app=foo\\nTopology key: hostname\\napp: foo\\nFigure 16.7\\nUsing pod anti-affinity to keep pods away from nodes that run pods \\nwith a certain label.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'get', 'description': 'command', 'category': 'software'},\n",
       "   {'entity': 'po', 'description': 'resource type', 'category': 'software'},\n",
       "   {'entity': 'wide', 'description': 'output format', 'category': 'software'},\n",
       "   {'entity': 'backend-257820-ssrgj',\n",
       "    'description': 'pod name',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'frontend-941083-3mff9',\n",
       "    'description': 'pod name',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'frontend-941083-7fp7d',\n",
       "    'description': 'pod name',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'frontend-941083-cq23b',\n",
       "    'description': 'pod name',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'frontend-941083-m70sw',\n",
       "    'description': 'pod name',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'frontend-941083-wsjv8',\n",
       "    'description': 'pod name',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'podAffinity',\n",
       "    'description': 'scheduling property',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'podAntiAffinity',\n",
       "    'description': 'scheduling property',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'label selector',\n",
       "    'description': 'selector',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Topology key', 'description': 'key', 'category': 'software'},\n",
       "   {'entity': 'hostname', 'description': 'key', 'category': 'hardware'},\n",
       "   {'entity': 'app=foo',\n",
       "    'description': 'label selector',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"run command to get pod information\", \"destination_entity\": \"po\"},\\n  {\"source_entity\": \"hostname\", \"description\": \"specify node where pods should not be scheduled\", \"destination_entity\": \"podAntiAffinity\"},\\n  {\"source_entity\": \"Topology key\", \"description\": \"specify node characteristic for pod affinity/anti-affinity\", \"destination_entity\": \"podAffinity\"},\\n  {\"source_entity\": \"app=foo\", \"description\": \"label selector to avoid scheduling pods on same node\", \"destination_entity\": \"backend-257820-ssrgj\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"run command to get pod information\", \"destination_entity\": \"frontend-941083-3mff9\"},\\n  {\"source_entity\": \"frontend-941083-3mff9\", \"description\": \"running on node with label selector app=foo\", \"destination_entity\": \"podAntiAffinity\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"run command to get pod information\", \"destination_entity\": \"frontend-941083-cq23b\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"run command to get pod information\", \"destination_entity\": \"frontend-941083-m70sw\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"run command to get pod information\", \"destination_entity\": \"frontend-941083-jslv8\"},\\n  {\"source_entity\": \"podAntiAffinity\", \"description\": \"avoid scheduling pods on same node with app=foo label\", \"destination_entity\": \"frontend-941083-wsjv8\"}\\n]\\n```'},\n",
       " {'page': 507,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '475\\nCo-locating pods with pod affinity and anti-affinity\\nUSING ANTI-AFFINITY TO SPREAD APART PODS OF THE SAME DEPLOYMENT\\nLet’s see how to force your frontend pods to be scheduled to different nodes. The fol-\\nlowing listing shows how the pods’ anti-affinity is configured.\\napiVersion: extensions/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: frontend\\nspec:\\n  replicas: 5\\n  template:\\n    metadata:\\n      labels:                  \\n        app: frontend          \\n    spec:\\n      affinity:\\n        podAntiAffinity:                                      \\n          requiredDuringSchedulingIgnoredDuringExecution:     \\n          - topologyKey: kubernetes.io/hostname            \\n            labelSelector:                                 \\n              matchLabels:                                 \\n                app: frontend                              \\n      containers: ...\\nThis time, you’re defining podAntiAffinity instead of podAffinity, and you’re mak-\\ning the labelSelector match the same pods that the Deployment creates. Let’s see\\nwhat happens when you create this Deployment. The pods created by it are shown in\\nthe following listing.\\n$ kubectl get po -l app=frontend -o wide\\nNAME                    READY  STATUS   RESTARTS  AGE  IP         NODE\\nfrontend-286632-0lffz   0/1    Pending  0         1m   <none>\\nfrontend-286632-2rkcz   1/1    Running  0         1m   10.47.0.1  node2.k8s\\nfrontend-286632-4nwhp   0/1    Pending  0         1m   <none>\\nfrontend-286632-h4686   0/1    Pending  0         1m   <none>\\nfrontend-286632-st222   1/1    Running  0         1m   10.44.0.4  node1.k8s\\nAs you can see, only two pods were scheduled—one to node1, the other to node2. The\\nthree remaining pods are all Pending, because the Scheduler isn’t allowed to schedule\\nthem to the same nodes.\\nUSING PREFERENTIAL POD ANTI-AFFINITY\\nIn this case, you probably should have specified a soft requirement instead (using the\\npreferredDuringSchedulingIgnoredDuringExecution property). After all, it’s not\\nsuch a big problem if two frontend pods run on the same node. But in scenarios where\\nthat’s a problem, using requiredDuringScheduling is appropriate. \\nListing 16.18\\nPods with anti-affinity: frontend-podantiaffinity-host.yaml\\nListing 16.19\\nPods created by the Deployment\\nThe frontend pods have \\nthe app=frontend label.\\nDefining hard-\\nrequirements for \\npod anti-affinity\\nA frontend pod must not \\nbe scheduled to the same \\nmachine as a pod with \\napp=frontend label.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'pod affinity',\n",
       "    'description': 'a scheduling strategy that places pods on nodes based on labels and selectors',\n",
       "    'category': 'network/application'},\n",
       "   {'entity': 'deployment',\n",
       "    'description': 'a Kubernetes object that manages the rollout of new versions of an application',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'replica',\n",
       "    'description': 'a number of identical pod instances running in a cluster',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'a lightweight and standalone execution environment for an application',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'label selector',\n",
       "    'description': 'a mechanism to select pods based on labels attached to them',\n",
       "    'category': 'network/application'},\n",
       "   {'entity': 'matchLabels',\n",
       "    'description': 'a key-value pair used to match labels on a pod',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'podAntiAffinity',\n",
       "    'description': 'a scheduling strategy that prevents pods from being scheduled on the same node',\n",
       "    'category': 'network/application'},\n",
       "   {'entity': 'requiredDuringSchedulingIgnoredDuringExecution',\n",
       "    'description': 'a property of pod anti-affinity that requires pods to be scheduled on different nodes',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'topologyKey',\n",
       "    'description': 'a key used to identify the topology of a node, such as kubernetes.io/hostname',\n",
       "    'category': 'hardware/network'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'a physical or virtual machine in a cluster that runs pods',\n",
       "    'category': 'hardware/computer'},\n",
       "   {'entity': 'scheduler',\n",
       "    'description': 'a component of the Kubernetes control plane that schedules pods on nodes',\n",
       "    'category': 'software/process'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'the command-line tool used to interact with a Kubernetes cluster',\n",
       "    'category': 'software/command'},\n",
       "   {'entity': 'get po',\n",
       "    'description': 'a kubectl command that lists pods in a cluster',\n",
       "    'category': 'software/command'}],\n",
       "  'relationships': '[{\"source_entity\": \"replica\", \"description\": \"defines the number of replicas for a deployment\", \"destination_entity\": \"deployment\"}, \\n {\"source_entity\": \"node\", \"description\": \"hosts a pod and provides resources to it\", \"destination_entity\": \"pod\"}, \\n {\"source_entity\": \"get po\", \"description\": \"commands the scheduler to list all pods with a specific label\", \"destination_entity\": \"scheduler\"}, \\n {\"source_entity\": \"podAntiAffinity\", \"description\": \"specifies that pods should not be scheduled on the same node\", \"destination_entity\": \"node\"}, \\n {\"source_entity\": \"matchLabels\", \"description\": \"defines which labels should match for pod affinity or anti-affinity\", \"destination_entity\": \"label selector\"}, \\n {\"source_entity\": \"requiredDuringSchedulingIgnoredDuringExecution\", \"description\": \" specifies a required during scheduling rule for pod anti-affinity\", \"destination_entity\": \"podAntiAffinity\"}, \\n {\"source_entity\": \"scheduler\", \"description\": \"assigns pods to nodes based on resource availability and affinity rules\", \"destination_entity\": \"node\"}, \\n {\"source_entity\": \"kubectl\", \"description\": \"commands the scheduler to perform an action (e.g., get po)\", \"destination_entity\": \"scheduler\"}, \\n {\"source_entity\": \"pod affinity\", \"description\": \"specifies that pods should be scheduled on the same node\", \"destination_entity\": \"node\"}, \\n {\"source_entity\": \"label selector\", \"description\": \"defines which labels should match for pod selection\", \"destination_entity\": \"deployment\"}, \\n {\"source_entity\": \"topologyKey\", \"description\": \" specifies a topology key for pod affinity or anti-affinity\", \"destination_entity\": \"podAntiAffinity\"}, \\n {\"source_entity\": \"deployment\", \"description\": \"manages the lifecycle of a set of replicas (pods)\", \"destination_entity\": \"replica\"}, \\n {\"source_entity\": \"container\", \"description\": \"runs an application within a pod\", \"destination_entity\": \"pod\"}]'},\n",
       " {'page': 508,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '476\\nCHAPTER 16\\nAdvanced scheduling\\n As with pod affinity, the topologyKey property determines the scope of where the\\npod shouldn’t be deployed to. You can use it to ensure pods aren’t deployed to the\\nsame rack, availability zone, region, or any custom scope you create using custom\\nnode labels.\\n16.4\\nSummary\\nIn this chapter, we looked at how to ensure pods aren’t scheduled to certain nodes or\\nare only scheduled to specific nodes, either because of the node’s labels or because of\\nthe pods running on them.\\n You learned that\\n\\uf0a1If you add a taint to a node, pods won’t be scheduled to that node unless they\\ntolerate that taint.\\n\\uf0a1Three types of taints exist: NoSchedule completely prevents scheduling, Prefer-\\nNoSchedule isn’t as strict, and NoExecute even evicts existing pods from a node.\\n\\uf0a1The NoExecute taint is also used to specify how long the Control Plane should\\nwait before rescheduling the pod when the node it runs on becomes unreach-\\nable or unready.\\n\\uf0a1Node affinity allows you to specify which nodes a pod should be scheduled to. It\\ncan be used to specify a hard requirement or to only express a node preference.\\n\\uf0a1Pod affinity is used to make the Scheduler deploy pods to the same node where\\nanother pod is running (based on the pod’s labels). \\n\\uf0a1Pod affinity’s topologyKey specifies how close the pod should be deployed to\\nthe other pod (onto the same node or onto a node in the same rack, availability\\nzone, or availability region).\\n\\uf0a1Pod anti-affinity can be used to keep certain pods away from each other. \\n\\uf0a1Both pod affinity and anti-affinity, like node affinity, can either specify hard\\nrequirements or preferences.\\nIn the next chapter, you’ll learn about best practices for developing apps and how to\\nmake them run smoothly in a Kubernetes environment.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'taint',\n",
       "    'description': 'a property that prevents scheduling of pods on a node unless they tolerate it',\n",
       "    'category': 'node'},\n",
       "   {'entity': 'NoSchedule',\n",
       "    'description': 'type of taint that completely prevents scheduling',\n",
       "    'category': 'taint'},\n",
       "   {'entity': 'Prefer-NoSchedule',\n",
       "    'description': \"type of taint that isn't as strict, only prefer not to schedule\",\n",
       "    'category': 'taint'},\n",
       "   {'entity': 'NoExecute',\n",
       "    'description': 'type of taint that evicts existing pods from a node and prevents scheduling',\n",
       "    'category': 'taint'},\n",
       "   {'entity': 'node affinity',\n",
       "    'description': 'property that specifies which nodes a pod should be scheduled to',\n",
       "    'category': 'pod'},\n",
       "   {'entity': 'hard requirement',\n",
       "    'description': 'type of node affinity that requires a pod to be scheduled on a specific node',\n",
       "    'category': 'node affinity'},\n",
       "   {'entity': 'preference',\n",
       "    'description': 'type of node affinity that expresses a preference for a pod to be scheduled on a specific node',\n",
       "    'category': 'node affinity'},\n",
       "   {'entity': 'pod affinity',\n",
       "    'description': 'property that makes the Scheduler deploy pods to the same node where another pod is running',\n",
       "    'category': 'pod'},\n",
       "   {'entity': 'topologyKey',\n",
       "    'description': 'property of pod affinity that specifies how close a pod should be deployed to other pods',\n",
       "    'category': 'pod affinity'},\n",
       "   {'entity': 'rack',\n",
       "    'description': \"a custom scope for topologyKey that ensures pods aren't deployed on the same rack\",\n",
       "    'category': 'scope'},\n",
       "   {'entity': 'availability zone',\n",
       "    'description': \"a custom scope for topologyKey that ensures pods aren't deployed in the same availability zone\",\n",
       "    'category': 'scope'},\n",
       "   {'entity': 'availability region',\n",
       "    'description': \"a custom scope for topologyKey that ensures pods aren't deployed in the same availability region\",\n",
       "    'category': 'scope'},\n",
       "   {'entity': 'Control Plane',\n",
       "    'description': 'the part of the Kubernetes system that makes scheduling decisions',\n",
       "    'category': 'scheduler'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes Scheduler\", \"description\": \"Prevents scheduling\", \"destination_entity\": \"NoSchedule taint\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"Won\\'t be scheduled unless they tolerate\", \"destination_entity\": \"Tainted node\"},\\n  {\"source_entity\": \"Control Plane\", \"description\": \"Wait before rescheduling pod\", \"destination_entity\": \"NoExecute taint\"},\\n  {\"source_entity\": \"Kubernetes Scheduler\", \"description\": \"Specify hard requirement or preference\", \"destination_entity\": \"Node affinity\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"Deploy to same node as another pod\", \"destination_entity\": \"Pod affinity\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"Deploy to close location (rack, AZ, AR)\", \"destination_entity\": \"topologyKey\"},\\n  {\"source_entity\": \"Kubernetes Scheduler\", \"description\": \"Keep certain pods away from each other\", \"destination_entity\": \"Pod anti-affinity\"},\\n  {\"source_entity\": \"Taint\", \"description\": \"Prevents or prefers scheduling\", \"destination_entity\": \"NoSchedule, Prefer-NoSchedule taints\"},\\n  {\"source_entity\": \"Control Plane\", \"description\": \"Specify wait time before rescheduling pod\", \"destination_entity\": \"NoExecute taint\"},\\n  {\"source_entity\": \"Node labels\", \"description\": \"Create custom scope for scheduling\", \"destination_entity\": \"topologyKey\"}\\n]\\n```\\n\\nNote: I\\'ve tried to match the entities with their respective actions in the document, but if any relations seem incorrect or incomplete, please let me know and I\\'ll be happy to adjust them.'},\n",
       " {'page': 509,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '477\\nBest practices\\nfor developing apps\\nWe’ve now covered most of what you need to know to run your apps in Kubernetes.\\nWe’ve explored what each individual resource does and how it’s used. Now we’ll see\\nhow to combine them in a typical application running on Kubernetes. We’ll also\\nlook at how to make an application run smoothly. After all, that’s the whole point\\nof using Kubernetes, isn’t it? \\n Hopefully, this chapter will help to clear up any misunderstandings and explain\\nthings that weren’t explained clearly yet. Along the way, we’ll also introduce a few\\nadditional concepts that haven’t been mentioned up to this point.\\nThis chapter covers\\n\\uf0a1Understanding which Kubernetes resources \\nappear in a typical application\\n\\uf0a1Adding post-start and pre-stop pod lifecycle hooks\\n\\uf0a1Properly terminating an app without breaking \\nclient requests\\n\\uf0a1Making apps easy to manage in Kubernetes\\n\\uf0a1Using init containers in a pod\\n\\uf0a1Developing locally with Minikube\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Pod Lifecycle Hooks',\n",
       "    'description': 'Mechanism for executing custom code before or after the creation of a pod',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Client Requests',\n",
       "    'description': 'Incoming requests from clients to an application',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Init Containers',\n",
       "    'description': 'Containers that run before the main containers in a pod',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'Tool for running Kubernetes locally on a single node',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes Resources',\n",
       "    'description': 'Components of the Kubernetes system, such as pods, services, and persistent volumes',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Applications',\n",
       "    'description': 'Software programs that run in Kubernetes ',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"manages\", \"destination_entity\": \"Applications\"},\\n  {\"source_entity\": \"Client Requests\", \"description\": \"interrupt\", \"destination_entity\": \"Application Termination\"},\\n  {\"source_entity\": \"Init Containers\", \"description\": \"initialize\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Pod Lifecycle Hooks\", \"description\": \"trigger\", \"destination_entity\": \"App Terminations\"},\\n  {\"source_entity\": \"Kubernetes Resources\", \"description\": \"utilize\", \"destination_entity\": \"Applications\"},\\n  {\"source_entity\": \"Minikube\", \"description\": \"develop\", \"destination_entity\": \"Applications locally\"},\\n  {\"source_entity\": \"Applications\", \"description\": \"require\", \"destination_entity\": \"Kubernetes Resources\"}\\n]\\n```'},\n",
       " {'page': 510,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '478\\nCHAPTER 17\\nBest practices for developing apps\\n17.1\\nBringing everything together\\nLet’s start by looking at what an actual application consists of. This will also give you a\\nchance to see if you remember everything you’ve learned so far and look at the big\\npicture. Figure 17.1 shows the Kubernetes components used in a typical application.\\nA typical application manifest contains one or more Deployment and/or StatefulSet\\nobjects. Those include a pod template containing one or more containers, with a live-\\nness probe for each of them and a readiness probe for the service(s) the container\\nprovides (if any). Pods that provide services to others are exposed through one or\\nmore Services. When they need to be reachable from outside the cluster, the Services\\nare either configured to be LoadBalancer or NodePort-type Services, or exposed\\nthrough an Ingress resource. \\n The pod templates (and the pods created from them) usually reference two types\\nof Secrets—those for pulling container images from private image registries and those\\nused directly by the process running inside the pods. The Secrets themselves are\\nusually not part of the application manifest, because they aren’t configured by the\\napplication developers but by the operations team. Secrets are usually assigned to\\nServiceAccounts, which are assigned to individual pods. \\nDeﬁned in the app manifest by the developer\\nPod template\\nDeployment\\nlabels\\nPod(s)\\nLabel selector\\nlabels\\nCreated automatically at runtime\\nCreated by a cluster admin beforehand\\nContainer(s)\\nVolume(s)\\nReplicaSet(s)\\nEndpoints\\n• Health probes\\n• Environment variables\\n• Volume mounts\\n• Resource reqs/limits\\nHorizontal\\nPodAutoscaler\\nStatefulSet\\nDaemonSet\\nJob\\nCronJob\\nPersistent\\nVolume\\nConﬁgMap\\nService\\nPersistent\\nVolume\\nClaim\\nSecret(s)\\nService\\naccount\\nStorage\\nClass\\nLimitRange\\nResourceQuota\\nIngress\\nimagePullSecret\\nFigure 17.1\\nResources in a typical application\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Defined in the app manifest by the developer\\nDeployment\\nPod template labels imagePullSe\\nContainer(s)\\nHorizontal • Health probes\\nPodAutoscaler • Environment variables\\n• Volume mounts\\n• Resource reqs/limits\\nStatefulSet\\nPersistent\\nVolume(s)\\nDaemonSet Volume\\nClaim\\nJob\\nConfigMap\\nCronJob\\nIngress Service  \\\n",
       "   0                                               None                                                                                                                                                                                                                                                                           \n",
       "   1                                               None                                                                                                                                                                                                                                                                           \n",
       "   2                                               None                                                                                                                                                                                                                                                                           \n",
       "   3                                               None                                                                                                                                                                                                                                                                           \n",
       "   4                                               None                                                                                                                                                                                                                                                                           \n",
       "   \n",
       "      Col1  Col2  \\\n",
       "   0  None         \n",
       "   1     e   cre   \n",
       "   2  None  None   \n",
       "   3  None         \n",
       "   4  None         \n",
       "   \n",
       "     Created by a cluster admin beforehand\\nService\\naccount\\nt\\nSecret(s)\\nLimitRange\\nStorage\\nClass ResourceQuota  \n",
       "   0                                               None                                                               \n",
       "   1                                                  t                                                               \n",
       "   2  Created automatically at runtime\\nPersistent\\n...                                                               \n",
       "   3                                               None                                                               \n",
       "   4                                               None                                                               ,\n",
       "     Deployment\\nPod template labels\\nContainer(s)\\n• Health probes\\n• Environment variables\\n• Volume mounts\\n• Resource reqs/limits\\nVolume(s)  \\\n",
       "   0                                               None                                                                                            \n",
       "   1                                               None                                                                                            \n",
       "   2                                               None                                                                                            \n",
       "   \n",
       "      Col1  Col2  Col3  \n",
       "   0        None        \n",
       "   1  None        None  \n",
       "   2  None        None  ],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': '',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'StatefulSet', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'Pods', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'Services', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'Ingress', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'Secrets', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'ServiceAccounts', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'Pod template',\n",
       "    'description': 'template used to create a pod',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Container(s)', 'description': '', 'category': 'container'},\n",
       "   {'entity': 'Volume(s)', 'description': '', 'category': 'storage'},\n",
       "   {'entity': 'ReplicaSet(s)', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'Endpoints', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'Health probes', 'description': '', 'category': 'process'},\n",
       "   {'entity': 'Environment variables',\n",
       "    'description': '',\n",
       "    'category': 'environment variable'},\n",
       "   {'entity': 'Volume mounts', 'description': '', 'category': 'storage'},\n",
       "   {'entity': 'Resource reqs/limits',\n",
       "    'description': '',\n",
       "    'category': 'resource management'},\n",
       "   {'entity': 'Horizontal PodAutoscaler',\n",
       "    'description': '',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Persistent Volume', 'description': '', 'category': 'storage'},\n",
       "   {'entity': 'ConfigMap', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'Service account storage Class',\n",
       "    'description': '',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'LimitRange',\n",
       "    'description': '',\n",
       "    'category': 'resource management'},\n",
       "   {'entity': 'ResourceQuota',\n",
       "    'description': '',\n",
       "    'category': 'resource management'},\n",
       "   {'entity': 'imagePullSecret', 'description': '', 'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Deployment\", \"description\": \"contains pod template with one or more containers and probes\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"ServiceAccounts\", \"description\": \"assigned to individual pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Secrets\", \"description\": \"reference by ServiceAccounts assigned to pods\", \"destination_entity\": \"ServiceAccounts\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"defines labels for pod(s)\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"creates ReplicaSet(s) and Endpoints\", \"destination_entity\": \"ReplicaSet(s)\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"creates Endpoints\", \"destination_entity\": \"Endpoints\"},\\n  {\"source_entity\": \"ServiceAccounts\", \"description\": \"reference by Service\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"Secrets\", \"description\": \"not part of the application manifest, but managed by operations team\", \"destination_entity\": \"Operations team\"},\\n  {\"source_entity\": \"Pod template\", \"description\": \"references imagePullSecret and other Secrets\", \"destination_entity\": \"imagePullSecret\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"defines liveness probe for each container\", \"destination_entity\": \"Container(s)\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"defines readiness probe for service(s)\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"Ingress\", \"description\": \"exposes Services to outside the cluster\", \"destination_entity\": \"Services\"},\\n  {\"source_entity\": \"Service\", \"description\": \"exposed through LoadBalancer or NodePort-type Services\", \"destination_entity\": \"LoadBalancer\"},\\n  {\"source_entity\": \"ResourceQuota\", \"description\": \"limits resource usage by pods and Deployments\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"LimitRange\", \"description\": \"defines limits for resource usage\", \"destination_entity\": \"Resources\"},\\n  {\"source_entity\": \"ConfigMap\", \"description\": \"stores configuration data\", \"destination_entity\": \"Applications\"},\\n  {\"source_entity\": \"StatefulSet\", \"description\": \"manages Persistent Volume and Claim\", \"destination_entity\": \"Persistent Volume Claim\"},\\n  {\"source_entity\": \"Horizontal PodAutoscaler\", \"description\": \"auto-scales the number of pods based on CPU usage\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Pod template\", \"description\": \"defines volume mounts for containers\", \"destination_entity\": \"Volume(s)\"},\\n  {\"source_entity\": \"Persistent Volume Claim\", \"description\": \"requests storage from Persistent Volume\", \"destination_entity\": \"Persistent Volume\"},\\n  {\"source_entity\": \"Resource reqs/limits\", \"description\": \"defined by Deployment and used by pods\", \"destination_entity\": \"Pods\"},\\n  {\"source_entity\": \"Health probes\", \"description\": \"used to monitor the health of containers\", \"destination_entity\": \"Container(s)\"},\\n  {\"source_entity\": \"Environment variables\", \"description\": \"set in pod templates and used by containers\", \"destination_entity\": \"Container(s)\"}\\n]\\n```'},\n",
       " {'page': 511,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '479\\nUnderstanding the pod’s lifecycle\\n The application also contains one or more ConfigMaps, which are either used to\\ninitialize environment variables or mounted as a configMap volume in the pod. Cer-\\ntain pods use additional volumes, such as an emptyDir or a gitRepo volume, whereas\\npods requiring persistent storage use persistentVolumeClaim volumes. The Persistent-\\nVolumeClaims are also part of the application manifest, whereas StorageClasses refer-\\nenced by them are created by system administrators upfront. \\n In certain cases, an application also requires the use of Jobs or CronJobs. Daemon-\\nSets aren’t normally part of application deployments, but are usually created by sysad-\\nmins to run system services on all or a subset of nodes. HorizontalPodAutoscalers\\nare either included in the manifest by the developers or added to the system later by\\nthe ops team. The cluster administrator also creates LimitRange and ResourceQuota\\nobjects to keep compute resource usage of individual pods and all the pods (as a\\nwhole) under control.\\n After the application is deployed, additional objects are created automatically by\\nthe various Kubernetes controllers. These include service Endpoints objects created\\nby the Endpoints controller, ReplicaSets created by the Deployment controller, and\\nthe actual pods created by the ReplicaSet (or Job, CronJob, StatefulSet, or DaemonSet)\\ncontrollers.\\n Resources are often labeled with one or more labels to keep them organized. This\\ndoesn’t apply only to pods but to all other resources as well. In addition to labels, most\\nresources also contain annotations that describe each resource, list the contact infor-\\nmation of the person or team responsible for it, or provide additional metadata for\\nmanagement and other tools. \\n At the center of all this is the Pod, which arguably is the most important Kuberne-\\ntes resource. After all, each of your applications runs inside it. To make sure you know\\nhow to develop apps that make the most out of their environment, let’s take one last\\nclose look at pods—this time from the application’s perspective. \\n17.2\\nUnderstanding the pod’s lifecycle\\nWe’ve said that pods can be compared to VMs dedicated to running only a single\\napplication. Although an application running inside a pod is not unlike an application\\nrunning in a VM, significant differences do exist. One example is that apps running in\\na pod can be killed any time, because Kubernetes needs to relocate the pod to\\nanother node for a reason or because of a scale-down request. We’ll explore this\\naspect next.\\n17.2.1 Applications must expect to be killed and relocated\\nOutside Kubernetes, apps running in VMs are seldom moved from one machine to\\nanother. When an operator moves the app, they can also reconfigure the app and\\nmanually check that the app is running fine in the new location. With Kubernetes,\\napps are relocated much more frequently and automatically—no human operator\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'a pod is a container that runs one or more application containers, it can be compared to VMs dedicated to running only a single application',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ConfigMaps',\n",
       "    'description': 'are used to initialize environment variables or mounted as a configMap volume in the pod',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'emptyDir',\n",
       "    'description': 'a type of volume that can be used by pods, it is not persistent and will be deleted when the pod is deleted',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'gitRepo',\n",
       "    'description': 'a type of volume that allows a pod to access a git repository',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'persistentVolumeClaim',\n",
       "    'description': 'a request for storage resources, it is used by pods that require persistent storage',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'StorageClass',\n",
       "    'description': 'a resource that defines the characteristics of a storage class, it is created by system administrators upfront',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'Job',\n",
       "    'description': 'a controller object that manages one-off tasks or batch jobs',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'CronJob',\n",
       "    'description': 'a controller object that schedules the execution of a job at specific intervals',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'DaemonSet',\n",
       "    'description': 'a controller object that ensures a certain number of replicas of a pod are running across all nodes in the cluster, it is usually created by system administrators to run system services on all or a subset of nodes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'HorizontalPodAutoscaler',\n",
       "    'description': 'a resource that automatically scales the number of replicas of a deployment based on CPU utilization or other metrics',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'LimitRange',\n",
       "    'description': 'a resource that sets limits on compute resources, such as memory and cpu, for pods in a namespace',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'ResourceQuota',\n",
       "    'description': 'a resource that sets quotas on compute resources, such as memory and cpu, for pods in a namespace',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Endpoints controller',\n",
       "    'description': 'a controller object that manages the service endpoints of a pod',\n",
       "    'category': 'controller'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'a controller object that ensures a certain number of replicas of a pod are running across all nodes in the cluster, it is created by the deployment controller',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'a resource that defines how to deploy an application to a cluster',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'a controller object that manages stateful applications, such as databases or key-value stores',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'a resource that exposes an application running in a pod to the outside world',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Label',\n",
       "    'description': 'a tag used to identify and select pods, it can be used to keep resources organized',\n",
       "    'category': 'metadata'},\n",
       "   {'entity': 'Annotation',\n",
       "    'description': 'additional metadata that can be attached to a resource to provide additional information or contact details',\n",
       "    'category': 'metadata'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"creates Persistent-VolumeClaims\", \"destination_entity\": \"persistentVolumeClaim\"},\\n  {\"source_entity\": \"system administrators\", \"description\": \"create StorageClasses referenced by Persistent-VolumeClaims\", \"destination_entity\": \"StorageClass\"},\\n  {\"source_entity\": \"application developers\", \"description\": \"include HorizontalPodAutoscalers in the manifest\", \"destination_entity\": \"HorizontalPodAutoscaler\"},\\n  {\"source_entity\": \"ops team\", \"description\": \"adds HorizontalPodAutoscalers to the system later\", \"destination_entity\": \"HorizontalPodAutoscaler\"},\\n  {\"source_entity\": \"cluster administrator\", \"description\": \"creates LimitRange and ResourceQuota objects\", \"destination_entity\": \"LimitRange\"},\\n  {\"source_entity\": \"cluster administrator\", \"description\": \"creates LimitRange and ResourceQuota objects\", \"destination_entity\": \"ResourceQuota\"},\\n  {\"source_entity\": \"various Kubernetes controllers\", \"description\": \"create service Endpoints objects\", \"destination_entity\": \"service Endpoints\"},\\n  {\"source_entity\": \"Deployment controller\", \"description\": \"creates ReplicaSets\", \"destination_entity\": \"ReplicaSet\"},\\n  {\"source_entity\": \"DaemonSet controller\", \"description\": \"creates DaemonSets\", \"destination_entity\": \"DaemonSet\"},\\n  {\"source_entity\": \"Job, CronJob, StatefulSet, or DaemonSet controllers\", \"description\": \"create actual pods\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"various Kubernetes controllers\", \"description\": \"create service Endpoints objects\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"applications\", \"description\": \"run inside Pods\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"killed apps running in a pod and relocates them to another node\", \"destination_entity\": \"application\"},\\n  {\"source_entity\": \"applications\", \"description\": \"are relocated automatically by Kubernetes\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"developers\", \"description\": \"use ConfigMaps to initialize environment variables or mount as configMap volume\", \"destination_entity\": \"ConfigMaps\"},\\n  {\"source_entity\": \"pods\", \"description\": \"use additional volumes such as emptyDir or gitRepo\", \"destination_entity\": \"emptyDir\"},\\n  {\"source_entity\": \"pods\", \"description\": \"require persistent storage and use persistentVolumeClaim volumes\", \"destination_entity\": \"persistentVolumeClaim\"},\\n  {\"source_entity\": \"applications\", \"description\": \"requires the use of Jobs or CronJobs\", \"destination_entity\": \"Job\"},\\n  {\"source_entity\": \"applications\", \"description\": \"requires the use of Jobs or CronJobs\", \"destination_entity\": \"CronJob\"}\\n]\\n```\\n\\nNote that some relations have been inferred based on the context and entities provided. Let me know if you\\'d like me to clarify any of these!'},\n",
       " {'page': 512,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '480\\nCHAPTER 17\\nBest practices for developing apps\\nreconfigures them and makes sure they still run properly after the move. This means\\napplication developers need to make sure their apps allow being moved relatively\\noften. \\nEXPECTING THE LOCAL IP AND HOSTNAME TO CHANGE\\nWhen a pod is killed and run elsewhere (technically, it’s a new pod instance replac-\\ning the old one; the pod isn’t relocated), it not only has a new IP address but also a\\nnew name and hostname. Most stateless apps can usually handle this without any\\nadverse effects, but stateful apps usually can’t. We’ve learned that stateful apps can\\nbe run through a StatefulSet, which ensures that when the app starts up on a new\\nnode after being rescheduled, it will still see the same host name and persistent state\\nas before. The pod’s IP will change nevertheless. Apps need to be prepared for that\\nto happen. The application developer therefore should never base membership in a\\nclustered app on the member’s IP address, and if basing it on the hostname, should\\nalways use a StatefulSet.\\nEXPECTING THE DATA WRITTEN TO DISK TO DISAPPEAR\\nAnother thing to keep in mind is that if the app writes data to disk, that data may not be\\navailable after the app is started inside a new pod, unless you mount persistent storage at\\nthe location the app is writing to. It should be clear this happens when the pod is\\nrescheduled, but files written to disk will disappear even in scenarios that don’t involve\\nany rescheduling. Even during the lifetime of a single pod, the files written to disk by\\nthe app running in the pod may disappear. Let me explain this with an example.\\n Imagine an app that has a long and computationally intensive initial startup proce-\\ndure. To help the app come up faster on subsequent startups, the developers make\\nthe app cache the results of the initial startup on disk (an example of this would be\\nthe scanning of all Java classes for annotations at startup and then writing the results\\nto an index file). Because apps in Kubernetes run in containers by default, these files\\nare written to the container’s filesystem. If the container is then restarted, they’re all\\nlost, because the new container starts off with a completely new writable layer (see fig-\\nure 17.2).\\n Don’t forget that individual containers may be restarted for several reasons, such\\nas because the process crashes, because the liveness probe returned a failure, or\\nbecause the node started running out of memory and the process was killed by the\\nOOMKiller. When this happens, the pod is still the same, but the container itself is\\ncompletely new. The Kubelet doesn’t run the same container again; it always creates a\\nnew container. \\nUSING VOLUMES TO PRESERVE DATA ACROSS CONTAINER RESTARTS\\nWhen its container is restarted, the app in the example will need to perform the\\nintensive startup procedure again. This may or may not be desired. To make sure data\\nlike this isn’t lost, you need to use at least a pod-scoped volume. Because volumes live\\nand die together with the pod, the new container will be able to reuse the data written\\nto the volume by the previous container (figure 17.3).\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A pod is a logical host in Kubernetes that can contain one or more containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Stateless app',\n",
       "    'description': 'An application that does not store data and can run without any adverse effects when its IP address changes.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'A Kubernetes resource that ensures pods in a stateful set have the same hostname and persistent storage across pod restarts or rescheduling.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Stateful app',\n",
       "    'description': 'An application that stores data and cannot run without any adverse effects when its IP address changes.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Persistent storage',\n",
       "    'description': 'A type of storage in Kubernetes that allows data to be preserved across pod restarts or rescheduling.',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'The component of the Kubernetes node that runs and manages containers within pods.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'OOMKiller',\n",
       "    'description': 'A process in Kubernetes that kills a container when the node starts running out of memory.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Liveness probe',\n",
       "    'description': 'A mechanism used by Kubernetes to determine whether a container is running correctly and needs to be restarted.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Volume',\n",
       "    'description': 'A shared resource in Kubernetes that can be accessed by multiple containers within the same pod.',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'Container restart',\n",
       "    'description': 'The process of restarting a container within a pod, which may result in data loss if not properly managed.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Pod-scoped volume',\n",
       "    'description': 'A type of volume that is shared across all containers within the same pod and can be used to preserve data during container restarts.',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'Hostname',\n",
       "    'description': 'The name assigned to a pod in Kubernetes, which can change when the pod is rescheduled or restarted.',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[{\"source_entity\":\"Application developer\",\"description\":\"must ensure apps allow being moved often\",\"destination_entity\":\"Application\"},\\n\\n{\"source_entity\":\"Pod\",\"description\":\"has a new IP address and hostname when killed and run elsewhere\",\"destination_entity\":\"Application\"},\\n\\n{\"source_entity\":\"Stateful app\",\"description\":\"can\\'t usually handle pod relocation without adverse effects\",\"destination_entity\":\"Pod relocation\"},\\n\\n{\"source_entity\":\"Kubelet\",\"description\":\"creates a new container when the process crashes or node runs out of memory\",\"destination_entity\":\"Container\" },\\n\\n{\"source_entity\":\"OOMKiller\",\"description\":\"kills processes that are using too much memory\",\"destination_entity\":\"Process\" },\\n\\n{\"source_entity\":\"Liveness probe\",\"description\":\"returns a failure if the process is not running correctly\",\"destination_entity\":\"Process\" },\\n\\n{\"source_entity\":\"StatefulSet\",\"description\":\"ensures the app sees the same host name and persistent state after being rescheduled\",\"destination_entity\":\"Application\" },\\n\\n{\"source_entity\":\"Pod-scoped volume\",\"description\":\"preserves data written to disk across container restarts\",\"destination_entity\":\"Data\" },\\n\\n{\"source_entity\":\"Volume\",\"description\":\"lives and dies together with the pod\",\"destination_entity\":\"Pod\" },\\n\\n{\"source_entity\":\"Persistent storage\",\"description\":\"must be mounted at the location where the app writes data\",\"destination_entity\":\"Application\" },\\n\\n{\"source_entity\":\"Stateless app\",\"description\":\"usually can handle pod relocation without adverse effects\",\"destination_entity\":\"Pod relocation\" },\\n\\n{\"source_entity\":\"Container restart\",\"description\":\"causes data written to disk by the previous container to disappear\",\"destination_entity\":\"Data\" }]\\n\\nNote: These relations are based on the given document and entities, and may not be exhaustive.'},\n",
       " {'page': 513,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '481\\nUnderstanding the pod’s lifecycle\\nContainer\\nProcess\\nWrites to\\nFilesystem\\nWritable layer\\nRead-only layer\\nRead-only layer\\nImage layers\\nContainer crashes\\nor is killed\\nPod\\nNew container\\nNew process\\nFilesystem\\nNew writable layer\\nRead-only layer\\nRead-only layer\\nImage layers\\nNew container started\\n(part of the same pod)\\nNew container\\nstarts with new\\nwriteable layer:\\nall ﬁles are lost\\nFigure 17.2\\nFiles written to the container’s filesystem are lost when the container is restarted.\\nContainer\\nProcess\\nWrites to\\nCan read\\nthe same ﬁles\\nFilesystem\\nvolumeMount\\nContainer crashes\\nor is killed\\nPod\\nNew container\\nNew process\\nFilesystem\\nvolumeMount\\nNew container started\\n(part of the same pod)\\nNew process can\\nuse data preserved\\nin the volume\\nVolume\\nFigure 17.3\\nUsing a volume to persist data across container restarts\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Container',\n",
       "    'description': 'A lightweight and standalone executable software package that includes everything needed to run an application.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Process',\n",
       "    'description': 'An instance of a program running on a computer.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Filesystem',\n",
       "    'description': 'A file system is a collection of files and directories on a disk or other storage device.',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': 'Writable layer',\n",
       "    'description': 'A file system layer that allows writes to be made to it.',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': 'Read-only layer',\n",
       "    'description': 'A file system layer that does not allow writes to be made to it.',\n",
       "    'category': 'filesystem'},\n",
       "   {'entity': 'Image layers',\n",
       "    'description': 'A collection of files and directories on a disk or other storage device that make up an image.',\n",
       "    'category': 'image'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes, consisting of one or more containers.',\n",
       "    'category': 'pod'},\n",
       "   {'entity': 'VolumeMount',\n",
       "    'description': 'A mechanism to mount a volume into a container.',\n",
       "    'category': 'volumemount'}],\n",
       "  'relationships': '[{\"source_entity\":\"Container\",\"description\":\"writes to\",\"destination_entity\":\"Filesystem\"},{\"source_entity\":\"Container\",\"description\":\"crashes or is killed\",\"destination_entity\":\"Pod\"},{\"source_entity\":\"Process\",\"description\":\"writes to\",\"destination_entity\":\"Filesystem\"},{\"source_entity\":\"Filesystem\",\"description\":\"can read the same files as\",\"destination_entity\":\"Process\"},{\"source_entity\":\"Container\",\"description\":\"starts with new writable layer, all files are lost\",\"destination_entity\":\"Filesystem\"},{\"source_entity\":\"Filesystem\",\"description\":\"persists data across container restarts using a \",\"destination_entity\":\"Volume\"},{\"source_entity\":\"Pod\",\"description\":\"contains multiple containers\",\"destination_entity\":\"Container\"},{\"source_entity\":\"Pod\",\"description\":\"contains multiple processes\",\"destination_entity\":\"Process\"},{\"source_entity\":\"Filesystem\",\"description\":\"is writable layer of\",\"destination_entity\":\"Container\"},{\"source_entity\":\"Image layers\",\"description\":\"are part of read-only layer of\",\"destination_entity\":\"Container\"}]'},\n",
       " {'page': 514,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '482\\nCHAPTER 17\\nBest practices for developing apps\\nUsing a volume to preserve files across container restarts is a great idea sometimes,\\nbut not always. What if the data gets corrupted and causes the newly created process\\nto crash again? This will result in a continuous crash loop (the pod will show the\\nCrashLoopBackOff status). If you hadn’t used a volume, the new container would start\\nfrom scratch and most likely not crash. Using volumes to preserve files across con-\\ntainer restarts like this is a double-edged sword. You need to think carefully about\\nwhether to use them or not.\\n17.2.2 Rescheduling of dead or partially dead pods\\nIf a pod’s container keeps crashing, the Kubelet will keep restarting it indefinitely.\\nThe time between restarts will be increased exponentially until it reaches five minutes.\\nDuring those five minute intervals, the pod is essentially dead, because its container’s\\nprocess isn’t running. To be fair, if it’s a multi-container pod, certain containers may\\nbe running normally, so the pod is only partially dead. But if a pod contains only a sin-\\ngle container, the pod is effectively dead and completely useless, because no process is\\nrunning in it anymore.\\n You may find it surprising to learn that such pods aren’t automatically removed\\nand rescheduled, even if they’re part of a ReplicaSet or similar controller. If you cre-\\nate a ReplicaSet with a desired replica count of three, and then one of the containers\\nin one of those pods starts crashing, Kubernetes will not delete and replace the pod.\\nThe end result is a ReplicaSet with only two properly running replicas instead of the\\ndesired three (figure 17.4).\\nYou’d probably expect the pod to be deleted and replaced with another pod instance\\nthat might run successfully on another node. After all, the container may be crashing\\nbecause of a node-related problem that doesn’t manifest itself on other nodes. Sadly,\\nthat isn’t the case. The ReplicaSet controller doesn’t care if the pods are dead—all it\\nReplicaSet\\nDesired replicas: 3\\nActual replicas: 3\\nOnly two pods are actually\\nperforming their jobs\\nThird pod’s status is Running,\\nbut its container keeps crashing,\\nwith signiﬁcant delays between\\nrestarts (CrashLoopBackOff)\\nWe want\\nthree pods\\nPod\\nRunning\\ncontainer\\nPod\\nRunning\\ncontainer\\nPod\\nDead\\ncontainer\\nFigure 17.4\\nA ReplicaSet controller doesn’t reschedule dead pods.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'Containerization platform',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Lightweight and portable container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'Agent that runs on each node in a cluster',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'CrashLoopBackOff',\n",
       "    'description': 'Status indicating continuous crashes of a pod',\n",
       "    'category': 'error'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'Controller that ensures a specified number of replicas are running',\n",
       "    'category': 'controller'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'Isolated and portable runtime environment',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'volume',\n",
       "    'description': 'Persistent storage for containers',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'node',\n",
       "    'description': 'A physical or virtual machine in a Kubernetes cluster',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'controller',\n",
       "    'description': 'Component that manages the state of a system',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"Kubelet\", \"description\": \"restarts container indefinitely until it reaches five minutes\", \"destination_entity\": \"pod\"}, {\"source_entity\": \"Kubelet\", \"description\": \"increases time between restarts exponentially\", \"destination_entity\": \"pod\"}, {\"source_entity\": \"container\", \"description\": \"crashes and causes pod to show CrashLoopBackOff status\", \"destination_entity\": \"pod\"}, {\"source_entity\": \"volume\", \"description\": \"preserves files across container restarts, but can cause continuous crash loop\", \"destination_entity\": \"pod\"}, {\"source_entity\": \"Kubelet\", \"description\": \"keeps restarting crashed container\", \"destination_entity\": \"container\"}, {\"source_entity\": \"controller\", \"description\": \"doesn\\'t reschedule dead pods\", \"destination_entity\": \"ReplicaSet\"}, {\"source_entity\": \"container\", \"description\": \"crashes due to node-related problem\", \"destination_entity\": \"node\"}, {\"source_entity\": \"Kubernetes\", \"description\": \"runs ReplicaSet with desired replica count of three\", \"destination_entity\": \"ReplicaSet\"}, {\"source_entity\": \"pod\", \"description\": \"shows CrashLoopBackOff status\", \"destination_entity\": \"container\"}]'},\n",
       " {'page': 515,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '483\\nUnderstanding the pod’s lifecycle\\ncares about is that the number of pods matches the desired replica count, which in\\nthis case, it does.\\n If you’d like to see for yourself, I’ve included a YAML manifest for a ReplicaSet\\nwhose pods will keep crashing (see file replicaset-crashingpods.yaml in the code\\narchive). If you create the ReplicaSet and inspect the pods that are created, the follow-\\ning listing is what you’ll see.\\n$ kubectl get po\\nNAME                  READY     STATUS             RESTARTS   AGE\\ncrashing-pods-f1tcd   0/1       CrashLoopBackOff   5          6m     \\ncrashing-pods-k7l6k   0/1       CrashLoopBackOff   5          6m\\ncrashing-pods-z7l3v   0/1       CrashLoopBackOff   5          6m\\n$ kubectl describe rs crashing-pods\\nName:           crashing-pods\\nReplicas:       3 current / 3 desired                       \\nPods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed      \\n$ kubectl describe po crashing-pods-f1tcd\\nName:           crashing-pods-f1tcd\\nNamespace:      default\\nNode:           minikube/192.168.99.102\\nStart Time:     Thu, 02 Mar 2017 14:02:23 +0100\\nLabels:         app=crashing-pods\\nStatus:         Running                      \\nIn a way, it’s understandable that Kubernetes behaves this way. The container will be\\nrestarted every five minutes in the hope that the underlying cause of the crash will be\\nresolved. The rationale is that rescheduling the pod to another node most likely\\nwouldn’t fix the problem anyway, because the app is running inside a container and\\nall the nodes should be mostly equivalent. That’s not always the case, but it is most of\\nthe time. \\n17.2.3 Starting pods in a specific order\\nOne other difference between apps running in pods and those managed manually is\\nthat the ops person deploying those apps knows about the dependencies between\\nthem. This allows them to start the apps in order. \\nUNDERSTANDING HOW PODS ARE STARTED\\nWhen you use Kubernetes to run your multi-pod applications, you don’t have a built-\\nin way to tell Kubernetes to run certain pods first and the rest only when the first pods\\nare already up and ready to serve. Sure, you could post the manifest for the first app\\nand then wait for the pod(s) to be ready before you post the second manifest, but your\\nListing 17.1\\nReplicaSet and pods that keep crashing\\nThe pod’s status shows the Kubelet is\\ndelaying the restart because the\\ncontainer keeps crashing.\\nNo action taken \\nby the controller, \\nbecause current \\nreplicas match \\ndesired replicas\\nThree \\nreplicas are \\nshown as \\nrunning.\\nkubectl describe \\nalso shows pod’s \\nstatus as running\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Pod',\n",
       "    'description': 'A lightweight and ephemeral clusterable unit that can run one or more containers',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'ReplicaSet',\n",
       "    'description': 'An object that ensures a specified number of replicas (identical copies) are running at any given time',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'The agent that runs on each node in a Kubernetes cluster and is responsible for starting, stopping, and managing containers',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'A process running in its own namespace, with its own memory, CPU, etc.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line interface for interacting with a Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'YAML manifest',\n",
       "    'description': 'A human-readable file format used to define the configuration of an object in Kubernetes',\n",
       "    'category': 'file format'},\n",
       "   {'entity': 'CrashLoopBackOff',\n",
       "    'description': \"An error condition that occurs when a pod's container crashes and the Kubelet delays restarting it due to repeated failures\",\n",
       "    'category': 'error'},\n",
       "   {'entity': 'Namespace',\n",
       "    'description': 'A logical grouping of resources in a Kubernetes cluster',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Node',\n",
       "    'description': 'A physical or virtual machine running the Kubelet and responsible for hosting containers',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n    {\"source_entity\": \"Kubernetes\", \"description\": \"behaves in a way that restarts the container every five minutes\", \"destination_entity\": \"container\"},\\n    {\"source_entity\": \"controller\", \"description\": \"does not take any action because current replicas match desired replicas\", \"destination_entity\": \"ReplicaSet\"},\\n    {\"source_entity\": \"Kubernetes\", \"description\": \"starts pods in a way that does not allow for specific order starting\", \"destination_entity\": \"pods\"},\\n    {\"source_entity\": \"ops person deploying apps\", \"description\": \"knows about dependencies between apps and starts them in order\", \"destination_entity\": \"apps\"},\\n    {\"source_entity\": \"Kubelet\", \"description\": \"delays the restart of pod because container keeps crashing\", \"destination_entity\": \"pod\"},\\n    {\"source_entity\": \"kubectl\", \"description\": \"gets the status of pods\", \"destination_entity\": \"pods\"},\\n    {\"source_entity\": \"kubectl\", \"description\": \"describes the ReplicaSet and shows its status as running\", \"destination_entity\": \"ReplicaSet\"},\\n    {\"source_entity\": \"kubectl\", \"description\": \"describes the pod and shows its status as running\", \"destination_entity\": \"pod\"},\\n    {\"source_entity\": \"YAML manifest\", \"description\": \"included in the code archive for a ReplicaSet whose pods will keep crashing\", \"destination_entity\": \"ReplicaSet\"},\\n    {\"source_entity\": \"minikube/192.168.99.102\", \"description\": \"is the Node where the pod is running\", \"destination_entity\": \"Node\"}\\n]\\n```\\n\\nNote: I\\'ve used the entities provided in the input list for the relations, even if they are not explicitly mentioned in the document page. If you want me to only extract relations between entities that are actually present in the document page, please let me know!'},\n",
       " {'page': 516,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '484\\nCHAPTER 17\\nBest practices for developing apps\\nwhole system is usually defined in a single YAML or JSON containing multiple Pods,\\nServices, and other objects. \\n The Kubernetes API server does process the objects in the YAML/JSON in the\\norder they’re listed, but this only means they’re written to etcd in that order. You have\\nno guarantee that pods will also be started in that order. \\n But you can prevent a pod’s main container from starting until a precondition is\\nmet. This is done by including an init containers in the pod. \\nINTRODUCING INIT CONTAINERS\\nIn addition to regular containers, pods can also include init containers. As the name\\nsuggests, they can be used to initialize the pod—this often means writing data to the\\npod’s volumes, which are then mounted into the pod’s main container(s).\\n A pod may have any number of init containers. They’re executed sequentially and\\nonly after the last one completes are the pod’s main containers started. This means\\ninit containers can also be used to delay the start of the pod’s main container(s)—for\\nexample, until a certain precondition is met. An init container could wait for a service\\nrequired by the pod’s main container to be up and ready. When it is, the init container\\nterminates and allows the main container(s) to be started. This way, the main con-\\ntainer wouldn’t use the service before it’s ready.\\n Let’s look at an example of a pod using an init container to delay the start of the\\nmain container. Remember the fortune pod you created in chapter 7? It’s a web\\nserver that returns a fortune quote as a response to client requests. Now, let’s imagine\\nyou have a fortune-client pod that requires the fortune Service to be up and run-\\nning before its main container starts. You can add an init container, which checks\\nwhether the Service is responding to requests. Until that’s the case, the init container\\nkeeps retrying. Once it gets a response, the init container terminates and lets the main\\ncontainer start.\\nADDING AN INIT CONTAINER TO A POD\\nInit containers can be defined in the pod spec like main containers but through the\\nspec.initContainers field. You’ll find the complete YAML for the fortune-client pod\\nin the book’s code archive. The following listing shows the part where the init con-\\ntainer is defined.\\nspec:\\n  initContainers:      \\n  - name: init\\n    image: busybox\\n    command:\\n    - sh\\n    - -c\\n    - \\'while true; do echo \"Waiting for fortune service to come up...\";  \\n    ➥ wget http://fortune -q -T 1 -O /dev/null >/dev/null 2>/dev/null   \\n    ➥ && break; sleep 1; done; echo \"Service is up! Starting main       \\n    ➥ container.\"\\'\\nListing 17.2\\nAn init container defined in a pod: fortune-client.yaml\\nYou’re defining \\nan init container, \\nnot a regular \\ncontainer.\\nThe init container runs a\\nloop that runs until the\\nfortune Service is up.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes API server',\n",
       "    'description': \"processes objects in YAML/JSON and writes them to etcd in order they're listed\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'distributed key-value store used by Kubernetes',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'basic execution unit in Kubernetes, usually defined in YAML/JSON with multiple objects',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'abstraction of a set of related applications or microservices in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'init containers',\n",
       "    'description': 'special type of container used to initialize pods and delay main container start',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'execution units in Kubernetes, usually defined in YAML/JSON with init containers and main containers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'pod spec',\n",
       "    'description': \"definition of a pod's configuration in YAML/JSON\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'spec.initContainers field',\n",
       "    'description': 'field in the pod spec that defines init containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'fortune-client pod',\n",
       "    'description': 'example pod that uses an init container to delay main container start until fortune Service is up',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'fortune Service',\n",
       "    'description': 'example Service in Kubernetes that provides a web server for fortune quotes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'busybox image',\n",
       "    'description': 'image used as an init container to delay main container start until fortune Service is up',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"fortune-client pod\", \"description\": \"uses an init container to delay start of main container until fortune Service is up and running\", \"destination_entity\": \"init containers\"},\\n  {\"source_entity\": \"pod spec\", \"description\": \"defines init containers like main containers but through the spec.initContainers field\", \"destination_entity\": \"spec.initContainers field\"},\\n  {\"source_entity\": \"Kubernetes API server\", \"description\": \"processes objects in YAML/JSON in order they\\'re listed and writes them to etcd in that order\", \"destination_entity\": \"etcd\"},\\n  {\"source_entity\": \"Kubernetes API server\", \"description\": \"does not guarantee that pods will be started in the same order as they\\'re listed\", \"destination_entity\": \"pods\"},\\n  {\"source_entity\": \"init containers\", \"description\": \"can prevent a pod\\'s main container from starting until a precondition is met\", \"destination_entity\": \"main containers\"},\\n  {\"source_entity\": \"busybox image\", \"description\": \"is used as the image for an init container\", \"destination_entity\": \"init container\"},\\n  {\"source_entity\": \"fortune Service\", \"description\": \"is checked by an init container to see if it\\'s up and running before main container starts\", \"destination_entity\": \"main container\"},\\n  {\"source_entity\": \"fortune-client pod\", \"description\": \"requires the fortune Service to be up and running before its main container starts\", \"destination_entity\": \"fortune Service\"},\\n  {\"source_entity\": \"init containers\", \"description\": \"can be used to delay start of a pod\\'s main container until certain precondition is met\", \"destination_entity\": \"main container\"}\\n]\\n```'},\n",
       " {'page': 517,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '485\\nUnderstanding the pod’s lifecycle\\nWhen you deploy this pod, only its init container is started. This is shown in the pod’s\\nstatus when you list pods with kubectl get:\\n$ kubectl get po\\nNAME             READY     STATUS     RESTARTS   AGE\\nfortune-client   0/1       Init:0/1   0          1m\\nThe STATUS column shows that zero of one init containers have finished. You can see\\nthe log of the init container with kubectl logs:\\n$ kubectl logs fortune-client -c init\\nWaiting for fortune service to come up...\\nWhen running the kubectl logs command, you need to specify the name of the init\\ncontainer with the -c switch (in the example, the name of the pod’s init container is\\ninit, as you can see in listing 17.2).\\n The main container won’t run until you deploy the fortune Service and the\\nfortune-server pod. You’ll find them in the fortune-server.yaml file. \\nBEST PRACTICES FOR HANDLING INTER-POD DEPENDENCIES\\nYou’ve seen how an init container can be used to delay starting the pod’s main con-\\ntainer(s) until a precondition is met (making sure the Service the pod depends on is\\nready, for example), but it’s much better to write apps that don’t require every service\\nthey rely on to be ready before the app starts up. After all, the service may also go\\noffline later, while the app is already running.\\n The application needs to handle internally the possibility that its dependencies\\naren’t ready. And don’t forget readiness probes. If an app can’t do its job because one\\nof its dependencies is missing, it should signal that through its readiness probe, so\\nKubernetes knows it, too, isn’t ready. You’ll want to do this not only because it pre-\\nvents the app from being added as a service endpoint, but also because the app’s read-\\niness is also used by the Deployment controller when performing a rolling update,\\nthereby preventing a rollout of a bad version. \\n17.2.4 Adding lifecycle hooks\\nWe’ve talked about how init containers can be used to hook into the startup of the\\npod, but pods also allow you to define two lifecycle hooks:\\n\\uf0a1Post-start hooks\\n\\uf0a1Pre-stop hooks\\nThese lifecycle hooks are specified per container, unlike init containers, which apply\\nto the whole pod. As their names suggest, they’re executed when the container starts\\nand before it stops. \\n Lifecycle hooks are similar to liveness and readiness probes in that they can either\\n\\uf0a1Execute a command inside the container\\n\\uf0a1Perform an HTTP GET request against a URL\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A pod is a logical host for one or more application containers',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Init container',\n",
       "    'description': 'A special type of container that runs before the main container(s)',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Main container',\n",
       "    'description': 'The primary container that runs an application',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'An abstraction layer that provides a network interface to access applications',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for interacting with Kubernetes',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'get po',\n",
       "    'description': 'A Kubernetes command to list pods',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'logs',\n",
       "    'description': 'A Kubernetes command to retrieve logs from a container',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'fortune-server.yaml',\n",
       "    'description': 'A configuration file for the fortune server pod',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Deployment controller',\n",
       "    'description': 'A component that manages rollouts and updates of deployments',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'readiness probe',\n",
       "    'description': 'A mechanism to check if an application is ready to serve traffic',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'lifecycle hooks',\n",
       "    'description': \"Functions executed at specific points in a container's lifecycle\",\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'post-start hook',\n",
       "    'description': 'A function executed when a container starts',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'pre-stop hook',\n",
       "    'description': 'A function executed before a container is terminated',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[{\"source_entity\": \"Deployment controller\", \"description\": \"uses readiness probe to prevent rollout of a bad version\", \"destination_entity\": \"readiness probe\"},{\"source_entity\": \"Deployment controller\", \"description\": \"knows when an app is ready\", \"destination_entity\": \"app\"},\\n\\n{\"source_entity\": \"fortune-client\", \"description\": \"has init container that waits for fortune service to come up\", \"destination_entity\": \"init container\"},\\n\\n{\"source_entity\": \"fortune-client\", \"description\": \"needs fortune Service and fortune-server pod to be running\", \"destination_entity\": \"Service\"},\\n\\n{\"source_entity\": \"fortune-client\", \"description\": \"can\\'t run main container until dependencies are met\", \"destination_entity\": \"main container\"},\\n\\n{\"source_entity\": \"app\", \"description\": \"should handle internally the possibility that its dependencies aren\\'t ready\", \"destination_entity\": \"dependencies\"},\\n\\n{\"source_entity\": \"Kubernetes\", \"description\": \"knows when a pod is not ready due to missing dependency\", \"destination_entity\": \"pod\"},\\n\\n{\"source_entity\": \"kubectl\", \"description\": \"can list pods and show their status\", \"destination_entity\": \"get po\"},\\n\\n{\"source_entity\": \"kubectl\", \"description\": \"can log init container output\", \"destination_entity\": \"logs\"},\\n\\n{\"source_entity\": \"fortune-server.yaml\", \"description\": \"defines fortune Service and fortune-server pod\", \"destination_entity\": \"Service\"},\\n\\n{\"source_entity\": \"post-start hook\", \"description\": \"is executed when a container starts\", \"destination_entity\": \"container\"},\\n\\n{\"source_entity\": \"pre-stop hook\", \"description\": \"is executed before a container is stopped\", \"destination_entity\": \"container\"},\\n\\n{\"source_entity\": \"lifecycle hooks\", \"description\": \"are specified per container and can execute commands or perform HTTP GET requests\", \"destination_entity\": \"container\"}]'},\n",
       " {'page': 518,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '486\\nCHAPTER 17\\nBest practices for developing apps\\nLet’s look at the two hooks individually to see what effect they have on the container\\nlifecycle.\\nUSING A POST-START CONTAINER LIFECYCLE HOOK\\nA post-start hook is executed immediately after the container’s main process is started.\\nYou use it to perform additional operations when the application starts. Sure, if you’re\\nthe author of the application running in the container, you can always perform those\\noperations inside the application code itself. But when you’re running an application\\ndeveloped by someone else, you mostly don’t want to (or can’t) modify its source\\ncode. Post-start hooks allow you to run additional commands without having to touch\\nthe app. These may signal to an external listener that the app is starting, or they may\\ninitialize the application so it can start doing its job.\\n The hook is run in parallel with the main process. The name might be somewhat\\nmisleading, because it doesn’t wait for the main process to start up fully (if the process\\nhas an initialization procedure, the Kubelet obviously can’t wait for the procedure to\\ncomplete, because it has no way of knowing when that is). \\n But even though the hook runs asynchronously, it does affect the container in two\\nways. Until the hook completes, the container will stay in the Waiting state with the\\nreason ContainerCreating. Because of this, the pod’s status will be Pending instead of\\nRunning. If the hook fails to run or returns a non-zero exit code, the main container\\nwill be killed. \\n A pod manifest containing a post-start hook looks like the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-with-poststart-hook\\nspec:\\n  containers:\\n  - image: luksa/kubia\\n    name: kubia\\n    lifecycle:          \\n      postStart:        \\n        exec:                                                               \\n          command:                                                          \\n          - sh                                                              \\n          - -c                                                              \\n          - \"echo \\'hook will fail with exit code 15\\'; sleep 5; exit 15\"     \\nIn the example, the echo, sleep, and exit commands are executed along with the\\ncontainer’s main process as soon as the container is created. Rather than run a com-\\nmand like this, you’d typically run a shell script or a binary executable file stored in\\nthe container image. \\n Sadly, if the process started by the hook logs to the standard output, you can’t see\\nthe output anywhere. This makes debugging lifecycle hooks painful. If the hook fails,\\nListing 17.3\\nA pod with a post-start lifecycle hook: post-start-hook.yaml\\nThe hook is executed as \\nthe container starts.\\nIt executes the\\npostStart.sh\\nscript in the /bin\\ndirectory inside\\nthe container.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Post-Start Hook',\n",
       "    'description': \"A hook executed immediately after the container's main process is started.\",\n",
       "    'category': 'Kubernetes Lifecycle'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'The unit of deployment in Kubernetes that runs an application.',\n",
       "    'category': 'Kubernetes Application'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A pod is the basic execution unit in a containerized system managed by Kubernetes.',\n",
       "    'category': 'Kubernetes'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'The agent that runs on each node in a cluster, responsible for starting and managing containers.',\n",
       "    'category': 'Kubernetes Daemon'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'A field in the pod manifest that specifies the API version of the resource.',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'A field in the pod manifest that specifies the type of resource being created.',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'A field in the pod manifest that contains metadata about the resource, such as its name and labels.',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'A field in the pod manifest that specifies the desired state of the resource.',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'A field in the pod manifest that lists the containers that make up the pod.',\n",
       "    'category': 'Kubernetes Resource'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'A field in the container specification that specifies the Docker image to use for the container.',\n",
       "    'category': 'Docker Image'},\n",
       "   {'entity': 'exec',\n",
       "    'description': \"A command in the pod manifest that executes a command in the container's main process.\",\n",
       "    'category': 'Kubernetes Lifecycle Hook'},\n",
       "   {'entity': 'command',\n",
       "    'description': 'A field in the exec specification that specifies the command to execute.',\n",
       "    'category': 'Docker Command'},\n",
       "   {'entity': 'sh',\n",
       "    'description': 'A shell used to execute commands in the container.',\n",
       "    'category': 'Unix Shell'},\n",
       "   {'entity': 'echo',\n",
       "    'description': 'A Unix command that outputs its arguments to the standard output.',\n",
       "    'category': 'Unix Command'},\n",
       "   {'entity': 'sleep',\n",
       "    'description': 'A Unix command that pauses for a specified amount of time.',\n",
       "    'category': 'Unix Command'},\n",
       "   {'entity': 'exit',\n",
       "    'description': 'A Unix command that exits the shell with a specified exit code.',\n",
       "    'category': 'Unix Command'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubelet\", \"description\": \"can\\'t wait for initialization procedure to complete\", \"destination_entity\": \"main process\"},\\n  {\"source_entity\": \"post-start hook\", \"description\": \"run additional commands without modifying app source code\", \"destination_entity\": \"application\"},\\n  {\"source_entity\": \"post-start hook\", \"description\": \"initialize application so it can start doing its job\", \"destination_entity\": \"application\"},\\n  {\"source_entity\": \"post-start hook\", \"description\": \"affect container in two ways\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"main process\", \"description\": \"stay in Waiting state with reason ContainerCreating\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"main process\", \"description\": \"will be killed if hook fails to run or returns non-zero exit code\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"echo command\", \"description\": \"log output which can\\'t be seen anywhere\", \"destination_entity\": \"standard output\"},\\n  {\"source_entity\": \"exec command\", \"description\": \"run shell script or binary executable file stored in container image\", \"destination_entity\": \"container image\"},\\n  {\"source_entity\": \"post-start hook\", \"description\": \"execute as container starts\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"sleep command\", \"description\": \"pause execution for a specified amount of time\", \"destination_entity\": \"process\"},\\n  {\"source_entity\": \"exit command\", \"description\": \"terminate process with specified exit code\", \"destination_entity\": \"process\"},\\n  {\"source_entity\": \"spec\", \"description\": \"contain container lifecycle hook configuration\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"containers\", \"description\": \"list of containers in a Pod\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"apiVersion\", \"description\": \"define version of API used by Kubernetes\", \"destination_entity\": \"Kubernetes\"},\\n  {\"source_entity\": \"kind\", \"description\": \"define type of resource being described\", \"destination_entity\": \"Pod\"}\\n]\\n```\\n\\nNote that I\\'ve kept the original text from the document page in mind while defining each relation.'},\n",
       " {'page': 519,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '487\\nUnderstanding the pod’s lifecycle\\nyou’ll only see a FailedPostStartHook warning among the pod’s events (you can see\\nthem using kubectl describe pod). A while later, you’ll see more information on why\\nthe hook failed, as shown in the following listing.\\nFailedSync   Error syncing pod, skipping: failed to \"StartContainer\" for \\n             \"kubia\" with PostStart handler: command \\'sh -c echo \\'hook \\n             will fail with exit code 15\\'; sleep 5 ; exit 15\\' exited \\n             with 15: : \"PostStart Hook Failed\" \\nThe number 15 in the last line is the exit code of the command. When using an HTTP\\nGET hook handler, the reason may look like the following listing (you can try this by\\ndeploying the post-start-hook-httpget.yaml file from the book’s code archive).\\nFailedSync   Error syncing pod, skipping: failed to \"StartContainer\" for \\n             \"kubia\" with PostStart handler: Get \\n             http://10.32.0.2:9090/postStart: dial tcp 10.32.0.2:9090: \\n             getsockopt: connection refused: \"PostStart Hook Failed\" \\nNOTE\\nThe post-start hook is intentionally misconfigured to use port 9090\\ninstead of the correct port 8080, to show what happens when the hook fails.\\nThe standard and error outputs of command-based post-start hooks aren’t logged any-\\nwhere, so you may want to have the process the hook invokes log to a file in the con-\\ntainer’s filesystem, which will allow you to examine the contents of the file with\\nsomething like this:\\n$ kubectl exec my-pod cat logfile.txt \\nIf the container gets restarted for whatever reason (including because the hook failed),\\nthe file may be gone before you can examine it. You can work around that by mount-\\ning an emptyDir volume into the container and having the hook write to it.\\nUSING A PRE-STOP CONTAINER LIFECYCLE HOOK\\nA pre-stop hook is executed immediately before a container is terminated. When a\\ncontainer needs to be terminated, the Kubelet will run the pre-stop hook, if config-\\nured, and only then send a SIGTERM to the process (and later kill the process if it\\ndoesn’t terminate gracefully). \\n A pre-stop hook can be used to initiate a graceful shutdown of the container, if it\\ndoesn’t shut down gracefully upon receipt of a SIGTERM signal. They can also be used\\nto perform arbitrary operations before shutdown without having to implement those\\noperations in the application itself (this is useful when you’re running a third-party\\napp, whose source code you don’t have access to and/or can’t modify). \\n Configuring a pre-stop hook in a pod manifest isn’t very different from adding a\\npost-start hook. The previous example showed a post-start hook that executes a com-\\nListing 17.4\\nPod’s events showing the exit code of the failed command-based hook\\nListing 17.5\\nPod’s events showing the reason why an HTTP GET hook failed\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'PostStart Hook',\n",
       "    'description': 'A command executed after a container starts',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'tool'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in Kubernetes',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'A process that runs on each node in a cluster and is responsible for running containers',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'SIGTERM',\n",
       "    'description': 'A signal sent to a process to terminate it',\n",
       "    'category': 'signal'},\n",
       "   {'entity': 'emptyDir',\n",
       "    'description': 'A volume that persists across restarts of a container',\n",
       "    'category': 'volume'},\n",
       "   {'entity': 'docker',\n",
       "    'description': 'A command-line tool for running containers',\n",
       "    'category': 'container runtime'},\n",
       "   {'entity': 'HTTP GET hook handler',\n",
       "    'description': 'A type of hook that executes an HTTP request',\n",
       "    'category': 'hook handler'},\n",
       "   {'entity': 'port 9090',\n",
       "    'description': 'An example port number used in a failed POST-start hook',\n",
       "    'category': 'network port'}],\n",
       "  'relationships': '[{\"source_entity\": \"HTTP GET hook handler\", \"description\": \"failed to execute due to connection refused\", \"destination_entity\": \"PostStart Hook\"},\\n {\"source_entity\": \"Kubelet\", \"description\": \"runs pre-stop hook and sends SIGTERM signal\", \"destination_entity\": \"container\"},\\n {\"source_entity\": \"kubectl\", \"description\": \"executes command to examine file contents\", \"destination_entity\": \"logfile.txt\"},\\n {\"source_entity\": \"PostStart Hook\", \"description\": \"failed to execute due to misconfigured port\", \"destination_entity\": \"port 9090\"},\\n {\"source_entity\": \"docker\", \"description\": \"used to examine file contents with kubectl exec\", \"destination_entity\": \"logfile.txt\"},\\n {\"source_entity\": \"pre-stop hook\", \"description\": \"executed immediately before container termination\", \"destination_entity\": \"container\"},\\n {\"source_entity\": \"kubectl\", \"description\": \"configures pre-stop hook in pod manifest\", \"destination_entity\": \"Pod\"},\\n {\"source_entity\": \"emptyDir\", \"description\": \"mounted into container to persist log file\", \"destination_entity\": \"container\"}]'},\n",
       " {'page': 520,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '488\\nCHAPTER 17\\nBest practices for developing apps\\nmand, so we’ll look at a pre-stop hook that performs an HTTP GET request now. The\\nfollowing listing shows how to define a pre-stop HTTP GET hook in a pod.\\n    lifecycle:\\n      preStop:            \\n        httpGet:          \\n          port: 8080          \\n          path: shutdown      \\nThe pre-stop hook defined in this listing performs an HTTP GET request to http:/\\n/\\nPOD_IP:8080/shutdown as soon as the Kubelet starts terminating the container.\\nApart from the port and path shown in the listing, you can also set the fields scheme\\n(HTTP or HTTPS) and host, as well as httpHeaders that should be sent in the\\nrequest. The host field defaults to the pod IP. Be sure not to set it to localhost,\\nbecause localhost would refer to the node, not the pod.\\n In contrast to the post-start hook, the container will be terminated regardless of\\nthe result of the hook—an error HTTP response code or a non-zero exit code when\\nusing a command-based hook will not prevent the container from being terminated.\\nIf the pre-stop hook fails, you’ll see a FailedPreStopHook warning event among the\\npod’s events, but because the pod is deleted soon afterward (after all, the pod’s dele-\\ntion is what triggered the pre-stop hook in the first place), you may not even notice\\nthat the pre-stop hook failed to run properly. \\nTIP\\nIf the successful completion of the pre-stop hook is critical to the proper\\noperation of your system, verify whether it’s being executed at all. I’ve wit-\\nnessed situations where the pre-stop hook didn’t run and the developer\\nwasn’t even aware of that.\\nUSING A PRE-STOP HOOK BECAUSE YOUR APP DOESN’T RECEIVE THE SIGTERM SIGNAL\\nMany developers make the mistake of defining a pre-stop hook solely to send a SIGTERM\\nsignal to their apps in the pre-stop hook. They do this because they don’t see their appli-\\ncation receive the SIGTERM signal sent by the Kubelet. The reason why the signal isn’t\\nreceived by the application isn’t because Kubernetes isn’t sending it, but because the sig-\\nnal isn’t being passed to the app process inside the container itself. If your container\\nimage is configured to run a shell, which in turn runs the app process, the signal may be\\neaten up by the shell itself, instead of being passed down to the child process.\\n In such cases, instead of adding a pre-stop hook to send the signal directly to your\\napp, the proper fix is to make sure the shell passes the signal to the app. This can be\\nachieved by handling the signal in the shell script running as the main container pro-\\ncess and then passing it on to the app. Or you could not configure the container image\\nto run a shell at all and instead run the application binary directly. You do this by using\\nthe exec form of ENTRYPOINT or CMD in the Dockerfile: ENTRYPOINT [\"/mybinary\"]\\ninstead of ENTRYPOINT /mybinary.\\nListing 17.6\\nA pre-stop hook YAML snippet: pre-stop-hook-httpget.yaml\\nThis is a pre-stop hook that \\nperforms an HTTP GET request.\\nThe request is sent to \\nhttp://POD_IP:8080/shutdown.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubelet',\n",
       "    'description': 'A program that runs on each node in a Kubernetes cluster and is responsible for starting and stopping containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'HTTP GET request',\n",
       "    'description': 'An HTTP method used to retrieve data from a server.',\n",
       "    'category': 'protocol'},\n",
       "   {'entity': 'Pod IP',\n",
       "    'description': 'The internal IP address of a pod in a Kubernetes cluster.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Pre-stop hook',\n",
       "    'description': 'A lifecycle hook that is executed before a container is terminated.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'POST-start hook',\n",
       "    'description': 'A lifecycle hook that is executed after a container has started.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Dockerfile',\n",
       "    'description': 'A text file that contains instructions for building a Docker image.',\n",
       "    'category': 'file format'},\n",
       "   {'entity': 'ENTRYPOINT',\n",
       "    'description': 'A command in a Dockerfile that specifies the default command to run when a container is started.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CMD',\n",
       "    'description': 'A command in a Dockerfile that specifies the default command to run when a container is started.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'SIGTERM signal',\n",
       "    'description': 'A signal sent by the Kubelet to indicate that a process should terminate.',\n",
       "    'category': 'signal'},\n",
       "   {'entity': 'shell script',\n",
       "    'description': 'A script written in shell language that is used to automate tasks.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes cluster',\n",
       "    'description': 'A group of machines (nodes) that run containerized applications and are managed by the Kubernetes system.',\n",
       "    'category': 'hardware/network'}],\n",
       "  'relationships': '[{\"source_entity\": \"Dockerfile\", \"description\": \"specifies how to run a shell or not\", \"destination_entity\": \"container image\"},\\n\\n {\"source_entity\": \"CMD\", \"description\": \"specifies the command to be executed\", \"destination_entity\": \"container process\"},\\n\\n {\"source_entity\": \"ENTRYPOINT\", \"description\": \"specifies the command to be executed at startup\", \"destination_entity\": \"container process\"},\\n\\n {\"source_entity\": \"Pre-stop hook\", \"description\": \"performs an HTTP GET request\", \"destination_entity\": \"Kubernetes cluster\"},\\n\\n {\"source_entity\": \"Kubelet\", \"description\": \"sends a SIGTERM signal\", \"destination_entity\": \"application process\"},\\n\\n {\"source_entity\": \"shell script\", \"description\": \"handles the signal and passes it to the app\", \"destination_entity\": \"application process\"},\\n\\n {\"source_entity\": \"Pod IP\", \"description\": \"defaults to the pod IP as the host for HTTP requests\", \"destination_entity\": \"HTTP request\"},\\n\\n {\"source_entity\": \"POST-start hook\", \"description\": \"runs after the container starts\", \"destination_entity\": \"container process\"},\\n\\n {\"source_entity\": \"SIGTERM signal\", \"description\": \"sent by Kubelet to terminate the application\", \"destination_entity\": \"application process\"},\\n\\n {\"source_entity\": \"Kubernetes cluster\", \"description\": \"manages a group of nodes as a single unit\", \"destination_entity\": \"node\"},\\n\\n {\"source_entity\": \"HTTP GET request\", \"description\": \"sent by pre-stop hook to shutdown the app\", \"destination_entity\": \"application process\"}]\\n\\nNote: I\\'ve extracted relations between entities and described them in a brief summary. Let me know if you\\'d like me to clarify or modify any of these relations!'},\n",
       " {'page': 521,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '489\\nUnderstanding the pod’s lifecycle\\n A container using the first form runs the mybinary executable as its main process,\\nwhereas the second form runs a shell as the main process with the mybinary process\\nexecuted as a child of the shell process.\\nUNDERSTANDING THAT LIFECYCLE HOOKS TARGET CONTAINERS, NOT PODS\\nAs a final thought on post-start and pre-stop hooks, let me emphasize that these lifecy-\\ncle hooks relate to containers, not pods. You shouldn’t use a pre-stop hook for run-\\nning actions that need to be performed when the pod is terminating. The reason is\\nthat the pre-stop hook gets called when the container is being terminated (most likely\\nbecause of a failed liveness probe). This may happen multiple times in the pod’s life-\\ntime, not only when the pod is in the process of being shut down. \\n17.2.5 Understanding pod shutdown\\nWe’ve touched on the subject of pod termination, so let’s explore this subject in more\\ndetail and go over exactly what happens during pod shutdown. This is important for\\nunderstanding how to cleanly shut down an application running in a pod.\\n Let’s start at the beginning. A pod’s shut-down is triggered by the deletion of the\\nPod object through the API server. Upon receiving an HTTP DELETE request, the\\nAPI server doesn’t delete the object yet, but only sets a deletionTimestamp field in it.\\nPods that have the deletionTimestamp field set are terminating. \\n Once the Kubelet notices the pod needs to be terminated, it starts terminating\\neach of the pod’s containers. It gives each container time to shut down gracefully, but\\nthe time is limited. That time is called the termination grace period and is configu-\\nrable per pod. The timer starts as soon as the termination process starts. Then the fol-\\nlowing sequence of events is performed:\\n1\\nRun the pre-stop hook, if one is configured, and wait for it to finish.\\n2\\nSend the SIGTERM signal to the main process of the container.\\n3\\nWait until the container shuts down cleanly or until the termination grace\\nperiod runs out.\\n4\\nForcibly kill the process with SIGKILL, if it hasn’t terminated gracefully yet.\\nThe sequence of events is illustrated in figure 17.5.\\nPre-stop hook process\\nTermination grace period\\nMain container process\\nContainer shutdown\\ninitiated\\nContainer killed\\nif still running\\nTime\\nSIGTERM\\nSIGKILL\\nFigure 17.5\\nThe container termination sequence\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Main container process Col1 Col2\n",
       "   0                                 ],\n",
       "  'entities': [{'entity': 'pod',\n",
       "    'description': 'A Kubernetes object that can contain one or more containers.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'A process running within a pod, such as an executable or shell process.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'lifecycle hook',\n",
       "    'description': 'A mechanism to run actions before and after container startup and shutdown.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pre-stop hook',\n",
       "    'description': 'A lifecycle hook that runs before a container is shut down, typically used for cleanup actions.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'post-start hook',\n",
       "    'description': 'A lifecycle hook that runs after a container is started, typically used for initialization actions.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'A component of the Kubernetes cluster that manages pods and containers.',\n",
       "    'category': 'daemon'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The central management component of a Kubernetes cluster that provides a RESTful API for managing resources.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'deletionTimestamp field',\n",
       "    'description': 'A field in the Pod object that indicates the pod is being terminated.',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'termination grace period',\n",
       "    'description': 'The time allowed for a container to shut down cleanly before it is forcibly killed.',\n",
       "    'category': 'timeout'},\n",
       "   {'entity': 'SIGTERM signal',\n",
       "    'description': 'A signal sent to a process to indicate that it should terminate gracefully.',\n",
       "    'category': 'signal'},\n",
       "   {'entity': 'SIGKILL signal',\n",
       "    'description': \"A signal sent to a process to immediately kill it, if it doesn't respond to SIGTERM.\",\n",
       "    'category': 'signal'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"API server\", \"description\": \"sets deletionTimestamp field in Pod object\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"terminates each container in the pod\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"pre-stop hook\", \"description\": \"runs and waits to finish before terminating container\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"sends SIGTERM signal to main process of container\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"waits for container to shut down cleanly or until termination grace period runs out\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"forcibly kills process with SIGKILL if it hasn\\'t terminated yet\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"API server\", \"description\": \"triggers pod\\'s shut-down by deleting Pod object\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"pre-stop hook\", \"description\": \"relates to containers, not pods\", \"destination_entity\": \"lifecycle hook\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"gives each container time to shut down gracefully during termination\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"API server\", \"description\": \"sets deletionTimestamp field in Pod object, indicating pod is terminating\", \"destination_entity\": \"Pod\"}\\n]\\n```'},\n",
       " {'page': 522,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '490\\nCHAPTER 17\\nBest practices for developing apps\\nSPECIFYING THE TERMINATION GRACE PERIOD\\nThe termination grace period can be configured in the pod spec by setting the spec.\\nterminationGracePeriodSeconds field. It defaults to 30, which means the pod’s con-\\ntainers will be given 30 seconds to terminate gracefully before they’re killed forcibly. \\nTIP\\nYou should set the grace period to long enough so your process can fin-\\nish cleaning up in that time. \\nThe grace period specified in the pod spec can also be overridden when deleting the\\npod like this:\\n$ kubectl delete po mypod --grace-period=5\\nThis will make the Kubelet wait five seconds for the pod to shut down cleanly. When\\nall the pod’s containers stop, the Kubelet notifies the API server and the Pod resource\\nis finally deleted. You can force the API server to delete the resource immediately,\\nwithout waiting for confirmation, by setting the grace period to zero and adding the\\n--force option like this:\\n$ kubectl delete po mypod --grace-period=0 --force\\nBe careful when using this option, especially with pods of a StatefulSet. The Stateful-\\nSet controller takes great care to never run two instances of the same pod at the same\\ntime (two pods with the same ordinal index and name and attached to the same\\nPersistentVolume). By force-deleting a pod, you’ll cause the controller to create a\\nreplacement pod without waiting for the containers of the deleted pod to shut\\ndown. In other words, two instances of the same pod might be running at the same\\ntime, which may cause your stateful cluster to malfunction. Only delete stateful pods\\nforcibly when you’re absolutely sure the pod isn’t running anymore or can’t talk to\\nthe other members of the cluster (you can be sure of this when you confirm that the\\nnode that hosted the pod has failed or has been disconnected from the network and\\ncan’t reconnect). \\n Now that you understand how containers are shut down, let’s look at it from the\\napplication’s perspective and go over how applications should handle the shutdown\\nprocedure.\\nIMPLEMENTING THE PROPER SHUTDOWN HANDLER IN YOUR APPLICATION\\nApplications should react to a SIGTERM signal by starting their shut-down procedure\\nand terminating when it finishes. Instead of handling the SIGTERM signal, the applica-\\ntion can be notified to shut down through a pre-stop hook. In both cases, the app\\nthen only has a fixed amount of time to terminate cleanly. \\n But what if you can’t predict how long the app will take to shut down cleanly? For\\nexample, imagine your app is a distributed data store. On scale-down, one of the pod\\ninstances will be deleted and therefore shut down. In the shut-down procedure, the\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'terminationGracePeriodSeconds',\n",
       "    'description': 'A field in the pod spec that specifies the termination grace period.',\n",
       "    'category': 'software/network'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A container in Kubernetes.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'spec.terminationGracePeriodSeconds',\n",
       "    'description': 'The default value of the termination grace period field in the pod spec.',\n",
       "    'category': 'software/network'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'A Kubernetes component that runs on each node and manages containers.',\n",
       "    'category': 'software/container'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The central authority in a Kubernetes cluster that manages resources.',\n",
       "    'category': 'software/network'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line tool for interacting with a Kubernetes cluster.',\n",
       "    'category': 'software/network/command'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'A Kubernetes resource that manages stateful applications.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'PersistentVolume',\n",
       "    'description': 'A Kubernetes resource that provides persistent storage for a pod.',\n",
       "    'category': 'database/storage'},\n",
       "   {'entity': 'SIGTERM',\n",
       "    'description': 'A signal sent to an application to shut down cleanly.',\n",
       "    'category': 'software/process/signal'},\n",
       "   {'entity': 'pre-stop hook',\n",
       "    'description': 'A mechanism in Kubernetes that allows an application to be notified to shut down.',\n",
       "    'category': 'software/network/hook'},\n",
       "   {'entity': 'distributed data store',\n",
       "    'description': 'An example of a distributed system that uses a pod instance as a data store.',\n",
       "    'category': 'application/system/database'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubelet\", \"description\": \"notifies the API server of a pod\\'s shutdown\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"forces the deletion of a resource without waiting for confirmation\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"StatefulSet controller\", \"description\": \"creates a replacement pod without waiting for the deleted pod to shut down\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"kills a pod\\'s containers when they fail to terminate cleanly within the grace period\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"StatefulSet controller\", \"description\": \"causes the cluster to malfunction by running two instances of the same pod at the same time\", \"destination_entity\": \"cluster\"},\\n  {\"source_entity\": \"application\", \"description\": \"reacts to a SIGTERM signal by starting its shut-down procedure and terminating when it finishes\", \"destination_entity\": \"SIGTERM\"},\\n  {\"source_entity\": \"pre-stop hook\", \"description\": \"notifies the app to shut down through a pre-stop hook\", \"destination_entity\": \"app\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"overwrites the pod spec\\'s termination grace period when deleting the pod\", \"destination_entity\": \"pod spec\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"waits for the pod to shut down cleanly before notifying the API server and deleting the resource\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"distributed data store\", \"description\": \"experiences issues on scale-down due to a distributed nature\", \"destination_entity\": \"scale-down\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"uses the --force option to delete a pod immediately without waiting for confirmation\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"StatefulSet controller\", \"description\": \"takes great care to never run two instances of the same pod at the same time\", \"destination_entity\": \"pod\"}\\n]\\n```'},\n",
       " {'page': 523,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '491\\nUnderstanding the pod’s lifecycle\\npod needs to migrate all its data to the remaining pods to make sure it’s not lost.\\nShould the pod start migrating the data upon receiving a termination signal (through\\neither the SIGTERM signal or through a pre-stop hook)? \\n Absolutely not! This is not recommended for at least the following two reasons:\\n\\uf0a1A container terminating doesn’t necessarily mean the whole pod is being\\nterminated.\\n\\uf0a1You have no guarantee the shut-down procedure will finish before the process\\nis killed.\\nThis second scenario doesn’t happen only when the grace period runs out before the\\napplication has finished shutting down gracefully, but also when the node running\\nthe pod fails in the middle of the container shut-down sequence. Even if the node\\nthen starts up again, the Kubelet will not restart the shut-down procedure (it won’t\\neven start up the container again). There are absolutely no guarantees that the pod\\nwill be allowed to complete its whole shut-down procedure.\\nREPLACING CRITICAL SHUT-DOWN PROCEDURES WITH DEDICATED SHUT-DOWN PROCEDURE PODS\\nHow do you ensure that a critical shut-down procedure that absolutely must run to\\ncompletion does run to completion (for example, to ensure that a pod’s data is\\nmigrated to other pods)?\\n One solution is for the app (upon receipt of a termination signal) to create a new\\nJob resource that would run a new pod, whose sole job is to migrate the deleted pod’s\\ndata to the remaining pods. But if you’ve been paying attention, you’ll know that you\\nhave no guarantee the app will indeed manage to create the Job object every single\\ntime. What if the node fails exactly when the app tries to do that? \\n The proper way to handle this problem is by having a dedicated, constantly run-\\nning pod that keeps checking for the existence of orphaned data. When this pod finds\\nthe orphaned data, it can migrate it to the remaining pods. Rather than a constantly\\nrunning pod, you can also use a CronJob resource and run the pod periodically. \\n You may think StatefulSets could help here, but they don’t. As you’ll remember,\\nscaling down a StatefulSet leaves PersistentVolumeClaims orphaned, leaving the data\\nstored on the PersistentVolume stranded. Yes, upon a subsequent scale-up, the Persistent-\\nVolume will be reattached to the new pod instance, but what if that scale-up never\\nhappens (or happens after a long time)? For this reason, you may want to run a\\ndata-migrating pod also when using StatefulSets (this scenario is shown in figure 17.6).\\nTo prevent the migration from occurring during an application upgrade, the data-\\nmigrating pod could be configured to wait a while to give the stateful pod time to\\ncome up again before performing the migration.\\n \\n \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'pod',\n",
       "    'description': 'A container-based runtime for running applications.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'SIGTERM signal',\n",
       "    'description': 'A termination signal sent to a process.',\n",
       "    'category': 'signal'},\n",
       "   {'entity': 'pre-stop hook',\n",
       "    'description': 'A script run before shutting down a pod.',\n",
       "    'category': 'hook'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'An agent that runs on each node and is responsible for starting and stopping containers.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Job resource',\n",
       "    'description': 'A Kubernetes resource used to create a batch job.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'CronJob resource',\n",
       "    'description': 'A Kubernetes resource used to schedule jobs.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'StatefulSet',\n",
       "    'description': 'A Kubernetes resource used to manage stateful applications.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'PersistentVolumeClaim',\n",
       "    'description': 'A request for storage resources in a cluster.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'pod instance',\n",
       "    'description': 'An individual pod running an application.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'data-migrating pod',\n",
       "    'description': 'A dedicated pod responsible for migrating data from one pod to another.',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"needs to migrate all its data to the remaining pods to make sure it\\'s not lost\",\\n    \"destination_entity\": \"remaining pods\"\\n  },\\n  {\\n    \"source_entity\": \"SIGTERM signal\",\\n    \"description\": \"should not be used by the pod to start migrating data upon receiving a termination signal\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"container terminating\",\\n    \"description\": \"does not necessarily mean the whole pod is being terminated\",\\n    \"destination_entity\": \"whole pod\"\\n  },\\n  {\\n    \"source_entity\": \"shut-down procedure\",\\n    \"description\": \"will not finish before the process is killed if the node fails in the middle of the container shut-down sequence\",\\n    \"destination_entity\": \"process\"\\n  },\\n  {\\n    \"source_entity\": \"Kubelet\",\\n    \"description\": \"will not restart the shut-down procedure even when the node starts up again\",\\n    \"destination_entity\": \"shut-down procedure\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"may not be allowed to complete its whole shut-down procedure\",\\n    \"destination_entity\": \"whole pod\"\\n  },\\n  {\\n    \"source_entity\": \"app (upon receipt of a termination signal)\",\\n    \"description\": \"should create a new Job resource that would run a new pod to migrate the deleted pod\\'s data\",\\n    \"destination_entity\": \"Job resource\"\\n  },\\n  {\\n    \"source_entity\": \"node\",\\n    \"description\": \"may fail exactly when the app tries to create the Job object\",\\n    \"destination_entity\": \"app (upon receipt of a termination signal)\"\\n  },\\n  {\\n    \"source_entity\": \"dedicated pod\",\\n    \"description\": \"should keep checking for the existence of orphaned data\",\\n    \"destination_entity\": \"orphaned data\"\\n  },\\n  {\\n    \"source_entity\": \"data-migrating pod\",\\n    \"description\": \"can migrate the orphaned data to the remaining pods\",\\n    \"destination_entity\": \"remaining pods\"\\n  },\\n  {\\n    \"source_entity\": \"StatefulSet\",\\n    \"description\": \"does not help with preventing migration during an application upgrade\",\\n    \"destination_entity\": \"data-migrating pod\"\\n  },\\n  {\\n    \"source_entity\": \"data-migrating pod\",\\n    \"description\": \"could be configured to wait a while to give the stateful pod time to come up again before performing the migration\",\\n    \"destination_entity\": \"stateful pod\"\\n  },\\n  {\\n    \"source_entity\": \"pre-stop hook\",\\n    \"description\": \"does not necessarily mean the whole pod is being terminated when used\",\\n    \"destination_entity\": \"whole pod\"\\n  }\\n]\\n```\\n\\nNote that some of these relations may seem obvious or implicit, but they have been extracted according to the rules provided.'},\n",
       " {'page': 524,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '492\\nCHAPTER 17\\nBest practices for developing apps\\n17.3\\nEnsuring all client requests are handled properly\\nYou now have a good sense of how to make pods shut down cleanly. Now, we’ll look at\\nthe pod’s lifecycle from the perspective of the pod’s clients (clients consuming the ser-\\nvice the pod is providing). This is important to understand if you don’t want clients to\\nrun into problems when you scale pods up or down.\\n It goes without saying that you want all client requests to be handled properly. You\\nobviously don’t want to see broken connections when pods are starting up or shutting\\ndown. By itself, Kubernetes doesn’t prevent this from happening. Your app needs to\\nfollow a few rules to prevent broken connections. First, let’s focus on making sure all\\nconnections are handled properly when the pod starts up.\\n17.3.1 Preventing broken client connections when a pod is starting up\\nEnsuring each connection is handled properly at pod startup is simple if you under-\\nstand how Services and service Endpoints work. When a pod is started, it’s added as an\\nendpoint to all the Services, whose label selector matches the pod’s labels. As you may\\nremember from chapter 5, the pod also needs to signal to Kubernetes that it’s ready.\\nUntil it is, it won’t become a service endpoint and therefore won’t receive any requests\\nfrom clients. \\n If you don’t specify a readiness probe in your pod spec, the pod is always considered\\nready. It will start receiving requests almost immediately—as soon as the first kube-proxy\\nupdates the iptables rules on its node and the first client pod tries to connect to the\\nservice. If your app isn’t ready to accept connections by then, clients will see “connec-\\ntion refused” types of errors.\\n All you need to do is make sure that your readiness probe returns success only\\nwhen your app is ready to properly handle incoming requests. A good first step is to\\nadd an HTTP GET readiness probe and point it to the base URL of your app. In many\\nPod\\nA-0\\nPod\\nA-1\\nStatefulSet A\\nReplicas: 2\\nScale\\ndown\\nPVC\\nA-0\\nPV\\nPVC\\nA-1\\nPV\\nPod\\nA-0\\nStatefulSet A\\nReplicas: 1\\nTransfers data to\\nremaining pod(s)\\nConnects to\\norphaned PVC\\nData-migrating\\nPod\\nJob\\nPVC\\nA-0\\nPV\\nPVC\\nA-1\\nPV\\nFigure 17.6\\nUsing a dedicated pod to migrate data \\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  PV PV PV PV\\nConnects to\\norphaned PVC\\nPVC PVC PVC PVC\\nA-0 A-1 A-0 A-1\\nTransfers data to\\nremaining pod(s)\\nPod Pod Pod Data-migrating\\nA-0 A-1 A-0 Pod\\nScale\\ndown\\nStatefulSet A StatefulSet A\\nJob\\nReplicas: 2 Replicas: 1  \\\n",
       "   0                         StatefulSet A\\nReplicas: 2                                                                                                                                                                                   \n",
       "   \n",
       "     Col1  \n",
       "   0  Job  ],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': '',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Services', 'description': '', 'category': 'Software'},\n",
       "   {'entity': 'service Endpoints', 'description': '', 'category': 'Software'},\n",
       "   {'entity': 'Pods', 'description': '', 'category': 'Container'},\n",
       "   {'entity': 'Readiness Probe', 'description': '', 'category': 'Software'},\n",
       "   {'entity': 'HTTP GET readiness probe',\n",
       "    'description': '',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'kube-proxy', 'description': '', 'category': 'Software'},\n",
       "   {'entity': 'iptables rules', 'description': '', 'category': 'Software'},\n",
       "   {'entity': 'StatefulSet', 'description': '', 'category': 'Software'},\n",
       "   {'entity': 'Replicas', 'description': '', 'category': 'Software'},\n",
       "   {'entity': 'PVC', 'description': '', 'category': 'Software'},\n",
       "   {'entity': 'PV', 'description': '', 'category': 'Software'},\n",
       "   {'entity': 'Pod A-0', 'description': '', 'category': 'Container'},\n",
       "   {'entity': 'Pod A-1', 'description': '', 'category': 'Container'},\n",
       "   {'entity': 'Job', 'description': '', 'category': 'Software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Pod A-0\",\\n    \"description\": \"is added as an endpoint to all Services whose label selector matches its labels\",\\n    \"destination_entity\": \"Services\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"doesn\\'t prevent broken connections when pods are starting up or shutting down\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Your app\",\\n    \"description\": \"needs to follow rules to prevent broken connections when pods are scaling up or down\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Pod A-0\",\\n    \"description\": \"will start receiving requests immediately if no readiness probe is specified in its pod spec\",\\n    \"destination_entity\": \"Clients\"\\n  },\\n  {\\n    \"source_entity\": \"Your app\",\\n    \"description\": \"should add an HTTP GET readiness probe to return success only when it\\'s ready to handle incoming requests\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Pod A-0\",\\n    \"description\": \"will become a service endpoint and receive requests from clients if it signals to Kubernetes that it\\'s ready\",\\n    \"destination_entity\": \"Service Endpoints\"\\n  },\\n  {\\n    \"source_entity\": \"kube-proxy\",\\n    \"description\": \"updates iptables rules on its node when the first client pod tries to connect to the service\",\\n    \"destination_entity\": \"Pod A-0\"\\n  },\\n  {\\n    \"source_entity\": \"Pod A-1\",\\n    \"description\": \"will be orphaned if its PVC is transferred to another PV\",\\n    \"destination_entity\": \"PVC and PV\"\\n  },\\n  {\\n    \"source_entity\": \"Data-migrating Pod\",\\n    \"description\": \"transfers data to remaining pod(s) when scaling down StatefulSet A\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Connects to orphaned PVC\",\\n    \"description\": \"the Job will connect to the orphaned PVC after its PV has been transferred to another node\",\\n    \"destination_entity\": \"PVC and Job\"\\n  }\\n]'},\n",
       " {'page': 525,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '493\\nEnsuring all client requests are handled properly\\ncases that gets you far enough and saves you from having to implement a special read-\\niness endpoint in your app. \\n17.3.2 Preventing broken connections during pod shut-down\\nNow let’s see what happens at the other end of a pod’s life—when the pod is deleted and\\nits containers are terminated. We’ve already talked about how the pod’s containers\\nshould start shutting down cleanly as soon they receive the SIGTERM signal (or when its\\npre-stop hook is executed). But does that ensure all client requests are handled properly? \\n How should the app behave when it receives a termination signal? Should it con-\\ntinue to accept requests? What about requests that have already been received but\\nhaven’t completed yet? What about persistent HTTP connections, which may be in\\nbetween requests, but are open (when no active request exists on the connection)?\\nBefore we can answer those questions, we need to take a detailed look at the chain of\\nevents that unfolds across the cluster when a Pod is deleted. \\nUNDERSTANDING THE SEQUENCE OF EVENTS OCCURRING AT POD DELETION\\nIn chapter 11 we took an in-depth look at what components make up a Kubernetes clus-\\nter. You need to always keep in mind that those components run as separate processes on\\nmultiple machines. They aren’t all part of a single big monolithic process. It takes time\\nfor all the components to be on the same page regarding the state of the cluster. Let’s\\nexplore this fact by looking at what happens across the cluster when a Pod is deleted.\\n When a request for a pod deletion is received by the API server, it first modifies the\\nstate in etcd and then notifies its watchers of the deletion. Among those watchers are\\nthe Kubelet and the Endpoints controller. The two sequences of events, which happen\\nin parallel (marked with either A or B), are shown in figure 17.7.\\nA2. Stop\\ncontainers\\nAPI server\\nkube-proxy\\nKubelet\\nWorker node\\nEndpoints\\ncontroller\\nkube-proxy\\nPod\\n(containers)\\nClient\\nDelete\\npod\\nB1. Pod deletion\\nnotiﬁcation\\nB2. Remove pod\\nas endpoint\\nA1. Pod deletion\\nnotiﬁcation\\nB3. Endpoint\\nmodiﬁcation\\nnotiﬁcation\\nB4. Remove pod\\nfrom iptables\\nB4. Remove pod\\nfrom iptables\\niptables\\niptables\\nWorker node\\nFigure 17.7\\nSequence of events that occurs when a Pod is deleted\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'client requests',\n",
       "    'description': 'requests from clients to the server',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'SIGTERM signal',\n",
       "    'description': 'signal sent to a process to terminate it cleanly',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pre-stop hook',\n",
       "    'description': 'a function called before a container is stopped or terminated',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'a Kubernetes object that represents a set of one or more containers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'the central component of the Kubernetes control plane that exposes the API',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'a distributed key-value store used by Kubernetes to store its state',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'an agent that runs on each worker node and is responsible for managing the running containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Endpoints controller',\n",
       "    'description': 'a component that maintains information about the network endpoints for pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kube-proxy',\n",
       "    'description': 'a networking component that provides load balancing and connectivity services for pods',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'iptables',\n",
       "    'description': 'a Linux utility used to set up and maintain network packet filtering rules',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[{\"source_entity\":\"API server\",\"description\":\"modifies state in etcd and notifies watchers of deletion\",\"destination_entity\":\"etcd\"},{\"source_entity\":\"API server\",\"description\":\"notifies its watchers of deletion\",\"destination_entity\":\"Kubelet\"},{\"source_entity\":\"API server\",\"description\":\"notifies its watchers of deletion\",\"destination_entity\":\"Endpoints controller\"},{\"source_entity\":\"Kubelet\",\"description\":\"receives notification and removes Pod as endpoint\",\"destination_entity\":\"Pod\"},{\"source_entity\":\"Kubelet\",\"description\":\"stops containers\",\"destination_entity\":\"containers\"},{\"source_entity\":\"Kubelet\",\"description\":\"removes pod from iptables\",\"destination_entity\":\"iptables\"},{\"source_entity\":\"Endpoints controller\",\"description\":\"modifies endpoints and notifies its watchers of deletion\",\"destination_entity\":\"API server\"},{\"source_entity\":\"Endpoints controller\",\"description\":\"notifies Kubelet of Pod deletion notification\",\"destination_entity\":\"Kubelet\"},{\"source_entity\":\"SIGTERM signal\",\"description\":\"is received by containers to shut down cleanly\",\"destination_entity\":\"containers\"},{\"source_entity\":\"pre-stop hook\",\"description\":\"is executed before shutting down containers\",\"destination_entity\":\"containers\"},{\"source_entity\":\"client requests\",\"description\":\"are handled properly when Pod is deleted\",\"destination_entity\":\"Pod\"},{\"source_entity\":\"Pod\",\"description\":\"receives SIGTERM signal and its containers shut down cleanly\",\"destination_entity\":\"containers\"}]\\n\\nNote: I\\'ve only included the entities provided in the list as destination_entities, even if they were mentioned multiple times in the document.'},\n",
       " {'page': 526,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '494\\nCHAPTER 17\\nBest practices for developing apps\\nIn the A sequence of events, you’ll see that as soon as the Kubelet receives the notifica-\\ntion that the pod should be terminated, it initiates the shutdown sequence as explained\\nin section 17.2.5 (run the pre-stop hook, send SIGTERM, wait for a period of time, and\\nthen forcibly kill the container if it hasn’t yet terminated on its own). If the app\\nresponds to the SIGTERM by immediately ceasing to receive client requests, any client\\ntrying to connect to it will receive a Connection Refused error. The time it takes for\\nthis to happen from the time the pod is deleted is relatively short because of the direct\\npath from the API server to the Kubelet.\\n Now, let’s look at what happens in the other sequence of events—the one leading\\nup to the pod being removed from the iptables rules (sequence B in the figure).\\nWhen the Endpoints controller (which runs in the Controller Manager in the Kuber-\\nnetes Control Plane) receives the notification of the Pod being deleted, it removes\\nthe pod as an endpoint in all services that the pod is a part of. It does this by modify-\\ning the Endpoints API object by sending a REST request to the API server. The API\\nserver then notifies all clients watching the Endpoints object. Among those watchers\\nare all the kube-proxies running on the worker nodes. Each of these proxies then\\nupdates the iptables rules on its node, which is what prevents new connections\\nfrom being forwarded to the terminating pod. An important detail here is that\\nremoving the iptables rules has no effect on existing connections—clients who are\\nalready connected to the pod will still send additional requests to the pod through\\nthose existing connections.\\n Both of these sequences of events happen in parallel. Most likely, the time it takes\\nto shut down the app’s process in the pod is slightly shorter than the time required for\\nthe iptables rules to be updated. The chain of events that leads to iptables rules\\nbeing updated is considerably longer (see figure 17.8), because the event must first\\nreach the Endpoints controller, which then sends a new request to the API server, and\\nA2. Send\\nSIGTERM\\nAPI server\\nAPI server\\nKubelet\\nEndpoints\\ncontroller\\nContainer(s)\\nA1. Watch\\nnotiﬁcation\\n(pod modiﬁed)\\nB1. Watch\\nnotiﬁcation\\n(pod modiﬁed)\\nB2. Remove pod’s IP\\nfrom endpoints\\nkube-proxy\\nB4. Update\\niptables\\nrules\\niptables\\nkube-proxy\\niptables\\nTime\\nB3. Watch notiﬁcation\\n(endpoints changed)\\nFigure 17.8\\nTimeline of events when pod is deleted\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubelet',\n",
       "    'description': 'The agent that runs on each node in a Kubernetes cluster, responsible for running and managing containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'SIGTERM',\n",
       "    'description': 'A signal sent to a process to terminate its execution cleanly.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'The central component of the Kubernetes control plane, responsible for storing and retrieving data from the cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'The agent that runs on each node in a Kubernetes cluster, responsible for running and managing containers.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A collection of one or more containers with shared resources, networking, and storage.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Endpoints controller',\n",
       "    'description': 'The component that manages the endpoints API object, responsible for updating iptables rules when a pod is deleted.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Iptables',\n",
       "    'description': 'A firewall and network packet filtering system in Linux.',\n",
       "    'category': 'hardware/network'},\n",
       "   {'entity': 'Kube-proxy',\n",
       "    'description': 'A component that runs on each node, responsible for managing iptables rules and forwarding connections to pods.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'The central component of the Kubernetes control plane, responsible for running controllers and managing the cluster state.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'REST request',\n",
       "    'description': 'A type of HTTP request used to send data from a client to a server.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Client',\n",
       "    'description': 'An application or service that sends requests to a server.',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\":\"Kubelet\",\"description\":\"receives notification to terminate a pod\",\"destination_entity\":\"Pod\"},{\"source_entity\":\"Kubelet\",\"description\":\"initiates shutdown sequence by running pre-stop hook, sending SIGTERM, and waiting for termination\",\"destination_entity\":\"Container(s)\"},{\"source_entity\":\"Client\",\"description\":\"receives Connection Refused error when pod is deleted\",\"destination_entity\":\"Pod\"},{\"source_entity\":\"Endpoints controller\",\"description\":\"removes pod as an endpoint in all services that the pod is a part of\",\"destination_entity\":\"Services\"},{\"source_entity\":\"Endpoints controller\",\"description\":\"sends REST request to API server to update Endpoints object\",\"destination_entity\":\"API server\"},{\"source_entity\":\"API server\",\"description\":\"notifies all clients watching the Endpoints object\",\"destination_entity\":\"Clients\"},{\"source_entity\":\"Kube-proxy\",\"description\":\"updates iptables rules on its node to prevent new connections to terminating pod\",\"destination_entity\":\"Iptables\"},{\"source_entity\":\"Controller Manager\",\"description\":\"runs Endpoints controller in the Kubernetes Control Plane\",\"destination_entity\":\"Kubernetes Control Plane\"},{\"source_entity\":\"API server\",\"description\":\"notifies Kubelet to terminate pod\",\"destination_entity\":\"Kubelet\"},{\"source_entity\":\"SIGTERM\",\"description\":\"signal sent by Kubelet to Container(s) to initiate shutdown sequence\",\"destination_entity\":\"Container(s)\"}]'},\n",
       " {'page': 527,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '495\\nEnsuring all client requests are handled properly\\nthen the API server must notify the kube-proxy before the proxy finally modifies the\\niptables rules. A high probability exists that the SIGTERM signal will be sent well\\nbefore the iptables rules are updated on all nodes.\\n The end result is that the pod may still receive client requests after it was sent the\\ntermination signal. If the app closes the server socket and stops accepting connections\\nimmediately, this will cause clients to receive “Connection Refused” types of errors\\n(similar to what happens at pod startup if your app isn’t capable of accepting connec-\\ntions immediately and you don’t define a readiness probe for it). \\nSOLVING THE PROBLEM\\nGoogling solutions to this problem makes it seem as though adding a readiness probe\\nto your pod will solve the problem. Supposedly, all you need to do is make the readi-\\nness probe start failing as soon as the pod receives the SIGTERM. This is supposed to\\ncause the pod to be removed as the endpoint of the service. But the removal would\\nhappen only after the readiness probe fails for a few consecutive times (this is configu-\\nrable in the readiness probe spec). And, obviously, the removal then still needs to\\nreach the kube-proxy before the pod is removed from iptables rules. \\n In reality, the readiness probe has absolutely no bearing on the whole process at\\nall. The Endpoints controller removes the pod from the service Endpoints as soon as\\nit receives notice of the pod being deleted (when the deletionTimestamp field in the\\npod’s spec is no longer null). From that point on, the result of the readiness probe\\nis irrelevant.\\n What’s the proper solution to the problem? How can you make sure all requests\\nare handled fully?\\n It’s clear the pod needs to keep accepting connections even after it receives the ter-\\nmination signal up until all the kube-proxies have finished updating the iptables\\nrules. Well, it’s not only the kube-proxies. There may also be Ingress controllers or\\nload balancers forwarding connections to the pod directly, without going through the\\nService (iptables). This also includes clients using client-side load-balancing. To\\nensure none of the clients experience broken connections, you’d have to wait until all\\nof them somehow notify you they’ll no longer forward connections to the pod. \\n That’s impossible, because all those components are distributed across many dif-\\nferent computers. Even if you knew the location of every one of them and could wait\\nuntil all of them say it’s okay to shut down the pod, what do you do if one of them\\ndoesn’t respond? How long do you wait for the response? Remember, during that\\ntime, you’re holding up the shut-down process. \\n The only reasonable thing you can do is wait for a long-enough time to ensure all\\nthe proxies have done their job. But how long is long enough? A few seconds should\\nbe enough in most situations, but there’s no guarantee it will suffice every time. When\\nthe API server or the Endpoints controller is overloaded, it may take longer for the\\nnotification to reach the kube-proxy. It’s important to understand that you can’t solve\\nthe problem perfectly, but even adding a 5- or 10-second delay should improve the\\nuser experience considerably. You can use a longer delay, but don’t go overboard,\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'SIGTERM signal',\n",
       "    'description': 'A signal sent to a process to terminate it.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kube-proxy',\n",
       "    'description': 'A Kubernetes component that modifies iptables rules for pod communication.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'iptables rules',\n",
       "    'description': 'Rules used by the kernel to filter and redirect network traffic.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'readiness probe',\n",
       "    'description': 'A Kubernetes feature that checks if a pod is ready to receive traffic.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Endpoints controller',\n",
       "    'description': 'A Kubernetes component responsible for managing service Endpoints.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'termination signal',\n",
       "    'description': 'A signal sent to a process to terminate it.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'client requests',\n",
       "    'description': 'Requests made by clients to a server.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'A Kubernetes component that provides the API for interacting with the cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Ingress controllers',\n",
       "    'description': 'Components responsible for managing incoming HTTP requests.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'load balancers',\n",
       "    'description': 'Components that distribute traffic across multiple servers.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'client-side load-balancing',\n",
       "    'description': 'A technique used by clients to balance their own traffic across multiple servers.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'iptables',\n",
       "    'description': 'A Linux utility for managing network packet filtering rules.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Endpoints controller\",\\n    \"description\": \"removes pod from service Endpoints as soon as it receives notice of the pod being deleted\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"kube-proxy\",\\n    \"description\": \"updates iptables rules\",\\n    \"destination_entity\": \"iptables rules\"\\n  },\\n  {\\n    \"source_entity\": \"SIGTERM signal\",\\n    \"description\": \"sent to pod before it closes server socket and stops accepting connections\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"readiness probe\",\\n    \"description\": \"has no bearing on the process of removing pod from service Endpoints\",\\n    \"destination_entity\": \"Endpoints controller\"\\n  },\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"notifies kube-proxy before it modifies iptables rules\",\\n    \"destination_entity\": \"kube-proxy\"\\n  },\\n  {\\n    \"source_entity\": \"iptables rules\",\\n    \"description\": \"updated on all nodes after kube-proxy receives notification from API server\",\\n    \"destination_entity\": \"all nodes\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"may still receive client requests after it was sent the termination signal\",\\n    \"destination_entity\": \"client requests\"\\n  },\\n  {\\n    \"source_entity\": \"readiness probe\",\\n    \"description\": \"supposedly solves the problem by causing pod to be removed as endpoint of service\",\\n    \"destination_entity\": \"service\"\\n  },\\n  {\\n    \"source_entity\": \"client-side load-balancing\",\\n    \"description\": \"forwards connections to pod directly, without going through Service (iptables)\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"load balancers\",\\n    \"description\": \"forwards connections to pod directly, without going through Service (iptables)\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"Ingress controllers\",\\n    \"description\": \"forward connections to pod directly, without going through Service (iptables)\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"client requests\",\\n    \"description\": \"may still be received by pod after it was sent the termination signal\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"termination signal\",\\n    \"description\": \"sent to pod before it closes server socket and stops accepting connections\",\\n    \"destination_entity\": \"pod\"\\n  }\\n]\\n```'},\n",
       " {'page': 528,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '496\\nCHAPTER 17\\nBest practices for developing apps\\nbecause the delay will prevent the container from shutting down promptly and will\\ncause the pod to be shown in lists long after it has been deleted, which is always frus-\\ntrating to the user deleting the pod.\\nWRAPPING UP THIS SECTION\\nTo recap—properly shutting down an application includes these steps:\\n\\uf0a1Wait for a few seconds, then stop accepting new connections. \\n\\uf0a1Close all keep-alive connections not in the middle of a request.\\n\\uf0a1Wait for all active requests to finish.\\n\\uf0a1Then shut down completely.\\nTo understand what’s happening with the connections and requests during this pro-\\ncess, examine figure 17.9 carefully.\\nNot as simple as exiting the process immediately upon receiving the termination sig-\\nnal, right? Is it worth going through all this? That’s for you to decide. But the least you\\ncan do is add a pre-stop hook that waits a few seconds, like the one in the following\\nlisting, perhaps.\\n    lifecycle:                    \\n      preStop:                    \\n        exec:                     \\n          command:                \\n          - sh\\n          - -c\\n          - \"sleep 5\"\\nListing 17.7\\nA pre-stop hook for preventing broken connections\\nDelay (few seconds)\\nKey:\\nConnection\\nRequest\\niptables rules\\nupdated on all nodes\\n(no new connections\\nafter this point)\\nStop\\naccepting new\\nconnections\\nClose inactive\\nkeep-alive\\nconnections\\nand wait for\\nactive requests\\nto ﬁnish\\nWhen last\\nactive request\\ncompletes,\\nshut down\\ncompletely\\nTime\\nSIGTERM\\nFigure 17.9\\nProperly handling existing and new connections after receiving a termination signal\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'container',\n",
       "    'description': 'A container used for running an application.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A pod that includes one or more containers.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'lifecycle',\n",
       "    'description': 'The process of shutting down an application.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'preStop hook',\n",
       "    'description': 'A command executed before shutting down a container.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'sleep',\n",
       "    'description': 'A command used to pause the execution of a script for a specified time.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'SIGTERM',\n",
       "    'description': 'A signal sent to an application to indicate it should terminate.',\n",
       "    'category': 'signal'},\n",
       "   {'entity': 'connection',\n",
       "    'description': 'A connection between a client and server.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'request',\n",
       "    'description': 'An HTTP request made by a client to a server.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'iptables rules',\n",
       "    'description': 'Rules used to manage network traffic on Linux systems.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'termination signal',\n",
       "    'description': 'A signal sent to an application to indicate it should terminate.',\n",
       "    'category': 'signal'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"lifecycle\",\\n    \"description\": \"waits for a few seconds before stopping new connections\",\\n    \"destination_entity\": \"connection\"\\n  },\\n  {\\n    \"source_entity\": \"SIGTERM\",\\n    \"description\": \"initiates termination signal to stop application\",\\n    \"destination_entity\": \"application\"\\n  },\\n  {\\n    \"source_entity\": \"preStop hook\",\\n    \"description\": \"waits a few seconds before shutting down container\",\\n    \"destination_entity\": \"container\"\\n  },\\n  {\\n    \"source_entity\": \"lifecycle\",\\n    \"description\": \"closes inactive keep-alive connections and waits for active requests to finish\",\\n    \"destination_entity\": \"connection\"\\n  },\\n  {\\n    \"source_entity\": \"lifecycle\",\\n    \"description\": \"shuts down completely after last active request completes\",\\n    \"destination_entity\": \"application\"\\n  },\\n  {\\n    \"source_entity\": \"container\",\\n    \"description\": \"is prevented from shutting down promptly due to delay in terminating pod\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"preStop hook\",\\n    \"description\": \"waits a few seconds before updating iptables rules on all nodes\",\\n    \"destination_entity\": \"iptables rules\"\\n  },\\n  {\\n    \"source_entity\": \"lifecycle\",\\n    \"description\": \"stops accepting new connections after receiving termination signal\",\\n    \"destination_entity\": \"connection\"\\n  },\\n  {\\n    \"source_entity\": \"container\",\\n    \"description\": \"is shown in lists long after it has been deleted due to delay in terminating pod\",\\n    \"destination_entity\": \"pod\"\\n  }\\n]\\n```'},\n",
       " {'page': 529,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '497\\nMaking your apps easy to run and manage in Kubernetes\\nThis way, you don’t need to modify the code of your app at all. If your app already\\nensures all in-flight requests are processed completely, this pre-stop delay may be all\\nyou need.\\n17.4\\nMaking your apps easy to run and manage in Kubernetes\\nI hope you now have a better sense of how to make your apps handle clients nicely.\\nNow we’ll look at other aspects of how an app should be built to make it easier to man-\\nage in Kubernetes.\\n17.4.1 Making manageable container images\\nWhen you package your app into an image, you can choose to include the app’s\\nbinary executable and any additional libraries it needs, or you can package up a whole\\nOS filesystem along with the app. Way too many people do this, even though it’s usu-\\nally unnecessary.\\n Do you need every single file from an OS distribution in your image? Probably not.\\nMost of the files will never be used and will make your image larger than it needs to\\nbe. Sure, the layering of images makes sure each individual layer is downloaded only\\nonce, but even having to wait longer than necessary the first time a pod is scheduled\\nto a node is undesirable.\\n Deploying new pods and scaling them should be fast. This demands having small\\nimages without unnecessary cruft. If you’re building apps using the Go language, your\\nimages don’t need to include anything else apart from the app’s single binary execut-\\nable file. This makes Go-based container images extremely small and perfect for\\nKubernetes.\\nTIP\\nUse the FROM scratch directive in the Dockerfile for these images.\\nBut in practice, you’ll soon see these minimal images are extremely difficult to debug.\\nThe first time you need to run a tool such as ping, dig, curl, or something similar\\ninside the container, you’ll realize how important it is for container images to also\\ninclude at least a limited set of these tools. I can’t tell you what to include and what\\nnot to include in your images, because it depends on how you do things, so you’ll\\nneed to find the sweet spot yourself.\\n17.4.2 Properly tagging your images and using imagePullPolicy wisely\\nYou’ll also soon learn that referring to the latest image tag in your pod manifests will\\ncause problems, because you can’t tell which version of the image each individual pod\\nreplica is running. Even if initially all your pod replicas run the same image version, if\\nyou push a new version of the image under the latest tag, and then pods are resched-\\nuled (or you scale up your Deployment), the new pods will run the new version,\\nwhereas the old ones will still be running the old one. Also, using the latest tag\\nmakes it impossible to roll back to a previous version (unless you push the old version\\nof the image again).\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'app',\n",
       "    'description': 'Application running in Kubernetes',\n",
       "    'category': 'Software/Application'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'Lightweight and stand-alone executable binary package',\n",
       "    'category': 'Software/Container'},\n",
       "   {'entity': 'Dockerfile',\n",
       "    'description': 'Text file that contains commands for building a Docker image',\n",
       "    'category': 'Software/Framework'},\n",
       "   {'entity': 'FROM scratch directive',\n",
       "    'description': 'Directive in Dockerfile to create an empty container image',\n",
       "    'category': 'Software/Framework'},\n",
       "   {'entity': 'ping',\n",
       "    'description': 'Networking diagnostic tool',\n",
       "    'category': 'Hardware/Tool'},\n",
       "   {'entity': 'dig',\n",
       "    'description': 'Domain Information Groper tool for DNS lookups',\n",
       "    'category': 'Hardware/Tool'},\n",
       "   {'entity': 'curl',\n",
       "    'description': 'Command-line tool and library for transferring data to/from a web server using URL syntax',\n",
       "    'category': 'Software/Tool'},\n",
       "   {'entity': 'imagePullPolicy',\n",
       "    'description': 'Kubernetes configuration setting for controlling the pulling of container images',\n",
       "    'category': 'Software/Setting'},\n",
       "   {'entity': 'latest tag',\n",
       "    'description': 'Docker image tag that refers to the most recent version of an image',\n",
       "    'category': 'Software/Tag'},\n",
       "   {'entity': 'pod manifests',\n",
       "    'description': 'Kubernetes configuration files for defining and managing pods',\n",
       "    'category': 'Software/File'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"app\", \"description\": \"include binary executable and additional libraries\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"app\", \"description\": \"package up OS filesystem along with the app\", \"destination_entity\": \"image\"},\\n  {\"source_entity\": \"FROM scratch directive\", \"description\": \"use in Dockerfile for minimal images\", \"destination_entity\": \"Dockerfile\"},\\n  {\"source_entity\": \"app\", \"description\": \"need to include tools such as ping, dig, curl\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"latest tag\", \"description\": \"referencing the latest image tag causes problems\", \"destination_entity\": \"pod manifests\"},\\n  {\"source_entity\": \"imagePullPolicy\", \"description\": \"using wisely is important for managing images\", \"destination_entity\": \"Kubernetes\"},\\n  {\"source_entity\": \"app\", \"description\": \"handle clients nicely by making manageable container images\", \"destination_entity\": \"Kubernetes\"},\\n  {\"source_entity\": \"Dockerfile\", \"description\": \"use the FROM scratch directive to create minimal images\", \"destination_entity\": \"image\"},\\n  {\"source_entity\": \"ping\", \"description\": \"need to include tools such as ping inside the container\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"dig\", \"description\": \"need to include tools such as dig inside the container\", \"destination_entity\": \"container\"},\\n  {\"source_entity\": \"curl\", \"description\": \"need to include tools such as curl inside the container\", \"destination_entity\": \"container\"}\\n]\\n```'},\n",
       " {'page': 530,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '498\\nCHAPTER 17\\nBest practices for developing apps\\n It’s almost mandatory to use tags containing a proper version designator instead\\nof latest, except maybe in development. Keep in mind that if you use mutable tags\\n(you push changes to the same tag), you’ll need to set the imagePullPolicy field in\\nthe pod spec to Always. But if you use that in production pods, be aware of the big\\ncaveat associated with it. If the image pull policy is set to Always, the container run-\\ntime will contact the image registry every time a new pod is deployed. This slows\\ndown pod startup a bit, because the node needs to check if the image has been mod-\\nified. Worse yet, this policy prevents the pod from starting up when the registry can-\\nnot be contacted.\\n17.4.3 Using multi-dimensional instead of single-dimensional labels\\nDon’t forget to label all your resources, not only Pods. Make sure you add multiple\\nlabels to each resource, so they can be selected across each individual dimension. You\\n(or the ops team) will be grateful you did it when the number of resources increases.\\n Labels may include things like\\n\\uf0a1The name of the application (or perhaps microservice) the resource belongs to\\n\\uf0a1Application tier (front-end, back-end, and so on)\\n\\uf0a1Environment (development, QA, staging, production, and so on)\\n\\uf0a1Version\\n\\uf0a1Type of release (stable, canary, green or blue for green/blue deployments, and\\nso on)\\n\\uf0a1Tenant (if you’re running separate pods for each tenant instead of using name-\\nspaces)\\n\\uf0a1Shard for sharded systems\\nThis will allow you to manage resources in groups instead of individually and make it\\neasy to see where each resource belongs.\\n17.4.4 Describing each resource through annotations\\nTo add additional information to your resources use annotations. At the least,\\nresources should contain an annotation describing the resource and an annotation\\nwith contact information of the person responsible for it. \\n In a microservices architecture, pods could contain an annotation that lists the\\nnames of the other services the pod is using. This makes it possible to show dependen-\\ncies between pods. Other annotations could include build and version information\\nand metadata used by tooling or graphical user interfaces (icon names, and so on).\\n Both labels and annotations make managing running applications much easier, but\\nnothing is worse than when an application starts crashing and you don’t know why.\\n17.4.5 Providing information on why the process terminated\\nNothing is more frustrating than having to figure out why a container terminated\\n(or is even terminating continuously), especially if it happens at the worst possible\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'tags',\n",
       "    'description': 'Tags containing a proper version designator instead of latest',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'imagePullPolicy',\n",
       "    'description': 'Field in pod spec to set Always for mutable tags',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'registry',\n",
       "    'description': 'Image registry that needs to be contacted by container runtime',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Resource that needs to start up with correct image pull policy',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'Multi-dimensional labels for resources like Pods, including application name, tier, environment, version, release type, tenant, and shard',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'annotations',\n",
       "    'description': 'Additional information added to resources through annotations, including resource description, contact information, build and version info, metadata used by tooling or graphical user interfaces',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pod spec',\n",
       "    'description': 'Specification for pods that needs to include imagePullPolicy field',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'container runtime',\n",
       "    'description': 'Runtime that contacts the image registry every time a new pod is deployed',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'image registry',\n",
       "    'description': 'Repository where container images are stored and need to be pulled by container runtime',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system that manages resources, including pods and labels',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'container',\n",
       "    'description': 'Executable unit of software that can be run in a pod, with its own process and resources',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"You\", \"description\": \"need to use tags containing a proper version designator instead of latest, except maybe in development.\", \"destination_entity\": \"tags\"},\\n  {\"source_entity\": \"You\", \"description\": \"will need to set the imagePullPolicy field in the pod spec to Always if you use mutable tags.\", \"destination_entity\": \"imagePullPolicy\"},\\n  {\"source_entity\": \"The container runtime\", \"description\": \"will contact the image registry every time a new pod is deployed if the image pull policy is set to Always.\", \"destination_entity\": \"image registry\"},\\n  {\"source_entity\": \"You\", \"description\": \"should label all your resources, not only Pods.\", \"destination_entity\": \"resources\"},\\n  {\"source_entity\": \"You\", \"description\": \"should add multiple labels to each resource so they can be selected across each individual dimension.\", \"destination_entity\": \"labels\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"will allow you to manage resources in groups instead of individually and make it easy to see where each resource belongs.\", \"destination_entity\": \"resources\"},\\n  {\"source_entity\": \"You\", \"description\": \"should add an annotation describing the resource and an annotation with contact information of the person responsible for it.\", \"destination_entity\": \"annotations\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"could contain an annotation that lists the names of the other services the pod is using, making it possible to show dependencies between pods.\", \"destination_entity\": \"annotations\"},\\n  {\"source_entity\": \"The container runtime\", \"description\": \"slows down pod startup a bit because the node needs to check if the image has been modified if the image pull policy is set to Always.\", \"destination_entity\": \"pod\"},\\n  {\"source_entity\": \"You\", \"description\": \"should provide information on why a container terminated or is terminating continuously, especially if it happens at the worst possible moment.\", \"destination_entity\": \"container\"}\\n]'},\n",
       " {'page': 531,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '499\\nMaking your apps easy to run and manage in Kubernetes\\nmoment. Be nice to the ops people and make their lives easier by including all the\\nnecessary debug information in your log files. \\n But to make triage even easier, you can use one other Kubernetes feature that\\nmakes it possible to show the reason why a container terminated in the pod’s status.\\nYou do this by having the process write a termination message to a specific file in the\\ncontainer’s filesystem. The contents of this file are read by the Kubelet when the con-\\ntainer terminates and are shown in the output of kubectl describe pod. If an applica-\\ntion uses this mechanism, an operator can quickly see why the app terminated without\\neven having to look at the container logs. \\n The default file the process needs to write the message to is /dev/termination-log,\\nbut it can be changed by setting the terminationMessagePath field in the container\\ndefinition in the pod spec. \\n You can see this in action by running a pod whose container dies immediately, as\\nshown in the following listing.\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: pod-with-termination-message\\nspec:\\n  containers:\\n  - image: busybox\\n    name: main\\n    terminationMessagePath: /var/termination-reason         \\n    command:\\n    - sh\\n    - -c\\n    - \\'echo \"I\\'\\'ve had enough\" > /var/termination-reason ; exit 1\\'   \\nWhen running this pod, you’ll soon see the pod’s status shown as CrashLoopBackOff.\\nIf you then use kubectl describe, you can see why the container died, without having\\nto dig down into its logs, as shown in the following listing.\\n$ kubectl describe po\\nName:           pod-with-termination-message\\n...\\nContainers:\\n...\\n    State:      Waiting\\n      Reason:   CrashLoopBackOff\\n    Last State: Terminated\\n      Reason:   Error\\n      Message:  I\\'ve had enough          \\n      Exit Code:        1\\n      Started:          Tue, 21 Feb 2017 21:38:31 +0100\\n      Finished:         Tue, 21 Feb 2017 21:38:31 +0100\\nListing 17.8\\nPod writing a termination message: termination-message.yaml\\nListing 17.9\\nSeeing the container’s termination message with kubectl describe\\nYou’re overriding the \\ndefault path of the \\ntermination message file.\\nThe container\\nwill write the\\nmessage to\\nthe file just\\nbefore exiting.\\nYou can see the reason \\nwhy the container died \\nwithout having to \\ninspect its logs.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': '/dev/termination-log',\n",
       "    'description': 'File path for termination message',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'busybox',\n",
       "    'description': 'Image used in pod spec',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'API version for Kubernetes object',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'Type of Kubernetes object (Pod)',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Metadata for pod object',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'Specification for pod object',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'List of containers in pod spec',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'Image used by container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'Name of container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'terminationMessagePath',\n",
       "    'description': 'Path for termination message file',\n",
       "    'category': 'software'},\n",
       "   {'entity': '/var/termination-reason',\n",
       "    'description': 'Custom path for termination message file',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'sh', 'description': 'Command shell', 'category': 'software'},\n",
       "   {'entity': '-c',\n",
       "    'description': 'Option to execute command',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'echo',\n",
       "    'description': 'Command to print string',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'exit 1',\n",
       "    'description': 'Command to exit container',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl describe',\n",
       "    'description': 'Command to display pod information',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'po',\n",
       "    'description': 'Pod name used with kubectl command',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Name:',\n",
       "    'description': 'Key for displaying pod name',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Containers:',\n",
       "    'description': 'Key for displaying container information',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\":\"metadata\", \"description\": \"defines the metadata of the pod\", \"destination_entity\": \"pod\"}, \\n {\"source_entity\":\"containers\", \"description\": \"lists the containers within the pod\", \"destination_entity\": \"pod\"}, \\n {\"source_entity\":\"apiVersion\", \"description\": \"specifies the API version of the Kubernetes object\", \"destination_entity\": \"metadata\"}, \\n {\"source_entity\":\"kind\", \"description\": \"defines the type of the Kubernetes object\", \"destination_entity\": \"metadata\"}, \\n {\"source_entity\":\"image\", \"description\": \"specifies the Docker image to use for the container\", \"destination_entity\": \"containers\"}, \\n {\"source_entity\":\"echo\", \"description\": \"writes a termination message to the file /var/termination-reason\", \"destination_entity\":\"/var/termination-reason\"}, \\n {\"source_entity\":\"Name:\", \"description\": \"displays the name of the pod\", \"destination_entity\": \"kubectl describe\"}, \\n {\"source_entity\":\"containers\", \"description\": \"displays the containers within the pod\", \"destination_entity\": \"kubectl describe\"}, \\n {\"source_entity\":\"/dev/termination-log\", \"description\": \"defines the default path for the termination message file\", \"destination_entity\": \"Kubernetes\"}, \\n {\"source_entity\":\"/var/termination-reason\", \"description\": \"overridden path for the termination message file\", \"destination_entity\": \"containers\"}, \\n {\"source_entity\":\"spec\", \"description\": \"defines the specification of the pod\", \"destination_entity\": \"metadata\"}, \\n {\"source_entity\":\"kubectl describe\", \"description\": \"displays detailed information about the pod\", \"destination_entity\": \"pod\"}, \\n {\"source_entity\":\"Kubernetes\", \"description\": \"defines a feature that makes it possible to show the reason why a container terminated\", \"destination_entity\": \"/dev/termination-log\"}]'},\n",
       " {'page': 532,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '500\\nCHAPTER 17\\nBest practices for developing apps\\n    Ready:              False\\n    Restart Count:      6\\nAs you can see, the “I’ve had enough” message the process wrote to the file /var/ter-\\nmination-reason is shown in the container’s Last State section. Note that this mecha-\\nnism isn’t limited only to containers that crash. It can also be used in pods that run a\\ncompletable task and terminate successfully (you’ll find an example in the file termi-\\nnation-message-success.yaml). \\n This mechanism is great for terminated containers, but you’ll probably agree that\\na similar mechanism would also be useful for showing app-specific status messages of\\nrunning, not only terminated, containers. Kubernetes currently doesn’t provide any\\nsuch functionality and I’m not aware of any plans to introduce it.\\nNOTE\\nIf the container doesn’t write the message to any file, you can set the\\nterminationMessagePolicy field to FallbackToLogsOnError. In that case,\\nthe last few lines of the container’s log are used as its termination message\\n(but only when the container terminates unsuccessfully).\\n17.4.6 Handling application logs\\nWhile we’re on the subject of application logging, let’s reiterate that apps should write\\nto the standard output instead of files. This makes it easy to view logs with the kubectl\\nlogs command. \\nTIP\\nIf a container crashes and is replaced with a new one, you’ll see the new\\ncontainer’s log. To see the previous container’s logs, use the --previous\\noption with kubectl logs.\\nIf the application logs to a file instead of the standard output, you can display the log\\nfile using an alternative approach: \\n$ kubectl exec <pod> cat <logfile>\\nThis executes the cat command inside the container and streams the logs back to\\nkubectl, which prints them out in your terminal. \\nCOPYING LOG AND OTHER FILES TO AND FROM A CONTAINER\\nYou can also copy the log file to your local machine using the kubectl cp command,\\nwhich we haven’t looked at yet. It allows you to copy files from and into a container. For\\nexample, if a pod called foo-pod and its single container contains a file at /var/log/\\nfoo.log, you can transfer it to your local machine with the following command:\\n$ kubectl cp foo-pod:/var/log/foo.log foo.log\\nTo copy a file from your local machine into the pod, specify the pod’s name in the sec-\\nond argument:\\n$ kubectl cp localfile foo-pod:/etc/remotefile\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'container',\n",
       "    'description': 'A container is a runtime instance of a Docker image.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A pod is the basic execution unit in Kubernetes, representing a single instance of a running process or service.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Kubernetes command-line tool used for executing commands against APIs and clusters.',\n",
       "    'category': 'tool'},\n",
       "   {'entity': 'logs',\n",
       "    'description': 'Application logs written to standard output or file by containers.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'terminationMessagePolicy',\n",
       "    'description': 'Field in container spec that determines how the message is generated when a container terminates.',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'kubectl exec',\n",
       "    'description': 'Command used to execute a command inside a running container.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubectl logs',\n",
       "    'description': 'Command used to view logs of a pod or container.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubectl cp',\n",
       "    'description': 'Command used to copy files from and into a container.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Docker image',\n",
       "    'description': 'A Docker image is a package that contains an application and its dependencies, used as input for the build process.',\n",
       "    'category': 'image'},\n",
       "   {'entity': 'file',\n",
       "    'description': 'A file can be written by containers and accessed using kubectl exec or cp commands.',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'standard output',\n",
       "    'description': 'Output sent to terminal when container writes logs instead of writing them to a file.',\n",
       "    'category': 'output'},\n",
       "   {'entity': 'application logging',\n",
       "    'description': 'Process of writing logs by applications running inside containers.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'application status messages',\n",
       "    'description': 'Messages displayed to the user about the current state of an application or its components.',\n",
       "    'category': 'message'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"container\",\\n    \"description\": \"writes termination message to file\",\\n    \"destination_entity\": \"/var/termination-reason\"\\n  },\\n  {\\n    \"source_entity\": \"container\",\\n    \"description\": \"doesn\\'t write message to any file, uses FallbackToLogsOnError\",\\n    \"destination_entity\": \"last few lines of container\\'s log\"\\n  },\\n  {\\n    \"source_entity\": \"application\",\\n    \"description\": \"writes application logs to standard output\",\\n    \"destination_entity\": \"standard output\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"displays previous container\\'s logs using --previous option\",\\n    \"destination_entity\": \"previous container\\'s log\"\\n  },\\n  {\\n    \"source_entity\": \"container\",\\n    \"description\": \"logs to file instead of standard output\",\\n    \"destination_entity\": \"/var/log/foo.log\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl cp\",\\n    \"description\": \"copies file from container to local machine\",\\n    \"destination_entity\": \"local machine\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl cp\",\\n    \"description\": \"copies file from local machine to pod\",\\n    \"destination_entity\": \"/etc/remotefile\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"runs container and logs application status messages\",\\n    \"destination_entity\": \"application status messages\"\\n  },\\n  {\\n    \"source_entity\": \"container\",\\n    \"description\": \"writes log file to standard output using kubectl exec\",\\n    \"destination_entity\": \"kubectl exec\"\\n  },\\n  {\\n    \"source_entity\": \"Docker image\",\\n    \"description\": \"runs container and logs application status messages\",\\n    \"destination_entity\": \"application status messages\"\\n  },\\n  {\\n    \"source_entity\": \"application logging\",\\n    \"description\": \"writes application logs to standard output or file\",\\n    \"destination_entity\": \"logs\"\\n  }\\n]\\n```\\n\\nNote: I\\'ve used the entities provided in the input list to create these relations. If an entity is not present in the list, it may not be included in the relation.'},\n",
       " {'page': 533,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '501\\nMaking your apps easy to run and manage in Kubernetes\\nThis copies the file localfile to /etc/remotefile inside the pod’s container. If the pod has\\nmore than one container, you specify the container using the -c containerName option.\\nUSING CENTRALIZED LOGGING\\nIn a production system, you’ll want to use a centralized, cluster-wide logging solution,\\nso all your logs are collected and (permanently) stored in a central location. This\\nallows you to examine historical logs and analyze trends. Without such a system, a\\npod’s logs are only available while the pod exists. As soon as it’s deleted, its logs are\\ndeleted also. \\n Kubernetes by itself doesn’t provide any kind of centralized logging. The compo-\\nnents necessary for providing a centralized storage and analysis of all the container\\nlogs must be provided by additional components, which usually run as regular pods in\\nthe cluster. \\n Deploying centralized logging solutions is easy. All you need to do is deploy a few\\nYAML/JSON manifests and you’re good to go. On Google Kubernetes Engine, it’s\\neven easier. Check the Enable Stackdriver Logging checkbox when setting up the clus-\\nter. Setting up centralized logging on an on-premises Kubernetes cluster is beyond the\\nscope of this book, but I’ll give you a quick overview of how it’s usually done.\\n You may have already heard of the ELK stack composed of ElasticSearch, Logstash,\\nand Kibana. A slightly modified variation is the EFK stack, where Logstash is replaced\\nwith FluentD. \\n When using the EFK stack for centralized logging, each Kubernetes cluster node\\nruns a FluentD agent (usually as a pod deployed through a DaemonSet), which is\\nresponsible for gathering the logs from the containers, tagging them with pod-specific\\ninformation, and delivering them to ElasticSearch, which stores them persistently.\\nElasticSearch is also deployed as a pod somewhere in the cluster. The logs can then be\\nviewed and analyzed in a web browser through Kibana, which is a web tool for visualiz-\\ning ElasticSearch data. It also usually runs as a pod and is exposed through a Service.\\nThe three components of the EFK stack are shown in the following figure.\\nNOTE\\nIn the next chapter, you’ll learn about Helm charts. You can use charts\\ncreated by the Kubernetes community to deploy the EFK stack instead of cre-\\nating your own YAML manifests. \\nNode 1\\nContainer logs\\nKibana\\nWeb\\nbrowser\\nFluentD\\nNode 2\\nContainer logs\\nFluentD\\nNode 3\\nContainer logs\\nFluentD\\nElasticSearch\\nFigure 17.10\\nCentralized logging with FluentD, ElasticSearch, and Kibana\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Basic execution unit in Kubernetes, a collection of one or more containers',\n",
       "    'category': 'Software'},\n",
       "   {'entity': '-c containerName option',\n",
       "    'description': 'Option to specify the container name while copying files inside a pod',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Centralized logging',\n",
       "    'description': 'System for collecting and storing logs from all containers in a cluster',\n",
       "    'category': 'Application'},\n",
       "   {'entity': 'Logstash',\n",
       "    'description': 'Component of the ELK stack for centralized logging, responsible for gathering logs',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'ElasticSearch',\n",
       "    'description': 'Component of the EFK stack for centralized logging, stores logs persistently',\n",
       "    'category': 'Database'},\n",
       "   {'entity': 'Kibana',\n",
       "    'description': 'Web tool for visualizing ElasticSearch data and viewing logs',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'FluentD',\n",
       "    'description': 'Agent responsible for gathering logs from containers and delivering them to ElasticSearch',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'DaemonSet',\n",
       "    'description': 'Pod deployment strategy that ensures a specified number of replicas are running at any given time',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'Resource that provides a network identity and load balancing for accessing a group of pods',\n",
       "    'category': 'Network'},\n",
       "   {'entity': 'Helm charts',\n",
       "    'description': 'Package manager for Kubernetes that allows users to easily deploy applications using pre-built templates',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"provides a centralized logging solution\",\\n    \"destination_entity\": \"Centralized logging\"\\n  },\\n  {\\n    \"source_entity\": \"FluentD\",\\n    \"description\": \"gathers logs from containers, tags them with pod-specific information, and delivers them to ElasticSearch\",\\n    \"destination_entity\": \"ElasticSearch\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"-c containerName option allows specifying a container when copying files inside the pod\\'s container\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"DaemonSet\",\\n    \"description\": \"deploys FluentD agents on each Kubernetes cluster node\",\\n    \"destination_entity\": \"FluentD\"\\n  },\\n  {\\n    \"source_entity\": \"Kibana\",\\n    \"description\": \"allows viewing and analyzing logs in a web browser through visualizing ElasticSearch data\",\\n    \"destination_entity\": \"ElasticSearch\"\\n  },\\n  {\\n    \"source_entity\": \"Logstash\",\\n    \"description\": \"is replaced with FluentD in the EFK stack\",\\n    \"destination_entity\": \"FluentD\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes Engine\",\\n    \"description\": \"allows easy setup of centralized logging through Enable Stackdriver Logging checkbox\",\\n    \"destination_entity\": \"Centralized logging\"\\n  },\\n  {\\n    \"source_entity\": \"ElasticSearch\",\\n    \"description\": \"stores logs persistently and is deployed as a pod in the cluster\",\\n    \"destination_entity\": \"Pod\"\\n  },\\n  {\\n    \"source_entity\": \"FluentD\",\\n    \"description\": \"runs as a pod deployed through a DaemonSet on each Kubernetes cluster node\",\\n    \"destination_entity\": \"DaemonSet\"\\n  },\\n  {\\n    \"source_entity\": \"Kibana\",\\n    \"description\": \"usually runs as a pod and is exposed through a Service\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Helm charts\",\\n    \"description\": \"can be used to deploy the EFK stack instead of creating own YAML manifests\",\\n    \"destination_entity\": \"EFK stack\"\\n  }\\n]'},\n",
       " {'page': 534,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '502\\nCHAPTER 17\\nBest practices for developing apps\\nHANDLING MULTI-LINE LOG STATEMENTS\\nThe FluentD agent stores each line of the log file as an entry in the ElasticSearch\\ndata store. There’s one problem with that. Log statements spanning multiple lines,\\nsuch as exception stack traces in Java, appear as separate entries in the centralized\\nlogging system. \\n To solve this problem, you can have the apps output JSON instead of plain text.\\nThis way, a multiline log statement can be stored and shown in Kibana as a single\\nentry. But that makes viewing logs with kubectl logs much less human-friendly. \\n The solution may be to keep outputting human-readable logs to standard output,\\nwhile writing JSON logs to a file and having them processed by FluentD. This requires\\nconfiguring the node-level FluentD agent appropriately or adding a logging sidecar\\ncontainer to every pod. \\n17.5\\nBest practices for development and testing\\nWe’ve talked about what to be mindful of when developing apps, but we haven’t\\ntalked about the development and testing workflows that will help you streamline\\nthose processes. I don’t want to go into too much detail here, because everyone needs\\nto find what works best for them, but here are a few starting points.\\n17.5.1 Running apps outside of Kubernetes during development\\nWhen you’re developing an app that will run in a production Kubernetes cluster, does\\nthat mean you also need to run it in Kubernetes during development? Not really. Hav-\\ning to build the app after each minor change, then build the container image, push it\\nto a registry, and then re-deploy the pods would make development slow and painful.\\nLuckily, you don’t need to go through all that trouble.\\n You can always develop and run apps on your local machine, the way you’re used\\nto. After all, an app running in Kubernetes is a regular (although isolated) process\\nrunning on one of the cluster nodes. If the app depends on certain features the\\nKubernetes environment provides, you can easily replicate that environment on your\\ndevelopment machine.\\n I’m not even talking about running the app in a container. Most of the time, you\\ndon’t need that—you can usually run the app directly from your IDE. \\nCONNECTING TO BACKEND SERVICES\\nIn production, if the app connects to a backend Service and uses the BACKEND_SERVICE\\n_HOST and BACKEND_SERVICE_PORT environment variables to find the Service’s coordi-\\nnates, you can obviously set those environment variables on your local machine manu-\\nally and point them to the backend Service, regardless of if it’s running outside or\\ninside a Kubernetes cluster. If it’s running inside Kubernetes, you can always (at least\\ntemporarily) make the Service accessible externally by changing it to a NodePort or a\\nLoadBalancer-type Service. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'FluentD',\n",
       "    'description': 'The FluentD agent stores each line of the log file as an entry in the ElasticSearch data store.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ElasticSearch',\n",
       "    'description': 'A centralized logging system where logs are stored.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Kibana',\n",
       "    'description': 'A tool for viewing and interacting with logs in ElasticSearch.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'A command-line interface for running commands on a Kubernetes cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'JSON',\n",
       "    'description': 'A lightweight data interchange format used to output log statements.',\n",
       "    'category': 'programming language'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'log file',\n",
       "    'description': 'A file containing log messages from an application or service.',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'log statements',\n",
       "    'description': 'Messages written to a log file by an application or service.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'exception stack trace',\n",
       "    'description': 'A detailed error message showing the sequence of method calls that led to the error.',\n",
       "    'category': 'error'},\n",
       "   {'entity': 'Java',\n",
       "    'description': 'An object-oriented programming language used for developing applications.',\n",
       "    'category': 'programming language'},\n",
       "   {'entity': 'IDE',\n",
       "    'description': 'An integrated development environment where developers write and run code.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'container image',\n",
       "    'description': \"A binary package that contains an application's dependencies and code.\",\n",
       "    'category': 'container'},\n",
       "   {'entity': 'registry',\n",
       "    'description': 'A centralized repository for storing and retrieving container images.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'A deployment unit in Kubernetes where one or more containers run together.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A named resource that provides a network interface to access a backend service.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'BACKEND_SERVICE_HOST',\n",
       "    'description': 'An environment variable used to specify the host address of a backend service.',\n",
       "    'category': 'environment variable'},\n",
       "   {'entity': 'BACKEND_SERVICE_PORT',\n",
       "    'description': 'An environment variable used to specify the port number of a backend service.',\n",
       "    'category': 'environment variable'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"FluentD\",\\n    \"description\": \"stores log file as an entry in ElasticSearch data store\",\\n    \"destination_entity\": \"log file\"\\n  },\\n  {\\n    \"source_entity\": \"app\",\\n    \"description\": \"outputs human-readable logs to standard output\",\\n    \"destination_entity\": \"standard output\"\\n  },\\n  {\\n    \"source_entity\": \"FluentD\",\\n    \"description\": \"processes JSON logs from a file\",\\n    \"destination_entity\": \"JSON logs\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"provides environment for app to connect to backend Service\",\\n    \"destination_entity\": \"backend Service\"\\n  },\\n  {\\n    \"source_entity\": \"app\",\\n    \"description\": \"connects to backend Service using BACKEND_SERVICE_HOST and BACKEND_SERVICE_PORT variables\",\\n    \"destination_entity\": \"BACKEND_SERVICE_HOST and BACKEND_SERVICE_PORT variables\"\\n  },\\n  {\\n    \"source_entity\": \"registry\",\\n    \"description\": \"stores container image pushed by user\",\\n    \"destination_entity\": \"container image\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"displays logs from a pod to user\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"IDE\",\\n    \"description\": \"runs app directly on local machine without container\",\\n    \"destination_entity\": \"local machine\"\\n  },\\n  {\\n    \"source_entity\": \"Java\",\\n    \"description\": \"outputs exception stack trace as a multiline log statement\",\\n    \"destination_entity\": \"multiline log statement\"\\n  },\\n  {\\n    \"source_entity\": \"FluentD\",\\n    \"description\": \"stores each line of log file in ElasticSearch data store\",\\n    \"destination_entity\": \"log file\"\\n  },\\n  {\\n    \"source_entity\": \"Kibana\",\\n    \"description\": \"displays log statements from FluentD as a single entry\",\\n    \"destination_entity\": \"log statement\"\\n  },\\n  {\\n    \"source_entity\": \"app\",\\n    \"description\": \"outputs JSON logs to a file for processing by FluentD\",\\n    \"destination_entity\": \"JSON logs and file\"\\n  }\\n]'},\n",
       " {'page': 535,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '503\\nBest practices for development and testing\\nCONNECTING TO THE API SERVER\\nSimilarly, if your app requires access to the Kubernetes API server when running\\ninside a Kubernetes cluster, it can easily talk to the API server from outside the cluster\\nduring development. If it uses the ServiceAccount’s token to authenticate itself, you\\ncan always copy the ServiceAccount’s Secret’s files to your local machine with kubectl\\ncp. The API server doesn’t care if the client accessing it is inside or outside the cluster. \\n If the app uses an ambassador container like the one described in chapter 8, you\\ndon’t even need those Secret files. Run kubectl proxy on your local machine, run\\nyour app locally, and it should be ready to talk to your local kubectl proxy (as long as\\nit and the ambassador container bind the proxy to the same port).\\n In this case, you’ll need to make sure the user account your local kubectl is using\\nhas the same privileges as the ServiceAccount the app will run under.\\nRUNNING INSIDE A CONTAINER EVEN DURING DEVELOPMENT\\nWhen during development you absolutely have to run the app in a container for what-\\never reason, there is a way of avoiding having to build the container image every time.\\nInstead of baking the binaries into the image, you can always mount your local filesys-\\ntem into the container through Docker volumes, for example. This way, after you\\nbuild a new version of the app’s binaries, all you need to do is restart the container (or\\nnot even that, if hot-redeploy is supported). No need to rebuild the image.\\n17.5.2 Using Minikube in development\\nAs you can see, nothing forces you to run your app inside Kubernetes during develop-\\nment. But you may do that anyway to see how the app behaves in a true Kubernetes\\nenvironment.\\n You may have used Minikube to run examples in this book. Although a Minikube\\ncluster runs only a single worker node, it’s nevertheless a valuable method of trying\\nout your app in Kubernetes (and, of course, developing all the resource manifests that\\nmake up your complete application). Minikube doesn’t offer everything that a proper\\nmulti-node Kubernetes cluster usually provides, but in most cases, that doesn’t matter.\\nMOUNTING LOCAL FILES INTO THE MINIKUBE VM AND THEN INTO YOUR CONTAINERS\\nWhen you’re developing with Minikube and you’d like to try out every change to your\\napp in your Kubernetes cluster, you can mount your local filesystem into the Minikube\\nVM using the minikube mount command and then mount it into your containers\\nthrough a hostPath volume. You’ll find additional instructions on how to do that\\nin the Minikube documentation at https:/\\n/github.com/kubernetes/minikube/tree/\\nmaster/docs.\\nUSING THE DOCKER DAEMON INSIDE THE MINIKUBE VM TO BUILD YOUR IMAGES\\nIf you’re developing your app with Minikube and planning to build the container\\nimage after every change, you can use the Docker daemon inside the Minikube VM to\\ndo the building, instead of having to build the image through your local Docker dae-\\nmon, push it to a registry, and then have it pulled by the daemon in the VM. To use\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes API server',\n",
       "    'description': 'The server that manages Kubernetes resources.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'A service account provides an identity for an application to use when making requests to the Kubernetes API.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kubectl cp',\n",
       "    'description': 'A command used to copy files from a Secret resource to a local machine.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Secret files',\n",
       "    'description': 'Files containing sensitive information such as authentication tokens.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Ambassador container',\n",
       "    'description': 'A container that acts as an ambassador for the Kubernetes API server.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'kubectl proxy',\n",
       "    'description': 'A command used to start a proxy server that forwards requests from a local machine to the Kubernetes API server.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Docker volumes',\n",
       "    'description': 'A way to mount a local filesystem into a container for development purposes.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'A tool used to run a single-node Kubernetes cluster on a local machine.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'worker node',\n",
       "    'description': 'A node in a Kubernetes cluster that can run pods.',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'hostPath volume',\n",
       "    'description': 'A type of persistent volume that allows a container to access a host directory.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Docker daemon',\n",
       "    'description': 'The process responsible for building and running Docker containers.',\n",
       "    'category': 'process'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ServiceAccount\", \"description\": \"uses token to authenticate itself with Kubernetes API server\", \"destination_entity\": \"Kubernetes API server\"},\\n  {\"source_entity\": \"ServiceAccount\", \"description\": \"has Secret\\'s files copied to local machine with kubectl cp\", \"destination_entity\": \"local machine\"},\\n  {\"source_entity\": \"Ambassador container\", \"description\": \"binds proxy to same port as kubectl proxy\", \"destination_entity\": \"kubectl proxy\"},\\n  {\"source_entity\": \"Minikube\", \"description\": \"runs a single worker node in Kubernetes cluster\", \"destination_entity\": \"Kubernetes cluster\"},\\n  {\"source_entity\": \"Minikube\", \"description\": \"mounts local filesystem into VM with minikube mount command\", \"destination_entity\": \"local filesystem\"},\\n  {\"source_entity\": \"kubectl proxy\", \"description\": \"is accessed by app running locally and ambassador container\", \"destination_entity\": \"app running locally and ambassador container\"},\\n  {\"source_entity\": \"Docker daemon\", \"description\": \"runs inside Minikube VM to build container image\", \"destination_entity\": \"Minikube VM\"},\\n  {\"source_entity\": \"worker node\", \"description\": \"runs in Minikube cluster\", \"destination_entity\": \"Minikube cluster\"},\\n  {\"source_entity\": \"kubectl cp\", \"description\": \"copies Secret\\'s files from ServiceAccount to local machine\", \"destination_entity\": \"local machine\"},\\n  {\"source_entity\": \"Docker daemon\", \"description\": \"is used inside Minikube VM to build container image\", \"destination_entity\": \"Minikube VM\"},\\n  {\"source_entity\": \"Secret files\", \"description\": \"are copied from ServiceAccount to local machine with kubectl cp\", \"destination_entity\": \"local machine\"},\\n  {\"source_entity\": \"hostPath volume\", \"description\": \"is mounted into containers through Docker volumes\", \"destination_entity\": \"containers\"},\\n  {\"source_entity\": \"Docker volumes\", \"description\": \"mounts local filesystem into container\", \"destination_entity\": \"container\"}\\n]\\n```\\n\\nThis output contains a list of JSON objects, each representing a relation between two entities. The `source_entity` field represents the entity that performs some action (the subject), the `destination_entity` field represents the entity on which the action is performed (the object), and the `description` field provides a brief summary of what action was being carried out by the source entity on the destination entity.'},\n",
       " {'page': 536,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '504\\nCHAPTER 17\\nBest practices for developing apps\\nMinikube’s Docker daemon, all you need to do is point your DOCKER_HOST environ-\\nment variable to it. Luckily, this is much easier than it sounds. All you need to do is\\nrun the following command on your local machine:\\n$ eval $(minikube docker-env)\\nThis will set all the required environment variables for you. You then build your\\nimages the same way as if the Docker daemon was running on your local machine.\\nAfter you build the image, you don’t need to push it anywhere, because it’s already\\nstored locally on the Minikube VM, which means new pods can use the image immedi-\\nately. If your pods are already running, you either need to delete them or kill their\\ncontainers so they’re restarted.\\nBUILDING IMAGES LOCALLY AND COPYING THEM OVER TO THE MINIKUBE VM DIRECTLY\\nIf you can’t use the daemon inside the VM to build the images, you still have a way to\\navoid having to push the image to a registry and have the Kubelet running in the\\nMinikube VM pull it. If you build the image on your local machine, you can copy it\\nover to the Minikube VM with the following command:\\n$ docker save <image> | (eval $(minikube docker-env) && docker load)\\nAs before, the image is immediately ready to be used in a pod. But make sure the\\nimagePullPolicy in your pod spec isn’t set to Always, because that would cause the\\nimage to be pulled from the external registry again and you’d lose the changes you’ve\\ncopied over.\\nCOMBINING MINIKUBE WITH A PROPER KUBERNETES CLUSTER\\nYou have virtually no limit when developing apps with Minikube. You can even com-\\nbine a Minikube cluster with a proper Kubernetes cluster. I sometimes run my devel-\\nopment workloads in my local Minikube cluster and have them talk to my other\\nworkloads that are deployed in a remote multi-node Kubernetes cluster thousands of\\nmiles away. \\n Once I’m finished with development, I can move my local workloads to the remote\\ncluster with no modifications and with absolutely no problems thanks to how Kuber-\\nnetes abstracts away the underlying infrastructure from the app.\\n17.5.3 Versioning and auto-deploying resource manifests\\nBecause Kubernetes uses a declarative model, you never have to figure out the current\\nstate of your deployed resources and issue imperative commands to bring that state to\\nwhat you desire. All you need to do is tell Kubernetes your desired state and it will take\\nall the necessary actions to reconcile the cluster state with the desired state.\\n You can store your collection of resource manifests in a Version Control System,\\nenabling you to perform code reviews, keep an audit trail, and roll back changes\\nwhenever necessary. After each commit, you can run the kubectl apply command to\\nhave your changes reflected in your deployed resources. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'minikube',\n",
       "    'description': 'a tool for running a Kubernetes cluster locally on a machine',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'docker daemon',\n",
       "    'description': 'the Docker engine process responsible for creating and managing containers',\n",
       "    'category': 'container runtime'},\n",
       "   {'entity': 'DOCKER_HOST environment variable',\n",
       "    'description': 'an environment variable used to specify the location of the Docker daemon',\n",
       "    'category': 'environment variable'},\n",
       "   {'entity': '$eval $(minikube docker-env)',\n",
       "    'description': \"a command used to set up the necessary environment variables for working with Minikube's Docker daemon\",\n",
       "    'category': 'command'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'the basic execution unit in a Kubernetes cluster, consisting of one or more containers',\n",
       "    'category': 'container orchestration'},\n",
       "   {'entity': 'imagePullPolicy',\n",
       "    'description': 'a field in the pod specification that determines how to handle image updates for a container',\n",
       "    'category': 'kubernetes resource'},\n",
       "   {'entity': 'kubectl apply command',\n",
       "    'description': 'a command used to apply a configuration change to a Kubernetes cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Version Control System',\n",
       "    'description': 'a software system that helps manage changes to software source code',\n",
       "    'category': 'software tool'},\n",
       "   {'entity': 'resource manifests',\n",
       "    'description': 'configuration files that define the desired state of a Kubernetes resource',\n",
       "    'category': 'kubernetes resource'},\n",
       "   {'entity': 'Kubelet',\n",
       "    'description': 'the agent responsible for running pods on a node in a Kubernetes cluster',\n",
       "    'category': 'container orchestration'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"uses a declarative model to manage resources\", \"destination_entity\": \"desired state\"},\\n  {\"source_entity\": \"Kubelet\", \"description\": \"pulls images from external registry if imagePullPolicy is set to Always\", \"destination_entity\": \"image\"},\\n  {\"source_entity\": \"minikube\", \"description\": \"provides a Docker daemon for local development\", \"destination_entity\": \"DOCKER_HOST environment variable\"},\\n  {\"source_entity\": \"$eval $(minikube docker-env)\", \"description\": \"sets the required environment variables for minikube\", \"destination_entity\": \"local machine\"},\\n  {\"source_entity\": \"docker daemon\", \"description\": \"runs inside the Minikube VM to build images\", \"destination_entity\": \"images\"},\\n  {\"source_entity\": \"kubectl apply command\", \"description\": \"applies changes to deployed resources after each commit\", \"destination_entity\": \"resource manifests\"},\\n  {\"source_entity\": \"Version Control System\", \"description\": \"stores resource manifests and enables code reviews and audit trails\", \"destination_entity\": \"resource manifests\"},\\n  {\"source_entity\": \"imagePullPolicy\", \"description\": \"determines whether to pull images from external registry or use local image\", \"destination_entity\": \"images\"},\\n  {\"source_entity\": \"pod\", \"description\": \"can use the image immediately if built locally and copied over to Minikube VM\", \"destination_entity\": \"images\"},\\n  {\"source_entity\": \"$eval $(minikube docker-env)\", \"description\": \"saves an image from a local machine and loads it into the Minikube VM\", \"destination_entity\": \"image\"},\\n  {\"source_entity\": \"docker daemon\", \"description\": \"can build images locally without pushing to registry\", \"destination_entity\": \"images\"},\\n  {\"source_entity\": \"Minikube cluster\", \"description\": \"can be combined with a proper Kubernetes cluster for development and deployment\", \"destination_entity\": \"Kubernetes cluster\"}\\n]\\n```'},\n",
       " {'page': 537,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '505\\nBest practices for development and testing\\n If you run an agent that periodically (or when it detects a new commit) checks out\\nyour manifests from the Version Control System (VCS), and then runs the apply com-\\nmand, you can manage your running apps simply by committing changes to the VCS\\nwithout having to manually talk to the Kubernetes API server. Luckily, the people at\\nBox (which coincidently was used to host this book’s manuscript and other materials)\\ndeveloped and released a tool called kube-applier, which does exactly what I described.\\nYou’ll find the tool’s source code at https:/\\n/github.com/box/kube-applier.\\n You can use multiple branches to deploy the manifests to a development, QA, stag-\\ning, and production cluster (or in different namespaces in the same cluster).\\n17.5.4 Introducing Ksonnet as an alternative to writing YAML/JSON \\nmanifests\\nWe’ve seen a number of YAML manifests throughout the book. I don’t see writing\\nYAML as too big of a problem, especially once you learn how to use kubectl explain\\nto see the available options, but some people do. \\n Just as I was finalizing the manuscript for this book, a new tool called Ksonnet was\\nannounced. It’s a library built on top of Jsonnet, which is a data templating language\\nfor building JSON data structures. Instead of writing the complete JSON by hand, it\\nlets you define parameterized JSON fragments, give them a name, and then build a\\nfull JSON manifest by referencing those fragments by name, instead of repeating the\\nsame JSON code in multiple locations—much like you use functions or methods in a\\nprogramming language. \\n Ksonnet defines the fragments you’d find in Kubernetes resource manifests, allow-\\ning you to quickly build a complete Kubernetes resource JSON manifest with much\\nless code. The following listing shows an example.\\nlocal k = import \"../ksonnet-lib/ksonnet.beta.1/k.libsonnet\";\\nlocal container = k.core.v1.container;\\nlocal deployment = k.apps.v1beta1.deployment;\\nlocal kubiaContainer =                              \\n  container.default(\"kubia\", \"luksa/kubia:v1\") +    \\n  container.helpers.namedPort(\"http\", 8080);        \\ndeployment.default(\"kubia\", kubiaContainer) +    \\ndeployment.mixin.spec.replicas(3)                \\nThe kubia.ksonnet file shown in the listing is converted to a full JSON Deployment\\nmanifest when you run the following command:\\n$ jsonnet kubia.ksonnet\\nListing 17.10\\nThe kubia Deployment written with Ksonnet: kubia.ksonnet\\nThis defines a container called kubia, \\nwhich uses the luksa/kubia:v1 image \\nand includes a port called http.\\nThis will be expanded into a full \\nDeployment resource. The kubiaContainer \\ndefined here will be included in the \\nDeployment’s pod template.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command to interact with Kubernetes API server',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'VCS (Version Control System)',\n",
       "    'description': 'system for managing changes to codebase',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kube-applier',\n",
       "    'description': 'tool for automating Kubernetes deployments',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Ksonnet',\n",
       "    'description': 'library for building JSON data structures',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Jsonnet',\n",
       "    'description': 'data templating language',\n",
       "    'category': 'programming_language'},\n",
       "   {'entity': 'YAML',\n",
       "    'description': 'human-readable serialization format',\n",
       "    'category': 'file_format'},\n",
       "   {'entity': 'JSON',\n",
       "    'description': 'lightweight data interchange format',\n",
       "    'category': 'file_format'},\n",
       "   {'entity': 'Ksonnet-lib',\n",
       "    'description': 'library for building Kubernetes resource manifests',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubiaContainer',\n",
       "    'description': 'container definition in Ksonnet',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'deployment',\n",
       "    'description': 'Kubernetes deployment object',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'replicas',\n",
       "    'description': 'field for specifying number of replicas in deployment',\n",
       "    'category': 'field'}],\n",
       "  'relationships': '[{\"source_entity\": \"You\", \"description\": \"run an agent that periodically checks out manifests from VCS and runs apply command\", \"destination_entity\": \"VCS (Version Control System)\"},\\n{\"source_entity\": \"Box\", \"description\": \"developed a tool called kube-applier which does exactly what you described\", \"destination_entity\": \"kube-applier\"},\\n{\"source_entity\": \"You\", \"description\": \"use multiple branches to deploy manifests to development, QA, staging, and production cluster\", \"destination_entity\": \"clusters\"},\\n{\"source_entity\": \"People\", \"description\": \"find writing YAML as too big of a problem\", \"destination_entity\": \"YAML\"},\\n{\"source_entity\": \"You\", \"description\": \"learn how to use kubectl explain to see available options\", \"destination_entity\": \"kubectl\"},\\n{\"source_entity\": \"You\", \"description\": \"write Kubernetes resource manifests in YAML\", \"destination_entity\": \"Kubernetes\"},\\n{\"source_entity\": \"Ksonnet\", \"description\": \"defines parameterized JSON fragments, allowing you to quickly build a complete Kubernetes resource JSON manifest with much less code\", \"destination_entity\": \"JSON\"},\\n{\"source_entity\": \"You\", \"description\": \"use Ksonnet-lib library built on top of Jsonnet\", \"destination_entity\": \"Ksonnet-lib\"},\\n{\"source_entity\": \"Ksonnet\", \"description\": \"allows you to define parameterized JSON fragments and build a full JSON manifest by referencing those fragments by name\", \"destination_entity\": \"Jsonnet\"},\\n{\"source_entity\": \"You\", \"description\": \"use Ksonnet to define a container called kubia which uses the luksa/kubia:v1 image and includes a port called http\", \"destination_entity\": \"kubiaContainer\"},\\n{\"source_entity\": \"Ksonnet\", \"description\": \"defines a full JSON Deployment manifest when you run the command jsonnet kubia.ksonnet\", \"destination_entity\": \"JSON\"},\\n{\"source_entity\": \"Ksonnet\", \"description\": \"includes a port called http in the container called kubia\", \"destination_entity\": \"port\"},\\n{\"source_entity\": \"You\", \"description\": \"use Kubectl to explain available options for writing YAML manifests\", \"destination_entity\": \"kubectl\"}]\\n\\nNote that I\\'ve excluded the entities themselves and only kept the relations between them.'},\n",
       " {'page': 538,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '506\\nCHAPTER 17\\nBest practices for developing apps\\nThe power of Ksonnet and Jsonnet becomes apparent when you realize you can define\\nyour own higher-level fragments and make all your manifests consistent and duplica-\\ntion-free. You’ll find more information on using and installing Ksonnet and Jsonnet at\\nhttps:/\\n/github.com/ksonnet/ksonnet-lib.\\n17.5.5 Employing Continuous Integration and Continuous Delivery \\n(CI/CD)\\nWe’ve touched on automating the deployment of Kubernetes resources two sections\\nback, but you may want to set up a complete CI/CD pipeline for building your appli-\\ncation binaries, container images, and resource manifests and then deploying them in\\none or more Kubernetes clusters.\\n You’ll find many online resources talking about this subject. Here, I’d like to point\\nyou specifically to the Fabric8 project (http:/\\n/fabric8.io), which is an integrated\\ndevelopment platform for Kubernetes. It includes Jenkins, the well-known, open-\\nsource automation system, and various other tools to deliver a full CI/CD pipeline\\nfor DevOps-style development, deployment, and management of microservices on\\nKubernetes.\\n If you’d like to build your own solution, I also suggest looking at one of the Google\\nCloud Platform’s online labs that talks about this subject. It’s available at https:/\\n/\\ngithub.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes.\\n17.6\\nSummary\\nHopefully, the information in this chapter has given you an even deeper insight into\\nhow Kubernetes works and will help you build apps that feel right at home when\\ndeployed to a Kubernetes cluster. The aim of this chapter was to\\n\\uf0a1Show you how all the resources covered in this book come together to repre-\\nsent a typical application running in Kubernetes.\\n\\uf0a1Make you think about the difference between apps that are rarely moved\\nbetween machines and apps running as pods, which are relocated much more\\nfrequently.\\n\\uf0a1Help you understand that your multi-component apps (or microservices, if you\\nwill) shouldn’t rely on a specific start-up order.\\n\\uf0a1Introduce init containers, which can be used to initialize a pod or delay the start\\nof the pod’s main containers until a precondition is met.\\n\\uf0a1Teach you about container lifecycle hooks and when to use them.\\n\\uf0a1Gain a deeper insight into the consequences of the distributed nature of\\nKubernetes components and its eventual consistency model.\\n\\uf0a1Learn how to make your apps shut down properly without breaking client\\nconnections.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Ksonnet',\n",
       "    'description': 'A configuration management tool for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Jsonnet',\n",
       "    'description': 'A templating language for configuration files',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'An open-source container orchestration system',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'CI/CD pipeline',\n",
       "    'description': 'A process for automating the deployment of application binaries, container images, and resource manifests',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Jenkins',\n",
       "    'description': 'An open-source automation system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Fabric8 project',\n",
       "    'description': 'An integrated development platform for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Google Cloud Platform',\n",
       "    'description': 'A cloud computing platform',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'init containers',\n",
       "    'description': \"Containers that can be used to initialize a pod or delay the start of the pod's main containers until a precondition is met\",\n",
       "    'category': 'container'},\n",
       "   {'entity': 'container lifecycle hooks',\n",
       "    'description': \"Hooks that can be used to perform actions at specific points in a container's lifecycle\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'The basic execution unit in Kubernetes, consisting of one or more containers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'API servers',\n",
       "    'description': 'Components of the Kubernetes control plane that provide a RESTful API for interacting with cluster resources',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"init containers\",\\n    \"description\": \"can be used to initialize a pod or delay the start of the pod\\'s main containers until a precondition is met.\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"Fabric8 project\",\\n    \"description\": \"is an integrated development platform for Kubernetes.\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"CI/CD pipeline\",\\n    \"description\": \"can be used to build application binaries, container images, and resource manifests and then deploying them in one or more Kubernetes clusters.\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"container lifecycle hooks\",\\n    \"description\": \"can be used to make apps shut down properly without breaking client connections.\",\\n    \"destination_entity\": \"apps\"\\n  },\\n  {\\n    \"source_entity\": \"API servers\",\\n    \"description\": \"are not explicitly mentioned in the text, but are related to Kubernetes as they can be used with it.\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Google Cloud Platform\",\\n    \"description\": \"provides online labs that talk about CI/CD pipeline for DevOps-style development, deployment, and management of microservices on Kubernetes.\",\\n    \"destination_entity\": \"CI/CD pipeline\"\\n  },\\n  {\\n    \"source_entity\": \"Jenkins\",\\n    \"description\": \"is an open-source automation system used in the Fabric8 project to deliver a full CI/CD pipeline for DevOps-style development, deployment, and management of microservices on Kubernetes.\",\\n    \"destination_entity\": \"CI/CD pipeline\"\\n  },\\n  {\\n    \"source_entity\": \"Jsonnet\",\\n    \"description\": \"is a templating language that can be used with Ksonnet to define higher-level fragments and make all manifests consistent and duplication-free.\",\\n    \"destination_entity\": \"Ksonnet\"\\n  },\\n  {\\n    \"source_entity\": \"pods\",\\n    \"description\": \"are relocated much more frequently than apps that are rarely moved between machines.\",\\n    \"destination_entity\": \"apps\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"is a platform for automating the deployment, scaling, and management of containerized applications.\",\\n    \"destination_entity\": \"containerized applications\"\\n  },\\n  {\\n    \"source_entity\": \"Ksonnet\",\\n    \"description\": \"is a templating language that can be used with Jsonnet to define higher-level fragments and make all manifests consistent and duplication-free.\",\\n    \"destination_entity\": \"Jsonnet\"\\n  }\\n]'},\n",
       " {'page': 539,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '507\\nSummary\\n\\uf0a1Give you a few small tips on how to make your apps easier to manage by keep-\\ning image sizes small, adding annotations and multi-dimensional labels to all\\nyour resources, and making it easier to see why an application terminated.\\n\\uf0a1Teach you how to develop Kubernetes apps and run them locally or in Mini-\\nkube before deploying them on a proper multi-node cluster.\\nIn the next and final chapter, we’ll learn how you can extend Kubernetes with your\\nown custom API objects and controllers and how others have done it to create com-\\nplete Platform-as-a-Service solutions on top of Kubernetes.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': None,\n",
       "    'category': 'application'},\n",
       "   {'entity': 'image sizes',\n",
       "    'description': 'keeping image sizes small',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'annotations',\n",
       "    'description': 'adding annotations to resources',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'adding multi-dimensional labels to resources',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Mini-kube',\n",
       "    'description': 'running Kubernetes apps locally or in Mini-kube',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'API objects',\n",
       "    'description': 'custom API objects and controllers for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'controllers', 'description': None, 'category': 'software'},\n",
       "   {'entity': 'Platform-as-a-Service',\n",
       "    'description': 'complete PaaS solutions on top of Kubernetes',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"you\", \"description\": \"keep image sizes small\", \"destination_entity\": \"image sizes\"},\\n  {\"source_entity\": \"you\", \"description\": \"add annotations and multi-dimensional labels\", \"destination_entity\": \"labels\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"run apps locally or in Mini-kube\", \"destination_entity\": \"Mini-kube\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"deploy apps on a proper multi-node cluster\", \"destination_entity\": \"multi-node cluster\"},\\n  {\"source_entity\": \"you\", \"description\": \"develop Kubernetes apps\", \"destination_entity\": \"Kubernetes apps\"},\\n  {\"source_entity\": \"others\", \"description\": \"create Platform-as-a-Service solutions\", \"destination_entity\": \"Platform-as-a-Service solutions\"},\\n  {\"source_entity\": \"you\", \"description\": \"extend Kubernetes with custom API objects and controllers\", \"destination_entity\": \"API objects\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"terminate an application\", \"destination_entity\": \"application\"},\\n  {\"source_entity\": \"you\", \"description\": \"see why an application terminated\", \"destination_entity\": \"annotations\"}\\n]\\n```'},\n",
       " {'page': 540,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '508\\nExtending Kubernetes\\nYou’re almost done. To wrap up, we’ll look at how you can define your own API\\nobjects and create controllers for those objects. We’ll also look at how others have\\nextended Kubernetes and built Platform-as-a-Service solutions on top of it.\\n18.1\\nDefining custom API objects\\nThroughout the book, you’ve learned about the API objects that Kubernetes pro-\\nvides and how they’re used to build application systems. Currently, Kubernetes\\nusers mostly use only these objects even though they represent relatively low-level,\\ngeneric concepts. \\nThis chapter covers\\n\\uf0a1Adding custom objects to Kubernetes\\n\\uf0a1Creating a controller for the custom object\\n\\uf0a1Adding custom API servers\\n\\uf0a1Self-provisioning of services with the Kubernetes \\nService Catalog\\n\\uf0a1Red Hat’s OpenShift Container Platform\\n\\uf0a1Deis Workflow and Helm\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API objects',\n",
       "    'description': 'Application programming interface objects used to build application systems',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Controllers',\n",
       "    'description': 'Software components that manage and control the behavior of API objects',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Custom API objects',\n",
       "    'description': 'User-defined API objects created using Kubernetes API machinery',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service Catalog',\n",
       "    'description': 'Kubernetes component for self-provisioning services',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'OpenShift Container Platform',\n",
       "    'description': \"Red Hat's Platform-as-a-Service solution built on top of Kubernetes\",\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deis Workflow',\n",
       "    'description': 'Container application platform built on top of Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Helm',\n",
       "    'description': 'Package manager for Kubernetes',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"provides API objects for building application systems\",\\n    \"destination_entity\": \"API objects\"\\n  },\\n  {\\n    \"source_entity\": \"Custom API objects\",\\n    \"description\": \"can be added to Kubernetes\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Custom API objects\",\\n    \"description\": \"require a controller for management\",\\n    \"destination_entity\": \"Controllers\"\\n  },\\n  {\\n    \"source_entity\": \"OpenShift Container Platform\",\\n    \"description\": \"is an example of a PaaS solution built on top of Kubernetes\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Service Catalog\",\\n    \"description\": \"allows self-provisioning of services with Kubernetes\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Helm\",\\n    \"description\": \"is a package manager that can be used to deploy applications on top of Kubernetes\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Deis Workflow\",\\n    \"description\": \"is an example of a PaaS solution built on top of Kubernetes\",\\n    \"destination_entity\": \"Kubernetes\"\\n  }\\n]\\n```'},\n",
       " {'page': 541,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '509\\nDefining custom API objects\\n As the Kubernetes ecosystem evolves, you’ll see more and more high-level objects,\\nwhich will be much more specialized than the resources Kubernetes supports today.\\nInstead of dealing with Deployments, Services, ConfigMaps, and the like, you’ll create\\nand manage objects that represent whole applications or software services. A custom\\ncontroller will observe those high-level objects and create low-level objects based on\\nthem. For example, to run a messaging broker inside a Kubernetes cluster, all you’ll\\nneed to do is create an instance of a Queue resource and all the necessary Secrets,\\nDeployments, and Services will be created by a custom Queue controller. Kubernetes\\nalready provides ways of adding custom resources like this. \\n18.1.1 Introducing CustomResourceDefinitions\\nTo define a new resource type, all you need to do is post a CustomResourceDefinition\\nobject (CRD) to the Kubernetes API server. The CustomResourceDefinition object is\\nthe description of the custom resource type. Once the CRD is posted, users can then\\ncreate instances of the custom resource by posting JSON or YAML manifests to the\\nAPI server, the same as with any other Kubernetes resource.\\nNOTE\\nPrior to Kubernetes 1.7, custom resources were defined through Third-\\nPartyResource objects, which were similar to CustomResourceDefinitions, but\\nwere removed in version 1.8.\\nCreating a CRD so that users can create objects of the new type isn’t a useful feature if\\nthose objects don’t make something tangible happen in the cluster. Each CRD will\\nusually also have an associated controller (an active component doing something\\nbased on the custom objects), the same way that all the core Kubernetes resources\\nhave an associated controller, as was explained in chapter 11. For this reason, to prop-\\nerly show what CustomResourceDefinitions allow you to do other than adding\\ninstances of a custom object, a controller must be deployed as well. You’ll do that in\\nthe next example.\\nINTRODUCING THE EXAMPLE CUSTOMRESOURCEDEFINITION\\nLet’s imagine you want to allow users of your Kubernetes cluster to run static websites\\nas easily as possible, without having to deal with Pods, Services, and other Kubernetes\\nresources. What you want to achieve is for users to create objects of type Website that\\ncontain nothing more than the website’s name and the source from which the web-\\nsite’s files (HTML, CSS, PNG, and others) should be obtained. You’ll use a Git reposi-\\ntory as the source of those files. When a user creates an instance of the Website\\nresource, you want Kubernetes to spin up a new web server pod and expose it through\\na Service, as shown in figure 18.1.\\n To create the Website resource, you want users to post manifests along the lines of\\nthe one shown in the following listing.\\n \\n \\n \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API objects',\n",
       "    'description': 'High-level objects managed by Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployments',\n",
       "    'description': 'Kubernetes resource for running applications',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'Kubernetes resource for exposing application services',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMaps',\n",
       "    'description': 'Kubernetes resource for storing configuration data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Queue resource',\n",
       "    'description': 'Custom Kubernetes resource for messaging brokers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'Kubernetes resource for storing sensitive data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CustomResourceDefinitions',\n",
       "    'description': 'Kubernetes object for defining custom resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Third-PartyResource objects',\n",
       "    'description': 'Legacy Kubernetes object for defining custom resources (removed in 1.8)',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Controller',\n",
       "    'description': 'Active component managing custom resource instances',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Kubernetes resource for running application containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'Kubernetes resource for exposing application services',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Git repository',\n",
       "    'description': 'Source code management system used in example',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Web server pod',\n",
       "    'description': 'Containerized web server process managed by Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'Kubernetes resource for exposing application service',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Website resource',\n",
       "    'description': 'Custom Kubernetes resource for static websites (example)',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"API objects\",\\n    \"description\": \"are being evolved to be more specialized\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Deployments\",\\n    \"description\": \"will be created by a custom controller\",\\n    \"destination_entity\": \"Queue resource\"\\n  },\\n  {\\n    \"source_entity\": \"ConfigMaps\",\\n    \"description\": \"and the like will be replaced with more high-level objects\",\\n    \"destination_entity\": \"API objects\"\\n  },\\n  {\\n    \"source_entity\": \"CustomResourceDefinitions\",\\n    \"description\": \"are used to define new resource types\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Queue controller\",\\n    \"description\": \"will create necessary Secrets, Deployments, and Services\",\\n    \"destination_entity\": \"Queue resource\"\\n  },\\n  {\\n    \"source_entity\": \"Git repository\",\\n    \"description\": \"will be used as the source of website files\",\\n    \"destination_entity\": \"Website resource\"\\n  },\\n  {\\n    \"source_entity\": \"Controller\",\\n    \"description\": \"will observe custom high-level objects and create low-level objects\",\\n    \"destination_entity\": \"API objects\"\\n  },\\n  {\\n    \"source_entity\": \"CustomResourceDefinitions\",\\n    \"description\": \"are being used to define a new resource type called Website\",\\n    \"destination_entity\": \"Website resource\"\\n  },\\n  {\\n    \"source_entity\": \"Users\",\\n    \"description\": \"will create instances of the custom resource by posting JSON or YAML manifests\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Services\",\\n    \"description\": \"will be created to expose web server pods\",\\n    \"destination_entity\": \"Web server pod\"\\n  },\\n  {\\n    \"source_entity\": \"Third-PartyResource objects\",\\n    \"description\": \"are being removed in favor of CustomResourceDefinitions\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"and other Kubernetes resources will be replaced with more high-level objects\",\\n    \"destination_entity\": \"API objects\"\\n  },\\n  {\\n    \"source_entity\": \"Deployments\",\\n    \"description\": \"will be created by a custom controller to run static websites\",\\n    \"destination_entity\": \"Website resource\"\\n  },\\n  {\\n    \"source_entity\": \"Web server pod\",\\n    \"description\": \"will be spun up by Kubernetes when users create an instance of the Website resource\",\\n    \"destination_entity\": \"Kubernetes\"\\n  }\\n]\\n```'},\n",
       " {'page': 542,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '510\\nCHAPTER 18\\nExtending Kubernetes\\nkind: Website        \\nmetadata:\\n  name: kubia             \\nspec:\\n  gitRepo: https://github.com/luksa/kubia-website-example.git   \\nLike all other resources, your resource contains a kind and a metadata.name field,\\nand like most resources, it also contains a spec section. It contains a single field called\\ngitRepo (you can choose the name)—it specifies the Git repository containing the\\nwebsite’s files. You’ll also need to include an apiVersion field, but you don’t know yet\\nwhat its value must be for custom resources.\\n If you try posting this resource to Kubernetes, you’ll receive an error because\\nKubernetes doesn’t know what a Website object is yet:\\n$ kubectl create -f imaginary-kubia-website.yaml\\nerror: unable to recognize \"imaginary-kubia-website.yaml\": no matches for \\n➥ /, Kind=Website\\nBefore you can create instances of your custom object, you need to make Kubernetes\\nrecognize them.\\nCREATING A CUSTOMRESOURCEDEFINITION OBJECT\\nTo make Kubernetes accept your custom Website resource instances, you need to post\\nthe CustomResourceDefinition shown in the following listing to the API server.\\napiVersion: apiextensions.k8s.io/v1beta1       \\nkind: CustomResourceDefinition                 \\nmetadata:\\n  name: websites.extensions.example.com      \\nspec:\\n  scope: Namespaced                          \\nListing 18.1\\nAn imaginary custom resource: imaginary-kubia-website.yaml\\nListing 18.2\\nA CustomResourceDefinition manifest: website-crd.yaml\\nWebsite\\nkind: Website\\nmetadata:\\nname: kubia\\nspec:\\ngitRepo:\\ngithub.com/.../kubia.git\\nPod:\\nkubia-website\\nService:\\nkubia-website\\nFigure 18.1\\nEach Website object should result in the creation of a Service and an HTTP \\nserver Pod.\\nA custom \\nobject kind\\nThe name of the website \\n(used for naming the \\nresulting Service and Pod)\\nThe Git \\nrepository \\nholding the \\nwebsite’s files\\nCustomResourceDefinitions belong \\nto this API group and version.\\nThe full\\nname of\\nyour\\ncustom\\nobject\\nYou want Website resources \\nto be namespaced.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Website\n",
       "   kind: Website\n",
       "   metadata:\n",
       "   name: kubia\n",
       "   spec:\n",
       "   gitRepo:\n",
       "   github.com/.../kubia.git, Service: Pod:\n",
       "   kubia-website kubia-website]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Website',\n",
       "    'description': 'Custom resource object',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Linux container running an application',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'Abstraction of a network connection to a pod',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'GitRepo',\n",
       "    'description': \"Field specifying the Git repository containing the website's files\",\n",
       "    'category': 'database'},\n",
       "   {'entity': 'CustomResourceDefinition',\n",
       "    'description': 'Object defining a new custom resource type',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Server exposing the Kubernetes API',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'Field specifying the API version of a resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata.name',\n",
       "    'description': 'Field specifying the name of a resource',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"does not recognize custom Website resource instances\", \"destination_entity\": \"imaginary-kubia-website.yaml\"},\\n  {\"source_entity\": \"you\", \"description\": \"need to post CustomResourceDefinition to API server\", \"destination_entity\": \"API server\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"will create a Service and an HTTP server Pod for each Website object\", \"destination_entity\": \"Pod\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"will create a Service for each Website object\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"CustomResourceDefinition\", \"description\": \"belongs to the API group and version\", \"destination_entity\": \"API extensions.k8s.io/v1beta1\"},\\n  {\"source_entity\": \"metadata.name\", \"description\": \"is used for naming the resulting Service and Pod\", \"destination_entity\": \"Service\"},\\n  {\"source_entity\": \"gitRepo\", \"description\": \"specifies the Git repository containing the website\\'s files\", \"destination_entity\": \"website\\'s files\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"will create a namespaced instance for each Website object\", \"destination_entity\": \"namespaced instance\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"will post the custom resource to Kubernetes\", \"destination_entity\": \"custom resource\"},\\n  {\"source_entity\": \"Website resources\", \"description\": \"should be namespaced\", \"destination_entity\": \"namespace\"},\\n  {\"source_entity\": \"you\", \"description\": \"will need to include an apiVersion field for custom resources\", \"destination_entity\": \"apiVersion\"}\\n]\\n```'},\n",
       " {'page': 543,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '511\\nDefining custom API objects\\n  group: extensions.example.com                \\n  version: v1                                  \\n  names:                                    \\n    kind: Website                           \\n    singular: website                       \\n    plural: websites                        \\nAfter you post the descriptor to Kubernetes, it will allow you to create any number of\\ninstances of the custom Website resource. \\n You can create the CRD from the website-crd.yaml file available in the code archive:\\n$ kubectl create -f website-crd-definition.yaml\\ncustomresourcedefinition \"websites.extensions.example.com\" created\\nI’m sure you’re wondering about the long name of the CRD. Why not call it Website?\\nThe reason is to prevent name clashes. By adding a suffix to the name of the CRD\\n(which will usually include the name of the organization that created the CRD), you\\nkeep CRD names unique. Luckily, the long name doesn’t mean you’ll need to create\\nyour Website resources with kind: websites.extensions.example.com, but as kind:\\nWebsite, as specified in the names.kind property of the CRD. The extensions.exam-\\nple.com part is the API group of your resource. \\n You’ve seen how creating Deployment objects requires you to set apiVersion to\\napps/v1beta1 instead of v1. The part before the slash is the API group (Deployments\\nbelong to the apps API group), and the part after it is the version name (v1beta1 in\\nthe case of Deployments). When creating instances of the custom Website resource,\\nthe apiVersion property will need to be set to extensions.example.com/v1.\\nCREATING AN INSTANCE OF A CUSTOM RESOURCE\\nConsidering what you learned, you’ll now create a proper YAML for your Website\\nresource instance. The YAML manifest is shown in the following listing.\\napiVersion: extensions.example.com/v1       \\nkind: Website                               \\nmetadata:\\n  name: kubia                                \\nspec:\\n  gitRepo: https://github.com/luksa/kubia-website-example.git\\nThe kind of your resource is Website, and the apiVersion is composed of the API\\ngroup and the version number you defined in the CustomResourceDefinition.\\n Create your Website object now:\\n$ kubectl create -f kubia-website.yaml\\nwebsite \"kubia\" created\\nListing 18.3\\nA custom Website resource: kubia-website.yaml\\nDefine an API group and version \\nof the Website resource.\\nYou need to specify the various \\nforms of the custom object’s name.\\nYour custom API\\ngroup and version\\nThis manifest \\ndescribes a Website \\nresource instance.\\nThe name of the \\nWebsite instance\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'Resource object for managing deployments',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API group',\n",
       "    'description': 'Namespace for API resources',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'version name',\n",
       "    'description': 'Version of an API resource',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'CustomResourceDefinition',\n",
       "    'description': 'Resource object for defining custom API objects',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CRD',\n",
       "    'description': 'Short form for CustomResourceDefinition',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'Property of a resource object that specifies the API version',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'Property of a resource object that specifies its kind',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'metadata.name',\n",
       "    'description': 'Property of a resource object that specifies its name',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'spec.gitRepo',\n",
       "    'description': 'Property of a Website resource object that specifies its Git repository URL',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl create',\n",
       "    'description': 'Command for creating resources in Kubernetes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubia-website.yaml',\n",
       "    'description': 'YAML manifest file for creating a Website resource instance',\n",
       "    'category': 'file'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl create\", \"description\": \"create a custom Website resource instance\", \"destination_entity\": \"kubia-website.yaml\"},\\n  {\"source_entity\": \"metadata.name\", \"description\": \"specify the name of the Website resource instance\", \"destination_entity\": \"kubia\"},\\n  {\"source_entity\": \"kind\", \"description\": \"define the kind of the custom object\", \"destination_entity\": \"Website\"},\\n  {\"source_entity\": \"Deployment\", \"description\": \"belong to a specific API group (apps)\", \"destination_entity\": \"API group\"},\\n  {\"source_entity\": \"API group\", \"description\": \"define the API group for the Website resource\", \"destination_entity\": \"extensions.example.com\"},\\n  {\"source_entity\": \"apiVersion\", \"description\": \"specify the version number for the custom Website resource\", \"destination_entity\": \"version name (v1)\"},\\n  {\"source_entity\": \"CustomResourceDefinition\", \"description\": \"define a custom API object\", \"destination_entity\": \"CRD\"},\\n  {\"source_entity\": \"CRD\", \"description\": \"create a new instance of the custom Website resource\", \"destination_entity\": \"kubectl create\"},\\n  {\"source_entity\": \"spec.gitRepo\", \"description\": \"specify the Git repository for the Website resource\", \"destination_entity\": \"https://github.com/luksa/kubia-website-example.git\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"use to create and manage custom API objects\", \"destination_entity\": \"kubectl create\"}\\n]\\n```\\n\\nLet me know if you\\'d like me to explain any of the relations.'},\n",
       " {'page': 544,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '512\\nCHAPTER 18\\nExtending Kubernetes\\nThe response tells you that the API server has accepted and stored your custom\\nWebsite object. Let’s see if you can now retrieve it. \\nRETRIEVING INSTANCES OF A CUSTOM RESOURCE\\nList all the websites in your cluster:\\n$ kubectl get websites\\nNAME      KIND\\nkubia     Website.v1.extensions.example.com\\nAs with existing Kubernetes resources, you can create and then list instances of cus-\\ntom resources. You can also use kubectl describe to see the details of your custom\\nobject, or retrieve the whole YAML with kubectl get, as in the following listing.\\n$ kubectl get website kubia -o yaml\\napiVersion: extensions.example.com/v1\\nkind: Website\\nmetadata:\\n  creationTimestamp: 2017-02-26T15:53:21Z\\n  name: kubia\\n  namespace: default\\n  resourceVersion: \"57047\"\\n  selfLink: /apis/extensions.example.com/v1/.../default/websites/kubia\\n  uid: b2eb6d99-fc3b-11e6-bd71-0800270a1c50\\nspec:\\n  gitRepo: https://github.com/luksa/kubia-website-example.git\\nNote that the resource includes everything that was in the original YAML definition,\\nand that Kubernetes has initialized additional metadata fields the way it does with all\\nother resources. \\nDELETING AN INSTANCE OF A CUSTOM OBJECT\\nObviously, in addition to creating and retrieving custom object instances, you can also\\ndelete them:\\n$ kubectl delete website kubia\\nwebsite \"kubia\" deleted\\nNOTE\\nYou’re deleting an instance of a Website, not the Website CRD\\nresource. You could also delete the CRD object itself, but let’s hold off on that\\nfor a while, because you’ll be creating additional Website instances in the\\nnext section. \\nLet’s go over everything you’ve done. By creating a CustomResourceDefinition object,\\nyou can now store, retrieve, and delete custom objects through the Kubernetes API\\nserver. These objects don’t do anything yet. You’ll need to create a controller to make\\nthem do something. \\nListing 18.4\\nFull Website resource definition retrieved from the API server\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'get',\n",
       "    'description': 'Command to retrieve resources in Kubernetes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'websites',\n",
       "    'description': 'Custom resource type in Kubernetes',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'KIND',\n",
       "    'description': 'Field in API output showing the kind of resource',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'kubectl describe',\n",
       "    'description': 'Command to view detailed information about a resource in Kubernetes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'kubectl get',\n",
       "    'description': 'Command to retrieve resources in YAML format in Kubernetes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'Field in API output showing the version of the API used',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'Field in API output showing the kind of resource',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Section in API output containing metadata about a resource',\n",
       "    'category': 'section'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'Section in API output containing specifications for a resource',\n",
       "    'category': 'section'},\n",
       "   {'entity': 'gitRepo',\n",
       "    'description': 'Field in API output showing the Git repository URL associated with a custom object',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'kubectl delete',\n",
       "    'description': 'Command to delete resources in Kubernetes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Website CRD',\n",
       "    'description': 'Custom resource definition for websites in Kubernetes',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'api server',\n",
       "    'description': 'Component of Kubernetes responsible for handling API requests',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'Kubernetes API server',\n",
       "    'description': 'Server component of Kubernetes responsible for handling API requests',\n",
       "    'category': 'component'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to retrieve custom resources\", \"destination_entity\": \"websites\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to describe custom resources\", \"destination_entity\": \"website\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to delete a custom resource instance\", \"destination_entity\": \"kubia\"},\\n  {\"source_entity\": \"Kubernetes API server\", \"description\": \"accepts and stores custom resource instances\", \"destination_entity\": \"websites\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to get custom resources\", \"destination_entity\": \"website\"},\\n  {\"source_entity\": \"kubectl get\", \"description\": \"used to retrieve whole YAML of a custom resource instance\", \"destination_entity\": \"kubia\"},\\n  {\"source_entity\": \"api server\", \"description\": \"stores and retrieves custom object instances\", \"destination_entity\": \"websites\"},\\n  {\"source_entity\": \"spec\", \"description\": \"includes details of the original YAML definition\", \"destination_entity\": \"website\"},\\n  {\"source_entity\": \"metadata\", \"description\": \"initialized by Kubernetes with additional fields\", \"destination_entity\": \"website\"},\\n  {\"source_entity\": \"get\", \"description\": \"used to list all instances of a custom resource\", \"destination_entity\": \"websites\"},\\n  {\"source_entity\": \"kubectl delete\", \"description\": \"used to delete a custom resource instance\", \"destination_entity\": \"kubia\"},\\n  {\"source_entity\": \"KIND\", \"description\": \"describes the type of custom resource\", \"destination_entity\": \"website\"},\\n  {\"source_entity\": \"apiVersion\", \"description\": \"specifies the version of the custom resource\", \"destination_entity\": \"Website CRD\"},\\n  {\"source_entity\": \"gitRepo\", \"description\": \"used to specify a GitHub repository for a custom resource instance\", \"destination_entity\": \"kubia\"},\\n  {\"source_entity\": \"kubectl describe\", \"description\": \"used to see details of a custom resource instance\", \"destination_entity\": \"website\"}\\n]\\n```'},\n",
       " {'page': 545,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '513\\nDefining custom API objects\\n In general, the point of creating custom objects like this isn’t always to make some-\\nthing happen when the object is created. Certain custom objects are used to store data\\ninstead of using a more generic mechanism such as a ConfigMap. Applications run-\\nning inside pods can query the API server for those objects and read whatever is\\nstored in them. \\n But in this case, we said you wanted the existence of a Website object to result in\\nthe spinning up of a web server serving the contents of the Git repository referenced\\nin the object. We’ll look at how to do that next.\\n18.1.2 Automating custom resources with custom controllers\\nTo make your Website objects run a web server pod exposed through a Service, you’ll\\nneed to build and deploy a Website controller, which will watch the API server for the\\ncreation of Website objects and then create the Service and the web server Pod for\\neach of them. \\n To make sure the Pod is managed and survives node failures, the controller will\\ncreate a Deployment resource instead of an unmanaged Pod directly. The controller’s\\noperation is summarized in figure 18.2.\\nI’ve written a simple initial version of the controller, which works well enough to\\nshow CRDs and the controller in action, but it’s far from being production-ready,\\nbecause it’s overly simplified. The container image is available at docker.io/luksa/\\nwebsite-controller:latest, and the source code is at https:/\\n/github.com/luksa/k8s-\\nwebsite-controller. Instead of going through its source code, I’ll explain what the con-\\ntroller does.\\nAPI server\\nWebsites\\nWebsite:\\nkubia\\nDeployments\\nDeployment:\\nkubia-website\\nServices\\nService:\\nkubia-website\\nWebsite\\ncontroller\\nWatches\\nCreates\\nFigure 18.2\\nThe Website controller \\nwatches for Website objects and \\ncreates a Deployment and a Service.\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Watches\n",
       "   Website\n",
       "   controller\n",
       "   Creates, API server\n",
       "   Websites\n",
       "   Website:\n",
       "   kubia\n",
       "   Deployments\n",
       "   Deployment:\n",
       "   kubia-website\n",
       "   Services\n",
       "   Service:\n",
       "   kubia-website]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Custom API Objects',\n",
       "    'description': 'API objects created to store data or trigger actions',\n",
       "    'category': 'Software/Application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Applications running inside pods\",\\n    \"description\": \"query the API server for custom objects and read data stored in them\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"Website controller\",\\n    \"description\": \"watch the API server for creation of Website objects and create Service and web server Pod\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"store custom data in Website objects instead of using ConfigMap\",\\n    \"destination_entity\": \"ConfigMap\"\\n  },\\n  {\\n    \"source_entity\": \"Website controller\",\\n    \"description\": \"create Deployment resource for web server Pod to ensure management and node failure survival\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"Website controller\",\\n    \"description\": \"create Service resource for web server Pod exposure\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Website controller\",\\n    \"description\": \"create web server Pod for each Website object creation\",\\n    \"destination_entity\": \"Pod\"\\n  }\\n]\\n\\nNote that I\\'ve only extracted relations related to the entities listed, which is '},\n",
       " {'page': 546,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '514\\nCHAPTER 18\\nExtending Kubernetes\\nUNDERSTANDING WHAT THE WEBSITE CONTROLLER DOES\\nImmediately upon startup, the controller starts to watch Website objects by requesting\\nthe following URL:\\nhttp://localhost:8001/apis/extensions.example.com/v1/websites?watch=true\\nYou may recognize the hostname and port—the controller isn’t connecting to the\\nAPI server directly, but is instead connecting to the kubectl proxy process, which\\nruns in a sidecar container in the same pod and acts as the ambassador to the API\\nserver (we examined the ambassador pattern in chapter 8). The proxy forwards the\\nrequest to the API server, taking care of both TLS encryption and authentication\\n(see figure 18.3).\\nThrough the connection opened by this HTTP GET request, the API server will send\\nwatch events for every change to any Website object.\\n The API server sends the ADDED watch event every time a new Website object is cre-\\nated. When the controller receives such an event, it extracts the Website’s name and\\nthe URL of the Git repository from the Website object it received in the watch event\\nand creates a Deployment and a Service object by posting their JSON manifests to the\\nAPI server. \\n The Deployment resource contains a template for a pod with two containers\\n(shown in figure 18.4): one running an nginx server and another one running a git-\\nsync process, which keeps a local directory synced with the contents of a Git repo.\\nThe local directory is shared with the nginx container through an emptyDir volume\\n(you did something similar to that in chapter 6, but instead of keeping the local\\ndirectory synced with a Git repo, you used a gitRepo volume to download the Git\\nrepo’s contents at pod startup; the volume’s contents weren’t kept in sync with the\\nGit repo afterward). The Service is a NodePort Service, which exposes your web\\nserver pod through a random port on each node (the same port is used on all\\nnodes). When a pod is created by the Deployment object, clients can access the web-\\nsite through the node port.\\nPod: website-controller\\nContainer: main\\nWebsite controller\\nGET http://localhost:8001/apis/extensions.\\nexample.com/v1/websites?watch=true\\nGET https://kubernetes:443/apis/extensions.\\nexample.com/v1/websites?watch=true\\nAuthorization: Bearer <token>\\nContainer: proxy\\nkubectl proxy\\nAPI server\\nFigure 18.3\\nThe Website controller talks to the API server through a proxy (in the ambassador container).\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Website Controller',\n",
       "    'description': 'Controller responsible for watching Website objects',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': 'Server responsible for sending watch events and handling API requests',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'Resource that creates a pod with two containers (nginx server and git-sync process)',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'NodePort Service that exposes web server pod through a random port on each node',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Lightweight and portable container runtimes for computing resources',\n",
       "    'category': 'Hardware/Container'},\n",
       "   {'entity': 'Container',\n",
       "    'description': 'Process that runs in isolation from other processes on the same host',\n",
       "    'category': 'Hardware/Container'},\n",
       "   {'entity': 'nginx Server',\n",
       "    'description': 'Web server software',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'git-sync Process',\n",
       "    'description': 'Process responsible for keeping a local directory synced with the contents of a Git repository',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Git Repository',\n",
       "    'description': 'Version control system that stores source code and history',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'EmptyDir Volume',\n",
       "    'description': 'Volume type that is backed by temporary storage, which will be discarded when the pod is deleted',\n",
       "    'category': 'Hardware/Volume'},\n",
       "   {'entity': 'kubectl Proxy',\n",
       "    'description': 'Process that runs in a sidecar container and acts as an ambassador to the API server',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Token',\n",
       "    'description': 'Authorization token used for authentication with the API server',\n",
       "    'category': 'Security/Auth'},\n",
       "   {'entity': 'TLS Encryption',\n",
       "    'description': 'Transport Layer Security (TLS) encryption used to secure communication between the controller and API server',\n",
       "    'category': 'Security/Encryption'}],\n",
       "  'relationships': '[{\"source_entity\": \"Token\",\"description\": \"used for authentication\",\"destination_entity\": \"API Server\"},{\"source_entity\": \"Website Controller\",\"description\": \"requests watch events for Website objects\",\"destination_entity\": \"API Server\"},{\"source_entity\": \"API Server\",\"description\": \"sends watch events for every change to any Website object\",\"destination_entity\": \"Website Controller\"},{\"source_entity\": \"API Server\",\"description\": \"authorizes access with Token\",\"destination_entity\": \"API Server\"},{\"source_entity\": \"Website Controller\",\"description\": \"extracts Website name and Git repository URL from watch event\",\"destination_entity\": \"Git Repository\"},{\"source_entity\": \"Website Controller\",\"description\": \"creates Deployment object by posting JSON manifest to API server\",\"destination_entity\": \"Deployment\"},{\"source_entity\": \"Deployment\",\"description\": \"contains template for pod with nginx container and git-sync process\",\"destination_entity\": \"Pod\"},{\"source_entity\": \"Deployment\",\"description\": \"exposes website through NodePort Service\",\"destination_entity\": \"Service\"},{\"source_entity\": \"git-sync Process\",\"description\": \"keeps local directory synced with Git repository contents\",\"destination_entity\": \"EmptyDir Volume\"},{\"source_entity\": \"nginx Server\",\"description\": \"runs on container within pod\",\"destination_entity\": \"Container\"},{\"source_entity\": \"kubectl Proxy\",\"description\": \"forwards request to API server\",\"destination_entity\": \"API Server\"},{\"source_entity\": \"API Server\",\"description\": \"sends watch events for Website objects changes to Kubernetes\",\"destination_entity\": \"Kubernetes\"},{\"source_entity\": \"Website Controller\",\"description\": \"uses kubectl proxy as ambassador to API server\",\"destination_entity\": \"kubectl Proxy\"}]'},\n",
       " {'page': 547,\n",
       "  'img_cnt': 1,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '515\\nDefining custom API objects\\nThe API server also sends a DELETED watch event when a Website resource instance is\\ndeleted. Upon receiving the event, the controller deletes the Deployment and the Ser-\\nvice resources it created earlier. As soon as a user deletes the Website instance, the\\ncontroller will shut down and remove the web server serving that website.\\nNOTE\\nMy oversimplified controller isn’t implemented properly. The way it\\nwatches the API objects doesn’t guarantee it won’t miss individual watch\\nevents. The proper way to watch objects through the API server is to not only\\nwatch them, but also periodically re-list all objects in case any watch events\\nwere missed. \\nRUNNING THE CONTROLLER AS A POD\\nDuring development, I ran the controller on my local development laptop and used a\\nlocally running kubectl proxy process (not running as a pod) as the ambassador to\\nthe Kubernetes API server. This allowed me to develop quickly, because I didn’t need\\nto build a container image after every change to the source code and then run it\\ninside Kubernetes. \\n When I’m ready to deploy the controller into production, the best way is to run the\\ncontroller inside Kubernetes itself, the way you do with all the other core controllers.\\nTo run the controller in Kubernetes, you can deploy it through a Deployment resource.\\nThe following listing shows an example of such a Deployment.\\napiVersion: apps/v1beta1\\nkind: Deployment\\nmetadata:\\n  name: website-controller\\nspec:\\n  replicas: 1                      \\n  template:\\nListing 18.5\\nA Website controller Deployment: website-controller.yaml\\nPod\\nWebserver\\ncontainer\\nWeb client\\ngit-sync\\ncontainer\\nServes website to\\nweb client through\\na random port\\nClones Git repo\\ninto volume and\\nkeeps it synced\\nemptyDir\\nvolume\\nFigure 18.4\\nThe pod serving \\nthe website specified in the \\nWebsite object\\nYou’ll run a single \\nreplica of the \\ncontroller.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'API server',\n",
       "    'description': 'The API server sends DELETED watch events when a Website resource instance is deleted.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A Deployment resource created by the controller to run the web server.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'A Service resource created by the controller to expose the web server.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Website instance',\n",
       "    'description': 'The user-managed Website resource instance.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Controller',\n",
       "    'description': 'A custom API object controller that watches and deletes resources upon user deletion.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Web server',\n",
       "    'description': 'A process serving the website specified in the Website object.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'kubectl proxy',\n",
       "    'description': 'A locally running process used as an ambassador to the Kubernetes API server.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A Pod resource created by the Deployment to run the web server and controller.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Replica',\n",
       "    'description': 'A single replica of the controller deployed through a Deployment resource.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'git-sync',\n",
       "    'description': 'A container that clones and keeps synced a Git repository into an emptyDir volume.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'emptyDir',\n",
       "    'description': 'A Kubernetes volume type used for storing the cloned Git repository.',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'API objects',\n",
       "    'description': 'Custom API resources, such as Websites and Controllers, managed by the Kubernetes API server.',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"API server\",\\n    \"description\": \"sends DELETED watch event\",\\n    \"destination_entity\": \"Website resource instance\"\\n  },\\n  {\\n    \"source_entity\": \"Controller\",\\n    \"description\": \"deletes Deployment and Service resources\",\\n    \"destination_entity\": \"Deployment\"\\n  },\\n  {\\n    \"source_entity\": \"Controller\",\\n    \"description\": \"shuts down and removes web server\",\\n    \"destination_entity\": \"Web server\"\\n  },\\n  {\\n    \"source_entity\": \"API objects\",\\n    \"description\": \"should be watched through API server\",\\n    \"destination_entity\": \"Controller\"\\n  },\\n  {\\n    \"source_entity\": \"Controller\",\\n    \"description\": \"watches API objects and re-lists them periodically\",\\n    \"destination_entity\": \"API objects\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl proxy\",\\n    \"description\": \"used as ambassador to Kubernetes API server\",\\n    \"destination_entity\": \"Kubernetes API server\"\\n  },\\n  {\\n    \"source_entity\": \"Deployment\",\\n    \"description\": \"runs controller inside Kubernetes itself\",\\n    \"destination_entity\": \"Controller\"\\n  },\\n  {\\n    \"source_entity\": \"Deployment\",\\n    \"description\": \"deploys controller through Deployment resource\",\\n    \"destination_entity\": \"Controller\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"serves website specified in Website object\",\\n    \"destination_entity\": \"Website instance\"\\n  },\\n  {\\n    \"source_entity\": \"git-sync\",\\n    \"description\": \"clones Git repo into volume and keeps it synced\",\\n    \"destination_entity\": \"emptyDir\"\\n  },\\n  {\\n    \"source_entity\": \"Pod\",\\n    \"description\": \"serves website to web client through random port\",\\n    \"destination_entity\": \"Web server\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl proxy\",\\n    \"description\": \"runs locally and used as ambassador\",\\n    \"destination_entity\": \"Kubernetes API server\"\\n  },\\n  {\\n    \"source_entity\": \"Replica\",\\n    \"description\": \"runs single replica of controller\",\\n    \"destination_entity\": \"Controller\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"created by controller earlier\",\\n    \"destination_entity\": \"Deployment\"\\n  }\\n]\\n```\\n\\nNote that some relations have been inferred based on the context and entities provided.'},\n",
       " {'page': 548,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '516\\nCHAPTER 18\\nExtending Kubernetes\\n    metadata:\\n      name: website-controller\\n      labels:\\n        app: website-controller\\n    spec:\\n      serviceAccountName: website-controller    \\n      containers:                                    \\n      - name: main                                   \\n        image: luksa/website-controller              \\n      - name: proxy                                  \\n        image: luksa/kubectl-proxy:1.6.2             \\nAs you can see, the Deployment deploys a single replica of a two-container pod. One\\ncontainer runs your controller, whereas the other one is the ambassador container\\nused for simpler communication with the API server. The pod runs under its own spe-\\ncial ServiceAccount, so you’ll need to create it before you deploy the controller:\\n$ kubectl create serviceaccount website-controller\\nserviceaccount \"website-controller\" created\\nIf Role Based Access Control (RBAC) is enabled in your cluster, Kubernetes will not\\nallow the controller to watch Website resources or create Deployments or Services. To\\nallow it to do that, you’ll need to bind the website-controller ServiceAccount to the\\ncluster-admin ClusterRole, by creating a ClusterRoleBinding like this:\\n$ kubectl create clusterrolebinding website-controller \\n➥ --clusterrole=cluster-admin \\n➥ --serviceaccount=default:website-controller\\nclusterrolebinding \"website-controller\" created\\nOnce you have the ServiceAccount and ClusterRoleBinding in place, you can deploy\\nthe controller’s Deployment. \\nSEEING THE CONTROLLER IN ACTION\\nWith the controller now running, create the kubia Website resource again:\\n$ kubectl create -f kubia-website.yaml\\nwebsite \"kubia\" created\\nNow, let’s check the controller’s logs (shown in the following listing) to see if it has\\nreceived the watch event.\\n$ kubectl logs website-controller-2429717411-q43zs -c main\\n2017/02/26 16:54:41 website-controller started.\\n2017/02/26 16:54:47 Received watch event: ADDED: kubia: https://github.c...\\n2017/02/26 16:54:47 Creating services with name kubia-website in namespa... \\n2017/02/26 16:54:47 Response status: 201 Created\\n2017/02/26 16:54:47 Creating deployments with name kubia-website in name... \\n2017/02/26 16:54:47 Response status: 201 Created\\nListing 18.6\\nDisplaying logs of the Website controller\\nIt will run \\nunder a special \\nServiceAccount.\\nTwo containers: the \\nmain container and \\nthe proxy sidecar\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'metadata',\n",
       "    'description': 'Metadata section of the Deployment',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'Name of the metadata section',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'labels',\n",
       "    'description': 'Labels for the Deployment',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'app',\n",
       "    'description': 'Label key for the application name',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'spec',\n",
       "    'description': 'Specification of the Deployment',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'serviceAccountName',\n",
       "    'description': 'Name of the Service Account used by the Deployment',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'containers',\n",
       "    'description': 'List of containers in the pod',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'main',\n",
       "    'description': 'Main container of the pod',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'image',\n",
       "    'description': 'Image used for the main container',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'luksa/website-controller',\n",
       "    'description': 'Docker image name',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'proxy',\n",
       "    'description': 'Proxy sidecar container of the pod',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl-proxy:1.6.2',\n",
       "    'description': 'Docker image name',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'Kubernetes Deployment object',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ServiceAccount',\n",
       "    'description': 'Kubernetes Service Account object',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'website-controller',\n",
       "    'description': 'Name of the Service Account',\n",
       "    'category': 'application'},\n",
       "   {'entity': '$ kubectl create serviceaccount website-controller',\n",
       "    'description': 'Command to create a Service Account',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'serviceaccount',\n",
       "    'description': 'Kubernetes object type',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Role Based Access Control (RBAC)',\n",
       "    'description': 'Authorization mechanism in Kubernetes',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'ClusterRoleBinding',\n",
       "    'description': 'Kubernetes object that binds a Cluster Role to a Service Account',\n",
       "    'category': 'application'},\n",
       "   {'entity': '$ kubectl create clusterrolebinding website-controller',\n",
       "    'description': 'Command to create a Cluster Role Binding',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'clusterrolebinding',\n",
       "    'description': 'Kubernetes object type',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'kubectl logs website-controller-2429717411-q43zs -c main',\n",
       "    'description': 'Command to get logs from a pod',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'logs',\n",
       "    'description': 'Output of the command to display logs',\n",
       "    'category': 'output'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"needs to be created before deploying controller\",\\n    \"destination_entity\": \"website-controller\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"will not allow controller to watch Website resources or create Deployments or Services if RBAC is enabled\",\\n    \"destination_entity\": \"Role Based Access Control (RBAC)\"\\n  },\\n  {\\n    \"source_entity\": \"ServiceAccount\",\\n    \"description\": \"needs to be bound to cluster-admin ClusterRole\",\\n    \"destination_entity\": \"cluster-admin ClusterRole\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl create serviceaccount\",\\n    \"description\": \"creates a new ServiceAccount named website-controller\",\\n    \"destination_entity\": \"website-controller\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl create clusterrolebinding\",\\n    \"description\": \"binds ServiceAccount to cluster-admin ClusterRole\",\\n    \"destination_entity\": \"cluster-admin ClusterRole\"\\n  },\\n  {\\n    \"source_entity\": \"Deployment\",\\n    \"description\": \"deploys a single replica of a two-container pod\",\\n    \"destination_entity\": \"website-controller\"\\n  },\\n  {\\n    \"source_entity\": \"controller\",\\n    \"description\": \"watches Website resources and creates Deployments or Services\",\\n    \"destination_entity\": \"Role Based Access Control (RBAC)\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl logs website-controller-2429717411-q43zs -c main\",\\n    \"description\": \"displays logs of the Website controller\",\\n    \"destination_entity\": \"website-controller\"\\n  },\\n  {\\n    \"source_entity\": \"main container\",\\n    \"description\": \"runs the controller\",\\n    \"destination_entity\": \"controller\"\\n  },\\n  {\\n    \"source_entity\": \"$ kubectl create -f kubia-website.yaml\",\\n    \"description\": \"creates a new Website resource named kubia\",\\n    \"destination_entity\": \"kubia\"\\n  },\\n  {\\n    \"source_entity\": \"clusterrolebinding\",\\n    \"description\": \"binds ServiceAccount to cluster-admin ClusterRole\",\\n    \"destination_entity\": \"cluster-admin ClusterRole\"\\n  },\\n  {\\n    \"source_entity\": \"$ kubectl create serviceaccount website-controller\",\\n    \"description\": \"creates a new ServiceAccount named website-controller\",\\n    \"destination_entity\": \"website-controller\"\\n  }\\n]\\n```'},\n",
       " {'page': 549,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '517\\nDefining custom API objects\\nThe logs show that the controller received the ADDED event and that it created a Service\\nand a Deployment for the kubia-website Website. The API server responded with a\\n201 Created response, which means the two resources should now exist. Let’s verify\\nthat the Deployment, Service and the resulting Pod were created. The following list-\\ning lists all Deployments, Services and Pods.\\n$ kubectl get deploy,svc,po\\nNAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE  AGE\\ndeploy/kubia-website        1         1         1            1          4s\\ndeploy/website-controller   1         1         1            1          5m\\nNAME                CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\\nsvc/kubernetes      10.96.0.1      <none>        443/TCP        38d\\nsvc/kubia-website   10.101.48.23   <nodes>       80:32589/TCP   4s\\nNAME                                     READY     STATUS    RESTARTS   AGE\\npo/kubia-website-1029415133-rs715        2/2       Running   0          4s\\npo/website-controller-1571685839-qzmg6   2/2       Running   1          5m\\nThere they are. The kubia-website Service, through which you can access your web-\\nsite, is available on port 32589 on all cluster nodes. You can access it with your browser.\\nAwesome, right? \\n Users of your Kubernetes cluster can now deploy static websites in seconds, with-\\nout knowing anything about Pods, Services, or any other Kubernetes resources, except\\nyour custom Website resource. \\n Obviously, you still have room for improvement. The controller could, for exam-\\nple, watch for Service objects and as soon as the node port is assigned, write the URL\\nthe website is accessible at into the status section of the Website resource instance\\nitself. Or it could also create an Ingress object for each website. I’ll leave the imple-\\nmentation of these additional features to you as an exercise.\\n18.1.3 Validating custom objects\\nYou may have noticed that you didn’t specify any kind of validation schema in the Web-\\nsite CustomResourceDefinition. Users can include any field they want in the YAML of\\ntheir Website object. The API server doesn’t validate the contents of the YAML (except\\nthe usual fields like apiVersion, kind, and metadata), so users can create invalid\\nWebsite objects (without a gitRepo field, for example). \\n Is it possible to add validation to the controller and prevent invalid objects from\\nbeing accepted by the API server? It isn’t, because the API server first stores the object,\\nthen returns a success response to the client (kubectl), and only then notifies all the\\nwatchers (the controller is one of them). All the controller can really do is validate\\nthe object when it receives it in a watch event, and if the object is invalid, write the\\nerror message to the Website object (by updating the object through a new request to\\nthe API server). The user wouldn’t be notified of the error automatically. They’d have\\nListing 18.7\\nThe Deployment, Service, and Pod created for the kubia-website\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Controller',\n",
       "    'description': 'A component that receives events from the API server and responds by creating or updating resources.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'An object that provides a network interface for accessing applications or services within a cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployment',\n",
       "    'description': 'A component that manages the rollout of new versions of an application, by creating or updating pods and replicators.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'The basic execution unit in a cluster, representing a single instance of a process or service.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': 'A component that handles incoming requests and updates to resources within the cluster.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'The command-line interface for interacting with a Kubernetes cluster, used to create and manage resources.',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'YAML',\n",
       "    'description': 'A human-readable format for serializing structured data, commonly used in configuration files and resource definitions.',\n",
       "    'category': 'file format'},\n",
       "   {'entity': 'CustomResourceDefinition',\n",
       "    'description': 'A component that defines a custom resource type within the cluster, including its schema and validation rules.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Website resource',\n",
       "    'description': 'A custom resource type defined using CustomResourceDefinition, representing a static website deployment.',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'GitRepo field',\n",
       "    'description': 'A required field within the Website resource definition, specifying the source code repository for the website.',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'Ingress object',\n",
       "    'description': 'An object that provides a network interface for accessing an application or service from outside the cluster.',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\":\"API Server\",\"description\":\"store the object without validation\",\"destination_entity\":\"Users\"},{\"source_entity\":\"Controller\",\"description\":\"validate the object when it receives a watch event\",\"destination_entity\":\"Website objects\"},{\"source_entity\":\"Controller\",\"description\":\"write error message to Website object if invalid\",\"destination_entity\":\"Website objects\"},{\"source_entity\":\"kubectl\",\"description\":\"send request to API server to create object\",\"destination_entity\":\"API Server\"},{\"source_entity\":\"Deployment\",\"description\":\"created for kubia-website website\",\"destination_entity\":\"kubia-website\"},{\"source_entity\":\"Service\",\"description\":\"created for kubia-website website\",\"destination_entity\":\"kubia-website\"},{\"source_entity\":\"Pod\",\"description\":\"created for kubia-website website\",\"destination_entity\":\"kubia-website\"},{\"source_entity\":\"Controller\",\"description\":\"watch for Service objects and update Website resource\",\"destination_entity\":\"Website resource\"},{\"source_entity\":\"Controller\",\"description\":\"create Ingress object for each website\",\"destination_entity\":\"Ingress object\"},{\"source_entity\":\"API Server\",\"description\":\"respond with 201 Created response to controller\",\"destination_entity\":\"Controller\"},{\"source_entity\":\"Users\",\"description\":\"can create invalid Website objects without validation\",\"destination_entity\":\"Website CustomResourceDefinition\"},{\"source_entity\":\"YAML\",\"description\":\"contains field that can be included by users\",\"destination_entity\":\"Website objects\"},{\"source_entity\":\"GitRepo field\",\"description\":\"required field in Website object\",\"destination_entity\":\"Website objects\"}]'},\n",
       " {'page': 550,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '518\\nCHAPTER 18\\nExtending Kubernetes\\nto notice the error message by querying the API server for the Website object. Unless\\nthe user does this, they have no way of knowing whether the object is valid or not.\\n This obviously isn’t ideal. You’d want the API server to validate the object and\\nreject invalid objects immediately. Validation of custom objects was introduced in\\nKubernetes version 1.8 as an alpha feature. To have the API server validate your cus-\\ntom objects, you need to enable the CustomResourceValidation feature gate in the\\nAPI server and specify a JSON schema in the CRD.\\n18.1.4 Providing a custom API server for your custom objects\\nA better way of adding support for custom objects in Kubernetes is to implement your\\nown API server and have the clients talk directly to it. \\nINTRODUCING API SERVER AGGREGATION\\nIn Kubernetes version 1.7, you can integrate your custom API server with the main\\nKubernetes API server, through API server aggregation. Initially, the Kubernetes API\\nserver was a single monolithic component. From Kubernetes version 1.7, multiple\\naggregated API servers will be exposed at a single location. Clients can connect to the\\naggregated API and have their requests transparently forwarded to the appropriate\\nAPI server. This way, the client wouldn’t even be aware that multiple API servers han-\\ndle different objects behind the scenes. Even the core Kubernetes API server may\\neventually end up being split into multiple smaller API servers and exposed as a single\\nserver through the aggregator, as shown in figure 18.5.\\nIn your case, you could create an API server responsible for handling your Website\\nobjects. It could validate those objects the way the core Kubernetes API server validates\\nthem. You’d no longer need to create a CRD to represent those objects, because you’d\\nimplement the Website object type into the custom API server directly. \\n Generally, each API server is responsible for storing their own resources. As shown\\nin figure 18.5, it can either run its own instance of etcd (or a whole etcd cluster), or it\\nMain\\nAPI server\\nCustom\\nAPI server Y\\nCustom\\nAPI server X\\nkubectl\\nUses its own etcd instance\\nfor storing its resources\\nUses CustomResourceDeﬁnitions\\nin main API server as storage\\nmechanism\\netcd\\netcd\\nFigure 18.5\\nAPI server aggregation\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API Server',\n",
       "    'description': 'Server responsible for handling API requests',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CustomResourceValidation',\n",
       "    'description': 'Feature gate for validating custom resources',\n",
       "    'category': 'feature'},\n",
       "   {'entity': 'CRD (Custom Resource Definition)',\n",
       "    'description': 'Definition of a custom resource',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'Key-value store used by Kubernetes for storing data',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'JSON schema',\n",
       "    'description': 'Schema for validating JSON data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server aggregation',\n",
       "    'description': 'Mechanism for integrating multiple API servers behind a single interface',\n",
       "    'category': 'networking'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'Website object',\n",
       "    'description': 'Custom resource type used in example',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Resource validation',\n",
       "    'description': 'Mechanism for validating resources before storing them',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Feature gate',\n",
       "    'description': 'Mechanism for enabling or disabling features in Kubernetes',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\": \"API Server\", \"description\": \"validates custom objects\", \"destination_entity\": \"CustomResourceValidation\"}, \\n {\"source_entity\": \"API Server\", \"description\": \"uses a JSON schema\", \"destination_entity\": \"JSON schema\"}, \\n {\"source_entity\": \"API Server\", \"description\": \"enables feature gate\", \"destination_entity\": \"Feature gate\"}, \\n {\"source_entity\": \"Kubernetes API server\", \"description\": \"exposes multiple aggregated API servers\", \"destination_entity\": \"API server aggregation\"}, \\n {\"source_entity\": \"Kubernetes API server\", \"description\": \"provides storage mechanism for custom resources\", \"destination_entity\": \"CRD (Custom Resource Definition)\"}, \\n {\"source_entity\": \"Custom API server\", \"description\": \"stores its own resources in etcd\", \"destination_entity\": \"etcd\"}, \\n {\"source_entity\": \"kubectl\", \"description\": \"uses CustomResourceDefinitions as storage mechanism\", \"destination_entity\": \"CRD (Custom Resource Definition)\"}, \\n {\"source_entity\": \"API Server Aggregator\", \"description\": \"forwards requests to appropriate API server\", \"destination_entity\": \"Kubernetes API server\"}, \\n {\"source_entity\": \"Website object\", \"description\": \"is validated by custom API server\", \"destination_entity\": \"Custom API server\"}, \\n {\"source_entity\": \"Kubernetes API server\", \"description\": \"rejects invalid objects\", \"destination_entity\": \"API Server\"}, \\n {\"source_entity\": \"kubectl\", \"description\": \"queries API server for Website object\", \"destination_entity\": \"API Server\"}]'},\n",
       " {'page': 551,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '519\\nExtending Kubernetes with the Kubernetes Service Catalog\\ncan store its resources in the core API server’s etcd store by creating CRD instances in\\nthe core API server. In that case, it needs to create a CRD object first, before creating\\ninstances of the CRD, the way you did in the example.\\nREGISTERING A CUSTOM API SERVER\\nTo add a custom API server to your cluster, you’d deploy it as a pod and expose it\\nthrough a Service. Then, to integrate it into the main API server, you’d deploy a YAML\\nmanifest describing an APIService resource like the one in the following listing.\\napiVersion: apiregistration.k8s.io/v1beta1   \\nkind: APIService                             \\nmetadata:\\n  name: v1alpha1.extensions.example.com\\nspec:\\n  group: extensions.example.com           \\n  version: v1alpha1                      \\n  priority: 150\\n  service:                    \\n    name: website-api         \\n    namespace: default        \\nAfter creating the APIService resource from the previous listing, client requests sent\\nto the main API server that contain any resource from the extensions.example.com\\nAPI group and version v1alpha1 would be forwarded to the custom API server pod(s)\\nexposed through the website-api Service. \\nCREATING CUSTOM CLIENTS\\nWhile you can create custom resources from YAML files using the regular kubectl cli-\\nent, to make deployment of custom objects even easier, in addition to providing a cus-\\ntom API server, you can also build a custom CLI tool. This will allow you to add\\ndedicated commands for manipulating those objects, similar to how kubectl allows\\ncreating Secrets, Deployments, and other resources through resource-specific com-\\nmands like kubectl create secret or kubectl create deployment.\\n As I’ve already mentioned, custom API servers, API server aggregation, and other\\nfeatures related to extending Kubernetes are currently being worked on intensively, so\\nthey may change after the book is published. To get up-to-date information on the\\nsubject, refer to the Kubernetes GitHub repos at http:/\\n/github.com/kubernetes.\\n18.2\\nExtending Kubernetes with the Kubernetes Service \\nCatalog\\nOne of the first additional API servers that will be added to Kubernetes through API\\nserver aggregation is the Service Catalog API server. The Service Catalog is a hot topic\\nin the Kubernetes community, so you may want to know about it. \\n Currently, for a pod to consume a service (here I use the term generally, not in\\nrelation to Service resources; for example, a database service includes everything\\nListing 18.8\\nAn APIService YAML definition \\nThis is an APIService \\nresource.\\nThe API group this API \\nserver is responsible for\\nThe supported API version\\nThe Service the custom API \\nserver is exposed through\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'CRD (Custom Resource Definition)',\n",
       "    'description': 'Resource definition for Kubernetes core API server',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'Key-value store used by Kubernetes core API server',\n",
       "    'category': 'Database'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Lightweight and portable container runtime environment',\n",
       "    'category': 'Container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'Resource for exposing a pod to the network',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'APIService',\n",
       "    'description': 'Resource for registering a custom API server with Kubernetes',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'YAML manifest',\n",
       "    'description': 'File format for defining resources in Kubernetes',\n",
       "    'category': 'Data Format'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes',\n",
       "    'category': 'Tool'},\n",
       "   {'entity': 'custom API server',\n",
       "    'description': 'Server that provides a custom API for Kubernetes',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'API group',\n",
       "    'description': 'Namespace for APIs in Kubernetes',\n",
       "    'category': 'Data Format'},\n",
       "   {'entity': 'version',\n",
       "    'description': 'Version of an API in Kubernetes',\n",
       "    'category': 'Data Format'},\n",
       "   {'entity': 'priority',\n",
       "    'description': 'Priority of an API server in Kubernetes',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'service name',\n",
       "    'description': 'Name of a Service resource in Kubernetes',\n",
       "    'category': 'Data Format'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'Namespace for resources in Kubernetes',\n",
       "    'category': 'Data Format'},\n",
       "   {'entity': 'resource-specific commands',\n",
       "    'description': 'Commands for manipulating specific resources in Kubernetes',\n",
       "    'category': 'Tool'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'Resource type for sensitive data in Kubernetes',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Deployments',\n",
       "    'description': 'Resource type for containerized applications in Kubernetes',\n",
       "    'category': 'Software'},\n",
       "   {'entity': 'Service Catalog API server',\n",
       "    'description': 'API server that provides a service catalog for Kubernetes',\n",
       "    'category': 'Software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Service Catalog API server\",\\n    \"description\": \"added to Kubernetes through API server aggregation\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"API group\",\\n    \"description\": \"responsible for The API group this API server is responsible for\",\\n    \"destination_entity\": \"custom API server\"\\n  },\\n  {\\n    \"source_entity\": \"YAML manifest\",\\n    \"description\": \"describing an APIService resource\",\\n    \"destination_entity\": \"APIService\"\\n  },\\n  {\\n    \"source_entity\": \"priority\",\\n    \"description\": \"defined in the spec section of the APIService resource\",\\n    \"destination_entity\": \"APIService\"\\n  },\\n  {\\n    \"source_entity\": \"namespace\",\\n    \"description\": \"defined in the metadata section of the Service resource\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"resource-specific commands\",\\n    \"description\": \"allow creating custom resources from YAML files using kubectl client\",\\n    \"destination_entity\": \"kubectl\"\\n  },\\n  {\\n    \"source_entity\": \"pod\",\\n    \"description\": \"exposed through a Service and containing the custom API server\",\\n    \"destination_entity\": \"Service\"\\n  },\\n  {\\n    \"source_entity\": \"Secrets\",\\n    \"description\": \"created using resource-specific commands like kubectl create secret\",\\n    \"destination_entity\": \"kubectl\"\\n  },\\n  {\\n    \"source_entity\": \"Deployments\",\\n    \"description\": \"created using resource-specific commands like kubectl create deployment\",\\n    \"destination_entity\": \"kubectl\"\\n  },\\n  {\\n    \"source_entity\": \"APIService\",\\n    \"description\": \"defined in the APIService resource to integrate custom API server into main API server\",\\n    \"destination_entity\": \"custom API server\"\\n  },\\n  {\\n    \"source_entity\": \"version\",\\n    \"description\": \"supported by the Service Catalog API server\",\\n    \"destination_entity\": \"Service Catalog API server\"\\n  },\\n  {\\n    \"source_entity\": \"CRD (Custom Resource Definition)\",\\n    \"description\": \"stored in etcd store and created before creating instances of the CRD\",\\n    \"destination_entity\": \"etcd\"\\n  },\\n  {\\n    \"source_entity\": \"Service\",\\n    \"description\": \"exposed through a Service to access custom API server\",\\n    \"destination_entity\": \"custom API server\"\\n  }\\n]\\n```'},\n",
       " {'page': 552,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '520\\nCHAPTER 18\\nExtending Kubernetes\\nrequired to allow users to use a database in their app), someone needs to deploy the\\npods providing the service, a Service resource, and possibly a Secret so the client pod\\ncan use it to authenticate with the service. That someone is usually the same user\\ndeploying the client pod or, if a team is dedicated to deploying these types of general\\nservices, the user needs to file a ticket and wait for the team to provision the service.\\nThis means the user needs to either create the manifests for all the components of the\\nservice, know where to find an existing set of manifests, know how to configure it\\nproperly, and deploy it manually, or wait for the other team to do it. \\n But Kubernetes is supposed to be an easy-to-use, self-service system. Ideally, users\\nwhose apps require a certain service (for example, a web application requiring a back-\\nend database), should be able to say to Kubernetes. “Hey, I need a PostgreSQL data-\\nbase. Please provision one and tell me where and how I can connect to it.” This will\\nsoon be possible through the Kubernetes Service Catalog. \\n18.2.1 Introducing the Service Catalog\\nAs the name suggests, the Service Catalog is a catalog of services. Users can browse\\nthrough the catalog and provision instances of the services listed in the catalog by\\nthemselves without having to deal with Pods, Services, ConfigMaps, and other resources\\nrequired for the service to run. You’ll recognize that this is similar to what you did\\nwith the Website custom resource.\\n Instead of adding custom resources to the API server for each type of service, the\\nService Catalog introduces the following four generic API resources:\\n\\uf0a1A ClusterServiceBroker, which describes an (external) system that can provision\\nservices\\n\\uf0a1A ClusterServiceClass, which describes a type of service that can be provisioned\\n\\uf0a1A ServiceInstance, which is one instance of a service that has been provisioned\\n\\uf0a1A ServiceBinding, which represents a binding between a set of clients (pods)\\nand a ServiceInstance\\nThe relationships between those four resources are shown in the figure 18.6 and\\nexplained in the following paragraphs.\\nIn a nutshell, a cluster admin creates a ClusterServiceBroker resource for each service\\nbroker whose services they’d like to make available in the cluster. Kubernetes then asks\\nthe broker for a list of services that it can provide and creates a ClusterServiceClass\\nresource for each of them. When a user requires a service to be provisioned, they create\\nan ServiceInstance resource and then a ServiceBinding to bind that ServiceInstance to\\nClient pods\\nServiceBinding\\nServiceInstance\\nClusterServiceClass(es)\\nClusterServiceBroker\\nFigure 18.6\\nThe relationships between Service Catalog API resources. \\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [ClusterServiceBroker, Col1, ClusterServiceClass(es), Col3, Col4, ServiceInstance, Col6, Col7, ServiceBinding, Client pods]\n",
       "   Index: []],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Docker',\n",
       "    'description': 'Container runtime engine',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Lightweight and ephemeral containers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'Resources that define a set of pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'API objects to store sensitive information',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterServiceBroker',\n",
       "    'description': 'API resource describing an external system that can provision services',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ClusterServiceClass',\n",
       "    'description': 'API resource describing a type of service that can be provisioned',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceInstance',\n",
       "    'description': 'API resource representing one instance of a service',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceBinding',\n",
       "    'description': 'API resource representing a binding between clients and a ServiceInstance',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Manifests',\n",
       "    'description': 'Configuration files for deploying resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ConfigMaps',\n",
       "    'description': 'Resources that store configuration data',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"needs to create manifests for all components of the service\",\\n    \"destination_entity\": \"Manifests\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"needs to know where to find an existing set of manifests\",\\n    \"destination_entity\": \"Manifests\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"needs to know how to configure the service properly\",\\n    \"destination_entity\": \"ServiceInstance\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"needs to deploy the service manually\",\\n    \"destination_entity\": \"Pods\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"is supposed to be an easy-to-use, self-service system\",\\n    \"destination_entity\": \"User\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes Service Catalog\",\\n    \"description\": \"provides a catalog of services for users to browse and provision\",\\n    \"destination_entity\": \"ServiceInstance\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster admin\",\\n    \"description\": \"creates a ClusterServiceBroker resource for each service broker\",\\n    \"destination_entity\": \"ClusterServiceBroker\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes Service Catalog\",\\n    \"description\": \"asks the ClusterServiceBroker for a list of services it can provide\",\\n    \"destination_entity\": \"ClusterServiceBroker\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes Service Catalog\",\\n    \"description\": \"creates a ClusterServiceClass resource for each service provided by the broker\",\\n    \"destination_entity\": \"ClusterServiceClass\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"requires a service to be provisioned and creates an ServiceInstance resource\",\\n    \"destination_entity\": \"ServiceInstance\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes Service Catalog\",\\n    \"description\": \"binds the ServiceInstance to client pods using a ServiceBinding\",\\n    \"destination_entity\": \"ServiceBinding\"\\n  },\\n  {\\n    \"source_entity\": \"Cluster admin\",\\n    \"description\": \"creates multiple ClusterServiceClass resources for each service provided by the broker\",\\n    \"destination_entity\": \"ClusterServiceClass\"\\n  },\\n  {\\n    \"source_entity\": \"User\",\\n    \"description\": \"needs to authenticate with the ServiceInstance using a Secret resource\",\\n    \"destination_entity\": \"Secrets\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes Service Catalog\",\\n    \"description\": \"uses a ConfigMap resource to store configuration for the service\",\\n    \"destination_entity\": \"ConfigMaps\"\\n  },\\n  {\\n    \"source_entity\": \"Pods\",\\n    \"description\": \"are created by Kubernetes to run containers, e.g. Docker containers\",\\n    \"destination_entity\": \"Docker\"\\n  }\\n]\\n```'},\n",
       " {'page': 553,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '521\\nExtending Kubernetes with the Kubernetes Service Catalog\\ntheir pods. Those pods are then injected with a Secret that holds all the necessary cre-\\ndentials and other data required to connect to the provisioned ServiceInstance.\\n The Service Catalog system architecture is shown in figure 18.7.\\nThe components shown in the figure are explained in the following sections.\\n18.2.2 Introducing the Service Catalog API server and Controller \\nManager\\nSimilar to core Kubernetes, the Service Catalog is a distributed system composed of\\nthree components:\\n\\uf0a1Service Catalog API Server\\n\\uf0a1etcd as the storage\\n\\uf0a1Controller Manager, where all the controllers run\\nThe four Service Catalog–related resources we introduced earlier are created by post-\\ning YAML/JSON manifests to the API server. It then stores them into its own etcd\\ninstance or uses CustomResourceDefinitions in the main API server as an alternative\\nstorage mechanism (in that case, no additional etcd instance is required). \\n The controllers running in the Controller Manager are the ones doing some-\\nthing with those resources. They obviously talk to the Service Catalog API server, the\\nway other core Kubernetes controllers talk to the core API server. Those controllers\\ndon’t provision the requested services themselves. They leave that up to external\\nservice brokers, which are registered by creating ServiceBroker resources in the Ser-\\nvice Catalog API.\\nKubernetes cluster\\nExternal system(s)\\nKubernetes Service Catalog\\nClient pods\\nProvisioned\\nservices\\nBroker A\\nBroker B\\netcd\\nService\\nCatalog\\nAPI server\\nController\\nManager\\nkubectl\\nProvisioned\\nservices\\nClient pods use the\\nprovisioned services\\nFigure 18.7\\nThe architecture of the Service Catalog\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service Catalog',\n",
       "    'description': 'extension to Kubernetes for provisioned services',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'server component of Service Catalog',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Controller Manager',\n",
       "    'description': 'component of Service Catalog that runs controllers',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'etcd',\n",
       "    'description': 'key-value store used by Service Catalog for storage',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceBroker',\n",
       "    'description': 'external system registered with Service Catalog to provision services',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Secret',\n",
       "    'description': 'resource in Kubernetes that holds sensitive data',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'unit of execution in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceInstance',\n",
       "    'description': 'resource in Service Catalog that represents a provisioned service',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Client pods',\n",
       "    'description': 'pods that use the provisioned services',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'provisioned services',\n",
       "    'description': 'services provided by external systems and registered with Service Catalog',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceInstance resources',\n",
       "    'description': 'resources in Service Catalog that represent a provisioned service',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'YAML/JSON manifests',\n",
       "    'description': 'manifests used to create resources in Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'CustomResourceDefinitions',\n",
       "    'description': 'mechanism for defining custom resources in Kubernetes',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[{\"source_entity\":\"Controller Manager\",\"description\":\"manages and does something with resources\",\"destination_entity\":\"Service Instance resources\"},{\"source_entity\":\"Client pods\",\"description\":\"use the provisioned services\",\"destination_entity\":\"provisioned services\"},{\"source_entity\":\"API server\",\"description\":\"stores ServiceInstance resources\",\"destination_entity\":\"etcd\"},{\"source_entity\":\"API server\",\"description\":\"accepts YAML/JSON manifests\",\"destination_entity\":\"ServiceInstance resources\"},{\"source_entity\":\"Controller Manager\",\"description\":\"runs on the Controller Manager\",\"destination_entity\":\"etcd\"},{\"source_entity\":\"Service Broker\",\"description\":\"provisions requested services\",\"destination_entity\":\"provisioned services\"},{\"source_entity\":\"Kubernetes cluster\",\"description\":\"uses the Service Catalog API server\",\"destination_entity\":\"API server\"},{\"source_entity\":\"Client pods\",\"description\":\"are injected with a Secret\",\"destination_entity\":\"Secret\"},{\"source_entity\":\"Controller Manager\",\"description\":\"talks to the Service Catalog API server\",\"destination_entity\":\"API server\"},{\"source_entity\":\"Service Broker\",\"description\":\"is registered by creating ServiceBroker resources\",\"destination_entity\":\"API server\"},{\"source_entity\":\"Kubernetes cluster\",\"description\":\"uses kubectl\",\"destination_entity\":\"kubectl\"},{\"source_entity\":\"Client pods\",\"description\":\"use provisioned services\",\"destination_entity\":\"provisioned services\"},{\"source_entity\":\"Controller Manager\",\"description\":\"does something with resources\",\"destination_entity\":\"ServiceInstance resources\"}]'},\n",
       " {'page': 554,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '522\\nCHAPTER 18\\nExtending Kubernetes\\n18.2.3 Introducing Service Brokers and the OpenServiceBroker API\\nA cluster administrator can register one or more external ServiceBrokers in the Ser-\\nvice Catalog. Every broker must implement the OpenServiceBroker API.\\nINTRODUCING THE OPENSERVICEBROKER API\\nThe Service Catalog talks to the broker through that API. The API is relatively simple.\\nIt’s a REST API providing the following operations:\\n\\uf0a1Retrieving the list of services with GET /v2/catalog\\n\\uf0a1Provisioning a service instance (PUT /v2/service_instances/:id)\\n\\uf0a1Updating a service instance (PATCH /v2/service_instances/:id)\\n\\uf0a1Binding a service instance (PUT /v2/service_instances/:id/service_bind-\\nings/:binding_id)\\n\\uf0a1Unbinding an instance (DELETE /v2/service_instances/:id/service_bind-\\nings/:binding_id)\\n\\uf0a1Deprovisioning a service instance (DELETE /v2/service_instances/:id)\\nYou’ll find the OpenServiceBroker API spec at https:/\\n/github.com/openservicebro-\\nkerapi/servicebroker.\\nREGISTERING BROKERS IN THE SERVICE CATALOG\\nThe cluster administrator registers a broker by posting a ServiceBroker resource man-\\nifest to the Service Catalog API, like the one shown in the following listing.\\napiVersion: servicecatalog.k8s.io/v1alpha1    \\nkind: ClusterServiceBroker                                  \\nmetadata:\\n  name: database-broker                          \\nspec:\\n  url: http://database-osbapi.myorganization.org  \\nThe listing describes an imaginary broker that can provision databases of different\\ntypes. After the administrator creates the ClusterServiceBroker resource, a controller\\nin the Service Catalog Controller Manager connects to the URL specified in the\\nresource to retrieve the list of services this broker can provision.\\n After the Service Catalog retrieves the list of services, it creates a ClusterService-\\nClass resource for each of them. Each ClusterServiceClass resource describes a sin-\\ngle type of service that can be provisioned (an example of a ClusterServiceClass is\\n“PostgreSQL database”). Each ClusterServiceClass has one or more service plans asso-\\nciated with it. These allow the user to choose the level of service they need (for exam-\\nple, a database ClusterServiceClass could provide a “Free” plan, where the size of the\\nListing 18.9\\nA ClusterServiceBroker manifest: database-broker.yaml\\nThe resource kind and \\nthe API group and version\\nThe name of this broker\\nWhere the Service Catalog\\ncan contact the broker\\n(its OpenServiceBroker [OSB] API URL)\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service Brokers',\n",
       "    'description': 'External services that can be provisioned by Kubernetes',\n",
       "    'category': 'database/application'},\n",
       "   {'entity': 'OpenServiceBroker API',\n",
       "    'description': 'REST API for communication between Service Catalog and brokers',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Service Catalog',\n",
       "    'description': 'Component of Kubernetes that provides a catalog of available services',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ClusterServiceBroker',\n",
       "    'description': 'Resource manifest for registering brokers in the Service Catalog',\n",
       "    'category': 'software/process'},\n",
       "   {'entity': 'REST API',\n",
       "    'description': 'Protocol for communication between services',\n",
       "    'category': 'application/network'},\n",
       "   {'entity': 'GET /v2/catalog',\n",
       "    'description': 'Operation to retrieve the list of services from a broker',\n",
       "    'category': 'software/process/command'},\n",
       "   {'entity': 'PUT /v2/service_instances/:id',\n",
       "    'description': 'Operation to provision a service instance',\n",
       "    'category': 'software/process/command'},\n",
       "   {'entity': 'PATCH /v2/service_instances/:id',\n",
       "    'description': 'Operation to update a service instance',\n",
       "    'category': 'software/process/command'},\n",
       "   {'entity': 'PUT /v2/service_instances/:id/service_bindings/:binding_id',\n",
       "    'description': 'Operation to bind a service instance',\n",
       "    'category': 'software/process/command'},\n",
       "   {'entity': 'DELETE /v2/service_instances/:id/service_bindings/:binding_id',\n",
       "    'description': 'Operation to unbind a service instance',\n",
       "    'category': 'software/process/command'},\n",
       "   {'entity': 'DELETE /v2/service_instances/:id',\n",
       "    'description': 'Operation to deprovision a service instance',\n",
       "    'category': 'software/process/command'},\n",
       "   {'entity': 'ClusterServiceClass',\n",
       "    'description': 'Resource for describing a single type of service that can be provisioned',\n",
       "    'category': 'software/process/resource'},\n",
       "   {'entity': 'service plans',\n",
       "    'description': 'Allow users to choose the level of service they need',\n",
       "    'category': 'application/software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"GET /v2/catalog\", \"description\": \"retrieving services list\", \"destination_entity\": \"Service Catalog\"},\\n  {\"source_entity\": \"REST API\", \"description\": \"providing operations for service catalog\", \"destination_entity\": \"Service Catalog\"},\\n  {\"source_entity\": \"PUT /v2/service_instances/:id\", \"description\": \"provisioning a service instance\", \"destination_entity\": \"ClusterServiceBroker\"},\\n  {\"source_entity\": \"PATCH /v2/service_instances/:id\", \"description\": \"updating a service instance\", \"destination_entity\": \"ClusterServiceBroker\"},\\n  {\"source_entity\": \"PUT /v2/service_instances/:id/service_bindings/:binding_id\", \"description\": \"binding a service instance\", \"destination_entity\": \"ClusterServiceBroker\"},\\n  {\"source_entity\": \"DELETE /v2/service_instances/:id/service_bindings/:binding_id\", \"description\": \"unbinding an instance\", \"destination_entity\": \"ClusterServiceBroker\"},\\n  {\"source_entity\": \"DELETE /v2/service_instances/:id\", \"description\": \"deprovisioning a service instance\", \"destination_entity\": \"ClusterServiceBroker\"},\\n  {\"source_entity\": \"OpenServiceBroker API\", \"description\": \"implementing the OpenServiceBroker API for ClusterServiceBroker\", \"destination_entity\": \"ClusterServiceBroker\"},\\n  {\"source_entity\": \"cluster administrator\", \"description\": \"registering Service Brokers in the Service Catalog\", \"destination_entity\": \"Service Catalog\"},\\n  {\"source_entity\": \"cluster administrator\", \"description\": \"posting a ServiceBroker resource manifest to the Service Catalog API\", \"destination_entity\": \"Service Catalog\"},\\n  {\"source_entity\": \"ClusterServiceBroker\", \"description\": \"providing services to the Service Catalog\", \"destination_entity\": \"Service Catalog\"},\\n  {\"source_entity\": \"ClusterServiceClass\", \"description\": \"describing a single type of service that can be provisioned\", \"destination_entity\": \"Service Brokers\"},\\n  {\"source_entity\": \"service plans\", \"description\": \"allowing users to choose the level of service they need\", \"destination_entity\": \"ClusterServiceClass\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"managing Service Brokers and ClusterServiceBroker resources\", \"destination_entity\": \"ClusterServiceBroker\"},\\n  {\"source_entity\": \"Service Catalog\", \"description\": \"retrieving services from the broker through the OpenServiceBroker API\", \"destination_entity\": \"OpenServiceBroker API\"}\\n]\\n```'},\n",
       " {'page': 555,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '523\\nExtending Kubernetes with the Kubernetes Service Catalog\\ndatabase is limited and the underlying storage is a spinning disk, and a “Premium”\\nplan, with unlimited size and SSD storage). \\nLISTING THE AVAILABLE SERVICES IN A CLUSTER\\nUsers of the Kubernetes cluster can retrieve a list of all services that can be provi-\\nsioned in the cluster with kubectl get serviceclasses, as shown in the following\\nlisting.\\n$ kubectl get clusterserviceclasses\\nNAME                KIND\\npostgres-database   ClusterServiceClass.v1alpha1.servicecatalog.k8s.io\\nmysql-database      ServiceClass.v1alpha1.servicecatalog.k8s.io\\nmongodb-database    ServiceClass.v1alpha1.servicecatalog.k8s.io\\nThe listing shows ClusterServiceClasses for services that your imaginary database bro-\\nker could provide. You can compare ClusterServiceClasses to StorageClasses, which we\\ndiscussed in chapter 6. StorageClasses allow you to select the type of storage you’d like\\nto use in your pods, while ClusterServiceClasses allow you to select the type of service.\\n You can see details of one of the ClusterServiceClasses by retrieving its YAML. An\\nexample is shown in the following listing.\\n$ kubectl get serviceclass postgres-database -o yaml\\napiVersion: servicecatalog.k8s.io/v1alpha1\\nbindable: true\\nbrokerName: database-broker                     \\ndescription: A PostgreSQL database\\nkind: ClusterServiceClass\\nmetadata:\\n  name: postgres-database\\n  ...\\nplanUpdatable: false\\nplans:\\n- description: A free (but slow) PostgreSQL instance        \\n  name: free                                                \\n  osbFree: true                                             \\n  ...\\n- description: A paid (very fast) PostgreSQL instance      \\n  name: premium                                            \\n  osbFree: false                                           \\n  ...\\nThe ClusterServiceClass in the listing contains two plans—a free plan, and a premium\\nplan. You can see that this ClusterServiceClass is provided by the database-broker\\nbroker.\\nListing 18.10\\nList of ClusterServiceClasses in a cluster\\nListing 18.11\\nA ClusterServiceClass definition\\nThis ClusterServiceClass \\nis provided by the \\ndatabase-broker.\\nA free plan for \\nthis service\\nA paid plan\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service Catalog',\n",
       "    'description': 'Kubernetes extension for provisioning services',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'get serviceclasses',\n",
       "    'description': 'Command to retrieve a list of available services in a cluster',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'ClusterServiceClass',\n",
       "    'description': 'Kubernetes resource for selecting the type of service',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'StorageClasses',\n",
       "    'description': 'Kubernetes resource for selecting the type of storage',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'postgress-database',\n",
       "    'description': 'ClusterServiceClass for a PostgreSQL database service',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'mysql-database',\n",
       "    'description': 'ClusterServiceClass for a MySQL database service',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'mongodb-database',\n",
       "    'description': 'ClusterServiceClass for a MongoDB database service',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'postgres-database',\n",
       "    'description': 'ClusterServiceClass definition for a PostgreSQL database service',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'free plan',\n",
       "    'description': 'Plan for the free version of a PostgreSQL database service',\n",
       "    'category': 'plan'},\n",
       "   {'entity': 'premium plan',\n",
       "    'description': 'Plan for the paid version of a PostgreSQL database service',\n",
       "    'category': 'plan'},\n",
       "   {'entity': 'database-broker',\n",
       "    'description': 'Broker providing ClusterServiceClasses',\n",
       "    'category': 'service'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"database-broker\",\\n    \"description\": \"provides\",\\n    \"destination_entity\": \"postgres-database\"\\n  },\\n  {\\n    \"source_entity\": \"database-broker\",\\n    \"description\": \"provides\",\\n    \"destination_entity\": \"mysql-database\"\\n  },\\n  {\\n    \"source_entity\": \"database-broker\",\\n    \"description\": \"provides\",\\n    \"destination_entity\": \"mongodb-database\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"retrieves list of services\",\\n    \"destination_entity\": \"postgres-database\"\\n  },\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"retrieves details of ClusterServiceClass\",\\n    \"destination_entity\": \"postgres-database\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"uses\",\\n    \"destination_entity\": \"ClusterServiceClass\"\\n  },\\n  {\\n    \"source_entity\": \"get serviceclasses\",\\n    \"description\": \"executes command to retrieve list of services\",\\n    \"destination_entity\": \"postgres-database\"\\n  },\\n  {\\n    \"source_entity\": \"database-broker\",\\n    \"description\": \"provides free plan for\",\\n    \"destination_entity\": \"postgres-database\"\\n  },\\n  {\\n    \"source_entity\": \"database-broker\",\\n    \"description\": \"provides premium plan for\",\\n    \"destination_entity\": \"postgres-database\"\\n  },\\n  {\\n    \"source_entity\": \"ClusterServiceClass\",\\n    \"description\": \"defines service class for\",\\n    \"destination_entity\": \"postgres-database\"\\n  }\\n]\\n```\\n\\nNote that I have only extracted relations between entities mentioned in the document page. If you would like me to extract relations with other entities not mentioned in the document, please let me know!'},\n",
       " {'page': 556,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '524\\nCHAPTER 18\\nExtending Kubernetes\\n18.2.4 Provisioning and using a service\\nLet’s imagine the pods you’re deploying need to use a database. You’ve inspected the\\nlist of available ClusterServiceClasses and have chosen to use the free plan of the\\npostgres-database ClusterServiceClass. \\nPROVISIONING A SERVICEINSTANCE\\nTo have the database provisioned for you, all you need to do is create a Service-\\nInstance resource, as shown in the following listing.\\napiVersion: servicecatalog.k8s.io/v1alpha1\\nkind: ServiceInstance\\nmetadata:\\n  name: my-postgres-db                     \\nspec:\\n  clusterServiceClassName: postgres-database        \\n  clusterServicePlanName: free                             \\n  parameters:\\n    init-db-args: --data-checksums         \\nYou created a ServiceInstance called my-postgres-db (that will be the name of the\\nresource you’re deploying) and specified the ClusterServiceClass and the chosen\\nplan. You’re also specifying a parameter, which is specific for each broker and Cluster-\\nServiceClass. Let’s imagine you looked up the possible parameters in the broker’s doc-\\numentation.\\n As soon as you create this resource, the Service Catalog will contact the broker the\\nClusterServiceClass belongs to and ask it to provision the service. It will pass on the\\nchosen ClusterServiceClass and plan names, as well as all the parameters you specified.\\n It’s then completely up to the broker to know what to do with this information. In\\nyour case, your database broker will probably spin up a new instance of a PostgreSQL\\ndatabase somewhere—not necessarily in the same Kubernetes cluster or even in\\nKubernetes at all. It could run a Virtual Machine and run the database in there. The\\nService Catalog doesn’t care, and neither does the user requesting the service. \\n You can check if the service has been provisioned successfully by inspecting the\\nstatus section of the my-postgres-db ServiceInstance you created, as shown in the\\nfollowing listing.\\n$ kubectl get instance my-postgres-db -o yaml\\napiVersion: servicecatalog.k8s.io/v1alpha1\\nkind: ServiceInstance\\n...\\nstatus:\\n  asyncOpInProgress: false\\n  conditions:\\nListing 18.12\\nA ServiceInstance manifest: database-instance.yaml\\nListing 18.13\\nInspecting the status of a ServiceInstance\\nYou’re giving this \\nInstance a name.\\nThe ServiceClass \\nand Plan you want\\nAdditional parameters \\npassed to the broker\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service Catalog',\n",
       "    'description': 'API for provision and use services',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'postgres-database',\n",
       "    'description': 'Cluster Service Class for PostgreSQL database',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'free',\n",
       "    'description': 'Plan name for postgres-database Cluster Service Class',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceInstance',\n",
       "    'description': 'Resource for provisioning a service',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'clusterServiceClassName',\n",
       "    'description': 'Field in ServiceInstance resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'clusterServicePlanName',\n",
       "    'description': 'Field in ServiceInstance resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'parameters',\n",
       "    'description': 'Field in ServiceInstance resource for passing parameters to broker',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'init-db-args',\n",
       "    'description': 'Parameter passed to broker for initializing database',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'kubectl',\n",
       "    'description': 'Command-line tool for interacting with Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'get',\n",
       "    'description': 'Command used to retrieve information about a resource',\n",
       "    'category': 'command'},\n",
       "   {'entity': 'instance',\n",
       "    'description': 'Resource type in Service Catalog API',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'asyncOpInProgress',\n",
       "    'description': 'Field in ServiceInstance status section indicating asynchronous operation in progress',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'conditions',\n",
       "    'description': 'Field in ServiceInstance status section containing conditions related to service provisioning',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"ServiceInstance\", \\n   \"description\": \"provision a service instance for postgres-database ClusterServiceClass with free plan\",\\n   \"destination_entity\": \"postgres-database\"},\\n  \\n  {\"source_entity\": \"Service Catalog\", \\n   \"description\": \"contact the broker to provision the service\",\\n   \"destination_entity\": \"broker\"},\\n\\n  {\"source_entity\": \"Service Catalog\", \\n   \"description\": \"pass on ClusterServiceClass and plan names, as well as all parameters to the broker\",\\n   \"destination_entity\": \"broker\"},\\n\\n  {\"source_entity\": \"broker\", \\n   \"description\": \"provision a new instance of PostgreSQL database somewhere\",\\n   \"destination_entity\": \"postgresql database\"},\\n\\n  {\"source_entity\": \"kubectl\", \\n   \"description\": \"inspect the status section of the my-postgres-db ServiceInstance\",\\n   \"destination_entity\": \"ServiceInstance\"},\\n\\n  {\"source_entity\": \"conditions\", \\n   \"description\": \"check if the service has been provisioned successfully\",\\n   \"destination_entity\": \"service\"},\\n\\n  {\"source_entity\": \"clusterServiceClassName\", \\n   \"description\": \"specify the ClusterServiceClass for the ServiceInstance\",\\n   \"destination_entity\": \"postgres-database\"},\\n\\n  {\"source_entity\": \"init-db-args\", \\n   \"description\": \"pass a parameter specific to each broker and ClusterServiceClass\",\\n   \"destination_entity\": \"broker\"},\\n  \\n  {\"source_entity\": \"clusterServicePlanName\", \\n   \"description\": \"specify the chosen plan for the ServiceInstance\",\\n   \"destination_entity\": \"postgres-database\"},\\n\\n  {\"source_entity\": \"Kubernetes\", \\n   \"description\": \"run a Virtual Machine and run the database in there\",\\n   \"destination_entity\": \"Virtual Machine\"},\\n  \\n  {\"source_entity\": \"get\", \\n   \"description\": \"check if the service has been provisioned successfully\",\\n   \"destination_entity\": \"ServiceInstance\"}\\n]\\n```'},\n",
       " {'page': 557,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '525\\nExtending Kubernetes with the Kubernetes Service Catalog\\n  - lastTransitionTime: 2017-05-17T13:57:22Z\\n    message: The instance was provisioned successfully    \\n    reason: ProvisionedSuccessfully                       \\n    status: \"True\"\\n    type: Ready                   \\nA database instance is now running somewhere, but how do you use it in your pods?\\nTo do that, you need to bind it.\\nBINDING A SERVICEINSTANCE\\nTo use a provisioned ServiceInstance in your pods, you create a ServiceBinding\\nresource, as shown in the following listing.\\napiVersion: servicecatalog.k8s.io/v1alpha1\\nkind: ServiceBinding\\nmetadata:\\n  name: my-postgres-db-binding\\nspec:\\n  instanceRef:                          \\n    name: my-postgres-db                \\n  secretName: postgres-secret           \\nThe listing shows that you’re defining a ServiceBinding resource called my-postgres-\\ndb-binding, in which you’re referencing the my-postgres-db service instance you\\ncreated earlier. You’re also specifying a name of a Secret. You want the Service Catalog\\nto put all the necessary credentials for accessing the service instance into a Secret\\ncalled postgres-secret. But where are you binding the ServiceInstance to your pods?\\nNowhere, actually.\\n Currently, the Service Catalog doesn’t yet make it possible to inject pods with the\\nServiceInstance’s credentials. This will be possible when a new Kubernetes feature\\ncalled PodPresets is available. Until then, you can choose a name for the Secret\\nwhere you want the credentials to be stored in and mount that Secret into your pods\\nmanually.\\n When you submit the ServiceBinding resource from the previous listing to the Ser-\\nvice Catalog API server, the controller will contact the Database broker once again\\nand create a binding for the ServiceInstance you provisioned earlier. The broker\\nresponds with a list of credentials and other data necessary for connecting to the data-\\nbase. The Service Catalog creates a new Secret with the name you specified in the\\nServiceBinding resource and stores all that data in the Secret. \\nUSING THE NEWLY CREATED SECRET IN CLIENT PODS\\nThe Secret created by the Service Catalog system can be mounted into pods, so they\\ncan read its contents and use them to connect to the provisioned service instance (a\\nPostgreSQL database in the example). The Secret could look like the one in the fol-\\nlowing listing.\\nListing 18.14\\nA ServiceBinding: my-postgres-db-binding.yaml\\nThe database was \\nprovisioned successfully.\\nIt’s ready to be used.\\nYou’re referencing the \\ninstance you created \\nearlier.\\nYou’d like the credentials \\nfor accessing the service \\nstored in this Secret.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service Catalog',\n",
       "    'description': 'Extension to Kubernetes for provisioning services',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ServiceInstance',\n",
       "    'description': 'Provisioned instance of a service',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ServiceBinding',\n",
       "    'description': 'Resource that binds a ServiceInstance to a pod',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'Secret',\n",
       "    'description': 'Container for sensitive data such as credentials',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'PodPresets',\n",
       "    'description': 'Kubernetes feature for injecting service credentials into pods',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Database broker',\n",
       "    'description': 'Component that manages database instances',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Server that handles Service Catalog requests',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'PostgreSQL',\n",
       "    'description': 'Relational database management system',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Entities that run containers and can access services',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"API server\", \"description\": \"contacts Database broker to create a binding for ServiceInstance\", \"destination_entity\": \"Database broker\"},\\n  {\"source_entity\": \"Service Catalog\", \"description\": \"creates a new Secret with specified name and stores credentials in it\", \"destination_entity\": \"Secret\"},\\n  {\"source_entity\": \"Service Catalog\", \"description\": \"contacts Database broker to retrieve credentials for ServiceInstance\", \"destination_entity\": \"Database broker\"},\\n  {\"source_entity\": \"API server\", \"description\": \"accepts ServiceBinding resource and creates a new Secret with specified name\", \"destination_entity\": \"Secret\"},\\n  {\"source_entity\": \"Service Catalog\", \"description\": \"injects the necessary credentials into a Secret for accessing ServiceInstance\", \"destination_entity\": \"ServiceInstance\"},\\n  {\"source_entity\": \"Database broker\", \"description\": \"provides credentials and data for connecting to database instance\", \"destination_entity\": \"Secret\"},\\n  {\"source_entity\": \"Pods\", \"description\": \"reads contents of Secret created by Service Catalog to connect to provisioned service instance\", \"destination_entity\": \"Secret\"},\\n  {\"source_entity\": \"ServiceBinding\", \"description\": \"references the ServiceInstance created earlier and specifies name for Secret\", \"destination_entity\": \"ServiceInstance\"},\\n  {\"source_entity\": \"API server\", \"description\": \"accepts ServiceBinding resource and creates a new binding for ServiceInstance\", \"destination_entity\": \"Database broker\"},\\n  {\"source_entity\": \"Kubernetes\", \"description\": \"features PodPresets will allow injecting credentials into pods directly\", \"destination_entity\": \"Service Catalog\"},\\n  {\"source_entity\": \"Service Catalog\", \"description\": \"uses the newly created Secret in client Pods to connect to provisioned service instance\", \"destination_entity\": \"Pods\"}\\n]\\n\\nNote that some entities are not explicitly mentioned in the original text, but can be inferred based on the context. I\\'ve tried to accurately represent the relationships between the entities as described in the document page.'},\n",
       " {'page': 558,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '526\\nCHAPTER 18\\nExtending Kubernetes\\n$ kubectl get secret postgres-secret -o yaml\\napiVersion: v1\\ndata:\\n  host: <base64-encoded hostname of the database>     \\n  username: <base64-encoded username>                 \\n  password: <base64-encoded password>                 \\nkind: Secret\\nmetadata:\\n  name: postgres-secret\\n  namespace: default\\n  ...\\ntype: Opaque\\nBecause you can choose the name of the Secret yourself, you can deploy pods before\\nprovisioning or binding the service. As you learned in chapter 7, the pods won’t be\\nstarted until such a Secret exists. \\n If necessary, multiple bindings can be created for different pods. The service bro-\\nker can choose to use the same set of credentials in every binding, but it’s better to\\ncreate a new set of credentials for every binding instance. This way, pods can be pre-\\nvented from using the service by deleting the ServiceBinding resource.\\n18.2.5 Unbinding and deprovisioning\\nOnce you no longer need a ServiceBinding, you can delete it the way you delete other\\nresources:\\n$ kubectl delete servicebinding my-postgres-db-binding\\nservicebinding \"my-postgres-db-binding\" deleted\\nWhen you do this, the Service Catalog controller will delete the Secret and call the bro-\\nker to perform an unbinding operation. The service instance (in your case a PostgreSQL\\ndatabase) is still running. You can therefore create a new ServiceBinding if you want.\\n But if you don’t need the database instance anymore, you should delete the Service-\\nInstance resource also:\\n$ kubectl delete serviceinstance my-postgres-db\\nserviceinstance \"my-postgres-db \" deleted\\nDeleting the ServiceInstance resource causes the Service Catalog to perform a depro-\\nvisioning operation on the service broker. Again, exactly what that means is up to the\\nservice broker, but in your case, the broker should shut down the PostgreSQL data-\\nbase instance that it created when we provisioned the service instance.\\n18.2.6 Understanding what the Service Catalog brings\\nAs you’ve learned, the Service Catalog enables service providers make it possible to\\nexpose those services in any Kubernetes cluster by registering the broker in that cluster.\\nListing 18.15\\nA Secret holding the credentials for connecting to the service instance\\nThis is what the pod \\nshould use to connect to \\nthe database service.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'Command-line interface for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'get secret',\n",
       "    'description': 'Kubectl command to retrieve a Secret resource',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'postgres-secret',\n",
       "    'description': 'Name of the Secret resource holding database credentials',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'apiVersion',\n",
       "    'description': 'Field in the Secret resource YAML output',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'data',\n",
       "    'description': 'Section in the Secret resource YAML output containing database credentials',\n",
       "    'category': 'section'},\n",
       "   {'entity': 'host',\n",
       "    'description': 'Database hostname stored as a base64-encoded string in the Secret resource',\n",
       "    'category': 'database field'},\n",
       "   {'entity': 'username',\n",
       "    'description': 'Database username stored as a base64-encoded string in the Secret resource',\n",
       "    'category': 'database field'},\n",
       "   {'entity': 'password',\n",
       "    'description': 'Database password stored as a base64-encoded string in the Secret resource',\n",
       "    'category': 'database field'},\n",
       "   {'entity': 'kind',\n",
       "    'description': 'Field in the Secret resource YAML output indicating its type',\n",
       "    'category': 'field'},\n",
       "   {'entity': 'metadata',\n",
       "    'description': 'Section in the Secret resource YAML output containing metadata about the Secret resource',\n",
       "    'category': 'section'},\n",
       "   {'entity': 'name',\n",
       "    'description': 'Name of the Secret resource',\n",
       "    'category': 'resource field'},\n",
       "   {'entity': 'namespace',\n",
       "    'description': 'Namespace of the Secret resource',\n",
       "    'category': 'resource field'},\n",
       "   {'entity': 'type',\n",
       "    'description': 'Type of the Secret resource (Opaque)',\n",
       "    'category': 'resource field'},\n",
       "   {'entity': 'Secret',\n",
       "    'description': 'Resource in Kubernetes that holds sensitive information',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'ServiceBinding',\n",
       "    'description': 'Resource in Service Catalog that represents a binding between a service and a pod',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'pod',\n",
       "    'description': 'Compute resource in Kubernetes that can be used to run containers',\n",
       "    'category': 'resource'},\n",
       "   {'entity': 'service catalog',\n",
       "    'description': 'Component of Kubernetes that enables service providers to expose services in any cluster',\n",
       "    'category': 'component'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to retrieve a Secret resource\", \"destination_entity\": \"postgres-secret\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to delete a ServiceBinding resource\", \"destination_entity\": \"my-postgres-db-binding\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"used to delete a ServiceInstance resource\", \"destination_entity\": \"my-postgres-db\"},\\n  {\"source_entity\": \"service catalog\", \"description\": \"enables service providers to expose services in any Kubernetes cluster\", \"destination_entity\": \"Kubernetes cluster\"},\\n  {\"source_entity\": \"pod\", \"description\": \"uses the Secret to connect to the database service\", \"destination_entity\": \"postgres-secret\"},\\n  {\"source_entity\": \"Service Catalog controller\", \"description\": \"performs an unbinding operation when a ServiceBinding is deleted\", \"destination_entity\": \"Secret\"},\\n  {\"source_entity\": \"Service Catalog controller\", \"description\": \"performs a deprovisioning operation on the service broker when a ServiceInstance is deleted\", \"destination_entity\": \"service broker\"},\\n  {\"source_entity\": \"broker\", \"description\": \"is called by the Service Catalog to perform an unbinding operation\", \"destination_entity\": \"Secret\"},\\n  {\"source_entity\": \"broker\", \"description\": \"shuts down the PostgreSQL database instance when a ServiceInstance is deleted\", \"destination_entity\": \"PostgreSQL database instance\"},\\n  {\"source_entity\": \"get secret\", \"description\": \"command used to retrieve a Secret resource\", \"destination_entity\": \"postgres-secret\"}\\n]\\n```\\n\\nNote that I have assumed some entities (e.g. `Service Catalog controller`, `broker`) are not explicitly mentioned in the document page, but can be inferred from the context. Also, I have kept the description brief and concise as per your request. Let me know if you\\'d like me to elaborate on any of these relations!'},\n",
       " {'page': 559,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '527\\nPlatforms built on top of Kubernetes\\nFor example, I’ve been involved with the Service Catalog since early on and have\\nimplemented a broker, which makes it trivial to provision messaging systems and\\nexpose them to pods in a Kubernetes cluster. Another team has implemented a broker\\nthat makes it easy to provision Amazon Web Services. \\n In general, service brokers allow easy provisioning and exposing of services in\\nKubernetes and will make Kubernetes an even more awesome platform for deploying\\nyour applications. \\n18.3\\nPlatforms built on top of Kubernetes\\nI’m sure you’ll agree that Kubernetes is a great system by itself. Given that it’s easily\\nextensible across all its components, it’s no wonder companies that had previously\\ndeveloped their own custom platforms are now re-implementing them on top of\\nKubernetes. Kubernetes is, in fact, becoming a widely accepted foundation for the\\nnew generation of Platform-as-a-Service offerings.\\n Among the best-known PaaS systems built on Kubernetes are Deis Workflow and\\nRed Hat’s OpenShift. We’ll do a quick overview of both systems to give you a sense of\\nwhat they offer on top of all the awesome stuff Kubernetes already offers.\\n18.3.1 Red Hat OpenShift Container Platform\\nRed Hat OpenShift is a Platform-as-a-Service and as such, it has a strong focus on\\ndeveloper experience. Among its goals are enabling rapid development of applica-\\ntions, as well as easy deployment, scaling, and long-term maintenance of those apps.\\nOpenShift has been around much longer than Kubernetes. Versions 1 and 2 were\\nbuilt from the ground up and had nothing to do with Kubernetes, but when Kuberne-\\ntes was announced, Red Hat decided to rebuild OpenShift version 3 from scratch—\\nthis time on top of Kubernetes. When a company such as Red Hat decides to throw\\naway an old version of their software and build a new one on top of an existing tech-\\nnology like Kubernetes, it should be clear to everyone how great Kubernetes is.\\n Kubernetes automates rollouts and application scaling, whereas OpenShift also auto-\\nmates the actual building of application images and their automatic deployment with-\\nout requiring you to integrate a Continuous Integration solution into your cluster. \\n OpenShift also provides user and group management, which allows you to run a\\nproperly secured multi-tenant Kubernetes cluster, where individual users are only\\nallowed to access their own Kubernetes namespaces and the apps running in those\\nnamespaces are also fully network-isolated from each other by default. \\nINTRODUCING ADDITIONAL RESOURCES AVAILABLE IN OPENSHIFT\\nOpenShift provides some additional API objects in addition to all those available in\\nKubernetes. We’ll explain them in the next few paragraphs to give you a good over-\\nview of what OpenShift does and what it provides.\\n The additional resources include\\n\\uf0a1Users & Groups\\n\\uf0a1Projects\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Service Catalog',\n",
       "    'description': 'a platform built on top of Kubernetes for easy provisioning and exposing services',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'broker',\n",
       "    'description': 'a component that makes it trivial to provision messaging systems and expose them to pods in a Kubernetes cluster',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'Kubernetes',\n",
       "    'description': 'an extensible system for deploying applications, easily extensible across all its components',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deis Workflow',\n",
       "    'description': 'a PaaS system built on top of Kubernetes, offering rapid development and deployment of applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Red Hat OpenShift',\n",
       "    'description': 'a Platform-as-a-Service built on top of Kubernetes, enabling rapid development, deployment, scaling, and maintenance of applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'OpenShift Container Platform',\n",
       "    'description': 'a PaaS system that automates rollouts, application scaling, image building, and deployment, with user and group management for secure multi-tenancy',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Kubernetes cluster',\n",
       "    'description': 'a group of machines (physical or virtual) running the Kubernetes system for deploying applications',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'pods',\n",
       "    'description': 'the basic execution unit in a Kubernetes cluster, where containers run and are managed by the Kubernetes control plane',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'namespaces',\n",
       "    'description': 'a way to partition a single Kubernetes cluster into multiple virtual clusters for isolation and resource management',\n",
       "    'category': 'component'},\n",
       "   {'entity': 'users & groups',\n",
       "    'description': 'API objects provided by OpenShift for managing users, groups, and access control in a Kubernetes cluster',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'projects',\n",
       "    'description': 'API objects provided by OpenShift for creating and managing projects in a Kubernetes cluster, with their own resources and isolation',\n",
       "    'category': 'database'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Red Hat OpenShift\",\\n    \"description\": \"re-implements its custom platform on top of Kubernetes\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Deis Workflow\",\\n    \"description\": \"built on top of Kubernetes\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"OpenShift\",\\n    \"description\": \"automates the actual building of application images\",\\n    \"destination_entity\": \"application images\"\\n  },\\n  {\\n    \"source_entity\": \"OpenShift\",\\n    \"description\": \"deploys applications without requiring Continuous Integration solution\",\\n    \"destination_entity\": \"Continuous Integration solution\"\\n  },\\n  {\\n    \"source_entity\": \"Red Hat OpenShift\",\\n    \"description\": \"provides user and group management for multi-tenant Kubernetes cluster\",\\n    \"destination_entity\": \"Kubernetes cluster\"\\n  },\\n  {\\n    \"source_entity\": \"OpenShift\",\\n    \"description\": \"allows individual users to access their own Kubernetes namespaces\",\\n    \"destination_entity\": \"Kubernetes namespace\"\\n  },\\n  {\\n    \"source_entity\": \"OpenShift\",\\n    \"description\": \"fully network-isolates apps running in different namespaces\",\\n    \"destination_entity\": \"apps\"\\n  },\\n  {\\n    \"source_entity\": \"Service Catalog\",\\n    \"description\": \"provisions messaging systems and exposes them to pods in a Kubernetes cluster\",\\n    \"destination_entity\": \"pods\"\\n  },\\n  {\\n    \"source_entity\": \"broker\",\\n    \"description\": \"makes it easy to provision Amazon Web Services\",\\n    \"destination_entity\": \"Amazon Web Services\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"automates rollouts and application scaling\",\\n    \"destination_entity\": \"application\"\\n  },\\n  {\\n    \"source_entity\": \"OpenShift\",\\n    \"description\": \"provides additional API objects beyond those in Kubernetes\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"OpenShift\",\\n    \"description\": \"includes users & groups and projects as additional resources\",\\n    \"destination_entity\": \"users & groups\"\\n  },\\n  {\\n    \"source_entity\": \"OpenShift\",\\n    \"description\": \"includes users & groups and projects as additional resources\",\\n    \"destination_entity\": \"projects\"\\n  }\\n]\\n```\\n\\nNote that I\\'ve only extracted relations between entities present in the input list. Let me know if you have any further questions!'},\n",
       " {'page': 560,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '528\\nCHAPTER 18\\nExtending Kubernetes\\n\\uf0a1Templates\\n\\uf0a1BuildConfigs\\n\\uf0a1DeploymentConfigs\\n\\uf0a1ImageStreams\\n\\uf0a1Routes\\n\\uf0a1And others\\nUNDERSTANDING USERS, GROUPS, AND PROJECTS\\nWe’ve said that OpenShift provides a proper multi-tenant environment to its users.\\nUnlike Kubernetes, which doesn’t have an API object for representing an individual\\nuser of the cluster (but does have ServiceAccounts that represent services running in\\nit), OpenShift provides powerful user management features, which make it possible to\\nspecify what each user can do and what they cannot. These features pre-date the Role-\\nBased Access Control, which is now the standard in vanilla Kubernetes.\\n Each user has access to certain Projects, which are nothing more than Kubernetes\\nNamespaces with additional annotations. Users can only act on resources that reside\\nin the projects the user has access to. Access to the project is granted by a cluster\\nadministrator. \\nINTRODUCING APPLICATION TEMPLATES\\nKubernetes makes it possible to deploy a set of resources through a single JSON or\\nYAML manifest. OpenShift takes this a step further by allowing that manifest to be\\nparameterizable. A parameterizable list in OpenShift is called a Template; it’s a list of\\nobjects whose definitions can include placeholders that get replaced with parameter\\nvalues when you process and then instantiate a template (see figure 18.8).\\nThe template itself is a JSON or YAML file containing a list of parameters that are ref-\\nerenced in resources defined in that same JSON/YAML. The template can be stored\\nin the API server like any other object. Before a template can be instantiated, it needs\\nTemplate\\nParameters\\nAPP_NAME=\"kubia\"\\nVOL_CAPACITY=\"5 Gi\"\\n...\\nPod\\nname: $(APP_NAME)\\nService\\nname: $(APP_NAME)\\nTemplate\\nPod\\nname: kubia\\nService\\nname: kubia\\nPod\\nname: kubia\\nService\\nname: kubia\\nProcess\\nCreate\\nFigure 18.8\\nOpenShift templates\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Templates',\n",
       "    'description': 'A parameterizable list in OpenShift that allows a JSON or YAML manifest to be instantiated with parameter values.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'BuildConfigs',\n",
       "    'description': 'A configuration object in OpenShift that defines the build process for an application.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'DeploymentConfigs',\n",
       "    'description': 'A configuration object in OpenShift that defines how to deploy an application.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ImageStreams',\n",
       "    'description': 'A container image repository in OpenShift.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Routes',\n",
       "    'description': 'A network rule in OpenShift that allows access to an application.',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Users',\n",
       "    'description': 'An individual user of the cluster in OpenShift, with their own permissions and access control.',\n",
       "    'category': 'user'},\n",
       "   {'entity': 'Groups',\n",
       "    'description': 'A group of users in OpenShift, with shared permissions and access control.',\n",
       "    'category': 'user'},\n",
       "   {'entity': 'Projects',\n",
       "    'description': 'A Kubernetes Namespace in OpenShift with additional annotations, used to manage resources and access control.',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'ServiceAccounts',\n",
       "    'description': 'An object in Kubernetes that represents a service running in the cluster, with its own permissions and access control.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Roles',\n",
       "    'description': 'A set of permissions and access control rules in OpenShift that define what users can do.',\n",
       "    'category': 'user'},\n",
       "   {'entity': 'Role-Based Access Control',\n",
       "    'description': 'The standard access control mechanism in Kubernetes, which uses roles to determine user permissions.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'JSON or YAML manifest',\n",
       "    'description': 'A file format used to define a set of resources to be deployed in OpenShift or Kubernetes.',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Parameters',\n",
       "    'description': 'Values that can be referenced and replaced in a template, allowing for customizations and variations.',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'A basic execution unit in Kubernetes or OpenShift, representing a running application instance.',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Service',\n",
       "    'description': 'An object in Kubernetes or OpenShift that provides network access to an application or service.',\n",
       "    'category': 'network'}],\n",
       "  'relationships': '[{\"source_entity\": \"Users\", \"description\": \"have access to certain Projects\", \"destination_entity\": \"Projects\"},\\n{\"source_entity\": \"Cluster administrator\", \"description\": \"grant access to a project\", \"destination_entity\": \"Projects\"},\\n{\"source_entity\": \"Roles\", \"description\": \"define what each user can do and what they cannot\", \"destination_entity\": \"Users\"},\\n{\"source_entity\": \"Role-Based Access Control\", \"description\": \"defines permissions for Users\", \"destination_entity\": \"Users\"},\\n{\"source_entity\": \"Templates\", \"description\": \"can be instantiated to deploy a set of resources\", \"destination_entity\": \"Resources\"},\\n{\"source_entity\": \"Template parameters\", \"description\": \"get replaced with values when instantiating a template\", \"destination_entity\": \"Templates\"},\\n{\"source_entity\": \"DeploymentConfigs\", \"description\": \"are used to deploy an application\", \"destination_entity\": \"Applications\"},\\n{\"source_entity\": \"ServiceAccounts\", \"description\": \"represent services running in the cluster\", \"destination_entity\": \"Services\"},\\n{\"source_entity\": \"ImageStreams\", \"description\": \"can be used to store and manage images\", \"destination_entity\": \"Images\"},\\n{\"source_entity\": \"Routes\", \"description\": \"are used to expose applications to external traffic\", \"destination_entity\": \"Applications\"},\\n{\"source_entity\": \"Pod\", \"description\": \"is a lightweight container runtime environment\", \"destination_entity\": \"Container\"},\\n{\"source_entity\": \"JSON or YAML manifest\", \"description\": \"can be used to deploy resources through a single file\", \"destination_entity\": \"Resources\"}]\\n\\nNote: I have assumed that some entities (like \\'Applications\\', \\'Services\\', \\'Images\\', \\'Resources\\') are implied by the context, but not explicitly listed in the input list. If you want me to remove these relations, please let me know!'},\n",
       " {'page': 561,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '529\\nPlatforms built on top of Kubernetes\\nto be processed. To process a template, you supply the values for the template’s\\nparameters and then OpenShift replaces the references to the parameters with those\\nvalues. The result is a processed template, which is exactly like a Kubernetes resource\\nlist that can then be created with a single POST request.\\n OpenShift provides a long list of pre-fabricated templates that allow users to\\nquickly run complex applications by specifying a few arguments (or none at all, if the\\ntemplate provides good defaults for those arguments). For example, a template can\\nenable the creation of all the Kubernetes resources necessary to run a Java EE appli-\\ncation inside an Application Server, which connects to a back-end database, also\\ndeployed as part of that same template. All those components can be deployed with a\\nsingle command.\\nBUILDING IMAGES FROM SOURCE USING BUILDCONFIGS\\nOne of the best features of OpenShift is the ability to have OpenShift build and imme-\\ndiately deploy an application in the OpenShift cluster by pointing it to a Git repository\\nholding the application’s source code. You don’t need to build the container image at\\nall—OpenShift does that for you. This is done by creating a resource called Build-\\nConfig, which can be configured to trigger builds of container images immediately\\nafter a change is committed to the source Git repository. \\n Although OpenShift doesn’t monitor the Git repository itself, a hook in the repos-\\nitory can notify OpenShift of the new commit. OpenShift will then pull the changes\\nfrom the Git repository and start the build process. A build mechanism called Source\\nTo Image can detect what type of application is in the Git repository and run the\\nproper build procedure for it. For example, if it detects a pom.xml file, which is used\\nin Java Maven-formatted projects, it runs a Maven build. The resulting artifacts are\\npackaged into an appropriate container image, and are then pushed to an internal\\ncontainer registry (provided by OpenShift). From there, they can be pulled and run\\nin the cluster immediately. \\n By creating a BuildConfig object, developers can thus point to a Git repo and not\\nworry about building container images. Developers have almost no need to know\\nanything about containers. Once the ops team deploys an OpenShift cluster and\\ngives developers access to it, those developers can develop their code, commit, and\\npush it to a Git repo, the same way they used to before we started packaging apps into\\ncontainers. Then OpenShift takes care of building, deploying, and managing apps\\nfrom that code.\\nAUTOMATICALLY DEPLOYING NEWLY BUILT IMAGES WITH DEPLOYMENTCONFIGS\\nOnce a new container image is built, it can also automatically be deployed in the clus-\\nter. This is enabled by creating a DeploymentConfig object and pointing it to an\\nImageStream. As the name suggests, an ImageStream is a stream of images. When an\\nimage is built, it’s added to the ImageStream. This enables the DeploymentConfig to\\nnotice the newly built image and allows it to take action and initiate a rollout of the\\nnew image (see figure 18.9).\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': '',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'OpenShift', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'Templates',\n",
       "    'description': 'pre-fabricated templates for running complex applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'PostgreSQL',\n",
       "    'description': 'back-end database',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'BuildConfigs', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'Git repository',\n",
       "    'description': \"holding the application's source code\",\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Source To Image',\n",
       "    'description': 'build mechanism for detecting and running proper build procedure',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'pom.xml file',\n",
       "    'description': 'used in Java Maven-formatted projects',\n",
       "    'category': 'file'},\n",
       "   {'entity': 'BuildConfig object', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'DeploymentConfigs', 'description': '', 'category': 'software'},\n",
       "   {'entity': 'ImageStream',\n",
       "    'description': 'a stream of images',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"OpenShift\",\\n    \"description\": \"processes a template by replacing references to parameters with values\",\\n    \"destination_entity\": \"template\"\\n  },\\n  {\\n    \"source_entity\": \"Users\",\\n    \"description\": \"quickly run complex applications using pre-fabricated templates\",\\n    \"destination_entity\": \"Templates\"\\n  },\\n  {\\n    \"source_entity\": \"OpenShift\",\\n    \"description\": \"builds and deploys an application by pointing to a Git repository\",\\n    \"destination_entity\": \"Git repository\"\\n  },\\n  {\\n    \"source_entity\": \"BuildConfig object\",\\n    \"description\": \"triggers builds of container images immediately after changes are committed\",\\n    \"destination_entity\": \"container image\"\\n  },\\n  {\\n    \"source_entity\": \"OpenShift\",\\n    \"description\": \"pulls changes from the Git repository and starts the build process\",\\n    \"destination_entity\": \"Git repository\"\\n  },\\n  {\\n    \"source_entity\": \"Source To Image\",\\n    \"description\": \"detects what type of application is in the Git repository and runs the proper build procedure\",\\n    \"destination_entity\": \"application\"\\n  },\\n  {\\n    \"source_entity\": \"Maven\",\\n    \"description\": \"runs a Maven build when a pom.xml file is detected\",\\n    \"destination_entity\": \"container image\"\\n  },\\n  {\\n    \"source_entity\": \"OpenShift\",\\n    \"description\": \"packs the resulting artifacts into an appropriate container image\",\\n    \"destination_entity\": \"container image\"\\n  },\\n  {\\n    \"source_entity\": \"Developers\",\\n    \"description\": \"do not need to know anything about containers once they have access to an OpenShift cluster\",\\n    \"destination_entity\": \"OpenShift cluster\"\\n  },\\n  {\\n    \"source_entity\": \"DeploymentConfig object\",\\n    \"description\": \"initiates a rollout of the new image when a new container image is built\",\\n    \"destination_entity\": \"container image\"\\n  },\\n  {\\n    \"source_entity\": \"ImageStream\",\\n    \"description\": \"notifies the DeploymentConfig of the newly built image and initiates a rollout\",\\n    \"destination_entity\": \"DeploymentConfig object\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"can create resources necessary to run a Java EE application inside an Application Server\",\\n    \"destination_entity\": \"Application Server\"\\n  },\\n  {\\n    \"source_entity\": \"PostgreSQL\",\\n    \"description\": \"is deployed as part of the same template\",\\n    \"destination_entity\": \"template\"\\n  }\\n]'},\n",
       " {'page': 562,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '530\\nCHAPTER 18\\nExtending Kubernetes\\nA DeploymentConfig is almost identical to the Deployment object in Kubernetes, but\\nit pre-dates it. Like a Deployment object, it has a configurable strategy for transition-\\ning between Deployments. It contains a pod template used to create the actual pods,\\nbut it also allows you to configure pre- and post-deployment hooks. In contrast to a\\nKubernetes Deployment, it creates ReplicationControllers instead of ReplicaSets and\\nprovides a few additional features.\\nEXPOSING SERVICES EXTERNALLY USING ROUTES\\nEarly on, Kubernetes didn’t provide Ingress objects. To expose Services to the outside\\nworld, you needed to use NodePort or LoadBalancer-type Services. But at that time,\\nOpenShift already provided a better option through a Route resource. A Route is sim-\\nilar to an Ingress, but it provides additional configuration related to TLS termination\\nand traffic splitting. \\n Similar to an Ingress controller, a Route needs a Router, which is a controller that\\nprovides the load balancer or proxy. In contrast to Kubernetes, the Router is available\\nout of the box in OpenShift. \\nTRYING OUT OPENSHIFT\\nIf you’re interested in trying out OpenShift, you can start by using Minishift, which is\\nthe OpenShift equivalent of Minikube, or you can try OpenShift Online Starter at\\nhttps:/\\n/manage.openshift.com, which is a free multi-tenant, hosted solution provided\\nto get you started with OpenShift. \\n18.3.2 Deis Workflow and Helm\\nA company called Deis, which has recently been acquired by Microsoft, also provides a\\nPaaS called Workflow, which is also built on top of Kubernetes. Besides Workflow,\\nPods\\nBuilder pod\\nReplication\\nController\\nBuildConﬁg\\nGit repo\\nDeploymentConﬁg\\nImageStream\\nBuild trigger\\nClones Git repo, builds new\\nimage from source, and adds\\nit to the ImageStream\\nWatches for new images in ImageStream\\nand rolls out new version (similarly to a\\nDeployment)\\nFigure 18.9\\nBuildConfigs and DeploymentConfigs in OpenShift\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'DeploymentConfig',\n",
       "    'description': 'Object for transitioning between Deployments',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pod template',\n",
       "    'description': 'Template used to create actual pods',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'ReplicationControllers',\n",
       "    'description': 'Controller for managing pod replicas',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ReplicaSets',\n",
       "    'description': 'Controller for managing pod replicas',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Routes',\n",
       "    'description': 'Resource for exposing Services externally',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Ingress',\n",
       "    'description': 'Object for exposing Services externally',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'NodePort',\n",
       "    'description': 'Type of Service for exposing to outside world',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'LoadBalancer',\n",
       "    'description': 'Type of Service for exposing to outside world',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Minishift',\n",
       "    'description': 'OpenShift equivalent of Minikube',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Minikube',\n",
       "    'description': 'Lightweight Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'OpenShift Online Starter',\n",
       "    'description': 'Free hosted solution for trying out OpenShift',\n",
       "    'category': 'cloud'},\n",
       "   {'entity': 'Deis Workflow',\n",
       "    'description': 'PaaS built on top of Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Helm',\n",
       "    'description': 'Package manager for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Pods',\n",
       "    'description': 'Execution unit in containerized environment',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'Builder pod',\n",
       "    'description': 'Pod for building new image from source',\n",
       "    'category': 'hardware'},\n",
       "   {'entity': 'BuildConfig',\n",
       "    'description': 'Configuration for building new images',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'DeploymentConfig',\n",
       "    'description': 'Object for transitioning between Deployments',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'ImageStream',\n",
       "    'description': 'Resource for managing container images',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Build trigger',\n",
       "    'description': 'Trigger for building new images',\n",
       "    'category': 'hardware'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Build trigger\",\\n    \"description\": \"clones Git repo, builds new image from source and adds it to ImageStream\",\\n    \"destination_entity\": \"ImageStream\"\\n  },\\n  {\\n    \"source_entity\": \"Build trigger\",\\n    \"description\": \"adds new image to ImageStream\",\\n    \"destination_entity\": \"ImageStream\"\\n  },\\n  {\\n    \"source_entity\": \"DeploymentConfig\",\\n    \"description\": \"has a configurable strategy for transitioning between Deployments\",\\n    \"destination_entity\": \"Deployments\"\\n  },\\n  {\\n    \"source_entity\": \"ReplicationControllers\",\\n    \"description\": \"is created instead of ReplicaSets by DeploymentConfig\",\\n    \"destination_entity\": \"ReplicaSets\"\\n  },\\n  {\\n    \"source_entity\": \"Routes\",\\n    \"description\": \"provides additional configuration related to TLS termination and traffic splitting\",\\n    \"destination_entity\": \"Ingress\"\\n  },\\n  {\\n    \"source_entity\": \"Router\",\\n    \"description\": \"provides the load balancer or proxy for Route\",\\n    \"destination_entity\": \"Route\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"does not provide Ingress objects initially, uses NodePort or LoadBalancer-type Services instead\",\\n    \"destination_entity\": \"NodePort\"\\n  },\\n  {\\n    \"source_entity\": \"OpenShift Online Starter\",\\n    \"description\": \"is a free multi-tenant, hosted solution for trying out OpenShift\",\\n    \"destination_entity\": \"OpenShift\"\\n  },\\n  {\\n    \"source_entity\": \"Minishift\",\\n    \"description\": \"is the OpenShift equivalent of Minikube\",\\n    \"destination_entity\": \"Minikube\"\\n  },\\n  {\\n    \"source_entity\": \"Deis Workflow\",\\n    \"description\": \"provides a PaaS built on top of Kubernetes, similar to Helm\",\\n    \"destination_entity\": \"Helm\"\\n  },\\n  {\\n    \"source_entity\": \"BuildConfigs\",\\n    \"description\": \"watches for new images in ImageStream and rolls out new version (similarly to a Deployment)\",\\n    \"destination_entity\": \"DeploymentConfig\"\\n  }\\n]'},\n",
       " {'page': 563,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '531\\nPlatforms built on top of Kubernetes\\nthey’ve also developed a tool called Helm, which is gaining traction in the Kubernetes\\ncommunity as a standard way of deploying existing apps in Kubernetes. We’ll take a\\nbrief look at both.\\nINTRODUCING DEIS WORKFLOW\\nYou can deploy Deis Workflow to any existing Kubernetes cluster (unlike OpenShift,\\nwhich is a complete cluster with a modified API server and other Kubernetes compo-\\nnents). When you run Workflow, it creates a set of Services and ReplicationControllers,\\nwhich then provide developers with a simple, developer-friendly environment. \\n Deploying new versions of your app is triggered by pushing your changes with git\\npush deis master and letting Workflow take care of the rest. Similar to OpenShift,\\nWorkflow also provides a source to image mechanism, application rollouts and roll-\\nbacks, edge routing, and also log aggregation, metrics, and alerting, which aren’t\\navailable in core Kubernetes. \\n To run Workflow in your Kubernetes cluster, you first need to install the Deis Work-\\nflow and Helm CLI tools and then install Workflow into your cluster. We won’t go into\\nhow to do that here, but if you’d like to learn more, visit the website at https:/\\n/deis\\n.com/workflow. What we’ll explore here is the Helm tool, which can be used without\\nWorkflow and has gained popularity in the community.\\nDEPLOYING RESOURCES THROUGH HELM\\nHelm is a package manager for Kubernetes (similar to OS package managers like yum\\nor apt in Linux or homebrew in MacOS). \\n Helm is comprised of two things:\\n\\uf0a1A helm CLI tool (the client).\\n\\uf0a1Tiller, a server component running as a Pod inside the Kubernetes cluster.\\nThose two components are used to deploy and manage application packages in a\\nKubernetes cluster. Helm application packages are called Charts. They’re combined\\nwith a Config, which contains configuration information and is merged into a Chart\\nto create a Release, which is a running instance of an application (a combined Chart\\nand Config). You deploy and manage Releases using the helm CLI tool, which talks to\\nthe Tiller server, which is the component that creates all the necessary Kubernetes\\nresources defined in the Chart, as shown in figure 18.10.\\n You can create charts yourself and keep them on your local disk, or you can use\\nany existing chart, which is available in the growing list of helm charts maintained by\\nthe community at https:/\\n/github.com/kubernetes/charts. The list includes charts for\\napplications such as PostgreSQL, MySQL, MariaDB, Magento, Memcached, MongoDB,\\nOpenVPN, PHPBB, RabbitMQ, Redis, WordPress, and others.\\n Similar to how you don’t build and install apps developed by other people to your\\nLinux system manually, you probably don’t want to build and manage your own\\nKubernetes manifests for such applications, right? That’s why you’ll want to use Helm\\nand the charts available in the GitHub repository I mentioned. \\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration platform',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Helm',\n",
       "    'description': 'Package manager for Kubernetes',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Tiller',\n",
       "    'description': 'Server component running as a Pod inside the Kubernetes cluster',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Charts',\n",
       "    'description': 'Application packages in Helm',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Release',\n",
       "    'description': 'Running instance of an application created by combining Chart and Config',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Config',\n",
       "    'description': 'Configuration information merged into a Chart to create a Release',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Pod',\n",
       "    'description': 'Lightweight and portable runtime environment in Kubernetes',\n",
       "    'category': 'container'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'Abstraction for exposing an application running inside a Pod',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'ReplicationControllers',\n",
       "    'description': 'Guarantees that a specified number of replicas are running at any given time',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'Deis Workflow',\n",
       "    'description': 'Developer-friendly environment created by Deis when you run Workflow',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'git',\n",
       "    'description': 'Version control system used for tracking changes to code',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'docker',\n",
       "    'description': 'Containerization platform not explicitly mentioned but implied in the context of Kubernetes and Deis',\n",
       "    'category': 'container'}],\n",
       "  'relationships': '[{\"source_entity\": \"Deis Workflow\", \"description\": \"creates a set of Services and ReplicationControllers\", \"destination_entity\": \"Services\"}, {\"source_entity\": \"Workflow\", \"description\": \"provides developers with a simple, developer-friendly environment\", \"destination_entity\": \"developers\"}, {\"source_entity\": \"git\", \"description\": \"triggered by pushing changes\", \"destination_entity\": \"Deis Workflow\"}, {\"source_entity\": \"Helm CLI tool\", \"description\": \"deploys and manages application packages in a Kubernetes cluster\", \"destination_entity\": \"Kubernetes\"}, {\"source_entity\": \"Tiller server\", \"description\": \"creates all necessary Kubernetes resources defined in the Chart\", \"destination_entity\": \"Kubernetes\"}, {\"source_entity\": \"Charts\", \"description\": \"combined with Config to create a Release\", \"destination_entity\": \"Release\"}, {\"source_entity\": \"Config\", \"description\": \"merged into a Chart to create a Release\", \"destination_entity\": \"Release\"}, {\"source_entity\": \"Helm CLI tool\", \"description\": \"deploys and manages Releases\", \"destination_entity\": \"Releases\"}, {\"source_entity\": \"Tiller server\", \"description\": \"runs as a Pod inside the Kubernetes cluster\", \"destination_entity\": \"Pods\"}, {\"source_entity\": \"Kubernetes cluster\", \"description\": \"has a growing list of helm charts maintained by the community\", \"destination_entity\": \"helm charts\"}, {\"source_entity\": \"Users\", \"description\": \"don\\'t want to build and manage their own Kubernetes manifests for applications\", \"destination_entity\": \"Kubernetes manifests\"}, {\"source_entity\": \"Helm\", \"description\": \"provides a package manager for Kubernetes\", \"destination_entity\": \"Kubernetes\"}, {\"source_entity\": \"Charts\", \"description\": \"are available in the growing list of helm charts maintained by the community\", \"destination_entity\": \"helm charts\"}]\\n\\nNote: I had to add an entity called \"Users\" to include the relation between users and Kubernetes manifests.'},\n",
       " {'page': 564,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '532\\nCHAPTER 18\\nExtending Kubernetes\\nWhen you want to run a PostgreSQL or a MySQL database in your Kubernetes cluster,\\ndon’t start writing manifests for them. Instead, check if someone else has already gone\\nthrough the trouble and prepared a Helm chart for it. \\n Once someone prepares a Helm chart for a specific application and adds it to the\\nHelm chart GitHub repo, installing the whole application takes a single one-line com-\\nmand. For example, to run MySQL in your Kubernetes cluster, all you need to do is\\nclone the charts Git repo to your local machine and run the following command (pro-\\nvided you have Helm’s CLI tool and Tiller running in your cluster):\\n$ helm install --name my-database stable/mysql\\nThis will create all the necessary Deployments, Services, Secrets, and PersistentVolu-\\nmeClaims needed to run MySQL in your cluster. You don’t need to concern yourself\\nwith what components you need and how to configure them to run MySQL properly.\\nI’m sure you’ll agree this is awesome.\\nTIP\\nOne of the most interesting charts available in the repo is an OpenVPN\\nchart, which runs an OpenVPN server inside your Kubernetes cluster and\\nallows you to enter the pod network through VPN and access Services as if\\nyour local machine was a pod in the cluster. This is useful when you’re devel-\\noping apps and running them locally.\\nThese were several examples of how Kubernetes can be extended and how companies\\nlike Red Hat and Deis (now Microsoft) have extended it. Now go and start riding the\\nKubernetes wave yourself!\\nKubernetes cluster\\nChart\\nand\\nConﬁg\\nHelm\\nCharts\\n(ﬁles on\\nlocal disk)\\nTiller\\n(pod)\\nDeployments,\\nServices, and\\nother objects\\nhelm\\nCLI tool\\nManages\\ncharts\\nCombines Chart and\\nConﬁg into a Release\\nCreates Kubernetes objects\\ndeﬁned in the Release\\nFigure 18.10\\nOverview of Helm\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'PostgreSQL',\n",
       "    'description': 'Database management system',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'MySQL',\n",
       "    'description': 'Database management system',\n",
       "    'category': 'database'},\n",
       "   {'entity': 'Helm chart',\n",
       "    'description': 'Package for distributing and managing Kubernetes applications',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Deployments',\n",
       "    'description': 'Kubernetes object for running replicated applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Services',\n",
       "    'description': 'Kubernetes object for exposing an application to the network',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Secrets',\n",
       "    'description': 'Kubernetes object for storing sensitive information',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'PersistentVolumeClaims',\n",
       "    'description': 'Kubernetes object for requesting storage resources',\n",
       "    'category': 'storage'},\n",
       "   {'entity': 'OpenVPN server',\n",
       "    'description': 'Virtual private network service',\n",
       "    'category': 'network'},\n",
       "   {'entity': 'Tiller',\n",
       "    'description': 'Helm component that runs in the Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'helm CLI tool',\n",
       "    'description': 'Command-line interface for managing Helm charts',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Release',\n",
       "    'description': 'Helm concept for combining a chart and configuration into a managed application',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[{\"source_entity\": \"helm CLI tool\", \"description\": \"manages charts\", \"destination_entity\": \"charts\"},\\n{\"source_entity\": \"helm CLI tool\", \"description\": \"combines chart and config into a release\", \"destination_entity\": \"Release\"},\\n{\"source_entity\": \"helm CLI tool\", \"description\": \"creates Kubernetes objects defined in the Release\", \"destination_entity\": \"Kubernetes cluster\"},\\n{\"source_entity\": \"helm CLI tool\", \"description\": \"installs MySQL\", \"destination_entity\": \"MySQL\"},\\n{\"source_entity\": \"Helm chart\", \"description\": \"prepared for specific application\", \"destination_entity\": \"application\"},\\n{\"source_entity\": \"Tiller\", \"description\": \"manages charts\", \"destination_entity\": \"charts\"},\\n{\"source_entity\": \"Kubernetes cluster\", \"description\": \"extends with Deployments, Services, and other objects\", \"destination_entity\": \"Deployments\"},\\n{\"source_entity\": \"Kubernetes cluster\", \"description\": \"extends with PersistentVolumeClaims\", \"destination_entity\": \"PersistentVolumeClaims\"},\\n{\"source_entity\": \"OpenVPN server\", \"description\": \"runs inside Kubernetes cluster through VPN\", \"destination_entity\": \"Services\"},\\n{\"source_entity\": \"Release\", \"description\": \"defined by Helm CLI tool and Tiller\", \"destination_entity\": \"Kubernetes cluster\"},\\n{\"source_entity\": \"Release\", \"description\": \"creates Deployments, Services, and other objects\", \"destination_entity\": \"Deployments\"},\\n{\"source_entity\": \"Release\", \"description\": \"manages Secrets\", \"destination_entity\": \"Secrets\"}]'},\n",
       " {'page': 565,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '533\\nSummary\\n18.4\\nSummary\\nThis final chapter has shown you how you can go beyond the existing functionalities\\nKubernetes provides and how companies like Dies and Red Hat have done it. You’ve\\nlearned how\\n\\uf0a1Custom resources can be registered in the API server by creating a Custom-\\nResourceDefinition object.\\n\\uf0a1Instances of custom objects can be stored, retrieved, updated, and deleted with-\\nout having to change the API server code.\\n\\uf0a1A custom controller can be implemented to bring those objects to life.\\n\\uf0a1Kubernetes can be extended with custom API servers through API aggregation.\\n\\uf0a1Kubernetes Service Catalog makes it possible to self-provision external services\\nand expose them to pods running in the Kubernetes cluster.\\n\\uf0a1Platforms-as-a-Service built on top of Kubernetes make it easy to build contain-\\nerized applications inside the same Kubernetes cluster that then runs them. \\n\\uf0a1A package manager called Helm makes deploying existing apps without requir-\\ning you to build resource manifests for them.\\nThank you for taking the time to read through this long book. I hope you’ve learned\\nas much from reading it as I have from writing it.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'Kubernetes',\n",
       "    'description': 'Container orchestration system',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Custom Resource Definition',\n",
       "    'description': 'Object used to register custom resources in the API server',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'API server',\n",
       "    'description': 'Component that manages access to Kubernetes resources',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Controller',\n",
       "    'description': 'Component that runs custom code to manage custom objects',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Custom API server',\n",
       "    'description': 'Custom API server implemented through API aggregation',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Service Catalog',\n",
       "    'description': 'Mechanism for self-provisioning external services in Kubernetes cluster',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Platforms-as-a-Service',\n",
       "    'description': 'Deployment model built on top of Kubernetes for containerized applications',\n",
       "    'category': 'application'},\n",
       "   {'entity': 'Helm',\n",
       "    'description': 'Package manager for deploying existing apps in Kubernetes cluster',\n",
       "    'category': 'software'}],\n",
       "  'relationships': '[\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"provides functionalities\",\\n    \"destination_entity\": \"Companies\"\\n  },\\n  {\\n    \"source_entity\": \"Custom Resource Definition\",\\n    \"description\": \"registered in API server by creating object\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"Instances of custom objects\",\\n    \"description\": \"stored, retrieved, updated and deleted without changing API server code\",\\n    \"destination_entity\": \"API server\"\\n  },\\n  {\\n    \"source_entity\": \"Custom controller\",\\n    \"description\": \"implemented to bring custom objects to life\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Custom API server\",\\n    \"description\": \"extended Kubernetes through API aggregation\",\\n    \"destination_entity\": \"Kubernetes\"\\n  },\\n  {\\n    \"source_entity\": \"Service Catalog\",\\n    \"description\": \"makes self-provisioning of external services possible\",\\n    \"destination_entity\": \"Pods running in Kubernetes cluster\"\\n  },\\n  {\\n    \"source_entity\": \"Platforms-as-a-Service\",\\n    \"description\": \"make it easy to build containerized applications\",\\n    \"destination_entity\": \"Kubernetes cluster\"\\n  },\\n  {\\n    \"source_entity\": \"Helm\",\\n    \"description\": \"makes deploying existing apps possible without building resource manifests\",\\n    \"destination_entity\": \"Existing apps\"\\n  },\\n  {\\n    \"source_entity\": \"Author of the book\",\\n    \"description\": \"has learned much from writing the book\",\\n    \"destination_entity\": \"Book\"\\n  }\\n]'},\n",
       " {'page': 566,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '534\\nappendix A\\nUsing kubectl\\nwith multiple clusters\\nA.1\\nSwitching between Minikube and Google Kubernetes \\nEngine\\nThe examples in this book can either be run in a cluster created with Minikube, or\\none created with Google Kubernetes Engine (GKE). If you plan on using both, you\\nneed to know how to switch between them. A detailed explanation of how to use\\nkubectl with multiple clusters is described in the next section. Here we look at how\\nto switch between Minikube and GKE.\\nSWITCHING TO MINIKUBE\\nLuckily, every time you start up a Minikube cluster with minikube start, it also\\nreconfigures kubectl to use it:\\n$ minikube start\\nStarting local Kubernetes cluster...\\n...\\nSetting up kubeconfig...                            \\nKubectl is now configured to use the cluster.       \\nAfter switching from Minikube to GKE, you can switch back by stopping Minikube\\nand starting it up again. kubectl will then be re-configured to use the Minikube clus-\\nter again.\\nSWITCHING TO GKE\\nTo switch to using the GKE cluster, you can use the following command:\\n$ gcloud container clusters get-credentials my-gke-cluster\\nThis will configure kubectl to use the GKE cluster called my-gke-cluster.\\nMinikube sets up kubectl every \\ntime you start the cluster.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': [],\n",
       "  'entities': [{'entity': 'kubectl',\n",
       "    'description': 'command-line tool for managing Kubernetes clusters',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'minikube',\n",
       "    'description': 'tool for running a single-node Kubernetes cluster locally',\n",
       "    'category': 'software'},\n",
       "   {'entity': 'Google Kubernetes Engine (GKE)',\n",
       "    'description': 'managed container environment service provided by Google Cloud',\n",
       "    'category': 'service'},\n",
       "   {'entity': 'kubeconfig',\n",
       "    'description': 'configuration file used by kubectl to connect to a Kubernetes cluster',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'credentials',\n",
       "    'description': 'authorization information for accessing a Kubernetes cluster',\n",
       "    'category': 'process'},\n",
       "   {'entity': 'clusters',\n",
       "    'description': 'groups of Kubernete nodes that can run applications and services',\n",
       "    'category': 'application'}],\n",
       "  'relationships': '[\\n  {\"source_entity\": \"minikube\", \"description\": \"reconfigure kubectl to use Minikube cluster\", \"destination_entity\": \"kubeconfig\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"configured to use a specific cluster\", \"destination_entity\": \"clusters\"},\\n  {\"source_entity\": \"gcloud container\", \"description\": \"configure kubectl to use GKE cluster\", \"destination_entity\": \"kubectl\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"re-configured to use Minikube cluster again\", \"destination_entity\": \"minikube\"},\\n  {\"source_entity\": \"kubernetes engine (GKE)\", \"description\": \"configured kubectl to use GKE cluster called my-gke-cluster\", \"destination_entity\": \"credentials\"},\\n  {\"source_entity\": \"kubectl\", \"description\": \"reconfigure kubectl to use Minikube cluster\", \"destination_entity\": \"minikube\"}\\n]\\n```\\n\\nNote that I extracted six relations, each with the three keys: `source_entity`, `description`, and `destination_entity`. These relations describe how the entities interact with each other based on the text in the document page.'},\n",
       " {'page': 567,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '535\\nUsing kubectl with multiple clusters or namespaces\\nGOING FURTHER\\nThese two methods should be enough to get you started quickly, but to understand\\nthe complete picture of using kubectl with multiple clusters, study the next section. \\nA.2\\nUsing kubectl with multiple clusters or namespaces\\nIf you need to switch between different Kubernetes clusters, or if you want to work in a\\ndifferent namespace than the default and don’t want to specify the --namespace\\noption every time you run kubectl, here’s how to do it.\\nA.2.1\\nConfiguring the location of the kubeconfig file\\nThe config used by kubectl is usually stored in the ~/.kube/config file. If it’s stored\\nsomewhere else, the KUBECONFIG environment variable needs to point to its location. \\nNOTE\\nYou can use multiple config files and have kubectl use them all at\\nonce by specifying all of them in the KUBECONFIG environment variable (sepa-\\nrate them with a colon).\\nA.2.2\\nUnderstanding the contents of the kubeconfig file\\nAn example config file is shown in the following listing.\\napiVersion: v1\\nclusters:\\n- cluster:                                                 \\n    certificate-authority: /home/luksa/.minikube/ca.crt    \\n    server: https://192.168.99.100:8443                    \\n  name: minikube                                           \\ncontexts:\\n- context:                          \\n    cluster: minikube               \\n    user: minikube                  \\n    namespace: default              \\n  name: minikube                    \\ncurrent-context: minikube             \\nkind: Config\\npreferences: {}\\nusers:\\n- name: minikube                                             \\n  user:                                                      \\n    client-certificate: /home/luksa/.minikube/apiserver.crt  \\n    client-key: /home/luksa/.minikube/apiserver.key          \\nThe kubeconfig file consists of four sections:\\n■\\nA list of clusters\\n■\\nA list of users\\n■\\nA list of contexts\\n■\\nThe name of the current context\\nListing A.1\\nExample kubeconfig file\\nContains \\ninformation about a \\nKubernetes cluster\\nDefines a \\nkubectl \\ncontext\\nThe current context \\nkubectl uses\\nContains \\na user’s \\ncredentials\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 568,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '536\\nAPPENDIX A\\nUsing kubectl with multiple clusters\\nEach cluster, user, and context has a name. The name is used to refer to the context,\\nuser, or cluster. \\nCLUSTERS\\nA cluster entry represents a Kubernetes cluster and contains the URL of the API\\nserver, the certificate authority (CA) file, and possibly a few other configuration\\noptions related to communication with the API server. The CA certificate can be\\nstored in a separate file and referenced in the kubeconfig file, or it can be included in\\nit directly in the certificate-authority-data field.\\nUSERS\\nEach user defines the credentials to use when talking to an API server. This can be a\\nusername and password pair, an authentication token, or a client key and certificate.\\nThe certificate and key can be included in the kubeconfig file (through the client-\\ncertificate-data and client-key-data properties) or stored in separate files and\\nreferenced in the config file, as shown in listing A.1.\\nCONTEXTS\\nA context ties together a cluster, a user, and the default namespace kubectl should use\\nwhen performing commands. Multiple contexts can point to the same user or cluster. \\nTHE CURRENT CONTEXT\\nWhile there can be multiple contexts defined in the kubeconfig file, at any given time\\nonly one of them is the current context. Later we’ll see how the current context can\\nbe changed.\\nA.2.3\\nListing, adding, and modifying kube config entries\\nYou can edit the file manually to add, modify, and remove clusters, users, or contexts,\\nbut you can also do it through one of the kubectl config commands.\\nADDING OR MODIFYING A CLUSTER\\nTo add another cluster, use the kubectl config set-cluster command:\\n$ kubectl config set-cluster my-other-cluster \\n➥ --server=https://k8s.example.com:6443 \\n➥ --certificate-authority=path/to/the/cafile\\nThis will add a cluster called my-other-cluster with the API server located at https:/\\n/\\nk8s.example.com:6443. To see additional options you can pass to the command, run\\nkubectl config set-cluster to have it print out usage examples.\\n If a cluster by that name already exists, the set-cluster command will overwrite\\nits configuration options. \\nADDING OR MODIFYING USER CREDENTIALS\\nAdding and modifying users is similar to adding or modifying a cluster. To add a user\\nthat authenticates with the API server using a username and password, run the follow-\\ning command:\\n$ kubectl config set-credentials foo --username=foo --password=pass\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 569,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': \"537\\nUsing kubectl with multiple clusters or namespaces\\nTo use token-based authentication, run the following instead:\\n$ kubectl config set-credentials foo --token=mysecrettokenXFDJIQ1234\\nBoth these examples store user credentials under the name foo. If you use the same\\ncredentials for authenticating against different clusters, you can define a single user\\nand use it with both clusters. \\nTYING CLUSTERS AND USER CREDENTIALS TOGETHER\\nA context defines which user to use with which cluster, but can also define the name-\\nspace that kubectl should use, when you don’t specify the namespace explicitly with\\nthe --namespace or -n option.\\n The following command is used to create a new context that ties together the clus-\\nter and the user you created:\\n$ kubectl config set-context some-context --cluster=my-other-cluster \\n➥ --user=foo --namespace=bar\\nThis creates a context called some-context that uses the my-other-cluster cluster\\nand the foo user credentials. The default namespace in this context is set to bar. \\n You can also use the same command to change the namespace of your current\\ncontext, for example. You can get the name of the current context like so:\\n$ kubectl config current-context\\nminikube\\nYou then change the namespace like this:\\n$ kubectl config set-context minikube --namespace=another-namespace\\nRunning this simple command once is much more user-friendly compared to having\\nto include the --namespace option every time you run kubectl.\\nTIP\\nTo easily switch between namespaces, define an alias like this: alias\\nkcd='kubectl config set-context $(kubectl config current-context)\\n--namespace '. You can then switch between namespaces with kcd some-\\nnamespace.\\nA.2.4\\nUsing kubectl with different clusters, users, and contexts\\nWhen you run kubectl commands, the cluster, user, and namespace defined in the\\nkubeconfig’s current context are used, but you can override them using the following\\ncommand-line options:\\n■\\n--user to use a different user from the kubeconfig file.\\n■\\n--username and --password to use a different username and/or password (they\\ndon’t need to be specified in the config file). If using other types of authentica-\\ntion, you can use --client-key and --client-certificate or --token.\\n■\\n--cluster to use a different cluster (must be defined in the config file).\\n \\n\",\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 570,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '538\\nAPPENDIX A\\nUsing kubectl with multiple clusters\\n■\\n--server to specify the URL of a different server (which isn’t in the config file).\\n■\\n--namespace to use a different namespace.\\nA.2.5\\nSwitching between contexts \\nInstead of modifying the current context as in one of the previous examples, you can\\nalso use the set-context command to create an additional context and then switch\\nbetween contexts. This is handy when working with multiple clusters (use set-clus-\\nter to create cluster entries for them). \\n Once you have multiple contexts set up, switching between them is trivial:\\n$ kubectl config use-context my-other-context\\nThis switches the current context to my-other-context. \\nA.2.6\\nListing contexts and clusters\\nTo list all the contexts defined in your kubeconfig file, run the following command:\\n$ kubectl config get-contexts\\nCURRENT   NAME          CLUSTER       AUTHINFO            NAMESPACE\\n*         minikube      minikube      minikube            default\\n          rpi-cluster   rpi-cluster   admin/rpi-cluster\\n          rpi-foo       rpi-cluster   admin/rpi-cluster   foo\\nAs you can see, I’m using three different contexts. The rpi-cluster and the rpi-foo\\ncontexts use the same cluster and credentials, but default to different namespaces.\\n Listing clusters is similar:\\n$ kubectl config get-clusters\\nNAME\\nrpi-cluster\\nminikube\\nCredentials can’t be listed for security reasons.\\nA.2.7\\nDeleting contexts and clusters\\nTo clean up the list of contexts or clusters, you can either delete the entries from the\\nkubeconfig file manually or use the following two commands:\\n$ kubectl config delete-context my-unused-context\\nand\\n$ kubectl config delete-cluster my-old-cluster\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 571,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '539\\nappendix B\\nSetting up a multi-node\\ncluster with kubeadm\\nThis appendix shows how to install a Kubernetes cluster with multiple nodes. You’ll\\nrun the nodes inside virtual machines through VirtualBox, but you can also use a\\ndifferent virtualization tool or bare-metal machines. To set up both the master and\\nthe worker nodes, you’ll use the kubeadm tool.\\nB.1\\nSetting up the OS and required packages\\nFirst, you need to download and install VirtualBox, if you don’t have it installed\\nalready. You can download it from https:/\\n/www.virtualbox.org/wiki/Downloads.\\nOnce you have it running, download the CentOS 7 minimal ISO image from\\nwww.centos.org/download. You can also use a different Linux distribution, but\\nmake sure it’s supported by checking the http:/\\n/kubernetes.io website.\\nB.1.1\\nCreating the virtual machine\\nNext, you’ll create the VM for your Kubernetes master. Start by clicking the New\\nicon in the upper-left corner. Then enter “k8s-master” as the name, and select\\nLinux as the Type and Red Hat (64-bit) as the version, as shown in figure B.1.\\n After clicking the Next button, you can set the VM’s memory size and set up the\\nhard disk. If you have enough memory, select at least 2GB (keep in mind you’ll run\\nthree such VMs). When creating the hard disk, leave the default options selected.\\nHere’s what they were in my case:\\n■\\nHard disk file type: VDI (VirtualBox Disk Image)\\n■\\nStorage on physical hard disk: Dynamically allocated\\n■\\nFile location and size: k8s-master, size 8GB\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 572,\n",
       "  'img_cnt': 1,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '540\\nAPPENDIX B\\nSetting up a multi-node cluster with kubeadm\\nB.1.2\\nConfiguring the network adapter for the VM\\nOnce you’re done creating the VM, you need to configure its network adapter,\\nbecause the default won’t allow you to run multiple nodes properly. You’ll configure\\nthe adapter so it uses the Bridged Adapter mode. This will connect your VMs to the\\nsame network your host computer is in. Each VM will get its own IP address, the same\\nway as if it were a physical machine connected to the same switch your host computer\\nis connected to. Other options are much more complicated, because they usually\\nrequire two network adapters to be set up.\\n To configure the network adapter, make sure the VM is selected in the main Virtual-\\nBox window and then click the Settings icon (next to the New icon you clicked before). \\n A window like the one shown in figure B.2 will appear. On the left-hand side, select\\nNetwork and then, in the main panel on the right, select Attached to: Bridged Adapter,\\nas shown in the figure. In the Name drop-down menu, select your host machine’s\\nadapter, which you use to connect your machine to the network.\\nFigure B.1\\nCreating a Virtual Machine in VirtualBox\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 573,\n",
       "  'img_cnt': 2,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '541\\nSetting up the OS and required packages\\nB.1.3\\nInstalling the operating system\\nYou’re now ready to run the VM and install the operating system. Ensure the VM is still\\nselected in the list and click on the Start icon at the top of the VirtualBox main window.\\nSELECTING THE START-UP DISK\\nBefore the VM starts up, VirtualBox will ask you what start-up disk to use. Click the icon\\nnext to the drop-down list (shown in figure B.3) and then find and select the CentOS\\nISO image you downloaded earlier. Then click Start to boot up the VM.\\nFigure B.2\\nConfiguring the network adapter for the VM\\nFigure B.3\\nSelecting \\nthe installation ISO image\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 574,\n",
       "  'img_cnt': 1,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '542\\nAPPENDIX B\\nSetting up a multi-node cluster with kubeadm\\nINITIATING THE INSTALL\\nWhen the VM starts up, a textual menu screen will appear. Use the cursor up key to\\nselect the Install CentOS Linux 7 option and press the Enter button. \\nSETTING INSTALLATION OPTIONS\\nAfter a few moments, a graphical Welcome to CentOS Linux 7 screen will appear,\\nallowing you to select the language you wish to use. I suggest you keep the language\\nset to English. Click the Continue button to get to the main setup screen as shown in\\nfigure B.4.\\nTIP\\nWhen you click into the VM’s window, your keyboard and mouse will be\\ncaptured by the VM. To release them, press the key shown at the bottom-right\\ncorner of the VirtualBox window the VM is running in. This is usually the Right\\nControl key on Windows and Linux or the left Command key on MacOS.\\nFirst, click Installation Destination and then immediately click the Done button on\\nthe screen that appears (you don’t need to click anywhere else). \\n Then click on Network & Host Name. On the next screen, first enable the network\\nadapter by clicking the ON/OFF switch in the top right corner. Then enter the host\\nFigure B.4\\nThe main setup screen\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 575,\n",
       "  'img_cnt': 2,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '543\\nSetting up the OS and required packages\\nname into the field at the bottom left, as shown in figure B.5. You’re currently setting\\nup the master, so set the host name to master.k8s. Click the Apply button next to the\\ntext field to confirm the new host name.\\nTo return to the main setup screen, click the Done button in the top-left corner.\\n You also need to set the correct time zone. Click Date & Time and then, on the\\nscreen that opens, select the Region and City or click your location on the map. Return\\nto the main screen by clicking the Done button in the top-left corner.\\nRUNNING THE INSTALL\\nTo start the installation, click the Begin Installation button in the bottom-right corner.\\nA screen like the one in figure B.6 will appear. While the OS is being installed, set the\\nFigure B.5\\nSetting the hostname and configuring the network adapter\\nFigure B.6\\nSetting the root password while the OS is being installed and rebooting afterward\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 576,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '544\\nAPPENDIX B\\nSetting up a multi-node cluster with kubeadm\\nroot password and create a user account, if you want. When the installation completes,\\nclick the Reboot button at the bottom right.\\nB.1.4\\nInstalling Docker and Kubernetes\\nLog into the machine as root. First, you need to disable two security features: SELinux\\nand the firewall.\\nDISABLING SELINUX\\nTo disable SELinux, run the following command:\\n# setenforce 0\\nBut this only disables it temporarily (until the next reboot). To disable it perma-\\nnently, edit the /etc/selinux/config file and change the SELINUX=enforcing line to\\nSELINUX=permissive.\\nDISABLING THE FIREWALL\\nYou’ll also disable the firewall, so you don’t run into any firewall-related problems.\\nRun the following command:\\n# systemctl disable firewalld && systemctl stop firewalld\\nRemoved symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1...\\nRemoved symlink /etc/systemd/system/basic.target.wants/firewalld.service.\\nADDING THE KUBERNETES YUM REPO\\nTo make the Kubernetes RPM packages available to the yum package manager, you’ll\\nadd a kubernetes.repo file to the /etc/yum.repos.d/ directory as shown in the follow-\\ning listing.\\n# cat <<EOF > /etc/yum.repos.d/kubernetes.repo\\n[kubernetes]\\nname=Kubernetes\\nbaseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64\\nenabled=1\\ngpgcheck=1\\nrepo_gpgcheck=1\\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg\\n        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\\nEOF\\nNOTE\\nMake sure no whitespace exists after EOF if you’re copying and pasting. \\nINSTALLING DOCKER, KUBELET, KUBEADM, KUBECTL, AND KUBERNETES-CNI\\nNow you’re ready to install all the packages you need:\\n# yum install -y docker kubelet kubeadm kubectl kubernetes-cni\\nListing B.1\\nAdding the Kubernetes RPM repo\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 577,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '545\\nSetting up the OS and required packages\\nAs you can see, you’re installing quite a few packages. Here’s what they are:\\n■\\ndocker—The container runtime\\n■\\nkubelet—The Kubernetes node agent, which will run everything for you\\n■\\nkubeadm—A tool for deploying multi-node Kubernetes clusters\\n■\\nkubectl—The command line tool for interacting with Kubernetes\\n■\\nkubernetes-cni—The Kubernetes Container Networking Interface\\nOnce they’re all installed, you need to manually enable the docker and the kubelet\\nservices:\\n# systemctl enable docker && systemctl start docker\\n# systemctl enable kubelet && systemctl start kubelet\\nENABLING THE NET.BRIDGE.BRIDGE-NF-CALL-IPTABLES KERNEL OPTION\\nI’ve noticed that something disables the bridge-nf-call-iptables kernel parameter,\\nwhich is required for Kubernetes services to operate properly. To rectify the problem,\\nyou need to run the following two commands:\\n# sysctl -w net.bridge.bridge-nf-call-iptables=1\\n# echo \"net.bridge.bridge-nf-call-iptables=1\" > /etc/sysctl.d/k8s.conf\\nDISABLING SWAP\\nThe Kubelet won’t run if swap is enabled, so you’ll disable it with the following\\ncommand:\\n# swapoff -a &&  sed -i \\'/ swap / s/^/#/\\' /etc/fstab\\nB.1.5\\nCloning the VM\\nEverything you’ve done up to this point must be done on every machine you plan to\\nuse in your cluster. If you’re doing this on bare metal, you need to repeat the process\\ndescribed in the previous section at least two more times—for each worker node. If\\nyou’re building the cluster using virtual machines, now’s the time to clone the VM, so\\nyou end up with three different VMs.\\nSHUTTING DOWN THE VM\\nTo clone the machine in VirtualBox, first shut down the VM by running the shutdown\\ncommand:\\n# shutdown now\\nCLONING THE VM\\nNow, right-click on the VM in the VirtualBox UI and select Clone. Enter the name for\\nthe new machine as shown in figure B.7 (for example, k8s-node1 for the first clone or\\nk8s-node2 for the second one). Make sure you check the Reinitialize the MAC address\\nof all network cards option, so each VM uses different MAC addresses (because\\nthey’re going to be located in the same network).\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 578,\n",
       "  'img_cnt': 1,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '546\\nAPPENDIX B\\nSetting up a multi-node cluster with kubeadm\\nClick the Next button and then make sure the Full clone option is selected before\\nclicking Next again. Then, on the next screen, click Clone (leave the Current machine\\nstate option selected).\\n Repeat the process for the VM for the second node and then start all three VMs by\\nselecting all three and clicking the Start icon. \\nCHANGING THE HOSTNAME ON THE CLONED VMS\\nBecause you created two clones from your master VM, all three VMs have the same host-\\nname configured. Therefore, you need to change the hostnames of the two clones. To\\ndo that, log into each of the two nodes (as root) and run the following command:\\n# hostnamectl --static set-hostname node1.k8s\\nNOTE\\nBe sure to set the hostname to node2.k8s on the second node.\\nCONFIGURING NAME RESOLUTION FOR ALL THREE HOSTS\\nYou need to ensure that all three nodes are resolvable either by adding records to a\\nDNS server or by editing the /etc/hosts file on all of them. For example, you need to\\nadd the following three lines to the hosts file (replace the IPs with those of your VMs),\\nas shown in the following listing.\\n192.168.64.138 master.k8s\\n192.168.64.139 node1.k8s\\n192.168.64.140 node2.k8s\\nListing B.2\\nEntries to add to /etc/hosts on each cluster node\\nFigure B.7\\nCloning the master VM\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 579,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '547\\nConfiguring the master with kubeadm\\nYou can get each node’s IP by logging into the node as root, running ip addr and\\nfinding the IP address associated with the enp0s3 network adapter, as shown in the fol-\\nlowing listing.\\n# ip addr\\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1\\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\\n    inet 127.0.0.1/8 scope host lo\\n       valid_lft forever preferred_lft forever\\n    inet6 ::1/128 scope host\\n       valid_lft forever preferred_lft forever\\n2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state \\nUP qlen 1000\\n    link/ether 08:00:27:db:c3:a4 brd ff:ff:ff:ff:ff:ff\\n    inet 192.168.64.138/24 brd 192.168.64.255 scope global dynamic enp0s3\\n       valid_lft 59414sec preferred_lft 59414sec\\n    inet6 fe80::77a9:5ad6:2597:2e1b/64 scope link\\n       valid_lft forever preferred_lft forever\\nThe command’s output in the previous listing shows that the machine’s IP address is\\n192.168.64.138. You’ll need to run this command on each of your nodes to get all\\ntheir IPs.\\nB.2\\nConfiguring the master with kubeadm\\nYou’re now ready to finally set up the Kubernetes Control Plane on your master node. \\nRUNNING KUBEADM INIT TO INITIALIZE THE MASTER\\nThanks to the awesome kubeadm tool, all you need to do to initialize the master is run\\na single command, as shown in the following listing.\\n# kubeadm init\\n[kubeadm] WARNING: kubeadm is in beta, please do not use it for production \\nclusters.\\n[init] Using Kubernetes version: v.1.8.4\\n...\\nYou should now deploy a pod network to the cluster.\\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\\n  http://kubernetes.io/docs/admin/addons/\\nYou can now join any number of machines by running the following on each node \\nas root:\\nkubeadm join --token eb3877.3585d0423978c549 192.168.64.138:6443 \\n--discovery-token-ca-cert-hash \\nsha256:037d2c5505294af196048a17f184a79411c7b1eac48aaa0ad137075be3d7a847\\nNOTE\\nWrite down the command shown in the last line of kubeadm init’s out-\\nput. You’ll need it later.\\nListing B.3\\nLooking up each node’s IP address\\nListing B.4\\nInitializing the master node with kubeadm init\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 580,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '548\\nAPPENDIX B\\nSetting up a multi-node cluster with kubeadm\\nKubeadm has deployed all the necessary Control Plane components, including etcd,\\nthe API server, Scheduler, and Controller Manager. It has also deployed the kube-\\nproxy, making Kubernetes services available from the master node. \\nB.2.1\\nUnderstanding how kubeadm runs the components\\nAll these components are running as containers. You can use the docker ps command\\nto confirm this. But kubeadm doesn’t use Docker directly to run them. It deploys their\\nYAML descriptors to the /etc/kubernetes/manifests directory. This directory is moni-\\ntored by the Kubelet, which then runs these components through Docker. The com-\\nponents run as Pods. You can see them with the kubectl get command. But first, you\\nneed to configure kubectl.\\nRUNNING KUBECTL ON THE MASTER\\nYou installed kubectl along with docker, kubeadm, and other packages in one of the\\ninitial steps. But you can’t use kubectl to talk to your cluster without first configuring\\nit through a kubeconfig file.\\n Luckily, the necessary configuration is stored in the /etc/kubernetes/admin.conf\\nfile. All you need to do is make kubectl use it by setting the KUBECONFIG environment\\nvariable, as explained in appendix A:\\n# export KUBECONFIG=/etc/kubernetes/admin.conf\\nLISTING THE PODS\\nTo test kubectl, you can list the pods of the Control Plane (they’re in the kube-system\\nnamespace), as shown in the following listing.\\n# kubectl get po -n kube-system\\nNAME                                 READY     STATUS    RESTARTS   AGE\\netcd-master.k8s                      1/1       Running   0          21m\\nkube-apiserver-master.k8s            1/1       Running   0          22m\\nkube-controller-manager-master.k8s   1/1       Running   0          21m\\nkube-dns-3913472980-cn6kz            0/3       Pending   0          22m\\nkube-proxy-qb709                     1/1       Running   0          22m\\nkube-scheduler-master.k8s            1/1       Running   0          21m\\nLISTING NODES\\nYou’re finished with setting up the master, but you still need to set up the nodes.\\nAlthough you already installed the Kubelet on both of your two worker nodes (you\\neither installed each node separately or cloned the initial VM after you installed all\\nthe required packages), they aren’t part of your Kubernetes cluster yet. You can see\\nthat by listing nodes with kubectl:\\n# kubectl get node\\nNAME         STATUS     ROLES     AGE       VERSION\\nmaster.k8s   NotReady   master    2m        v1.8.4\\nListing B.5\\nSystem pods in the kube-system namespace\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 581,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '549\\nConfiguring worker nodes with kubeadm\\nSee, only the master is listed as a node. And even the master is shown as being Not-\\nReady. You’ll see why later. Now, you’ll set up your two nodes.\\nB.3\\nConfiguring worker nodes with kubeadm\\nWhen using kubeadm, configuring worker nodes is even easier than configuring the\\nmaster. In fact, when you ran the kubeadm init command to set up your master, it\\nalready told you how to configure your worker nodes (repeated in the next listing).\\nYou can now join any number of machines by running the following on each node \\nas root:\\nkubeadm join --token eb3877.3585d0423978c549 192.168.64.138:6443 \\n--discovery-token-ca-cert-hash \\nsha256:037d2c5505294af196048a17f184a79411c7b1eac48aaa0ad137075be3d7a847\\nAll you need to do is run the kubeadm join command with the specified token and the\\nmaster’s IP address/port on both of your nodes. It then takes less than a minute for\\nthe nodes to register themselves with the master. You can confirm they’re registered\\nby running the kubectl get node command on the master again:\\n# kubectl get nodes\\nNAME         STATUS     ROLES     AGE       VERSION\\nmaster.k8s   NotReady   master    3m        v1.8.4\\nnode1.k8s    NotReady   <none>    3s        v1.8.4\\nnode2.k8s    NotReady   <none>    5s        v1.8.4\\nOkay, you’ve made progress. Your Kubernetes cluster now consists of three nodes, but\\nnone of them are ready. Let’s investigate.\\n Let’s use the kubectl describe command in the following listing to see more\\ninformation. Somewhere at the top, you’ll see a list of Conditions, showing the\\ncurrent conditions on the node. One of them will show the following Reason and\\nMessage.\\n# kubectl describe node node1.k8s\\n...\\nKubeletNotReady    runtime network not ready: NetworkReady=false \\n                   reason:NetworkPluginNotReady message:docker: \\n                   network plugin is not ready: cni config uninitialized\\nAccording to this, the Kubelet isn’t fully ready, because the container network (CNI)\\nplugin isn’t ready, which is expected, because you haven’t deployed the CNI plugin\\nyet. You’ll deploy one now.\\nListing B.6\\nLast part of the output of the kubeadm init command\\nListing B.7\\nKubectl describe shows why the node isn’t ready\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 582,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '550\\nAPPENDIX B\\nSetting up a multi-node cluster with kubeadm\\nB.3.1\\nSetting up the container network\\nYou’ll install the Weave Net container networking plugin, but several alternatives are\\nalso available. They’re listed among the available Kubernetes add-ons at http:/\\n/kuber-\\nnetes.io/docs/admin/addons/.\\n Deploying the Weave Net plugin (like most other add-ons) is as simple as this:\\n$ kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl \\nversion | base64 | tr -d \\'\\\\n\\')\\nThis will deploy a DaemonSet and a few security-related resources (refer to chapter 12\\nfor an explanation of the ClusterRole and ClusterRoleBinding, which are deployed\\nalongside the DaemonSet).\\n Once the DaemonSet controller creates the pods and they’re started on all your\\nnodes, the nodes should become ready:\\n# k get node\\nNAME         STATUS    ROLES     AGE       VERSION\\nmaster.k8s   Ready     master    9m        v1.8.4\\nnode1.k8s    Ready     <none>    5m        v1.8.4\\nnode2.k8s    Ready     <none>    5m        v1.8.4\\nAnd that’s it. You now have a fully functioning three-node Kubernetes cluster with an\\noverlay network provided by Weave Net. All the required components, except for the\\nKubelet itself, are running as pods, managed by the Kubelet, as shown in the follow-\\ning listing.\\n# kubectl get po --all-namespaces\\nNAMESPACE     NAME                                 READY     STATUS    AGE\\nkube-system   etcd-master.k8s                      1/1       Running   1h\\nkube-system   kube-apiserver-master.k8s            1/1       Running   1h\\nkube-system   kube-controller-manager-master.k8s   1/1       Running   1h\\nkube-system   kube-dns-3913472980-cn6kz            3/3       Running   1h\\nkube-system   kube-proxy-hcqnx                     1/1       Running   24m\\nkube-system   kube-proxy-jvdlr                     1/1       Running   24m\\nkube-system   kube-proxy-qb709                     1/1       Running   1h\\nkube-system   kube-scheduler-master.k8s            1/1       Running   1h\\nkube-system   weave-net-58zbk                      2/2       Running   7m\\nkube-system   weave-net-91kjd                      2/2       Running   7m\\nkube-system   weave-net-vt279                      2/2       Running   7m\\nB.4\\nUsing the cluster from your local machine\\nUp to this point, you’ve used kubectl on the master node to talk to the cluster. You’ll\\nprobably want to configure the kubectl instance on your local machine, too. \\n To do that, you need to copy the /etc/kubernetes/admin.conf file from the mas-\\nter to your local machine with the following command:\\n$ scp root@192.168.64.138:/etc/kubernetes/admin.conf ~/.kube/config2\\nListing B.8\\nSystem pods in the kube-system namespace after deploying Weave Net\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 583,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '551\\nUsing the cluster from your local machine\\nReplace the IP with that of your master. Then you point the KUBECONFIG environment\\nvariable to the ~/.kube/config2 file like this:\\n$ export KUBECONFIG=~/.kube/config2\\nKubectl will now use this config file. To switch back to using the previous one, unset\\nthe environment variable. \\n You’re now all set to use the cluster from your local machine.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 584,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '552\\nappendix C\\nUsing other container\\nruntimes\\nC.1\\nReplacing Docker with rkt\\nWe’ve mentioned rkt (pronounced rock-it) a few times in this book. Like Docker, it\\nruns applications in isolated containers, using the same Linux technologies as\\nthose used by Docker. Let’s look at how rkt differs from Docker and how to try it in\\nMinikube.\\n The first great thing about rkt is that it directly supports the notion of a Pod\\n(running multiple related containers), unlike Docker, which only runs individual\\ncontainers. Rkt is based on open standards and was built with security in mind from\\nthe start (for example, images are signed, so you can be sure they haven’t been tam-\\npered with). Unlike Docker, which initially had a client-server based architecture\\nthat didn’t play well with init systems such as systemd, rkt is a CLI tool that runs\\nyour container directly, instead of telling a daemon to run it. A nice thing about rkt\\nis that it can run existing Docker-formatted container images, so you don’t need to\\nrepackage your applications to get started with rkt.\\nC.1.1\\nConfiguring Kubernetes to use rkt\\nAs you may remember from chapter 11, the Kubelet is the only Kubernetes\\ncomponent that talks to the Container Runtime. To get Kubernetes to use rkt\\ninstead of Docker, you need to configure the Kubelet to use it by running it with\\nthe --container-runtime=rkt command-line option. But be aware that support\\nfor rkt isn’t as mature as support for Docker. \\n Please refer to the Kubernetes documentation for more information on how to\\nuse rkt and what is or isn’t supported. Here, we’ll go over a quick example to get\\nyou started.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 585,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '553\\nReplacing Docker with rkt\\nC.1.2\\nTrying out rkt with Minikube\\nLuckily, to get started with rkt on Kubernetes, all you need is the same Minikube exe-\\ncutable you’re already using. To use rkt as the container runtime in Minikube, all you\\nneed to do is start Minikube with the following two options:\\n$ minikube start --container-runtime=rkt --network-plugin=cni \\nNOTE\\nYou may need to run minikube delete to delete the existing Minikube\\nVM first. \\nThe --container-runtime=rkt option obviously configures the Kubelet to use rkt as\\nthe Container Runtime, whereas the --network-plugin=cni makes it use the Con-\\ntainer Network Interface as the network plugin. Without this option, pods won’t run,\\nso it’s imperative you use it.\\nRUNNING A POD\\nOnce the Minikube VM is up, you can interact with Kubernetes exactly like before.\\nYou can deploy the kubia app with the kubectl run command, for example:\\n$ kubectl run kubia --image=luksa/kubia --port 8080\\ndeployment \"kubia\" created\\nWhen the pod starts up, you can see it’s running through rkt by inspecting its contain-\\ners with kubectl describe, as shown in the following listing.\\n$ kubectl describe pods\\nName:           kubia-3604679414-l1nn3\\n...\\nStatus:         Running\\nIP:             10.1.0.2\\nControllers:    ReplicaSet/kubia-3604679414\\nContainers:\\n  kubia:\\n    Container ID:       rkt://87a138ce-...-96e375852997:kubia    \\n    Image:              luksa/kubia\\n    Image ID:           rkt://sha512-5bbc5c7df6148d30d74e0...    \\n...\\nYou can also try hitting the pod’s HTTP port to see if it’s responding properly to\\nHTTP requests. You can do this by creating a NodePort Service or by using kubectl\\nport-forward, for example. \\nINSPECTING THE RUNNING CONTAINERS IN THE MINIKUBE VM\\nTo get more familiar with rkt, you can try logging into the Minikube VM with the fol-\\nlowing command:\\n$ minikube ssh\\nListing C.1\\nPod running with rkt\\nThe container \\nand image IDs \\nmention rkt \\ninstead of \\nDocker.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 586,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '554\\nAPPENDIX C\\nUsing other container runtimes\\nThen, you can use rkt list to see the running pods and containers, as shown in the\\nfollowing listing.\\n$ rkt list\\nUUID      APP                 IMAGE NAME                       STATE   ...\\n4900e0a5  k8s-dashboard       gcr.io/google_containers/kun...  running ...\\n564a6234  nginx-ingr-ctrlr    gcr.io/google_containers/ngi...  running ...\\n5dcafffd  dflt-http-backend   gcr.io/google_containers/def...  running ...\\n707a306c  kube-addon-manager  gcr.io/google-containers/kub...  running ...\\n87a138ce  kubia               registry-1.docker.io/luksa/k...  running ...\\nd97f5c29  kubedns             gcr.io/google_containers/k8s...  running ...\\n          dnsmasq             gcr.io/google_containers/k8...            \\n          sidecar             gcr.io/google_containers/k8...\\nYou can see the kubia container, as well as other system containers running (the ones\\ndeployed in pods in the kube-system namespace). Notice how the bottom two con-\\ntainers don’t have anything listed in the UUID or STATE columns? That’s because they\\nbelong to the same pod as the kubedns container listed above them. \\n Rkt prints containers belonging to the same pod grouped together. Each pod\\n(instead of each container) has its own UUID and state. If you tried doing this when\\nyou were using Docker as the Container Runtime, you’ll appreciate how much easier\\nit is to see all the pods and their containers with rkt. You’ll notice no infrastructure\\ncontainer exists for each pod (we explained them in chapter 11). That’s because of\\nrkt’s native support for pods.\\nLISTING CONTAINER IMAGES\\nIf you’ve played around with Docker CLI commands, you’ll get familiar quickly with\\nrkt’s commands. Run rkt without any arguments and you’ll see all the commands you\\ncan run. For example, to list container images, you run the command in the follow-\\ning listing.\\n$ rkt image list\\nID           NAME                          SIZE    IMPORT TIME  LAST USED\\nsha512-a9c3  ...addon-manager:v6.4-beta.1  245MiB  24 min ago   24 min ago\\nsha512-a078  .../rkt/stage1-coreos:1.24.0  224MiB  24 min ago   24 min ago\\nsha512-5bbc  ...ker.io/luksa/kubia:latest  1.3GiB  23 min ago   23 min ago\\nsha512-3931  ...es-dashboard-amd64:v1.6.1  257MiB  22 min ago   22 min ago\\nsha512-2826  ...ainers/defaultbackend:1.0  15MiB   22 min ago   22 min ago\\nsha512-8b59  ...s-controller:0.9.0-beta.4  233MiB  22 min ago   22 min ago\\nsha512-7b59  ...dns-kube-dns-amd64:1.14.2  100MiB  21 min ago   21 min ago\\nsha512-39c6  ...nsmasq-nanny-amd64:1.14.2  86MiB   21 min ago   21 min ago\\nsha512-89fe  ...-dns-sidecar-amd64:1.14.2  85MiB   21 min ago   21 min ago\\nThese are all Docker-formatted container images. You can also try building images in\\nthe OCI image format (OCI stands for Open Container Initiative) with the acbuild\\nListing C.2\\nListing running containers with rkt list\\nListing C.3\\nListing images with rkt image list\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 587,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '555\\nUsing other container runtimes through the CRI\\ntool (available at https:/\\n/github.com/containers/build) and running them with rkt.\\nDoing that is outside the scope of this book, so I’ll let you try it on your own.\\n The information explained in this appendix so far should be enough to get you\\nstarted using rkt with Kubernetes. Refer to the rkt documentation at https:/\\n/coreos\\n.com/rkt and Kubernetes documentation at https:/\\n/kubernetes.io/docs for addi-\\ntional information.\\nC.2\\nUsing other container runtimes through the CRI\\nKubernetes’ support for other container runtimes doesn’t stop with Docker and rkt.\\nBoth of those runtimes were initially integrated directly into Kubernetes, but in\\nKubernetes version 1.5, the Container Runtime Interface (CRI) was introduced. CRI\\nis a plugin API enabling easy integration of other container runtimes with Kuberne-\\ntes. People are now free to plug other container runtimes into Kubernetes without\\nhaving to dig deep into Kubernetes code. All they need to do is implement a few inter-\\nface methods. \\n From Kubernetes version 1.6 onward, CRI is the default interface the Kubelet uses.\\nBoth Docker and rkt are now used through the CRI (no longer directly).\\nC.2.1\\nIntroducing the CRI-O Container Runtime\\nBeside Docker and rkt, a new CRI implementation called CRI-O allows Kubernetes to\\ndirectly launch and manage OCI-compliant containers, without requiring you to\\ndeploy any additional Container Runtime. \\n You can try CRI-O with Minikube by starting it with --container-runtime=crio.\\nC.2.2\\nRunning apps in virtual machines instead of containers \\nKubernetes is a container orchestration system, right? Throughout the book, we\\nexplored many features that show that it’s much more than an orchestration system,\\nbut the bottom line is that when you run an app with Kubernetes, the app always runs\\ninside a container, right? You may find it surprising that’s no longer the case.\\n New CRI implementations are being developed that allow Kubernetes to run apps\\nin virtual machines instead of in containers. One such implementation, called Frakti,\\nallows you to run regular Docker-based container images directly through a hypervi-\\nsor, which means each container runs its own kernel. This allows much better isola-\\ntion between containers compared to when they use the same kernel. \\n And there’s more. Another CRI implementation is the Mirantis Virtlet, which\\nmakes it possible to run actual VM images (in the QCOW2 image file format, which is\\none of the formats used by the QEMU virtual machine tool) instead of container\\nimages. When you use the Virtlet as the CRI plugin, Kubernetes spins up a VM for\\neach pod. How awesome is that?\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 588,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '556\\nappendix D\\nCluster Federation\\nIn the section about high availability in chapter 11 we explored how Kubernetes\\ncan deal with failures of individual machines and even failures of whole server racks\\nor the supporting infrastructure. But what if the whole datacenter goes dark?\\n To make sure you’re not susceptible to datacenter-wide outages, apps should be\\ndeployed in multiple datacenters or cloud availability zones. When one of those\\ndatacenters or availability zones becomes unavailable, client requests can be routed\\nto the apps running in the remaining healthy datacenters or zones.\\n While Kubernetes doesn’t require you to run the Control Plane and the nodes\\nin the same datacenter, you’ll almost always want to do that to keep network latency\\nbetween them low and to reduce the possibility of them becoming disconnected\\nfrom each other. Instead of having a single cluster spread across multiple locations,\\na better alternative is to have an individual Kubernetes cluster in every location.\\nWe’ll explore this approach in this appendix.\\nD.1\\nIntroducing Kubernetes Cluster Federation\\nKubernetes allows you to combine multiple clusters into a cluster of clusters\\nthrough Cluster Federation. It allows users to deploy and manage apps across mul-\\ntiple clusters running in different locations in the world, but also across different\\ncloud providers combined with on-premises clusters (hybrid cloud). The motiva-\\ntion for Cluster Federation isn’t only to ensure high availability, but also to com-\\nbine multiple heterogeneous clusters into a single super-cluster managed through\\na single management interface. \\n For example, by combining an on-premises cluster with one running on a cloud\\nprovider’s infrastructure, you can run privacy-sensitive components of your applica-\\ntion system on-premises, while the non-sensitive parts can run in the cloud.\\nAnother example is initially running your application only in a small on-premises\\ncluster, but when the application’s compute requirements exceed the cluster’s\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 589,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '557\\nUnderstanding the architecture\\ncapacity, letting the application spill over to a cloud-based cluster, which is automati-\\ncally provisioned on the cloud provider’s infrastructure.\\nD.2\\nUnderstanding the architecture\\nLet’s take a quick look at what Kubernetes Cluster Federation is. A cluster of clusters\\ncan be compared to a regular cluster where instead of nodes, you have complete clus-\\nters. Just as a Kubernetes cluster consists of a Control Plane and multiple worker\\nnodes, a federated cluster consists of a Federated Control Plane and multiple Kuber-\\nnetes clusters. Similar to how the Kubernetes Control Plane manages applications\\nacross a set of worker nodes, the Federated Control Plane does the same thing, but\\nacross a set of clusters instead of nodes. \\n The Federated Control Plane consists of three things:\\n■\\netcd for storing the federated API objects\\n■\\nFederation API server\\n■\\nFederation Controller Manager\\nThis isn’t much different from the regular Kubernetes Control Plane. etcd stores the\\nfederated API objects, the API server is the REST endpoint all other components talk\\nto, and the Federation Controller Manager runs the various federation controllers\\nthat perform operations based on the API objects you create through the API server. \\n Users talk to the Federation API server to create federated API objects (or feder-\\nated resources). The federation controllers watch these objects and then talk to the\\nunderlying clusters’ API servers to create regular Kubernetes resources. The architec-\\nture of a federated cluster is shown in figure D.1.\\nSan Francisco\\nControl Plane\\netcd\\nFederation Controller Manager\\nController\\nManager\\nAPI server\\netcd\\nFederation\\nAPI server\\nScheduler\\nWorker node\\nKubelet\\nWorker node\\nKubelet\\nWorker node\\nKubelet\\nControl Plane\\netcd\\nController\\nManager\\nAPI server\\nScheduler\\nWorker node\\nKubelet\\nWorker node\\nKubelet\\nWorker node\\nKubelet\\nLondon\\nOther\\nlocations\\nFigure D.1\\nCluster Federation with clusters in different geographical locations\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [  Federation\\netcd Federation Controller Manager\\nAPI server\\nControl Plane Control Plane\\netcd API server Scheduler etcd API server Scheduler\\nController Controller\\nManager Manager\\nKubelet Kubelet Kubelet Kubelet Kubelet Kubelet\\nWorker node Worker node Worker node Worker node Worker node Worker node\\nOther\\nlocations\\nSan Francisco London  \\\n",
       "   0  Control Plane\\netcd API server Scheduler\\nCont...                                                                                                                                                                                                                                                                                                       \n",
       "   \n",
       "                  Col1  \n",
       "   0  Other\\nlocations  ]},\n",
       " {'page': 590,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '558\\nAPPENDIX A\\nCluster Federation\\nD.3\\nUnderstanding federated API objects\\nThe federated API server allows you to create federated variants of the objects you\\nlearned about throughout the book. \\nD.3.1\\nIntroducing federated versions of Kubernetes resources\\nAt the time of writing this, the following federated resources are supported:\\n■\\nNamespaces\\n■\\nConfigMaps and Secrets\\n■\\nServices and Ingresses\\n■\\nDeployments, ReplicaSets, Jobs, and Daemonsets\\n■\\nHorizontalPodAutoscalers\\nNOTE\\nCheck the Kubernetes Cluster Federation documentation for an up-to-\\ndate list of supported federated resources.\\nIn addition to these resources, the Federated API server also supports the Cluster\\nobject, which represents an underlying Kubernetes cluster, the same way a Node object\\nrepresents a worker node in a regular Kubernetes cluster. To help you visualize how fed-\\nerated objects relate to the objects created in the underlying clusters, see figure D.2.\\nSan Francisco\\nNamespace: foo\\nReplicaSet X-432\\nReplicas: 3\\nPod X-432-5\\nDeployment X\\nReplicas: 3\\nFederated resources\\nSecret W\\nSecret Y\\nNamespace: foo\\nDeployment X\\nReplicas: 5\\nSecret Y\\nLondon\\nNamespace: foo\\nReplicaSet X-432\\nReplicas: 2\\nPod X-432-8\\nDeployment X\\nReplicas: 2\\nSecret W\\nSecret Y\\nCluster:\\nLondon\\nCluster:\\nSan Francisco\\nSecret W\\nIngress Z\\nFigure D.2\\nThe relationship between federated resources and regular resources in underlying clusters\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [Empty DataFrame\n",
       "   Columns: [Deployment X\n",
       "   Replicas: 5, Secret Y]\n",
       "   Index: []]},\n",
       " {'page': 591,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '559\\nUnderstanding federated API objects\\nD.3.2\\nUnderstanding what federated resources do\\nFor part of the federated objects, when you create the object in the Federation API\\nserver, the controllers running in the Federation Controller Manager will create regu-\\nlar cluster-scoped resources in all underlying Kubernetes clusters and manage them\\nuntil the federated object is deleted. \\n For certain federated resource types, the resources created in the underlying clus-\\nters are exact replicas of the federated resource; for others, they’re slightly modified\\nversions, whereas certain federated resources don’t cause anything to be created in\\nthe underlying clusters at all. The replicas are kept in sync with the original federated\\nversions. But the synchronization is one-directional only—from the federation server\\ndown to the underlying clusters. If you modify the resource in an underlying cluster,\\nthe changes will not be synced up to the Federation API server.\\n For example, if you create a namespace in the federated API server, a namespace\\nwith the same name will be created in all underlying clusters. If you then create a fed-\\nerated ConfigMap inside that namespace, a ConfigMap with that exact name and con-\\ntents will be created in all underlying clusters, in the same namespace. This also applies\\nto Secrets, Services, and DaemonSets.\\n ReplicaSets and Deployments are different. They aren’t blindly copied to the\\nunderlying clusters, because that’s not what the user usually wants. After all, if you cre-\\nate a Deployment with a desired replica count of 10, you probably don’t want 10 pod\\nreplicas running in each underlying cluster. You want 10 replicas in total. Because of\\nthis, when you specify a desired replica count in a Deployment or ReplicaSet, the Fed-\\neration controllers create underlying Deployments/ReplicaSets so that the sum of\\ntheir desired replica counts equals the desired replica count specified in the federated\\nDeployment or ReplicaSet. By default, the replicas are spread evenly across the clus-\\nters, but this can be overridden.\\nNOTE\\nCurrently, you need to connect to each cluster’s API server individu-\\nally to get the list of pods running in that cluster. You can’t list all the clusters’\\npods through the Federated API server.\\nA federated Ingress resource, on the other hand, doesn’t result in the creation of any\\nIngress objects in the underlying clusters. You may remember from chapter 5 that an\\nIngress represents a single point of entry for external clients to a Service. Because of\\nthis, a federated Ingress resource creates a global, multi-cluster-wide entry point to the\\nServices across all underlying clusters. \\nNOTE\\nAs for regular Ingresses, a federated Ingress controller is required\\nfor this. \\nSetting up federated Kubernetes clusters is outside the scope of this book, so you can\\nlearn more about the subject by referring to the Cluster Federation sections in the\\nuser and administration guides in the Kubernetes online documentation at http:/\\n/\\nkubernetes.io/docs/.\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 592,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': ' \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 593,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': '561\\nindex\\nSymbols\\n$(ENV_VAR) syntax 199\\n$(ENV_VARIABLE_NAME) \\nsyntax 205\\n$(VAR) syntax 198\\n* (asterisk) character 117\\n- - (double dash) 125\\nNumerics\\n137 exit code 88\\n143 exit code 89\\n8080 port 48, 67\\n8888 port 67\\nA\\n-a option 35\\nABAC (Attribute-based access \\ncontrol) 353\\naccess controls. See RBAC\\naccounts. See service accounts\\nactions 353–354\\nactiveDeadlineSeconds \\nproperty 116\\nad hoc tasks 112\\nadapters\\nnetwork, configuring for \\nVMs 540\\nnode networks, shutting \\ndown 304–305\\nADDED watch event 514\\nadd-ons 328–330\\ncomponents 310\\ndeploying 328–329\\nDNS servers 329\\nIngress controllers 329\\nusing 329–330\\naddresses attribute 138\\naddresses, of API servers 239–240\\nadmin ClusterRole, granting full \\ncontrol of namespaces \\nwith 372\\nalgorithms, default \\nscheduling 319\\naliases\\ncreating for external \\nservices 134\\nfor kubectl 41–42\\nall keyword 82\\n--all option 81\\n--all-namespaces option 144\\nallocatable resources 408\\nallowed capabilities, \\nconfiguring 394–395\\nadding capabilities to all \\ncontainers 395\\ndropping capabilities from \\ncontainers 395\\nspecifying which capabilities \\ncan be added to \\ncontainers 394\\nallowedCapabilities 394\\nAlwaysPullImages 317\\nAmazon Web Services. See AWS\\nambassador containers\\ncommunicating with API servers \\nthrough 245\\npatterns 244\\nrunning curl pods with 244–245\\nsimplifying API server commu-\\nnication with 243–245\\nannotations\\nadding 76\\ndescribing resources through\\n498\\nmodifying 76\\nof objects, looking up 75–76\\nupdating 232\\nAPI (application program inter-\\nface)\\naggregation 518–519\\ncustom, providing for custom \\nobjects 518–519\\ndefining custom objects 508–519\\nautomating custom resources \\nwith custom \\ncontrollers 513–517\\nCRD 509–513\\nproviding custom API \\nservers for custom \\nobjects 518–519\\nvalidating custom \\nobjects 517–518\\nfederated objects 558–559\\nService Catalog 521\\nSee also OpenServiceBroker API\\nAPI servers 233–248, 316–318, \\n346–374\\naccessing through kubectl \\nproxy 234–235\\nauthentication of 346–353\\ngroups 346–348\\nservice accounts 348–353\\nusers 347–348\\nauthorizing clients 317\\ncommunicating from within \\npods 238–243\\ncommunicating with pods \\nthrough 295–297\\ncommunicating with, through \\nambassador containers 245\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 594,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n562\\nAPI servers (continued)\\nconnecting to 503\\nconnecting to cluster-internal \\nservices through 299\\nexploring 235–236, 248\\nfinding addresses 239–240\\nmodifying resources in \\nrequests 317\\nnotifying clients of resource \\nchanges 318–319\\npersistently storing \\nresources 318\\nrunning multiple instances \\nof 343\\nrunning static pods \\nwithout 326–327\\nsecuring clusters with \\nRBAC 353–373\\nbinding roles to service \\naccounts 359–360\\ndefault ClusterRoleBindings\\n371–373\\ndefault ClusterRoles 371–373\\ngranting authorization \\npermissions 373\\nincluding service accounts \\nfrom other namespaces \\nin RoleBinding 361\\nRBAC authorization \\nplugins 353–354\\nRBAC resources 355–357\\nusing ClusterRoleBindings\\n362–371\\nusing ClusterRoles 362–371\\nusing RoleBindings 358–359\\nusing Roles 358–359\\nsimplifying communication\\n243–245\\nambassador container \\npatterns 244\\nrunning curl pod 244–245\\nusing client libraries to commu-\\nnicate with 246–248\\nbuilding libraries with \\nOpenAPI 248\\nbuilding libraries with \\nSwagger 248\\ninteracting with FABRIC8 \\nJava client 247–248\\nusing existing client \\nlibraries 246–247\\nusing custom service account \\ntokens to communicate \\nwith 352–353\\nvalidating resources 318\\nverifying identity of 240–241\\nSee also REST API\\napiVersion property 106, 358, 510\\napp=kubia label 98, 123\\napp=pc label selector 72\\napplication logs 65–66\\napplication program interface. See \\nAPI\\napplication templates, in Red Hat \\nOpenShift Container \\nplatform 528–529\\napplications\\naccessing 32\\nbest practices for \\ndeveloping 477, 502, \\n506–507\\nauto-deploying resource \\nmanifests 504–505\\nemploying CI/CD 506\\nensuring client requests are \\nhandled properly 492–\\n497\\nKsonnet as alternative to \\nwriting JSON/YAML \\nmanifests 505–506\\npod lifecycles 479–491\\nusing Minikube 503–504\\nversioning resource \\nmanifests 504–505\\ncompromised 373\\ncontainerized, \\nconfiguring 191–192\\ncreating 290–291\\ncreating versions of 268\\ndeploying through \\nStatefulSets 291–295\\ncreating governing \\nservices 292–294\\ncreating persistent \\nvolumes 291–292\\ncreating StatefulSets 294\\nexamining PersistentVolume-\\nClaims 295\\nexamining stateful pods\\n294–295\\ndescribing resources through \\nannotations 498\\ndescriptions, effect on running \\ncontainers 19–20\\nexamining nodes 51–52\\ndisplaying pod IP when list-\\ning pods 51\\ndisplaying pod node when \\nlisting pods 51\\ninspecting details of pod with \\nkubectl describe 52\\nexposing through services using \\nsingle YAML file 255\\nhandling logs 500–502\\ncopying files to and from \\ncontainers 500–501\\ncopying logs to and from \\ncontainers 500–501\\nhandling multi-line log \\nstatements 502\\nusing centralized \\nlogging 501\\nhighly available 341\\nhorizontally scaling 48–50\\nincreasing replica count 49\\nrequests hitting three pods \\nwhen hitting services 50\\nresults of scale-out 49–50\\nvisualizing new states 50\\nimplementing shutdown han-\\ndlers in 490–491\\nin containers, limits as seen \\nby 415–416\\nkilling 479–482\\nexpecting data written to disk \\nto disappear 480\\nexpecting hostnames to \\nchange 480\\nexpecting local IP to \\nchange 480\\nusing volumes to preserve \\ndata across container \\nrestarts 480–482\\nmaking container images 497\\nmanaging 497–502\\nmonolithic, vs. microservices\\n3–6\\nmulti-tier, splitting into multiple \\npods 59\\nNode.js\\ncreating 28–29\\ndeploying 42–44\\nnon-horizontally scalable, using \\nleader-election for 341\\noverview 478–479\\nproviding consistent environ-\\nment to 6\\nproviding information about pro-\\ncess termination 498–500\\nrelocating 479–482\\nexpecting data written to disk \\nto disappear 480\\nexpecting hostnames to \\nchange 480\\nexpecting local IP to \\nchange 480\\nusing volumes to preserve \\ndata across container \\nrestarts 480–482\\nrunning 19–21, 497–502\\nin VMs instead of \\ncontainers 555\\nkeeping containers \\nrunning 20–21\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 595,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n563\\napplications (continued)\\nlocating containers 21\\noutside of Kubernetes during \\ndevelopment 502–503\\nscaling number of copies 21\\nthrough services using single \\nYAML file 255\\nshut-down procedures 496–497\\nsplitting into microservices 3–4\\ntagging images 497–498\\nupdating declaratively using \\nDeployment 261–278\\nblocking rollouts of bad \\nversions 274–278\\ncontrolling rate of \\nrollout 271–273\\ncreating Deployments\\n262–264\\npausing rollout process\\n273–274\\nrolling back \\ndeployments 268–270\\nupdating Deployments\\n264–268\\nusing imagePullPolicy 497–498\\nusing multi-dimensional labels \\nvs. single-dimensional \\nlabels 498\\nusing pre-stop hooks when not \\nreceiving SIGTERM \\nsignal 488–489\\nargs array 196\\narguments\\ndefining in Docker 193–195\\nCMD instruction 193\\nENTRYPOINT instruction\\n193\\nmaking INTERVAL configu-\\nrable in fortune images\\n194–195\\nshell forms vs. exec \\nforms 193–194\\noverriding in Kubernetes\\n195–196\\nSee also command-line argu-\\nments\\nasterisks 117\\nat-most-one semantics 290\\nAttribute-based access control. See \\nABAC\\nauthenticating\\nAPI servers 346–353\\ngroups 347–348\\nservice accounts 348, \\n351–353\\nusers 347–348\\nclients with authentication \\nplugins 317\\ncreating Secrets for, with \\nDocker registries 223\\nwith API servers 241–242\\nauthorization plugins, RBAC\\n353–354\\nauthorizations\\nclients with plugins for 317\\ngranting permissions 373\\nservice accounts tying into 349\\nauto-deploying, resource \\nmanifests 504–505\\nautomatic scaling 23\\nautomating custom resources \\nwith custom controllers\\n513–517\\nrunning controllers as \\npods 515–516\\nWebsite controllers 514–515\\nAutoscaler, using to scale up \\nDeployments 446–447\\nautoscaling\\nmetrics appropriate for 450\\nprocess of 438–441\\ncalculating required number \\nof pods 439\\nobtaining pod metrics\\n438–439\\nupdating replica count on \\nscaled resources 440\\nSee also horizontal autoscaling\\navailability zones\\nco-locating pods in same 471\\ndeploying pods in same\\n471–472\\navailability-zone label 467\\nAWS (Amazon Web Services) 37, \\n174, 454\\nawsElasticBlockStore volume 162, \\n174\\nazureDisk volume 162, 174\\nazureFile volume 174\\nB\\nbackend services, connecting \\nto 502\\nbackend-database 129–130\\nbase images 29\\nbash process 33\\nbatch API groups, REST \\nendpoints 236–237\\nBestEffort class, assigning pods \\nto 417\\nbinary data, using Secrets for 217\\nbinding\\nRoles to service accounts\\n359–360\\nservice instances 525\\nto host ports without using \\nhost network namespaces\\n377–379\\nblocking rollouts 274–278\\nconfiguring deadlines for \\nrollouts 278\\ndefining readiness probes to \\nprevent rollouts 275\\nminReadySeconds 274–275\\npreventing rollouts with readi-\\nness probes 277–278\\nupdating deployments with \\nkubectl apply 276–277\\nblue-green deployment 253\\nBorg 16\\nbrokers. See service brokers\\nBuildConfigs, building images \\nfrom source using 529\\nBurstable QoS class, assigning to \\npods 418\\nbusybox image 26, 112\\nC\\n-c option 66, 245\\nCA (certificate authority) 170, \\n240\\n--cacert option 240\\ncAdvisor 431\\ncanary release 69, 273\\ncapabilities\\nadding to all containers 395\\ndropping from containers\\n385–386, 395\\nkernel, adding to \\ncontainers 384–385\\nspecifying which can be added \\nto containers 394\\nSee also allowed capabilities\\ncapacity, of nodes 407–408\\nCAP_CHOWN capability 385, 395\\nCAP_SYS_TIME capability 384\\ncategorizing worker nodes with \\nlabels 74\\nCentOS ISO image 541\\ncentral processing units. See CPUs\\ncephfs volume 163\\ncert files, from Secrets 221\\ncertificate-authority-data field 536\\ncertificates, TLS 147–149\\nCI/CD (Continuous Integration \\nand Continuous \\nDelivery) 506\\ncinder volume 163\\nclaiming\\nbenefits of 182\\nPersistentVolumes 179–181\\nclient binaries, downloading 39\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 596,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n564\\nclient libraries, communicating \\nwith API servers through\\n246–248\\nbuilding libraries with \\nOpenAPI 248\\nbuilding libraries with \\nSwagger 248\\nexploring API servers with \\nSwagger UI 248\\ninteracting with FABRIC8 Java \\nclient 247–248\\nusing existing client \\nlibraries 246–247\\nclient-certificate-data \\nproperty 536\\nclient-key-data property 536\\nclients\\nauthenticating 317\\nauthorizing 317\\ncustom, creating 519\\nhandling requests 492–497\\nnon-preservation of IP 142\\nnotifying of resource \\nchanges 318–319\\npods, using newly created \\nSecrets in 525–526\\npreventing broken \\nconnections 492–497\\nsequence of events at pod \\ndeletion 493–495\\ntroubleshooting 495–496\\nSee also external clients\\ncloning VMs 545–547\\nchanging hostname on 546\\nconfiguring name resolution for \\nhosts 546–547\\nshutting down 545\\nCloud infrastructure, requesting \\nnodes from 452–453\\nCluster Autoscaler 452–453\\nenabling 454\\nrelinquishing nodes 453\\nrequesting additional nodes \\nfrom Cloud infrastructure\\n452–453\\ncluster events, observing 332–333\\nCluster Federation 556–559\\narchitecture of 557\\nfederated API objects 558–559\\noverview 556–557\\ncluster nodes\\ndisplaying CPU usage for 431\\ndisplaying memory usage \\nfor 431\\nhorizontal scaling of 452–456\\nsecuring 375–403\\nconfiguring container secu-\\nrity contexts 380–389\\nisolating pod networks\\n399–402\\nrestricting use of security-\\nrelated features 389–399\\nusing host node namespaces \\nin pods 376–380\\nclustered data stores 303–304\\ncluster-internal services, connect-\\ning through API servers 299\\nclusterIP field 154, 293\\nClusterIP service 45\\ncluster-level resources, allowing \\naccess to 362–365\\nClusterRoleBindings\\ncombing with \\nClusterRoles 370–371\\ncombining with Role \\nresources 370–371\\ncombining with \\nRoleBindings 370–371\\ndefault 371–373\\nusing 362–371\\nallowing access to cluster-level \\nresources 362–365\\nallowing access to non-\\nresource URLs 365–367\\nClusterRoles 362–371\\nallowing access to cluster-level \\nresources 362–365\\nallowing access to non-resource \\nURLs 365–367\\ncombining with ClusterRole-\\nBindings 370–371\\ncombining with \\nRoleBindings 370–371\\ncombining with Roles 370–371\\ndefault 371–373\\nallowing modifying \\nresources 372\\nallowing read-only access to \\nresources 372\\ngranting full control of \\nnamespaces 372\\nusing to grant access to \\nresources in specific \\nnamespaces 367–370\\nclusters\\nadding 536\\ncombining Minikube with 504\\nconfirming communication \\nwith kubectl 38\\nconnecting to services living \\noutside of 131–134\\ncreating alias for external \\nservices 134\\nservice endpoints 131–134\\ncreating with three nodes 39\\ndeleting 538\\nenabling RBAC resources \\nin 356\\netcd, running 342–343\\nhighly available, running\\n341–345\\nmaking applications highly \\navailable 341\\nmaking Control Plane com-\\nponents highly \\navailable 342–345\\nhosted, using with GKE 38–41\\nin Docker 36–42\\nrunning local single-node \\nclusters with Minikube\\n37–38\\nsetting up aliases for \\nkubectl 41–42\\nsetting up command-line \\ncompletion for \\nkubectl 41–42\\nin kubeconfig files 536\\nKubernetes, architecture of\\n18–19\\nlimiting service disruption during \\nscale-down of 454–456\\nlisting 538\\nlisting job instances in 237–238\\nlisting nodes 40\\nlisting services available in 523\\nlocal single-node, running with \\nMinikube 37–38\\nmodifying 536\\nmulti-node with kubeadm\\n539–551\\nconfiguring masters 547–549\\nconfiguring worker \\nnodes 549–550\\nsetting up operating \\nsystems 539\\nsetting up required \\npackages 539\\noverview of 39\\nrunning Grafana in 433\\nrunning InfluxDB in 433\\nsecuring with RBAC 353–373\\nbinding roles to service \\naccounts 359–360\\ndefault ClusterRoleBindings\\n371–373\\ndefault ClusterRoles 371–373\\ngranting authorization \\npermissions 373\\nincluding service accounts \\nfrom other \\nnamespaces 361\\nRBAC authorization \\nplugins 353–354\\nRBAC resources 355–357\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 597,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n565\\nclusters (continued)\\nstarting with Minikube 37–38\\ntesting services from within 124\\ntwo-node, deploying pods \\nin 468\\ntying with user credentials 537\\nusing from local machines\\n550–551\\nusing kubectl with \\nmultiple 534–538\\nadding kube config \\nentries 536–537\\nconfiguring location of \\nkubeconfig files 535\\ncontents of kubeconfig \\nfiles 535–536\\ndeleting contexts 538\\nlisting contexts 538\\nlisting kube config \\nentries 536–537\\nmodifying kube config \\nentries 536–537\\nswitching between \\ncontexts 538\\nswitching between Minikube \\nand GKE 534–535\\nSee also ClusterRoleBindings; \\nClusterRoles; RoleBind-\\nings; Roles; cluster nodes\\nCMD instruction 193\\nCNI (Container Network \\nInterface) 335, 338, 549\\ncollecting resource usages\\n430–432\\nCOMMAND column 334\\ncommand-line arguments\\npassing ConfigMap entries \\nas 204–205\\npassing to containers 192–196\\ncommand-line completion, for \\nkubectl 41–42\\ncommands\\ndefining in Docker 193–195\\nCMD instruction 193\\nENTRYPOINT instruction\\n193\\nmaking INTERVAL configu-\\nrable in fortune images\\n194–195\\nshell forms vs. exec forms\\n193–194\\noverriding in Kubernetes\\n195–196\\nremotely executing, in running \\ncontainers 124–126\\ncommunication\\nbetween pods on different \\nnodes 337–338\\nbetween pods on same \\nnode 336–337\\nof components 311\\nwith API servers 352–353\\nwith pods through API \\nservers 295–297\\ncompletions property 114\\ncomponents\\nadd-ons 310\\ncommunicating 311\\ninvolved in controllers 330\\nisolating with Linux container \\ntechnologies 8\\nof Control Plane 310\\nmaking highly available\\n342–345\\nusing leader-election in 344\\nof Kubernetes 310–312\\nrunning 311–312\\nrunning with kubeadm 548–549\\nComponentStatus 311\\ncomputational resources 404–436\\nlimiting resources available to \\ncontainers 412–416\\nexceeding limits 414–415\\nlimits as seen by applications \\nin containers 415–416\\nsetting hard limits for \\nresources containers can \\nuse 412–413\\nlimiting total resources available \\nin namespaces 425–429\\nlimiting objects that can be \\ncreated 427–428\\nResourceQuota \\nresources 425–427\\nspecifying quotas for per-\\nsistent storage 427\\nspecifying quotas for specific \\npod states 429\\nspecifying quotas for specific \\nQoS classes 429\\nmonitoring pod resource \\nusage 430–434\\nanalyzing historical resource \\nconsumption \\nstatistics 432–434\\ncollecting resource \\nusages 430–432\\nretrieving resource \\nusages 430–432\\nstoring historical resource \\nconsumption \\nstatistics 432–434\\npod QoS classes 417–421\\ndefining QoS classes for \\npods 417–419\\nkilling processes 420–421\\nrequesting resources for pod \\ncontainers 405–412\\ncreating pods with resource \\nrequests 405–406\\ndefining custom resources\\n411–412\\neffect of CPU requests on \\nCPU time sharing 411\\neffect of resource requests on \\nscheduling 406–410\\nrequesting custom \\nresources 411–412\\nsetting default limits for pods \\nper namespace 421–425\\napplying default resource \\nlimits 424–425\\ncreating LimitRange \\nobjects 422–423\\nenforcing limits 423–424\\nLimitRange resources\\n421–422\\nconditions, using multiple in label \\nselectors 72\\nconfig, updating without restart-\\ning application 211–213\\nediting ConfigMap 212\\nsignaling Nginx to reload \\nconfig 212\\nupdating ConfigMap 213\\nupdating files automatically\\n212–213\\nconfig-file.conf file 201\\nCONFIG_FOO-BAR variable 204\\nconfigMap volume\\nexamining mounted \\ncontents 209\\nexposing ConfigMap entries as \\nfiles 205–211\\nsetting file permissions for files \\nin 211\\nConfigMaps\\ncreating 199–201, 206–207\\ncombining options 201\\nentries from contents of \\nfiles 201\\nfrom files in directories 201\\nusing kubectl create \\nConfigMap command\\n200–201\\ndecoupling configurations \\nwith 198–213\\nediting 212\\nexposing entries as files 205–211\\nexamining mounted config-\\nMap volume contents\\n209\\nmounting directory hiding \\nexisting files 210\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 598,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n566\\nConfigMaps (continued)\\nsetting file permissions for \\nfiles 211\\nverifying Nginx uses mounted \\nconfig file 208\\nexposing entries in volume\\n209–210\\nmounting entries as files\\n210–211\\nnon-existing, referencing in \\npods 203\\noverview 198\\npassing entries 202–205\\nupdating 213\\nusing entries in volume 207–208\\nversus Secrets 217–218\\nconfigurations, decoupling with \\nConfigMap\\ncreating ConfigMaps 200–201\\npassing ConfigMap \\nentries 202–205\\nupdating application config \\nwithout restarting \\napplication 211–213\\nusing configMap volume to \\nexpose ConfigMap entries \\nas files 205–211\\nconfiguring\\nallowed capabilities 394–395\\nadding capabilities to all \\ncontainers 395\\ndropping capabilities from \\ncontainers 395\\nspecifying which capabilities \\ncan be added to \\ncontainers 394\\ncontainer security \\ncontexts 380–389\\nadding individual kernel \\ncapabilities to \\ncontainers 384–385\\ndropping capabilities from \\ncontainers 385–386\\npreventing containers from \\nrunning as root 382\\npreventing processes from \\nwriting to container \\nfilesystems 386–387\\nrunning containers as specific \\nuser 381–382\\nrunning pods in privileged \\nmode 382–384\\nrunning pods without specify-\\ning security contexts\\n381\\nsharing volumes when con-\\ntainers run as different \\nusers 387–389\\ncontainerized applications\\n191–192\\ndeadlines for rollouts 278\\ndefault capabilities 394–395\\nadding capabilities to all \\ncontainers 395\\ndropping capabilities from \\ncontainers 395\\nspecifying which capabilities \\ncan be added to \\ncontainers 394\\ndisallowed capabilities 394–395\\nadding capabilities to all \\ncontainers 395\\ndropping capabilities from \\ncontainers 395\\nspecifying which capabilities \\ncan be added to \\ncontainers 394\\nhost in Ingress pointing to \\nIngress IP address 145\\nIngress, to handle TLS \\ntraffic 147–149\\nINTERVAL in fortune \\nimages 194–195\\nJob templates 117\\nKubernetes to use rkt 552\\nlocation of kubeconfig files 535\\nmasters with kubeadm 547–549\\nrunning components with \\nkubeadm 548–549\\nrunning kubeadm init to ini-\\ntialize masters 547–548\\nname resolution for hosts\\n546–547\\nnetwork adapters for VMs 540\\npod rescheduling after node \\nfailures 462\\nproperties of liveness \\nprobes 88–89\\nresource requests \\nautomatically 451\\nschedule 117\\nservice endpoints \\nmanually 132–134\\ncreating endpoints resource \\nfor services without \\nselectors 133–134\\ncreating services without \\nselectors 132–133\\nsession affinity on services 126\\ntab completion for kubectl 41–42\\nworker nodes with kubeadm\\n549–550\\nconnections\\nexternal 141–142\\npreventing breakage when pods \\nshut down 493–497\\nsequence of events at pod \\ndeletion 493–495\\ntroubleshooting 495–496\\npreventing breakage when pods \\nstart up 492–493\\nsignaling when pods ready to \\naccept 149–153\\nto API servers 503\\nto backend services 502\\nto services living outside \\ncluster 131–134\\ncreating alias for external \\nservices 134\\nservice endpoints 131–134\\nto services through FQDN 130\\nto services through load \\nbalancers 139–141\\ncontainer images\\nbuilding 29–32\\nimage layers 30–31\\noverview 30\\nwith Dockerfile vs. \\nmanually 31–32\\ncreating Dockerfile for 29\\ncreating Node.js \\napplications 28–29\\nin Docker container \\nplatform 15\\npushing image to image \\nregistry 35–36\\npushing images to Docker \\nHub 36\\nrunning images on different \\nmachines 36\\ntagging images under addi-\\ntional tags 35\\nremoving containers 34–35\\nrunning 32–33\\naccessing applications 32\\nlisting all running \\ncontainers 32\\nobtaining information about \\ncontainers 33\\nstopping containers 34–35\\nversioning 28\\nviewing environment of run-\\nning containers 33–34\\nisolating filesystems 34\\nprocesses running in host \\noperating system 34\\nrunning shells inside \\nexisting 33\\nwith out-of-range user IDs 393\\nContainer Network Interface. See \\nCNI\\ncontainer networking interface. \\nSee Kubernetes-CNI\\ncontainer ports, specifying 63–65\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 599,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n567\\ncontainer runtimes 552–555\\nreplacing Docker with rkt\\n552–555\\nconfiguring Kubernetes to \\nuse rkt 552\\nusing rkt with Minikube\\n553–555\\nusing through CRI 555\\nCRI-O Container Runtime\\n555\\nrunning applications in VMs \\ninstead of \\ncontainers 555\\nCONTAINER_CPU_REQUEST\\n_MILLICORES variable 228\\nContainerCreating 486\\nCONTAINER_MEMORY_LIMIT\\n_KIBIBYTES variable 228\\n--container-runtime=rkt option\\n552–553\\ncontainers 7–16\\nadding capabilities to all 395\\nadding individual kernel capa-\\nbilities to 384–385\\ncomparing VMs to 8–10\\nconfiguring security \\ncontexts 380–389\\npreventing containers from \\nrunning as root 382\\nrunning containers as specific \\nuser 381–382\\nrunning pods in privileged \\nmode 382–384\\nrunning pods without \\nspecifying security \\ncontexts 381\\ncopying files to and from\\n500–501\\ncopying logs to and from\\n500–501\\ndetermining QoS class of 418\\nDocker container platform\\n12–15\\nbuilding images 13\\ncomparing VMs to 14–15\\nconcepts 12–13\\ndistributing images 13\\nimage layers 15\\nportability limitations of con-\\ntainer images 15\\nrunning images 13\\ndropping capabilities \\nfrom 385–386, 395\\nexisting, running shells inside 33\\nexploring 33\\nimages\\ncreating 290–291\\nlisting 554–555\\nInit containers, adding to \\npods 484–485\\ninstead of VMs, running appli-\\ncations in 555\\nisolating filesystems 34\\nlimiting resource available \\nto 412–416\\nlimits as seen by applications \\nin 415–416\\nLinux, isolating components \\nwith 8\\nlisting all 32\\nlocating 21\\nmaking images 497\\nmechanisms for isolation 11\\nmounting local files into 503\\nmultiple vs. one with multiple \\nprocesses 56–57\\nobtaining information about 33\\nof pods\\nrequesting resources \\nfor 405–412\\nresources requests for\\n411–412\\nrunning with kubelet 332\\norganizing across pods 58–60\\nsplitting into multiple pods \\nfor scaling 59\\nsplitting multi-tier applica-\\ntions into multiple \\npods 59\\noverview 8–12\\nisolating processes with Linux \\nNamespaces 11\\nlimiting resources available to \\nprocess 11–12\\npartial isolation between 57\\npassing command-line argu-\\nments to 192–196\\ndefining arguments in \\nDocker 193–195\\ndefining command in \\nDocker 193–195\\noverriding arguments in \\nKubernetes 195–196\\noverriding command in \\nKubernetes 195–196\\nrunning fortune pods with \\ncustom interval 195–196\\npassing ConfigMap entries \\nto 202–203\\npods with multiple, determin-\\ning QoS classes of 419\\npost-start, using lifecycle \\nhooks 486–487\\npre-stop\\nusing lifecycle hooks\\n487–488\\nusing lifecycle hooks when \\napplication not receiving \\nSIGTERM signal\\n488–489\\npreventing processes from writ-\\ning to filesystems 386–387\\nprocesses running in host oper-\\nating system 34\\nremotely executing commands \\nin 124–126\\nremoving 34–35\\nrkt container platform, as alter-\\nnative to Docker 15–16\\nrunning 20–21\\neffect of application descrip-\\ntions on 19–20\\ninspecting in Minikube \\nVM 553–554\\nviewing environment of 33–34\\nrunning applications inside \\nduring development 503\\nrunning shells in 130–131\\nseeing all node CPU cores 416\\nseeing node memory 415–416\\nsetting environment variables \\nfor 196–198\\nconfiguring INTERVAL in \\nfortune images through \\nenvironment variables\\n197\\ndisadvantages of hardcoding \\nenvironment \\nvariables 198\\nreferring to environment \\nvariables in variable \\nvalues 198\\nsetting hard limits for resources \\nused by 412–413\\ncreating pods with resource \\nlimits 412–413\\novercommitting limits 413\\nsetting up networks 550\\nsharing data between with \\nvolumes 163–169\\nusing emptyDir volume\\n163–166\\nusing Git repository as \\nstarting point for \\nvolume 166–169\\nsharing IP 57\\nsharing port space 57\\nsharing volumes when running \\nas different users 387–389\\nspecifying environment vari-\\nables in definitions 197\\nspecifying name when retriev-\\ning logs of multi-container \\npods 66\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 600,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n568\\ncontainers (continued)\\nspecifying which capabilities \\ncan be added to 394\\nstopping 34–35\\ntargeting 489\\nusing Secrets to pass sensitive \\ndata to 213–223\\nConfigMaps versus Secrets\\n217–218\\ncreating Secrets 216\\ndefault token Secrets 214–215\\nimage pull Secrets 222–223\\noverview of Secrets 214\\nusing Secrets in pods\\n218–222\\nusing volumes to preserve data \\nacross restarts 480–482\\nwhen to use multiple in \\npod 59–60\\nwith same QoS classes, \\nhandling 420–421\\nSee also container images; side-\\ncar containers\\n--containers option 432\\ncontainers, Docker-based 13\\nContentAgent container 162\\ncontexts\\ncurrent, in kubeconfig files 536\\ndeleting 538\\nin kubeconfig files 536\\nlisting 538\\nswitching between 538\\nusing kubectl with 537–538\\nSee also security contexts\\ncontinuous delivery 6–7\\nbenefits of 7\\nrole of developers in 7\\nrole of sysadmins in 7\\nContinuous Integration and Con-\\ntinuous Delivery. See CI/CD\\nControl Plane 18–19\\ncomponents of 310, 344\\nmaking components highly \\navailable 342–345\\nensuring high availability of \\ncontrollers 343–344\\nensuring high availability of \\nScheduler 343–344\\nrunning etcd clusters 342–343\\nrunning multiple instances of \\nAPI servers 343\\nusing leader-election in Con-\\ntrol Plane components\\n344\\nController Manager, Control \\nPlane component 19\\ncontrollers 145, 321–326, 330–333\\nchain of events 331–332\\nDeployment controller cre-\\nates ReplicaSet 331\\nkubelet runs pod \\ncontainers 332\\nReplicaSet controller creates \\npods 332\\nScheduler assigns node to \\nnewly created pods 332\\ncomponents involved in 330\\ncustom, automating custom \\nresources with 513–517\\nDaemonSet 324\\nDeployment controllers 324\\nEndpoints controllers 325\\nensuring high availability \\nof 343–344\\nJob controllers 324\\nNamespace controllers 325\\nNode controllers 324\\nobserving cluster events 332–333\\noverview 322, 326\\nPersistentVolume \\ncontrollers 325–326\\nremoving pods from 100\\nReplicaSet 324\\nreplication managers 323–324\\nrunning as pods 515–516\\nService controllers 324\\nStatefulSet controllers 324\\nWebsite 514–515\\ncopies, scaling number of 21\\ncopying\\nfiles to and from containers\\n500–501\\nimages to Minikube VM \\ndirectly 504\\nlogs to and from \\ncontainers 500–501\\nCPUs (central processing units)\\ncreating Horizontal pod Auto-\\nscaler based on 442–443\\ncreating ResourceQuota \\nresources for 425–426\\ndisplaying usage\\nfor cluster nodes 431\\nfor pods 431–432\\nnodes, seen by containers 416\\nscaling based on 441–447\\nautomatic rescale \\nevents 444–445\\ncreating Horizontal pod \\nAutoscaler based on \\nCPU usage 442–443\\nmaximum rate of scaling 447\\nmodifying target metric val-\\nues on existing HPA \\nobjects 447\\ntriggering scale-ups 445–446\\nusing Autoscaler to scale up \\nDeployments 446–447\\ntime sharing 411\\nCrashLoopBackOff 414, 499\\nCRD (CustomResource-\\nDefinitions) 509–513\\ncreating CRD objects 510–511\\ncreating instances of custom \\nresources 511–512\\ndeleting instances of custom \\nobjects 512–513\\nexamples of 509–510\\nretrieving instances of custom \\nresources 512\\ncreation_method label 72\\ncredentials. See user credentials\\nCRI (Container Runtime Inter-\\nface), using container run-\\ntimes through 555\\nCronJob resources, creating\\n116–117\\nCRUD (Create, Read, Update, \\nDelete) 316\\nCSR (CertificateSigning-\\nRequest) 148\\ncurl command 124–126, 130–131, \\n167\\ncurl pods, running with additional \\nambassador containers\\n244–245\\nCURL_CA_BUNDLE variable 241\\nCURRENT column 49\\ncustom intervals, running fortune \\npods with 195–196\\ncustom-namespace.yaml file 78\\nD\\n-d flag 32\\nDaemonSets 324, 328, 550, 559\\ncreating 111\\ncreating YAML definition 110\\nexamples of 109\\nrunning one pod on each node \\nwith 108–112\\nrunning pods on certain nodes \\nwith 109–112\\nadding required label to \\nnodes 111\\nremoving required label from \\nnodes 111–112\\nrunning pods on every node \\nwith 109\\ndashboard 52–53\\naccessing when running in man-\\naged GKE 52\\naccessing when using \\nMinikube 53\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 601,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n569\\ndata\\nbinary, using Secrets for 217\\npassing to containers using \\nSecrets 213–223\\nConfigMaps versus \\nSecrets 217–218\\ncreating Secrets 216\\ndefault token Secrets\\n214–215\\nimage pull Secrets 222–223\\noverview of Secrets 214\\nusing Secrets in pods\\n218–222\\npersisted by previous pod\\n173–174\\nusing volumes to preserve \\nacross container \\nrestarts 480–482\\nwriting to persistent storage by \\nadding documents to Mon-\\ngoDB database 173\\nwritten to disks, \\ndisappearing 480\\ndata stores, clustered 303–304\\ndeadlines, configuring for \\nrollouts 278\\ndeclarative scaling 103\\ndecoupling\\nconfigurations with \\nConfigMap 198–213\\nConfigMaps 198–199\\ncreating ConfigMaps\\n200–201\\npassing all entries of Config-\\nMap as environment \\nvariables at once 204\\npassing ConfigMap entries as \\ncommand-line \\narguments 204–205\\npassing ConfigMap entries to \\ncontainers as environ-\\nment variables 202–203\\nupdating application config \\nwithout restarting \\napplication 211, 213\\nusing configMap volume to \\nexpose ConfigMap \\nentries as files 205, 211\\npods from underlying storage \\ntechnologies 176–184\\nbenefits of using claims 182\\nPersistentVolumeClaims\\n176–177, 179–181\\nPersistentVolumes 176–184\\ndefault capabilities, \\nconfiguring 394–395, 403\\nadding capabilities to all \\ncontainers 395\\ndropping capabilities from \\ncontainer 395\\nspecifying which capabilities \\ncan be added to \\ncontainer 394\\ndefault policy 397\\ndefault token Secret 214–215\\ndefaultAddCapabilities 394\\ndefaultMode property 232\\nDeis Helm package manager\\n530–533\\nDELETED watch event 515\\ndeleting\\nclusters 538\\ncontexts 538\\ninstances of custom \\nobjects 512–513\\nPersistentVolumeClaims 288\\nPet pods 297–298\\npods 80–82, 252–253, 306\\nby deleting whole \\nnamespaces 80–81\\nby name 80\\nforcibly 307\\nin namespace while keeping \\nnamespace 81–82\\nmanually 306–307\\nsequence of events 493–495\\nusing label selectors 80\\npods marked for 307\\nReplicationControllers 103–104\\nresources in namespace 82\\ndeletionTimestamp field 489, 495\\ndelivery. See continuous delivery\\ndependencies\\ninter-pod 485\\noverview 5\\ndeploying\\nadd-ons 328–329\\napplications through \\nStatefulSets 291–295\\ncreating governing \\nservices 292–294\\ncreating persistent \\nvolumes 291–292\\ncreating StatefulSets 294\\nexamining PersistentVolume-\\nClaims 295\\nexamining stateful pods\\n294–295\\napplications, simplifying 21–22\\nmicroservices 5\\nnewly built images automatically \\nwith DeploymentConfigs\\n529–530\\nNode.js applications 42–44\\nbehind the scenes 44\\nlisting pods 43–44\\npods\\nin same availability \\nzone 471–472\\nco-locating pods in same \\navailability zone 471\\ntopologyKey 471–472\\nin same geographical \\nregions 471–472\\nco-locating pods in \\nsame geographical \\nregion 471\\ntopologyKey 471–472\\nin same rack 471–472\\nin two-node clusters 468\\non same nodes, with \\ninter-pod affinity\\n468–471\\nwith container images \\nwith out-of-range user \\nIDs 393\\nwith pod affinity 470\\nwith runAsUser outside of \\npolicy ranges 393\\nprivileged containers, creating \\nPodSecurityPolicy to \\nallow 396–397\\nresources through Helm\\n531–533\\nversions 269\\nDeployment controllers\\ncreating ReplicaSet controller \\nwith 331\\noverview 324\\nDeployment resource\\nbenefits of 267–268\\ncreating 262–264\\nmanifest, creating 262\\nrolling back to specific \\nrevision 270\\nstrategies 264–265\\nupdating 264–268\\nslowing down rolling \\nupdates 265\\ntriggering rolling \\nupdates 265–267\\nusing for updating applications \\ndeclaratively 261–278\\nblocking rollouts of bad \\nversions 274–278\\ncontrolling rate of \\nrollout 271–273\\npausing rollout process\\n273–274\\nrolling back \\ndeployments 268–270\\nDeploymentConfigs, deploying \\nnewly built images automati-\\ncally with 529–530\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 602,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n570\\ndeployments 250–279\\ndisplaying rollout history of 270\\nperforming automatic rolling \\nupdates 254–261\\nobsolescence of kubectl roll-\\ning-update 260–261\\nperforming rolling updates \\nwith kubectl 256–260\\nrunning initial version of \\napplications 254–255\\nrolling back 268–270\\ncreating application \\nversions 268\\ndeploying versions 269\\nrolling back to specific Deploy-\\nment revision 270\\nundoing rollouts 269\\nrollouts, displaying status of 263\\nupdating applications running \\nin pods 251–253\\ndeleting old pods 252–253\\nreplacing old pods 252\\nspinning up new pods\\n252–253\\nupdating with kubectl \\napply 276–277\\nusing anti-affinity with pods of \\nsame 475\\nusing Autoscaler to scale \\nup 446–447\\ndeprovisioning, instances 526\\ndescribe command 409\\ndescriptors\\nJSON, creating pods from 61–67\\nYAML\\ncreating for pods 63–65\\ncreating pods from 61–67\\nof existing pods 61–63\\nDESIRED column 49\\ndevelopers, role in continuous \\ndelivery 7\\nDevOps 7\\ndirectories\\nmounting hides existing \\nfiles 210\\nusing multiple in same \\nvolume 282\\ndirectory traversal attack 353\\ndisabling, firewalls 544\\ndisallowed capabilities, \\nconfiguring 394–395\\nadding capabilities to all \\ncontainers 395\\ndropping capabilities from \\ncontainer 395\\nspecifying which capabilities \\ncan be added to \\ncontainer 394\\ndisk=ssd label 110\\ndisks, data written to \\ndisappearing 480\\ndisruptions, of services 454–456\\ndistributing Docker images 13\\nDNS (Domain Name System) 300\\ndiscovering pods through\\n155–156\\ndiscovering services through 129\\nimplementing peer discovery \\nthrough 301–302\\nrecords returned for headless \\nservice 155–156\\nservers 329\\ndnsPolicy property 129\\nDocker container platform 25–54\\nclusters in 36–42\\nrunning local single-node clus-\\nters with Minikube 37–38\\nsetting up aliases for \\nkubectl 41–42\\nsetting up command-line \\ncompletion for \\nkubectl 41–42\\nusing hosted clusters with \\nGKE 38–41\\ncomparing VMs to 14–15\\nconcepts 12–13\\ncontainer images 26–36\\nbuilding 29–32\\ncreating Dockerfile for 29\\ncreating Node.js \\napplications 28–29\\npushing images to image \\nregistry 35–36\\nremoving containers 34–35\\nrunning 32–33\\nstopping containers 34–35\\nviewing environment of run-\\nning containers 33–34\\ndefining arguments in 193–195\\nCMD instruction 193\\nENTRYPOINT \\ninstruction 193\\nmaking INTERVAL configu-\\nrable in fortune \\nimages 194–195\\nshell forms vs. exec \\nforms 193–194\\ndefining commands in 193–195\\nCMD instruction 193\\nENTRYPOINT instruction\\n193\\nmaking INTERVAL configu-\\nrable in fortune \\nimages 194–195\\nshell forms vs. exec \\nforms 193–194\\nimages\\nbuilding 13\\ndistributing 13\\nlayers 15\\nportability limitations of 15\\nrunning 13\\ninstalling 26–28\\nregistries, creating Secrets for \\nauthenticating with 223\\nrkt container platform as alter-\\nnative to 15–16\\nrunning first application on \\nKubernetes\\naccessing web applications\\n45–47\\ndeploying Node.js \\napplications 42–44\\nexamining which nodes \\napplication is running \\non 51–52\\nhorizontally scaling \\napplications 48–50\\nlogical parts of systems 47–48\\nusing Kubernetes \\ndashboard 52–53\\nrunning Hello World \\ncontainer 26–28\\nbehind the scenes 27\\nrunning other images 27\\nversioning container \\nimages 28\\nDocker Hub\\npushing images to 36\\nusing private image repositories \\non 222\\nDocker Hub ID 35\\ndocker images command 35\\nDocker platform\\nreplacing with rkt 552–555\\nconfiguring Kubernetes to \\nuse rkt 552\\nusing rkt with Minikube\\n553–555\\nusing daemon inside Minikube \\nVM 503–504\\ndocker ps command 548\\ndocker pull command 44\\nDocker Registry 13\\ndocker run command 26\\nDocker tool, installing 544–545\\ndisabling firewalls 544\\ndisabling SELinux 544\\nenabling net.bridge.bridge-nf-\\ncall-iptables Kernel \\noptions 545\\nDockerfile\\nbuilding images with 31–32\\ncreating for container images 29\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 603,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n571\\nDOCKER_HOST variable 32, 504\\nDocker-registry Secrets, using \\nin pod definitions 223\\ndocuments, writing data to \\npersistent storage by \\nadding to MongoDB \\ndatabase 173\\nDoesNotExist operator 107\\nDomain Name System. See DNS\\ndouble dash character 125\\ndownloading client binaries 39\\ndowntime, reducing by \\nrunning multiple \\ninstances 341\\nDownward API\\nbenefits of using 233\\npassing metadata through\\n226–233\\navailable metadata 226–227\\nexposing metadata through \\nenvironment \\nvariables 227–230\\npassing metadata through \\nfiles in downwardAPI \\nvolume 230–233\\ndownwardAPI volume, passing \\nmetadata through files \\nin 163, 230–233\\nbenefits of using Downward \\nAPI 233\\nreferring to container-level \\nmetadata in volume \\nspecification 233\\nupdating annotations 232\\nupdating labels 232\\ndownwardAPI.items attribute 231\\nDSL (Domain-Specific-\\nLanguage) 248\\ndynamic provisioning\\nof PersistentVolumes\\n184–189\\ndefining available storage \\ntypes through Storage-\\nClass resources 185\\nrequesting storage class in \\nPersistentVolume-\\nClaims 185–187\\nwithout specifying storage \\nclass 187–189\\nwithout specifying storage class\\ncreating PersistentVolume-\\nClaims 188–189\\nexamining default storage \\nclasses 188\\nlisting storage classes 187–188\\nPersistentVolumeClaims \\nbound to pre-provisioned \\nPersistent-Volumes 189\\nE\\necho command 27\\nedit ClusterRole, allowing modifi-\\ncation of resources with 372\\nediting\\nConfigMap 212\\ndefinitions, scaling Replication-\\nController by 102–103\\nEDITOR variable 102\\nEFK Stack 501\\nelastic block store volume, AWS 174\\nELK Stack 501\\nemptyDir volume 162–166\\ncreating pods 164–165\\nseeing pods in action 165\\nspecifying medium for 166\\nusing in pods 163–164\\n--enable-swagger-ui=true \\noption 248\\nEndpoints controllers 325\\nendpoints resources, creating for \\nservices without selectors\\n133–134\\nENTRYPOINT instruction 193\\nenv command 128\\nenv label 72\\nenv=debug label 70\\nenv=devel label 105\\nenv=prod label 70\\nenv=production label 105\\nenvFrom attribute 204\\nenvironment variables\\nconfiguring INTERVAL in \\nfortune images through\\n197–198\\ndisadvantages of \\nhardcoding 198\\ndiscovering services \\nthrough 128–129\\nexposing metadata \\nthrough 227–230\\nexposing Secrets entries \\nthrough 221–222\\npassing all ConfigMap entries \\nas 204\\npassing ConfigMap entries to \\ncontainers as 202–203\\nreferring to in variable \\nvalues 198\\nsetting for containers 196, 198\\nspecifying in container \\ndefinitions 197\\nephemeral pods 121\\netcd cluster 518\\netcd stores 312–316\\nensuring consistency of stored \\nobjects 314–315\\nensuring consistency when \\nclustered 315\\nensuring validity of stored \\nobjects 314–315\\nnumber of instances 316\\nrunning clusters 342–343\\nstoring resources in 313–314\\netcd, Control Plane component 19\\nexec forms, versus shell \\nforms 193–194\\nExec probe 86, 90, 150\\nExists operator 107, 461\\nexplain command 175\\nexposing\\nmultiple ports in same \\nservice 126–127\\nmultiple services through same \\nIngress 146–147\\nmapping different services to \\ndifferent hosts 147\\nmapping different services to \\ndifferent paths of same \\nhost 146\\nservices externally through \\nIngress resources 142–149\\nbenefits of using 142–143\\nconfiguring Ingress to handle \\nTLS traffic 147–149\\ncreating Ingress \\nresources 144\\nusing Ingress controllers\\n143–144\\nservices externally using \\nRoutes 530\\nservices to external clients\\n134–142\\nexternal connections 141–142\\nthrough external load \\nbalancers 138–141\\nusing NodePort \\nservices 135–138\\nexternal clients\\nallowing NodePort services \\naccess to 137–138\\nexposing services to 134–142\\nexternal connections\\n141–142\\nthrough external load \\nbalancers 138–141\\nusing NodePort \\nservices 135–138\\nexternal services, creating alias \\nfor 134\\nExternalIP 138\\nEXTERNAL-IP column 136\\nExternalName services, \\ncreating 134\\nexternal-traffic annotation 141\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 604,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n572\\nF\\nFABRIC8 Java client, interacting \\nwith 247–248\\nFailedPostStartHook 487\\nFailedPreStopHook 488\\nFallbackToLogsOnError 500\\nfeatures. See security-related fea-\\ntures\\nfederation. See Cluster Federation\\nfiles\\ncopying to and from \\ncontainers 500–501\\ncreating ConfigMap entries \\nfrom contents of 201\\nin configMap volumes, setting \\nfile permissions for 211\\nin directories, creating Config-\\nMap from 201\\nin downwardAPI volume, \\npassing metadata through\\n230–233\\nin sync with gitRepo \\nvolume 168\\nmounted config, verifying \\nNginx using 208\\nmounting 503\\nmounting ConfigMap entries \\nas 210–211\\nmounting directory hiding \\nexisting 210\\non worker node filesystems, \\naccessing 169–170\\nupdating automatically 212–213\\nusing configMap volume to \\nexpose ConfigMap entries \\nas 205, 211\\nfilesystems\\nof containers\\nisolating 34\\npreventing processes from \\nwriting to 386–387\\nworker node, accessing files \\non 169–170\\nfirewalls\\nchanging rules to let external \\nclients access NodePort \\nservices 137–138\\ndisabling 544\\nFlannel 312\\nflat networks, inter-pod 58\\nflexVolume volume 163\\nflocker volume 163\\nfoo namespace 358\\nfoo.default.svc.cluster.local \\ndomain 286\\nFOO_SECRET variable 221\\n--force option 490\\nfortune\\nimages, configuring INTERVAL \\nvariables in 194–195\\npods, running with custom \\nintervals 195–196\\nfortune command 163, 165\\nfortune images, INTERVAL vari-\\nables in 197\\nfortune-config config map, \\nmodifying to enable HTTPS\\n218–219\\nfortune-https directory 216\\nfortune-https Secret, mounting in \\npods 219–220\\nfortuneloop container 209\\nfortuneloop.sh script 164\\nfortune-pod.yaml file 164\\nforwarding, local network port to \\nport in pod 67\\nFQDN (fully qualified domain \\nname) 129–130, 134\\nFROM scratch directive 497\\n--from-file argument 201\\nfsGroup policies 392–394\\ndeploying pod with container \\nimage with an out-of-range \\nuser ID 393\\nusing MustRunAs rule 392–393\\nfsGroup property 388\\nG\\ngateways 58\\nGCE (Google Compute \\nEngine) 185, 454\\ncreating persistent disks 171–172\\nusing in pod volumes 171–174\\ncreating GCE persistent \\ndisks 171–172\\ncreating pods using gcePer-\\nsistentDisk volumes 172\\nre-creating pods 173–174\\nverifying pod can read data \\npersisted by previous \\npod 173–174\\nwriting data to persistent stor-\\nage by adding docu-\\nments to MongoDB \\ndatabase 173\\ngcePersistentDisk volumes\\ncreating pods 172\\noverview 162, 190\\ngcloud command 39, 171\\ngcloud compute ssh command 97\\n--generator flag 42\\ngeographical regions\\nco-locating pods in same 471\\ndeploying pods in same 471–472\\nGit repositories\\nas starting point for \\nvolumes 166–169\\nfiles in sync with gitRepo \\nvolume 168\\ngitRepo volume 169\\nsidecar containers 168\\ncloned, running web server pod \\nserving files from 167\\nprivate, using gitRepo volume \\nwith 168\\nGit sync container 168\\ngitRepo field 510, 517\\ngitRepo volumes\\nfiles in sync with 168\\noverview 169\\nusing with private Git \\nrepositories 168\\nGKE (Google Container \\nEngine) 36, 454\\naccessing dashboard when run-\\nning in 52\\nand Minikube, switching \\nbetween 534–535\\nswitching from Minikube \\nto 534\\nswitching to Minikube \\nfrom 534\\nusing hosted clusters with\\n38–41\\ncreating clusters with three \\nnodes 39\\ndownloading client \\nbinaries 39\\ngetting overview of \\nclusters 39\\nlisting cluster nodes 40\\nretrieving additional details \\nof objects 41\\nsetting up Google Cloud \\nprojects 39\\nglusterfs volume 163\\nGoogle Cloud, setting up \\nprojects 39\\nGoogle Compute Engine. See GCE\\nGoogle Container Registry 35\\ngoverning services\\ncreating 292–294\\noverview 285–287\\ngpu=true label 75, 464–465\\nGrafana suite\\nanalyzing resource usage \\nwith 433–434\\noverview 432\\nrunning in clusters 433\\ngroup ID (gid) 381\\ngrouping resources, with \\nnamespaces 76–80\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 605,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n573\\ngroups 347–348\\nassigning different PodSecurity-\\nPolicies to 396–399\\nin Red Hat OpenShift Con-\\ntainer platform 528\\nGuaranteed class, assigning pods \\nto 417–418\\nguarantees, StatefulSets 289–290\\nat-most-one semantics 290\\nimplications of stable \\nidentity 289–290\\nimplications of stable \\nstorage 289–290\\nH\\nhardcoding environment vari-\\nables, disadvantages of 198\\nhardware, improving utilization \\nof 22\\nheadless services\\ncreating 154–155\\nDNS returned for 155–156\\nusing for discovering individual \\npods 154–156\\ndiscovering all pods 156\\ndiscovering pods through \\nDNS 155–156\\nhealth checking 22–23\\nhealth HTTP endpoint 89\\nHeapster aggregator, \\nenabling 431\\nHello World container 26–28\\nbehind the scenes 27\\nrunning other images 27\\nversioning container images 28\\nHelm. See Deis Helm package \\nmanager\\nhooks. See lifecycle hooks\\nhops. See network hops\\nhorizontal autoscaling, of \\npods 438–451\\nautoscaling process 438–441\\nmetrics appropriate for \\nautoscaling 450\\nscaling\\ndown to zero replicas\\n450–451\\non CPU utilization 441–447\\non memory \\nconsumption 448\\non other and custom \\nmetrics 448–450\\nHorizontal pod Autoscaler, creat-\\ning based on CPU \\nusage 442–443\\nhorizontal scaling, of cluster \\nnodes 452–456\\nCluster Autoscaler 452–453\\nenabling Cluster \\nAutoscaler 454\\nlimiting service disruption \\nduring cluster scale-\\ndown 454–456\\nHost header 147\\nhost networks, namespaces\\n377–379\\nhost nodes, using namespaces in \\npods 376–380\\nhost operating systems, container \\nprocesses running in 34\\nhost ports, binding to 377–379\\nhostIPC property 380\\nhostnames\\nchanging on cloned VMs 546\\nexpecting changes to 480\\nhostNetwork property 376\\nhostPath volumes 162, 169–170\\nhostPort property 377\\nhosts\\nconfiguring name resolution \\nfor 546–547\\nmapping different services \\nto 147\\nmapping different services to \\npaths of same 146\\nHPA (HorizontalPodAutoscaler)\\nmodifying target metric values \\non existing objects 447\\noverview 443\\nhtml-generator container 165\\nHTTP GET probe 85, 150\\nHTTP-based liveness probes, \\ncreating 86–87\\nhttpGet liveness probe 87\\nHTTPS (Hypertext Transfer \\nProtocol Secure)\\n218–219\\nhybrid cloud 556\\nhypervisors 9\\nI\\nidentities\\nnetwork, providing 285–287\\ngoverning service, \\noverview 285–287\\nscaling StatefulSets 287\\nof API servers, verifying\\n240–241\\nproviding stable for pods\\n282–284\\nstable, implications of\\n289–290\\nif statement 268\\nIgnoredDuringExecution 464\\nimage layers 15, 30–31\\nimage pull Secrets 222–223, \\n351\\ncreating Secrets for authenticat-\\ning with Docker \\nregistries 223\\nspecifying on every pod 223\\nusing Docker-registry Secrets in \\npod definitions 223\\nusing private image repositories \\non Docker Hub 222\\nimage registry\\npushing images to 35–36\\nrunning images on different \\nmachines 36\\ntagging images under addi-\\ntional tags 35\\npushing images to Docker \\nHub 36\\nimagePullPolicy\\noverview 256, 317\\nusing 497–498\\nimagePullSecrets field 222\\nimages\\nbuilding\\nfrom source using \\nBuildConfigs 529\\nlocally 504\\nusing Docker daemon inside \\nMinikube VM 503–504\\ncopying to Minikube VM \\ndirectly 504\\ndeploying automatically \\nwith DeploymentConfigs\\n529–530\\nDocker\\nbuilding 13\\ndistributing 13\\nportability limitations of 15\\nrunning 13\\nfortune\\nconfiguring INTERVAL vari-\\nables in 194–195\\nconfiguring INTERVAL vari-\\nables through environ-\\nment variables 197\\nof containers\\ncreating 290–291, 497\\nlisting 554–555\\npushing to Docker Hub 36\\npushing to image registry\\n35–36\\nrunning on different \\nmachines 36\\ntagging 35, 497–498\\nSee also container images\\nImageStream 529\\nIn operator 107\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 606,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n574\\nInfluxDB database\\noverview 432\\nrunning in clusters 433\\nIngress resource 135\\naccessing pods through 145\\naccessing services through 145\\nbenefits of using 142–143\\nconfiguring to handle TLS \\ntraffic 147–149\\ncreating 144\\ncreating TLS certificate \\nfor 147–149\\nexposing multiple services \\nthrough 146–147\\nmapping different services to \\ndifferent hosts 147\\nmapping different services to \\ndifferent paths of same \\nhost 146\\nexposing services externally \\nthrough 142–149\\nobtaining IP address of 145\\noverview 145\\nIngresses 559\\nInit containers, adding to \\npods 484–485\\ninitialDelaySeconds property 88\\ninitializing masters with kubeadm \\ninit 547–548\\ninitiating install of OS 542\\ninspecting node capacity 407–408\\ninstalling\\nDocker 544–545\\ndisabling firewalls 544\\ndisabling SELinux 544\\nenabling net.bridge.bridge-\\nnf-call-iptables Kernel \\noptions 545\\nkubeadm 544–545\\nkubectl 38, 544–545\\nKubelet 544–545\\nKubernetes 544–545\\nadding Kubernetes yum \\nrepo 544\\ndisabling firewalls 544\\ndisabling SELinux 544\\nenabling net.bridge.bridge-\\nnf-call-iptables Kernel \\noption 545\\nKubernetes-CNI 544–545\\nMinikube 37\\nOS 541–544\\ninitiating 542\\nrunning install 543–544\\nselecting start-up disks 541\\nsetting installation \\noptions 542–543\\nInternet protocol. See IP\\ninter-pod affinity, to deploy pods \\non same nodes 468–471\\ndeploying pods with pod \\naffinity 470\\nspecifying pod affinity in pod \\ndefinitions 469\\nusing pod affinity rules with \\nScheduler 470–471\\nINTERVAL variables, configuring \\nin fortune images\\noverview 194–195\\nthrough environment \\nvariables 197\\nintervals, running fortune pods \\nwith 195–196\\nIP (Internet protocol)\\ncontainers sharing 57\\nexternal, accessing services \\nthrough 46–47\\nIngress address 145\\nlocal, expecting changes to 480\\nof client, non-preservation of 142\\nservice, pinging 131\\nIP addresses 121\\nIPC (Inter-Process Communication)\\noverview 11, 56, 376\\nusing namespaces 379–380\\niptables rules 327–328, 339–340, \\n345, 492, 494–495\\niscsi volume 163\\nisolating\\ncomponents with Linux con-\\ntainer technologies 8\\ncontainer filesystems 34\\ncontainers, mechanisms for 11\\nnetworks between Kubernetes \\nnamespaces 401–402\\npartially between containers of \\nsame pod 57\\npod networks 399–402\\nprocesses with Linux \\nNamespaces 11\\n-it option 33\\nitems attribute 138\\nJ\\nJob controllers 324\\nJob resources 112\\nconfiguring templates 117\\nrunning multiple pod instances \\nin 114–116\\nrunning Job pods in \\nparallel 115\\nrunning Job pods \\nsequentially 115\\nscaling Job 116\\nrunning on pods 114\\njobs\\ndefining 113–114\\nlisting instances in \\nclusters 237–238\\nretrieving instances by \\nname 238\\nscheduling 116–118\\ncreating CronJob 116–117\\noverview 117\\nJSON format\\ncreating pods from descriptors\\nsending requests to pods\\n66–67\\nusing kubectl create to create \\npods 65\\nviewing application logs\\n65–66\\nmanifests, writing 505–506\\nJSONPath 138\\nK\\nkeep-alive connections 140\\nkernel capabilities, adding to \\ncontainers 384–385\\nKernels, enabling \\nnet.bridge.bridge-nf-call-\\niptables option 545\\nkey files, from Secrets 221\\nkey=value format 232\\nkilling applications 479–482\\nexpecting data written to disk to \\ndisappear 480\\nexpecting hostnames to \\nchange 480\\nexpecting local IP to \\nchange 480\\nusing volumes to preserve data \\nacross container \\nrestarts 480–482\\nKsonnet, as alternative to writing \\nJSON manifests 505–506\\nkube config\\nadding entries 536–537\\nadding clusters 536\\nadding user credentials\\n536–537\\ntying clusters and user cre-\\ndentials together 537\\nlisting entries 536–537\\nadding clusters 536\\nadding user credentials\\n536–537\\nmodifying clusters 536\\nmodifying user \\ncredentials 536–537\\ntying clusters and user cre-\\ndentials together 537\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 607,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n575\\nkube config (continued)\\nmodifying entries 536–537\\nmodifying clusters 536\\nmodifying user credentials\\n536–537\\ntying clusters and user cre-\\ndentials together 537\\nkubeadm init command 549\\nkubeadm join command 549\\nkubeadm tool\\nconfiguring masters with\\n547–549\\nconfiguring worker nodes \\nwith 549–550\\ninstalling 544–545\\nrunning components with\\n548–549\\nlisting nodes 548–549\\nlisting pods 548\\nrunning kubectl on \\nmasters 548\\nrunning init to initialize \\nmasters 547–548\\nsetting up multi-node clusters \\nwith 539–551\\nsetting up operating \\nsystems 539\\nsetting up required \\npackages 539\\nusing clusters from local \\nmachines 550–551\\nkubeconfig files\\nconfiguring location of 535\\ncontents of 535–536\\nKUBECONFIG variable 535\\nkubectl\\naliases for 41–42\\ncommand-line completion\\n41–42\\nconfiguring tab completion \\nfor 41–42\\nconfirming cluster communicat-\\ning with 38\\ncreating additional users for 398\\ninstalling 38\\nlogs, retrieving with pod logs 66\\nperforming rolling updates \\nwith 256–260\\nreplacing old pods with new \\npods by scaling two \\nReplicationControllers\\n259–260\\nsteps performed before roll-\\ning update commences\\n258–259\\nproxy\\naccessing API servers \\nthrough 234–235\\nexploring Kubernetes API \\nthrough 235–236\\nusing with different \\nclusters 537–538\\nusing with different \\ncontexts 537–538\\nusing with different users\\n537–538\\nusing with multiple \\nclusters 534–538\\nadding kube config \\nentries 536–537\\nconfiguring location of \\nkubeconfig files 535\\ncontents of kubeconfig \\nfiles 535–536\\ndeleting clusters 538\\ndeleting contexts 538\\nlisting clusters 538\\nlisting contexts 538\\nlisting kube config \\nentries 536–537\\nmodifying kube config \\nentries 536–537\\nswitching between \\ncontexts 538\\nswitching between Minikube \\nand GKE 534–535\\nusing with multiple \\nNamespaces 535–538\\nadding kube config \\nentries 536–537\\nconfiguring location of \\nkubeconfig files 535\\ncontents of kubeconfig \\nfiles 535–536\\ndeleting clusters 538\\ndeleting contexts 538\\nlisting clusters 538\\nlisting contexts 538\\nlisting kube config \\nentries 536–537\\nmodifying kube config \\nentries 536–537\\nswitching between \\ncontexts 538\\nkubectl annotate command 76\\nkubectl apply command 266, \\n276–277, 504\\nkubectl autoscale command 443\\nkubectl cluster-info command 38, \\n52\\nkubectl command-line tool 39\\nkubectl cp command 500\\nkubectl create command 65, 94, \\n114, 178, 255\\nkubectl create configmap \\ncommand 200–201\\nkubectl create deployment \\ncommand 519\\nkubectl create -f command 65\\nkubectl create namespace \\ncommand 78–79\\nkubectl create role command 359\\nkubectl create secret \\ncommand 519\\nkubectl delete command 104\\nkubectl describe command 41, 52, \\n76, 95, 131, 408, 444, 512, \\n549, 553\\nkubectl describe node \\ncommand 409\\nkubectl describe pod \\ncommand 215, 487, 499\\nkubectl edit command 151\\nkubectl edit method 266\\nkubectl exec command 124, 128, \\n130, 152, 446\\nkubectl explain command 64, 505\\nkubectl expose command 66, 123\\nkubectl get command 48, 95\\nkubectl get events 332\\nkubectl get pods command 51, 124\\nkubectl get serviceclasses \\ncommand 523\\nkubectl get services command 46\\nkubectl get svc command 129\\nkubectl logs command 87, 485, 502\\nkubectl patch method 266\\nkubectl port-forward \\ncommand 67\\nkubectl proxy command 244, 503\\nkubectl proxy process 514–515\\nkubectl replace method 266\\nkubectl rolling-update \\ncommand 257, 260–261, 264\\nkubectl rollout status \\ncommand 271\\nkubectl run command 61, 82\\nkubectl scale command 102–103, \\n116\\nkubectl set image command\\n265–266\\nkubectl tool\\ninstalling 544–545\\nrunning on masters 548\\nkube-dns 129\\nKUBE_EDITOR environment \\nvariable 102\\nKubelet node agent, \\ninstalling 544–545\\nkubelets 326–327\\noverview 326\\nrunning pod containers with 332\\nrunning static pods without API \\nservers 326–327\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 608,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n576\\nkube-proxy 19, 312, 327, 330, 345\\noverview 339\\nusing iptables 339–340\\nkube-public namespace 77\\nKubernetes 16–24\\narchitecture of 310–330\\nadd-ons 328–330\\nAPI servers 316–319\\nclusters 18–19\\ncomponents 310–312\\ncomponents of Control \\nPlane 310\\ncomponents running on \\nworker nodes 310\\nController 321–326\\netcd 312–316\\nkubelet 326–327\\nScheduler 319–321\\nService Proxy 327–328\\nbenefits of using 21–24\\nautomatic scaling 23\\nhealth checking 22–23\\nimproving hardware \\nutilization 22\\nself-healing 22–23\\nsimplifying application \\ndeployment 21–22\\nsimplifying application \\ndevelopment 23–24\\ndashboard 52–53\\naccessing when running in \\nmanaged GKE 52\\naccessing when using \\nMinikube 53\\ninstalling 544–545\\nadding Kubernetes yum \\nrepo 544\\ndisabling firewalls 544\\ndisabling SELinux 544\\nenabling net.bridge.bridge-\\nnf-call-iptables Kernel \\noption 545\\nmaster, checking node status as \\nseen by 305\\norigins of 16\\noverriding arguments in\\n195–196\\noverriding commands in\\n195–196\\noverview 16–18\\nfocusing on core application \\nfeatures 17–18\\nimproving resource \\nutilization 18\\nrunning applications in 19–21\\neffect of application descrip-\\ntion on running \\ncontainers 19–20\\nkeeping containers \\nrunning 20–21\\nlocating containers 21\\nscaling number of copies 21\\nwhen indicated 2–7\\ncontinuous delivery 6–7\\nmicroservices vs. monolithic \\napplications 3–6\\nproviding consistent applica-\\ntion environment 6\\nKubernetes CNI (Container Net-\\nworking Interface) 545\\nKubernetes Control Plane 18\\nKUBERNETES_SERVICE_HOST \\nvariable 239\\nKUBERNETES_SERVICE_PORT \\nvariable 239\\nkube-scheduler resource 344\\nkube-system namespace 77, 312, \\n314, 344, 454, 548\\nkubia 42\\nkubia-2qneh 100\\nkubia-container 32–33\\nkubia-dmdck 100\\nkubia-gpu.yaml file 74\\nkubia-manual 63\\nkubia-rc.yaml file 93\\nKUBIA_SERVICE_HOST \\nvariable 129\\nKUBIA_SERVICE_PORT \\nvariable 129\\nkubia-svc.yaml file 123\\nkubia-website service 517\\nL\\nlabel selectors\\nchanging for Replication-\\nController 100–101\\ndeleting pods using 80\\neffect of changing 93\\nlisting pods using 71–72\\nlisting subsets of pods \\nthrough 71–72\\nReplicaSets 107\\nusing multiple conditions in 72\\nlabels\\nadding to nodes 111\\nadding to pods managed by \\nReplicationControllers 99\\ncategorizing worker nodes \\nwith 74\\nconstraining pod scheduling \\nwith 73–75\\ncategorizing worker nodes \\nwith labels 74\\nscheduling pods to specific \\nnodes 74–75\\nmulti-dimensional vs. single-\\ndimensional 498\\nof existing pods, modifying\\n70–71\\nof managed pods 99–100\\norganizing pods with 67, 71\\noverview 68–69\\nremoving from nodes 111–112\\nspecifying when creating \\npods 69–70\\nupdating 232\\nLAN (local area network) 58\\nlatest tag 28\\n--leader-elect option 343\\nleader-election\\nfor non-horizontally scalable \\napplications 341\\nusing in Control Plane \\ncomponents 344\\nLeastRequestedPriority 407\\nlibraries\\nbuilding with OpenAPI 248\\nbuilding with Swagger 248\\nSee also client libraries\\nlifecycles, of pods 479–491\\nadding lifecycle hooks 485–489\\nhooks\\nadding 485–489\\ntargeting containers with 489\\nusing post-start container\\n486–487\\nusing pre-stop container\\n487–488\\nusing pre-stop when applica-\\ntion not receiving \\nSIGTERM signal\\n488–489\\nkilling applications 479–482\\npod shutdowns 489–491\\nrelocating applications 479–482\\nrescheduling dead pods\\n482–483\\nrescheduling partially dead \\npods 482–483\\nstarting pods in specific \\norder 483–485\\nlimiting resources\\navailable in namespaces 425–429\\nlimiting objects that can be \\ncreated 427–428\\nResourceQuota \\nresources 425–427\\nspecifying quotas for per-\\nsistent storage 427\\nspecifying quotas for specific \\npod states 429\\nspecifying quotas for specific \\nQoS classes 429\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 609,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n577\\nlimiting resources (continued)\\navailable to containers 412–416\\nexceeding limits 414–415\\nlimits as seen by applications \\nin containers 415–416\\nsetting hard limits for \\nresources containers can \\nuse 412–413\\nLimitRange objects, creating\\n422–423\\nlimits\\nas seen by applications in \\ncontainers 415–416\\ncontainers seeing all node \\nCPU cores 416\\ncontainers seeing node \\nmemory 415–416\\nenforcing 423–424\\nexceeding 414–415\\nfor pods per namespace, setting \\ndefaults 421–425\\novercommitting 413\\nsetting for resources used by \\ncontainers 412–413\\nSee also resource limits\\nLinux Control Groups \\n(cgroups) 11\\nLinux Namespaces 11\\nLinux OS\\ncontainer technologies, isolat-\\ning components with 8\\nNamespaces, isolating pro-\\ncesses with 11\\nlisting\\nall running containers 32\\ncluster nodes 40\\nclusters 538\\ncontainer images 554–555\\ncontexts 538\\njob instances in clusters 237–238\\nkube config entries 536–537\\nadding clusters 536\\nadding user credentials\\n536–537\\nmodifying clusters 536\\nmodifying user credentials\\n536–537\\ntying clusters and user cre-\\ndentials together 537\\nnodes 548–549\\nPersistentVolumeClaims 180\\nPersistentVolumes 180–181\\npods 43–44, 51, 71–72, 548\\nservices 46\\nservices available in clusters 523\\nstorage classes 187–188\\nsubsets through label \\nselectors 71–72\\nliveness probes 85–86\\nconfiguring properties of\\n88–89\\ncreating 89–90\\nHTTP-based 86–87\\nimplementing retry loops \\nin 90\\nkeeping probes light 90\\nwhat liveness probe should \\ncheck 89–90\\nin action 87–88\\nload balancers\\nconnecting to services \\nthrough 139–141\\nexternal, exposing services \\nthrough 138–141\\nLoadBalancer service\\ncreating 139\\noverview 135, 530\\nlocal area network. See LAN\\nlocal machines, using clusters \\nfrom 550–551\\nLogRotator container 162\\nlogs\\ncentralized, using 501\\ncopying to and from \\ncontainers 500–501\\nhandling multi-line \\nstatements 502\\nkubectl, retrieving with pod \\nlogs 66\\nof applications 500–502\\nof multi-container pods, specify-\\ning container name when \\nretrieving 66\\npod, retrieving with kubectl \\nlogs 66\\nSee also application logs\\nlogVol 162\\nls command 151\\nM\\nmanifests\\nDeployment resource, \\ncreating 262\\nJSON, writing 505–506\\nresources 504–505\\nYAML, writing 505–506\\nmapping\\ndifferent services to different \\nhosts 147\\ndifferent services to different \\npaths of same host 146\\nmaster node 18\\nmasters\\nconfiguring with \\nkubeadm 547–549\\nrunning kubeadm init to \\ninitialize 547–548\\nrunning kubectl on 548\\nmatchExpressions property 107, \\n465, 470\\nmatchLabels field 105, 107, \\n470\\nmaxSurge property 271–272\\nmaxUnavailable property\\n271–274, 277, 455\\nmedium, specifying for emptyDir \\nvolume 166\\nmemory\\ncreating ResourceQuota \\nresources for 425–426\\ndisplaying usage\\nfor cluster nodes 431\\nfor pods 431–432\\nkilling processes when low\\n420–421\\nhandling containers with \\nsame QoS class 420–421\\nsequence of QoS classes 420\\nnodes, as seen by containers\\n415–416\\nscaling based on consumption \\nof 448\\nSecret volumes stored in \\nmemory 221\\nmetadata\\ncontainer-level, in volume \\nspecifications 233\\nexposing through environment \\nvariables 227–230\\npassing through Downward \\nAPI 226–233\\navailable metadata 226–227\\npassing through files in \\ndownwardAPI volume\\n230–233\\nmetadata section 201\\nmetadata.name field 510\\nmetadata.resourceVersion \\nfield 313\\nmetric types\\nObject, scaling based on\\n449–450\\npods\\nobtaining 438–439\\nscaling based on 449\\nresource, scaling based on 449\\nmetrics\\nappropriate for autoscaling\\n450\\ncustom, scaling based on\\n448–450\\nmodifying target values on exist-\\ning HPA objects 447\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 610,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n578\\nmicroservices 3–6\\ndeploying 5\\ndivergence of environment \\nrequirements 5–6\\nscaling 4\\nsplitting applications into 3–4\\nminAvailable field 455\\nminikube delete command 553\\nminikube mount command 503\\nminikube node 408\\nMinikube tool\\naccessing dashboard when \\nusing 53\\nand GKE, switching \\nbetween 534–535\\ncombining with Kubernetes \\nclusters 504\\ninspecting running containers \\nin 553–554\\ninstalling 37\\nrunning local single-node \\nKubernetes clusters with\\n37–38\\nstarting clusters with 37–38\\nswitching from GKE to 534\\nswitching to GKE from 534\\nusing in development 503–504\\nbuilding images locally 504\\ncopying images to Minikube \\nVM directly 504\\nmounting local files into \\ncontainers 503\\nmounting local files into \\nMinikube VM 503\\nusing Docker daemon inside \\nMinikube VM to build \\nimages 503–504\\nusing rkt with 553–555\\nlisting container images\\n554–555\\nrunning pods 553\\nMinikube VM (virtual machine)\\ndirectly copying images to 504\\nmounting local files into 503\\nusing Docker daemon inside, to \\nbuild images 503–504\\nMinishift 530\\nminReadySeconds attribute 265, \\n274–275\\nMongoDB database, adding docu-\\nments to 173\\nmonitoring pod resource \\nusage 430–434\\nanalyzing historical resource \\nconsumption statistics\\n432–434\\ncollecting resource usages\\n430–432\\nretrieving resource usages\\n430–432\\nstoring historical resource \\nconsumption statistics\\n432–434\\nmonolithic applications, vs. \\nmicroservices 3–6\\nMostRequestedPriority 407\\nMount (mnt) Namespace 11\\nmountable Secrets 350–351\\nmounted config files, verifying \\nNginx using 208\\nmounting\\nConfigMap entries as files\\n210–211\\ndirectories hides existing \\nfiles 210\\nfortune-https Secret in \\npods 219–220\\nlocal files into containers 503\\nlocal files into Minikube \\nVM 503\\nmulti-tier applications, splitting \\ninto multiple pods 59\\nMustRunAs rules, using 392–393\\nMustRunAsNonRoot rules, using \\nin runAsUser fields 394\\nmy-nginx-config.conf entry 209\\nmy-postgres-db service 525\\nMYSQL_ROOT_PASSWORD \\nvariable 192\\nN\\n-n flag 79\\n-n option 537\\nnames\\nconfiguring resolution for \\nhosts 546–547\\ndeleting pods by 80\\nof containers 66\\nretrieving job instances by 238\\nnames.kind property 511\\nNamespace controllers 325\\n--namespace option 535, 537\\nNamespaceLifecycle 317\\nnamespaces\\naccessing resources in, using \\nClusterRoles 367–370\\ncreating\\nfrom YAML files 78\\nwith kubectl create \\nnamespace 78–79\\ndeleting pods in while \\nkeeping 81–82\\ndiscovering pods of 77–78\\nenabling network isolation \\nin 399\\ngranting full control of, with \\nadmin ClusterRole 372\\ngrouping resources with 76–80\\nhost network, binding to host \\nports without using\\n377–379\\nhost nodes, using in pods\\n376–380\\nincluding service accounts in \\nRoleBindings 361\\nIPC, using 379–380\\nisolating networks between\\n401–402\\nisolation provided by 79–80\\nlimiting resources available \\nin 425–429\\nlimiting objects that can be \\ncreated 427–428\\nResourceQuota resources\\n425–427\\nspecifying quotas for per-\\nsistent storage 427\\nspecifying quotas for specific \\npod states 429\\nspecifying quotas for specific \\nQoS classes 429\\nLinux, isolating processes \\nwith 11\\nmanaging objects in 79\\nnode network, using in \\npods 376–377\\nnode PID 379–380\\nof pods 242\\npods in, allowing some to con-\\nnect to server pods 400\\nsetting default limits for pods \\nper 421–425\\napplying default resource \\nlimits 424–425\\ncreating LimitRange \\nobjects 422–423\\nenforcing limits 423–424\\nLimitRange resources 421–422\\nsetting default requests for pods \\nper 421–425\\napplying default resource \\nrequests 424–425\\ncreating LimitRange \\nobjects 422–423\\nenforcing limits 423–424\\nLimitRange resources\\n421–422\\nusing kubectl with multiple\\n535–538\\nadding kube config entries\\n536–537\\nconfiguring location of \\nkubeconfig files 535\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 611,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n579\\nnamespaces (continued)\\ncontents of kubeconfig \\nfiles 535–536\\ndeleting clusters 538\\ndeleting contexts 538\\nlisting clusters 538\\nlisting contexts 538\\nlisting kube config \\nentries 536–537\\nmodifying kube config \\nentries 536–537\\nswitching between \\ncontexts 538\\nwhy needed 77\\nNAS (network-attached \\nstorage) 171\\nNAT (Network Address \\nTranslation) 58, 335\\nnet.bridge.bridge-nf-call-iptables \\nKernel option 545\\nNetwork (net) namespace 11\\nnetwork hops, preventing 141\\nnetworks\\nbetween pods 335–338\\narchitecture of 335–336\\nCNI (Container Network \\nInterface) 338\\nenabling communication \\nbetween pods on differ-\\nent nodes 337–338\\nenabling communication \\nbetween pods on same \\nnode 336–337\\noverview 336–338\\nconfiguring adapters for \\nVMs 540\\nenabling isolation in \\nnamespaces 399\\nisolating between Kubernetes \\nnamespaces 401–402\\nlocal ports, forwarding port in \\npod 67\\nnode namespaces, using in \\npods 376–377\\nnode, shutting down \\nadapters 304–305\\nof containers, setting up 550\\nproviding stable identities\\n285–287\\ngoverning service, \\noverview 285–287\\nscaling StatefulSets 287\\nsecuring 375–403\\nconfiguring container \\nsecurity contexts\\n380–389\\nisolating pod networks\\n399–402\\nrestricting use of security-\\nrelated features in \\npods 389–399\\nusing host node namespaces \\nin pods 376–380\\nsimulating node disconnection \\nfrom 304–306\\nchecking node status as seen \\nby Kubernetes \\nmaster 305\\npods with unknown \\nstatus 305–306\\nSee also flat networks; host net-\\nworks\\nNFS (Network File System) 175\\nnfs volume 162\\nnginx container 334, 514\\nNginx software\\nsignaling to reload config 212\\nusing cert files from Secrets\\n221\\nusing key files from Secrets 221\\nverifying use of mounted config \\nfiles 208\\nnode affinity\\nspecifying hard rules 463–465\\nnodeAffinity attribute \\nnames 464–465\\nnodeSelectorTerms 465\\nspecifying preferential \\nrules 466–467\\nusing to attract pods to \\nnodes 462–468\\nexamining default node \\nlabels 462–463\\nprioritizing nodes when \\nscheduling 465–468\\nvs. node selectors 462–463\\nNode controllers 324\\nnode failures 304–307\\ndeleting pods manually 306–307\\nsimulating node disconnection \\nfrom network 304–306\\nchecking node status as seen \\nby Kubernetes \\nmaster 305\\npods with unknown \\nstatus 305–306\\nshutting down node network \\nadapters 304–305\\nnodeAffinity attribute names\\n464–465\\nNode.js\\ncreating applications 28–29\\ndeploying applications 42–44\\nbehind the scenes 44\\nlisting pods 43–44\\nNodeLost 306\\nNODE_NAME variable 228\\nNodePort services\\nchanging firewall rules to let \\nexternal clients access\\n137–138\\ncreating 135–136\\nexamining 136–137\\nusing 135–138\\nnodes\\nadding custom taints to 460\\napplications running on, \\nexamining 51–52\\nassigning to newly created pods \\nwith Scheduler 332\\nchecking status as seen by \\nKubernetes master 305\\nconfiguring pod rescheduling \\nafter failures 462\\nCPU cores, as seen by \\ncontainers 416\\ncreating clusters with three 39\\ncreating pods that don’t fit on \\nany 408–409\\ndisplaying taints 458\\nenabling communication \\nbetween pods on \\ndifferent 337–338\\nenabling communication \\nbetween pods on same\\n336–337\\nexamining default labels\\n462–463\\nfinding acceptable 320\\ninspecting capacity of 407–408\\nlabeling 466\\nlisting 40, 548–549\\nmemory, as seen by \\ncontainers 415–416\\nnetwork namespaces, using in \\npods 376–377\\nPID namespaces 379–380\\npreferences 467\\nprioritizing when scheduling \\npods 465–468\\ndeploying pods in two-node \\nclusters 468\\nspecifying preferential node \\naffinity rules 466–467\\nrelinquishing 453\\nReplicationControllers respond-\\ning to failures 97–98\\nrequesting from Cloud \\ninfrastructure 452–453\\nrunning one pod on each, with \\nDaemonSets 108–112\\nrunning pods on\\nadding required label to \\nnodes 111\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 612,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n580\\nnodes (continued)\\nremoving required label from \\nnodes 111–112\\nwith DaemonSets 109–112\\nrunning pods on every, with \\nDaemonSets 109\\nScheduler using pod requests \\nwhen selecting 407\\nscheduling pods to specific\\n74–75\\nselecting best for pods 320–321\\nselectors vs. affinity 462–463\\nsimulating disconnection from \\nnetwork 304–306\\nchecking node status as seen by \\nKubernetes master 305\\npods with unknown \\nstatus 305–306\\nshutting down node network \\nadapters 304–305\\nusing inter-pod affinity to \\ndeploy pods on 468–471\\ndeploying pods with pod \\naffinity 470\\nspecifying pod affinity in pod \\ndefinitions 469\\nusing pod affinity rules with \\nScheduler 470–471\\nusing node affinity to attract \\npods to 462–468\\ncomparing node affinity to \\nnode selectors 462–463\\nspecifying hard node affinity \\nrules 463–465\\nusing Scheduler to determine \\nwhether pods can fit on 406\\nusing taints to repel pods \\nfrom 457–462\\ntaints, overview 458–460\\nusing taints 461–462\\nusing tolerations to repel pods \\nfrom 457–462\\nadding tolerations to \\npods 460–461\\ntolerations, overview 458–460\\nusing tolerations 461–462\\nworker\\ncategorizing with labels 74\\ncomponents running on 310\\nSee also cluster nodes; node \\naffinity; node failures; \\nworker node filesystems; \\nworker nodes\\nnodeSelector field 75, 462–464\\nnodeSelectorTerms 465\\nNoExecute 460\\nnon-resource URLs, allowing \\naccess to 365–367\\nNoOps 7\\nNoSchedule 460\\nNotIn operator 107\\nNotReady status 97, 305\\nNotTerminating scope 429\\nNTP (Network Time Protocol) \\ndaemon 385\\nNTP daemon. See NTP\\nO\\n-o custom-columns option 312\\nobject fields 64\\nObject metric types, scaling based \\non 449–450\\nOCI (Open Container \\nInitiative) 15, 554\\nOmega 16\\nOnlyLocal annotation 142\\nOOM (OutOfMemory) score 420\\nOpenAPI interface, building \\nlibraries with 248\\nOpenServiceBroker API 522–523\\nlisting available services in \\nclusters 523\\noverview 522\\nregistering brokers in Service \\nCatalog 522–523\\nOpenShift. See Red Hat OpenShift \\nContainer platform\\noperations (ops) team 2\\noptimistic concurrency \\ncontrol 313\\nOS (operating systems), \\ninstalling 541–544\\ninitiating install 542\\nrunning install 543–544\\nselecting start-up disks 541\\nsetting installation options\\n542–543\\nOutOfMemoryErrors 85\\nout-of-range user IDs, container \\nimages with 393\\novercommitting limits 413\\noverriding arguments and \\ncommands 195–196\\n--overwrite option 70, 99\\nP\\nparallelism property 114\\npath traversa attack 353\\npaths, mapping services to 146\\npatterns, of ambassador \\ncontainers 244\\npausing rollouts 273–274\\nPD (Persistent Disk) 185\\nPDB (PodDisruptionBudget) 455\\npeers\\ndiscovering in StatefulSets\\n299–304\\nclustered data store 303–304\\nimplementing peer discovery \\nthrough DNS 301–302\\nSRV records, overview 300\\nupdating StatefulSets\\n302–303\\nimplementing discovery \\nthrough DNS 301–302\\nPending status 44\\npermissions 373\\npersistent disks, GCE 171–172\\npersistent storage 171–175\\nspecifying quotas for 427\\nusing GCE Persistent Disk in \\npod volumes 171–174\\ncreating GCE persistent \\ndisks 171–172\\ncreating pods using gcePer-\\nsistentDisk volumes 172\\nre-creating pods 173–174\\nverifying pod can read data \\npersisted by previous \\npod 173–174\\nusing volumes with underlying \\npersistent storage 174–175\\nusing AWS elastic block store \\nvolume 174\\nusing NFS volume 175\\nusing storage technologies\\n175\\nwriting data to, by adding docu-\\nments to MongoDB \\ndatabase 173\\nPersistentVolume \\ncontrollers 325–326\\nPersistentVolumeClaims. See PVC\\nPersistentVolumes 176–177, 288\\nbenefits of using 182\\nclaiming by creating Persistent-\\nVolumeClaims 179–181\\ncreating PersistentVolume-\\nClaims 179–180\\nlisting PersistentVolume-\\nClaims 180\\ncreating 177–178\\ndynamic provisioning of\\n184–189\\ndefining available storage \\ntypes through Storage-\\nClass resources 185\\nrequesting storage class in \\nPersistentVolumeClaims\\n185–187\\nwithout specifying storage \\nclass 187–189\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 613,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n581\\nPersistentVolumes (continued)\\ndynamically provisioned 186–187\\nlisting 180–181\\npre-provisioned, Persistent-\\nVolumeClaims bound \\nto 189\\nreclaiming automatically\\n183–184\\nreclaiming manually 183\\nrecycling 183–184\\nPet pods 295–299\\ncommunicating with pods \\nthrough API servers\\n295–297\\nconnecting to cluster-internal \\nservices through API \\nservers 299\\ndeleting 297–298\\nexposing through services\\n298–299\\nscaling StatefulSets 298\\nPetSets 284\\nphotonPersistentDisk volume 163\\nPID namespaces 379–380\\npinging service IP 131\\nplatforms 527–533\\nDeis Workflow 530–533\\nHelm 530–533\\nRed Hat OpenShift Container \\nplatform 527–530\\napplication templates\\n528–529\\nautomatically deploying \\nnewly built images with \\nDeploymentConfigs\\n529–530\\nbuilding images from source \\nusing BuildConfigs 529\\nexposing services externally \\nusing Routes 530\\ngroups 528\\nprojects 528\\nresources available in 527–528\\nusers 528\\nusing 530\\npod affinity\\nco-locating pods with 468–476\\ndeploying pods in same avail-\\nability zone 471–472\\ndeploying pods in same geo-\\ngraphical regions 471–\\n472\\ndeploying pods in same \\nrack 471–472\\nexpressing podAffinity pref-\\nerences instead of hard \\nrequirements 472–473\\ndeploying pods with 470\\nspecifying in pod \\ndefinitions 469\\nusing inter-pod affinity to \\ndeploy pods on same \\nnode 468–471\\nusing rules with \\nScheduler 470–471\\nSee also inter-pod affinity\\npodAffinity, preferences vs. \\nhard requirements\\n472–473\\npodAntiAffinity property 474\\nPodDisruptionBudget \\nresource 455\\nPodDisruptionBudget. See PDB\\nPOD_IP variable 228\\nPOD_NAME variable 228\\nPOD_NAMESPACE variable 228\\npods 20, 47–55, 83\\naccessing through Ingress 145\\naccessing through services 264\\nadding Init containers to\\n484–485\\nadding readiness probes \\nto 151–153\\nadding readiness probe \\nto pod template\\n151–152\\nhitting service with single \\nready pod 153\\nmodifying pod readiness \\nstatus 152\\nobserving pod readiness \\nstatus 152\\nadding tolerations to 460–461\\nadvanced scheduling of 321\\nannotating 75–76\\nadding annotations 76\\nlooking up object \\nannotations 75–76\\nmodifying annotations 76\\nassigning Burstable QoS class \\nto 418\\nassigning nodes to, with \\nScheduler 332\\nassigning service accounts \\nto 351–353\\nassigning to BestEffort class 417\\nassigning to Guaranteed \\nclass 417–418\\nattracting to nodes with node \\naffinity 462–468\\ncomparing node affinity to \\nnode selectors 462–463\\nexamining default node \\nlabels 462–463\\nspecifying hard node affinity \\nrules 463–465\\ncalculating required number \\nof 439\\nco-locating 468–476\\nexpressing podAffinity pref-\\nerences instead of hard \\nrequirements 472–473\\nusing inter-pod affinity to \\ndeploy pods on same \\nnode 468–471\\nwith pod anti-affinity 468–476\\ncommunicating with API \\nservers 238–243\\nauthenticating with API \\nservers 241–242\\nfinding API server \\naddresses 239–240\\nrunning pods to 239\\nverifying server identity\\n240–241\\ncommunicating with \\nKubernetes 243\\nconfiguring rescheduling after \\nnode failures 462\\nconnecting through port \\nforwarders 67\\ncontainers\\nresources requests for\\n411–412\\nrunning with kubelet 332\\ncontainers sharing IP 57\\ncontainers sharing port \\nspace 57\\ncreating 164–165\\nas different users 398–399\\nfrom JSON descriptors 61–67\\nfrom YAML descriptors\\n61–67\\nexamining YAML descrip-\\ntors of existing pods\\n61–63\\nviewing application \\nlogs 65–66\\nmanually 281\\nnew with Replication-\\nControllers 96\\nspecific service accounts for \\neach 373\\nusing custom service \\naccounts 351–352\\nwith gcePersistentDisk \\nvolumes 172\\nwith kubectl create 65\\nwith ReplicaSet \\ncontroller 332\\nwith resource limits 412–413\\nwith resource requests\\n405–406\\nYAML descriptors for 63–65\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 614,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n582\\npods (continued)\\ndead, rescheduling 482–483\\ndecoupling from underlying \\nstorage technologies\\n176–184\\nbenefits of using claims 182\\nusing PersistentVolume-\\nClaims 176–177, \\n179–181\\nusing PersistentVolumes\\n176–184\\ndefinitions 62–63\\ndeleting 306\\nby deleting whole \\nnamespace 80–81\\nby name 80\\ndeleting resources in \\nnamespace 82\\nforcibly 307\\nin namespace while keeping \\nnamespace 81–82\\nmanually 306–307\\nold 252–253\\nusing label selectors 80\\ndeploying\\nin same availability \\nzone 471–472\\nin same geographical \\nregions 471–472\\nin same rack 471–472\\nin two-node clusters 468\\nmanaged 84–118\\nwith container images with \\nout-of-range user \\nIDs 393\\nwith runAsUser outside of \\npolicy ranges 393\\ndiscovering all 156\\ndiscovering namespaces 242\\ndiscovering through DNS\\n155–156\\ndisplaying CPU usage for\\n431–432\\ndisplaying memory usage \\nfor 431–432\\ndisplaying pod IP when \\nlisting 51\\ndisplaying pod node when \\nlisting 51\\ndisplaying tolerations 459\\nflat inter-pod networks 58\\nfortune, running with custom \\nintervals 195–196\\nfreeing resources to \\nschedule 410\\nhorizontal autoscaling of\\n438–451\\nautoscaling process 438–441\\nmetrics appropriate for \\nautoscaling 450\\nscaling based on CPU \\nutilization 441–447\\nscaling based on memory \\nconsumption 448\\nscaling based on other and \\ncustom metrics 448–450\\nscaling down to zero \\nreplicas 450–451\\nhorizontally scaling 102–103\\ndeclarative approach to \\nscaling 103\\nscaling down with kubectl \\nscale command 103\\nscaling ReplicationController \\nby editing definitions\\n102–103\\nscaling up Replication-\\nController 102\\nin action 165\\nin namespaces, allowing some to \\nconnect to server pods 400\\ninspecting details with kubectl \\ndescribe 52\\nisolating networks 399–402\\nenabling network isolation in \\nnamespaces 399\\nisolating networks between \\nKubernetes \\nnamespaces 401–402\\nlifecycles of 479–491\\nadding lifecycle hooks\\n485–489\\nkilling applications 479–482\\nrelocating applications\\n479–482\\nlisting 43–44, 548\\nlisting subsets through label \\nselectors 71–72\\nlisting using label selectors\\n71–72\\nlogs, retrieving with kubectl \\nlogs 66\\nmanaged by ReplicationControl-\\nlers, adding labels to 99\\nmanaged, changing labels \\nof 99–100\\nmarked for deletion 307\\nmodifying labels of existing\\n70–71\\nmodifying resource requests \\nwhile running 451–452\\nmonitoring resource \\nusage 430–434\\nanalyzing historical resource \\nconsumption statistics\\n432–434\\ncollecting resource \\nusages 430–432\\nretrieving resource \\nusages 430–432\\nstoring historical resource \\nconsumption \\nstatistics 432–434\\nmoving in and out of scope of \\nReplicationControllers\\n98–101\\nadding labels to pods man-\\naged by Replication-\\nControllers 99\\nchanging label selector\\n100–101\\nmulti-container, logs of 66\\nnetworking between 335–338\\nCNI (Container Network \\nInterface) 338\\nenabling communication \\nbetween pods on differ-\\nent nodes 337–338\\nenabling communication \\nbetween pods on same \\nnode 336–337\\nnetwork architecture\\n335–336\\nnetworking overview 336–338\\nnot being scheduled 409–410\\nof clients, using newly created \\nSecrets in 525–526\\norganizing containers \\nacross 58–60\\nsplitting into multiple pods \\nfor scaling 59\\nsplitting multi-tier applications \\ninto multiple pods 59\\norganizing with labels 67–71\\noverview 43, 48–58, 60\\npartial isolation between con-\\ntainers of same pod 57\\npartially dead, rescheduling\\n482–483\\nperforming single completable \\ntask 112–116\\ndefining jobs 113–114\\nJob resource 112\\nrunning multiple pod \\ninstances in Job 114–116\\nseeing Job run pods 114\\npreventing broken client \\nconnection 492–497\\nprioritizing nodes when \\nscheduling 465–468\\nlabeling nodes 466\\nnode preferences 467\\nspecifying preferential node \\naffinity rules 466–467\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 615,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n583\\npods (continued)\\nproviding stable identity \\nfor 282–284\\nQoS classes 417–421\\ndefining 417–419\\nkilling processes when mem-\\nory is low 420–421\\nreading Secret entries in 218\\nreattaching PersistentVolume-\\nClaims to new instances \\nof 289\\nre-creating 173–174\\nreferencing non-existing config \\nmaps in 203\\nremoving from controllers 100\\nrepelling from nodes with \\ntaints 457–462\\nadding custom taints to \\nnode 460\\ntaints, overview 458–460\\nusing taints 461–462\\nrepelling from nodes with \\ntolerations 457–462\\nadding tolerations to \\npods 460–461\\ntolerations, overview\\n458–460\\nusing tolerations 461–462\\nreplacing old 252, 259–260\\nReplicationControllers respond-\\ning to deleted 95\\nrequesting resources for \\ncontainers 405–412\\ndefining custom \\nresources 411–412\\neffect of CPU requests on \\nCPU time sharing 411\\neffect of resource requests on \\nscheduling 406–410\\nrequests hitting three when hit-\\nting services 50\\nrestricting use of security-related \\nfeatures in 389–399\\nassigning different PodSecu-\\nrityPolicies to different \\ngroups 396–399\\nassigning different Pod-\\nSecurityPolicies to differ-\\nent users 396–399\\nconfiguring allowed \\ncapabilities 394–395\\nconfiguring default \\ncapabilities 394–395\\nconfiguring disallowed \\ncapabilities 394–395\\nconstraining types of vol-\\numes pods can use 395\\nfsGroup policies 392–394\\nPodSecurityPolicy \\nresources 389–392\\nrunAsUser policies 392–394\\nsupplementalGroups \\npolicies 392–394\\nretrieving whole definition \\nof 65\\nrunning 333–334, 357, 553\\nin privileged mode 382–384\\non certain nodes\\nadding required label to \\nnodes 111\\nDaemonSets 109–112\\nremoving required label \\nfrom nodes 111–112\\non every node, \\nDaemonSets 109\\none on each node, with \\nDaemonSets 108–112\\nshell in containers 130–131\\nwithout specifying security \\ncontexts 381\\nwithout writing YAML \\nmanifest 155\\nrunning controllers as 515–516\\nScheduler using requests when \\nselecting nodes 407\\nscheduling to specific \\nnodes 74–75\\nseeing newly created in list \\nof 65\\nselecting best nodes for\\n320–321\\nsending requests to 66–67\\nsequence of events at \\ndeletion 493–495\\nsetting default limits for, per \\nnamespace 421–425\\nsetting default requests for, per \\nnamespace 421–425\\nshutdowns of 489–491\\nimplementing proper shut-\\ndown handler in \\napplications 490–491\\nreplacing critical shut-down \\nprocedures with dedi-\\ncated shut-down proce-\\ndure pods 491\\nspecifying termination grace \\nperiods 490\\nsignaling when ready to accept \\nconnections 149–153\\nspecifying for quotas for specific \\nstates 429\\nspecifying image pull Secrets \\non 223\\nspecifying labels when \\ncreating 69–70\\nspecifying pod affinity in \\ndefinitions 469\\nspinning up new 252–253\\nstarting 483–484\\nstarting in specific order 483–485\\nbest practices for handling \\ninter-pod dependencies\\n485\\nInit containers 484\\nstatic, running without API \\nservers 326–327\\nstopping, deleting pods 80–82\\ntemplates\\nchanging 101\\neffect of changing 93\\nusing with volume claim \\ntemplates 288\\nupdating applications running \\nin 251–253\\nusing anti-affinity with pods of \\nsame Deployment 475\\nusing dedicated service for each \\ninstance 283–284\\nusing Docker-registry Secrets in \\ndefinitions 223\\nusing emptyDir volume in\\n163–164\\nusing GCE Persistent Disk in \\nvolumes 171–174\\ncreating GCE persistent \\ndisks 171–172\\ncreating pods using gce-\\nPersistentDisk volumes\\n172\\nwriting data to persistent stor-\\nage by adding docu-\\nments to MongoDB \\ndatabase 173\\nusing headless services to \\ndiscover 154–156\\nusing host node namespaces \\nin 376–380\\nbinding to host ports without \\nusing host network \\nnamespaces 377–379\\nusing node IPC \\nnamespaces 379–380\\nusing node PID \\nnamespaces 379–380\\nusing labels to constrain \\nscheduling 73–75\\ncategorizing worker nodes \\nwith labels 74\\nscheduling pods to specific \\nnodes 74–75\\nusing namespaces to group \\nresources 76, 80\\ncreating namespaces 78–79\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 616,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n584\\npods (continued)\\ndiscovering other namespaces \\nand their pods 77–78\\nisolation provided by 79–80\\nmanaging objects in other \\nnamespaces 79\\nnamespaces, why needed 77\\nusing node network name-\\nspaces in 376–377\\nusing one ReplicaSet per \\ninstance 281–282\\nusing PersistentVolumeClaims \\nin 181\\nusing Scheduler to determine \\nwhether pod can fit on \\nnode 406\\nusing Secrets in 218, 222\\nexposing Secret entries \\nthrough environment \\nvariables 221–222\\nmodifying fortune-config \\nconfig map to enable \\nHTTPS 218–219\\nmounting fortune-https \\nSecret in pod 219–220\\nSecret volumes stored in \\nmemory 221\\nverifying Nginx is using cert \\nfiles from Secrets 221\\nverifying Nginx is using key \\nfiles from Secrets 221\\nusing selectors to constrain \\nscheduling\\ncategorizing worker nodes \\nwith labels 74\\nscheduling pods to specific \\nnodes 74–75\\nverifying readability of data \\npersisted by previous \\npods 173–174\\nvertical autoscaling of 451–452\\nautomatically configuring \\nresource requests 451\\nmodifying resource requests \\nwhile pod is running\\n451–452\\nweb server, serving files from \\ncloned Git repository 167\\nwhen to use multiple containers \\nin 59–60\\nwhy needed 56–57\\nwith multiple containers, deter-\\nmining QoS classes of 419\\nwith pod affinity, deploying 470\\nwith unknown status 305–306\\nSee also curl pods; inter-pod \\naffinity; Pet pods; pod \\naffinity; server pods\\nPodSecurityPolicy resources\\n389–392\\nassigning to different \\ngroups 396–399\\nassigning to different \\nusers 396–399\\ncreating additional users for \\nkubectl 398\\ncreating pods as different \\nuser 398–399\\nwith RBAC 397–398\\ncreating, allowing privileged \\ncontainers to be \\ndeployed 396–397\\nexamining 391–392\\noverview 390–391\\npod.spec.containers.args field 204\\npolicy ranges, deploying pods with \\nrunAsUser outside of 393\\nport forwarding 66\\nport spaces, containers sharing 57\\nportability, limitations of Docker \\nimages 15\\nports\\nexposing multiple in same \\nservice 126–127\\nlocal network, forwarding port \\nin pod 67\\nnamed, using 127–128\\nSee also host ports\\nPost-start hooks 485\\nPreferNoSchedule 460\\npreserving data, with volumes\\n480–482\\nPre-stop hooks 485\\nprivate image repositories, using \\non Docker Hub 222\\nprivilege escalation 372\\nprivileged containers\\ndeployment of 396–397\\noverview 403\\nprivileged mode, running pods \\nin 382–384\\nprivileged policy 397\\nprivileged property 383\\nprobes. See liveness probes; readi-\\nness probes\\nprocedures. See shut-down proce-\\ndures\\nProcess ID (pid) Namespace 11\\nprocesses\\nisolating with Linux \\nNamespaces 11\\nkilling when memory is \\nlow 420–421\\nhandling containers with \\nsame QoS class 420–421\\nsequence of QoS classes 420\\nmultiple with one container vs. \\nmultiple containers 56–57\\npreventing from writing to con-\\ntainer filesystems 386–387\\nprojects, in Red Hat OpenShift \\nContainer platform 528\\nprovisioning. See dynamic provi-\\nsioning\\nProvisioningFailed event 186\\nproxies 327–328\\npublicHtml volume 162\\npushing\\nimages to Docker Hub 36\\nimages to image registry 35–36\\nrunning images on different \\nmachines 36\\ntagging images under addi-\\ntional tags 35\\nPVC (PersistentVolumeClaims)\\n176–177, 288, 423, 532\\nbound to pre-provisioned \\nPersistentVolumes 189\\ncreating 179–180, 288\\ncreating without specifying stor-\\nage class 188–189\\ndeleting 288\\nexamining 295\\nlisting 180\\nreattaching to new instances of \\npod 289\\nrequesting storage class in\\n185–187\\nby creating PersistentVolume-\\nClaim definition\\n185–186\\nexamining created Persistent-\\nVolumeClaims 186–187\\nexamining dynamically provi-\\nsioned Persistent-\\nVolumes 186–187\\nusing storage classes 187\\nusing in pods 181\\nQ\\nQCOW2 image file format 555\\nQEMU virtual machine tool 555\\nQoS (Quality of Service) 417\\nQoS classes 417–421\\ndefining for pods 417–419\\nassigning Burstable QoS class \\nto pods 418\\nassigning pods to BestEffort \\nclass 417\\nassigning pods to Guaranteed \\nclass 417–418\\nhandling containers with \\nsame 420–421\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 617,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n585\\nQoS classes (continued)\\nkilling processes when memory \\nis low 420–421\\nhandling containers with \\nsame QoS class 420–421\\nsequence of QoS classes 420\\nof containers, determining 418\\nof pods with multiple contain-\\ners, determining 419\\nsequence of 420\\nspecifying quotas for 429\\nQPS (Queries-Per-Second) 439, \\n449\\nquobyte volume 163\\nquorum 315\\nquotas, specifying 427–429\\nR\\nracks, deploying pods in 471–472\\nRBAC (role-based access \\ncontrol) 242\\nauthorization plugins 353–354\\nplugins 354\\nresources 355–357\\ncreating namespaces 357\\nenabling in clusters 356\\nlisting services from pods 357\\nrunning pods 357\\nsecuring clusters with 353–373\\nbinding roles to service \\naccounts 359–360\\ndefault ClusterRoleBindings\\n371–373\\ndefault ClusterRoles\\n371–373\\ngranting authorization \\npermissions 373\\nincluding service accounts \\nfrom other namespaces \\nin RoleBinding 361\\nRBAC authorization \\nplugins 353–354\\nRBAC resources 355–357\\nusing ClusterRoleBindings\\n362–371\\nusing ClusterRoles 362–371\\nusing RoleBindings 358–359\\nusing Roles 358–359\\nusing to assign different Pod-\\nSecurityPolicies to differ-\\nent users 397–398\\nrbd volume 163\\nrc (replicationcontroller) 46, 95\\nreadiness probes\\nadding to pods 151–153\\nadding readiness probe to \\npod template 151–152\\nhitting service with single \\nready pod 153\\nmodifying pod readiness \\nstatus 152\\nobserving pod readiness \\nstatus 152\\nbenefits of using 151\\ndefining 153\\ndefining to prevent rollouts 275\\nincluding pod shutdown logic \\nin 153\\noperation of 150\\noverview 149–151\\npreventing rollouts with 277–278\\ntypes of 150\\nreading, Secret entries in \\npods 218\\nread-only access, to resources with \\nview ClusterRole 372\\nReadOnlyMany access mode 180\\nreadOnlyRootFilesystem \\nproperty 387, 392\\nReadWriteMany access mode 180\\nReadWriteOnce access mode 180\\nREADY column 44, 152\\nreconciliation loops 92\\nrecords, DNS 155–156\\nre-creating pods 173–174\\nrecycling PersistentVolumes\\n183–184\\nautomatically 183–184\\nmanually 183\\nRed Hat OpenShift Container \\nplatform 527–530\\napplication templates 528–529\\nautomatically deploying newly \\nbuilt images with \\nDeploymentConfigs\\n529–530\\nbuilding images from source \\nusing BuildConfigs 529\\nexposing services externally \\nusing Routes 530\\ngroups 528\\nprojects 528\\nresources available in 527–528\\nusers 528\\nusing 530\\nreferencing non-existing config \\nmaps in pods 203\\nregistering\\ncustom API servers 519\\nservice brokers in Service \\nCatalog 522–523\\nregistries, creating Secrets for \\nauthenticating with 223\\nrel=canary label 80\\nrelinquishing, nodes 453\\nreloading config, signaling Nginx \\nto 212\\nrelocating applications 479–482\\nexpecting data written to disk to \\ndisappear 480\\nexpecting hostnames to \\nchange 480\\nexpecting local IP to \\nchange 480\\nusing volumes to preserve data \\nacross container \\nrestarts 480–482\\nremoving\\ncontainers 34–35\\nlabels from nodes 111–112\\npods from controllers 100\\nrepelling\\npods from nodes with \\ntaints 457–462\\nadding custom taints to \\nnode 460\\ntaints, overview 458–460\\nusing taints 461–462\\npods from nodes with \\ntolerations 457–462\\nadding tolerations to \\npods 460–461\\ntolerations, overview 458–460\\nusing tolerations 461–462\\nreplacing old pods\\nby scaling two Replication-\\nControllers 259–260\\noverview 252\\nreplica count 49, 92\\nreplicas\\nrunning multiple with separate \\nstorage for each 281–282\\ncreating pods manually 281\\nusing multiple directories in \\nsame volume 282\\nusing one ReplicaSet per pod \\ninstance 281–282\\nscaling down to zero 450–451\\nupdating count on scaled \\nresources 440\\nReplicaSets 104–108, 324, 559\\ncontrollers\\ncreating pods with 332\\ncreating with Deployment \\ncontroller 331\\ncreating 106–107, 263–264\\ndefining 105–106\\nexamining 106–107\\nusing label selectors 107\\nusing one per pod \\ninstance 281–282\\nvs. ReplicationControllers 105\\nvs. StatefulSets 284–285\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 618,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n586\\nreplicating stateful pods 281–284\\nproviding stable identity for \\npods 282–284\\nrunning multiple replicas with \\nseparate storage for \\neach 281–282\\nreplication controllers 47–48\\nreplication managers 323–324\\nReplicationControllers 90–104\\nbenefits of using 93\\nchanging pod templates 101\\ncreating 93–94\\ncreating new pods with 96\\ndeleting 103–104\\ngetting information about\\n95–96\\nhorizontally scaling pods\\n102–103\\nin action 94–98\\nmoving pods in and out of \\nscope of 98–101\\nadding labels to pods man-\\naged by 99\\nchanging label selectors\\n100–101\\nchanging labels of managed \\npod 99–100\\nremoving pods from \\ncontrollers 100\\noperation of 91–93\\nparts of 92–93\\nperforming automatic \\nrolling updates with\\n254–261\\nobsolescence of kubectl \\nrolling-update\\n260–261\\nperforming rolling updates \\nwith kubectl 256–260\\nrunning initial version of \\napplications 254–255\\nreconciliation loops 92\\nreplacing old pods with new \\npods by scaling 259–260\\nresponding to deleted pods 95\\nresponding to node \\nfailures 97–98\\nscaling by editing definitions\\n102–103\\nscaling up 102\\nvs. ReplicaSets 105\\nvs. StatefulSets 284–285\\nrepo files, adding to yum package \\nmanager 544\\nrequests\\nfor pods per namespace, setting \\ndefaults 421–425\\nfrom clients, handling 492–497\\npreventing broken client con-\\nnections when pod shuts \\ndown 493–497\\npreventing broken client con-\\nnections when pod starts \\nup 492–493\\nmodifying resources in, with \\nadmission control \\nplugins 317\\nsending to pods 66–67\\nconnecting to pods through \\nport forwarders 67\\nforwarding local network \\nport to port in pod 67\\nSee also CPU requests\\nrequiredDropCapabilities 394–395\\nrequiredDuringScheduling 464, \\n475\\nrescaling automatically 444–445\\nrescheduling\\ndead pods 482–483\\npartially dead pods 482–483\\nresource limits, creating pods \\nwith 412–413\\nresource metric types, scaling \\nbased on 449\\nresource requests\\nautomatically configuring 451\\ncreating pods with 405–406\\ndefault, applying 424–425\\ndefining custom resources\\n411–412\\neffect on scheduling 406–410\\ncreating pods that don’t fit on \\nany node 408–409\\nfreeing resources to schedule \\npods 410\\ninspecting node capacity\\n407–408\\npods not being scheduled\\n409–410\\nScheduler determining \\nwhether pod can fit on \\nnode 406\\nScheduler using pod requests \\nwhen selecting best \\nnodes 407\\nfor pod containers 411–412\\nmodifying while pod is \\nrunning 451–452\\nresourceFieldRef 233\\nresourceNames field 358\\nResourceQuota resources\\n425–427\\ncreating for CPUs 425–426\\ncreating for memory 425–426\\ncreating LimitRange along with \\nResourceQuota 427\\ninspecting Quota and Quota \\nusage 426\\nresources\\naccessing in specific \\nnamespaces 367–370\\nallowing modification of \\nresources with edit \\nClusterRole 372\\nanalyzing statistics for historical \\nconsumption of 432–434\\nGrafana 432\\nInfluxDB 432\\nrunning Grafana in \\nclusters 433\\nrunning InfluxDB in \\nclusters 433\\nusing information shown in \\ncharts 434\\nanalyzing usage with \\nGrafana 433–434\\nauto-deploying manifests\\n504–505\\navailable in namespaces, \\nlimiting 425–429\\nlimiting objects that can be \\ncreated 427–428\\nResourceQuota \\nresources 425–427\\nspecifying quotas for per-\\nsistent storage 427\\nspecifying quotas for specific \\npod states 429\\nspecifying quotas for specific \\nQoS classes 429\\navailable to containers, \\nlimiting 412–416\\nexceeding limits 414–415\\nlimits as seen by applications \\nin containers 415–416\\ncluster-level, allowing access \\nto 362–365\\ncollecting usage 430–432\\ndisplaying CPU usage for \\ncluster nodes 431\\ndisplaying CPU usage for \\nindividual pods 431–432\\ndisplaying memory usage for \\ncluster nodes 431\\ndisplaying memory usage for \\nindividual pods 431–432\\nenabling Heapster 431\\ncustom\\nautomating with custom \\ncontrollers 513–517\\ncreating instances of\\n511–512\\nretrieving instances of 512\\ndeleting in namespace 82\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 619,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n587\\nresources (continued)\\ndeploying through Helm\\n531–533\\ndescribing through \\nannotations 498\\nfederated\\nfunctions of 559\\nversions of 558\\nfreeing to schedule pods 410\\nin Red Hat OpenShift Con-\\ntainer platform 527–528\\nlimiting consumption of 11–12\\nmodifying in requests with \\nadmission control \\nplugins 317\\nnotifying clients of \\nchanges 318–319\\nof pods, monitoring usage \\nof 430–434\\nread-only access to 372\\nrequesting for pod \\ncontainers 405–412\\ncreating pods with resource \\nrequests 405–406\\ndefining custom \\nresources 411–412\\neffect of CPU requests on \\nCPU time sharing 411\\neffect of resource requests on \\nscheduling 406–410\\nretrieving usage 430–432\\ndisplaying CPU usage\\n431–432\\ndisplaying memory \\nusage 431–432\\nenabling Heapster 431\\nsetting hard limits for 412–413\\ncreating pod with resource \\nlimits 412–413\\novercommitting limits 413\\nstoring in etcd 313–314\\nstoring persistently 318\\nstoring statistics for historical \\nconsumption of 432–434\\nGrafana 432\\nInfluxDB 432\\nrunning Grafana in \\nclusters 433\\nrunning InfluxDB in \\nclusters 433\\nusing information shown in \\ncharts 434\\nusing namespaces to group\\n76–80\\ncreating namespaces 78–79\\ndiscovering other name-\\nspaces and their pods\\n77–78\\nisolation provided by 79–80\\nmanaging objects in other \\nnamespaces 79\\nnamespaces, why needed 77\\nvalidating 318\\nversioning manifests 504–505\\nSee also computational resources\\nREST API 234–238\\naccessing API server through \\nkubectl proxy 234–235\\nexploring batch API group \\nREST endpoints 236–237\\nexploring Kubernetes API \\nthrough kubectl proxy\\n235–236\\nlisting job instances in \\nclusters 237–238\\nretrieving job instances by \\nname 238\\n--restart=Never option 446\\nrestartPolicy property 113\\nrestarts, of containers 480–482\\nRESTful (REpresentational State \\nTransfer) APIs 4\\nresuming rollouts 274\\nretrieving\\ninstances of custom \\nresources 512\\njob instances by name 238\\nresource usages 430–432\\ndisplaying CPU usage for \\ncluster nodes 431\\ndisplaying CPU usage for \\nindividual pods 431–432\\ndisplaying memory usage for \\ncluster nodes 431\\ndisplaying memory usage for \\nindividual pods 431–432\\nenabling Heapster 431\\nretry loops, implementing in live-\\nness probes 90\\nrevisionHistoryLimit property 270\\nrevisions, rolling back Deployment \\nto 270\\nRHEL (Red Hat Enterprise \\nLinux) 12\\nrkt container system\\nconfiguring Kubernetes to \\nuse 552\\nreplacing Docker with 552–555\\nusing with Minikube 553–555\\ninspecting running contain-\\ners in Minikube VM\\n553–554\\nlisting container images\\n554–555\\nrunning pods 553\\n--rm option 446\\nRole resources\\nbinding to service \\naccounts 359–360\\ncreating 357–359\\nusing 358–359\\nrole-based access control. See \\nRBAC\\nRoleBindings\\ncombining with ClusterRole-\\nBindings 370–371\\ncombining with ClusterRoles\\n370–371\\ncombining with Roles 370–371\\nincluding service accounts from \\nother namespaces in 361\\nusing 358–359\\nrolling back deployments\\n268–270\\ncreating application \\nversions 268\\ndeploying versions 269\\ndisplaying deployment rollout \\nhistory 270\\nto specific Deployment \\nrevision 270\\nundoing rollouts 269\\nrolling updates 253\\nperforming automatically \\nwith Replication-\\nController 254–261\\nobsolescence of kubectl \\nrolling-update 260–261\\nrunning initial version of \\napplications 254–255\\nperforming with kubectl\\n256–260\\nreplacing old pods with new \\npods by scaling two \\nReplicationControllers\\n259–260\\nsteps performed before roll-\\ning update commences\\n258–259\\nslowing 265\\ntriggering 265–267\\nrolling-update command, in \\nkubectl 260–261\\nrollouts\\nblocking 274–278\\nconfiguring deadlines for \\nrollouts 278\\ndefining readiness probes to \\nprevent rollouts 275\\nminReadySeconds 274–275\\npreventing rollouts with read-\\niness probes 277–278\\nupdating deployments with \\nkubectl apply 276–277\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 620,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n588\\nrollouts (continued)\\nconfiguring deadlines for 278\\ncontrolling rate of 271–273\\nmaxSurge property 271–272\\nmaxUnavailable property\\n271–273\\nof deployments\\ndisplaying history of 270\\ndisplaying status of 263\\npausing 273–274\\npreventing\\nby using pause feature 274\\nwith readiness probes\\n277–278\\nresuming 274\\nundoing 269\\nroot, preventing containers from \\nrunning as 382\\nRoutes, exposing services exter-\\nnally with 530\\nrpi-cluster context 538\\nrpi-foo context 538\\nrun command 27, 42–43, 47\\nrunAsUser fields, using mustRun-\\nAsNonRoot rules in 394\\nrunAsUser policies 392–394\\ndeploying pods 393\\nusing MustRunAs rules 392–393\\nusing MustRunAsNonRoot rules \\nin runAsUser fields 394\\nrunAsUser property 381, 393\\nruntimes. See container runtimes\\nS\\nscaled resources, updating replica \\ncount on 440\\nscale-downs 456\\nof clusters, limiting service dis-\\nruption during 454–456\\nto zero replicas 450–451\\nscaleIO volume 163\\nscale-out, results of 49–50\\nscale-ups 456\\nof Deployments, with \\nAutoscaler 446–447\\ntriggering 445–446\\nscaling\\napplications, horizontally\\n48–50\\nautomatic 23\\nbased on CPU utilization\\n441–447\\nautomatic rescale events\\n444–445\\ncreating Horizontal pod \\nAutoscaler based on \\nCPU usage 442–443\\nmaximum rate of scaling 447\\nmodifying target metric val-\\nues on existing HPA \\nobjects 447\\ntriggering scale-ups 445–446\\nusing Autoscaler to scale up \\nDeployments 446–447\\nbased on custom metrics\\n448–450\\nObject metric types 449–450\\npods metric types 449\\nresource metric types 449\\nbased on memory \\nconsumption 448\\ncluster nodes horizontally\\n452–456\\nCluster Autoscaler 452–453\\nenabling Cluster Autoscaler\\n454\\nlimiting service disruption \\nduring cluster scale-\\ndown 454–456\\ndown to zero replicas 450–451\\nhorizontal autoscaling of \\npods 438–451\\nautoscaling process 438–441\\nmetrics appropriate for \\nautoscaling 450\\nJob resource 116\\nmaximum rate of 447\\nmicroservices 4\\nnumber of copies 21\\npods horizontally 102–103\\ndeclarative approach to \\nscaling 103\\nscaling down with kubectl \\nscale command 103\\nscaling ReplicationControl-\\nler by editing \\ndefinitions 102–103\\nscaling up Replication-\\nController 102\\nReplicationControllers\\n259–260\\nsplitting into multiple pods \\nfor 59\\nStatefulSets 287, 298\\nvertical autoscaling of \\npods 451–452\\nautomatically configuring \\nresource requests 451\\nmodifying resource requests \\nwhile pod is running\\n451–452\\nSee also autoscaling\\nscaling out 3\\nscaling up 3\\nschedule, configuring 117\\nScheduler 319–321\\nadvanced scheduling of \\npods 321\\nassigning nodes to newly cre-\\nated pods 332\\ndefault scheduling algorithm\\n319\\nensuring high availability \\nof 343–344\\nfinding acceptable nodes 320\\nselecting best node for \\npod 320–321\\nusing multiple schedulers 321\\nusing pod affinity rules \\nwith 470–471\\nusing pod requests when select-\\ning best nodes 407\\nusing to determine whether pod \\ncan fit on node 406\\nscheduling 44, 457–476\\nco-locating pods with pod \\naffinity 468–476\\ndeploying pods 471–472\\nexpressing podAffinity pref-\\nerences instead of hard \\nrequirements 472–473\\nusing inter-pod affinity to \\ndeploy pods on same \\nnode 468–471\\ndefault 319\\neffect of resource requests \\non 406–410\\ncreating pods that don’t fit on \\nany node 408–409\\nfreeing resources to schedule \\npods 410\\ninspecting node \\ncapacity 407–408\\npod not being \\nscheduled 409–410\\nScheduler determining \\nwhether pod can fit on \\nnode 406\\nScheduler using pod requests \\nwhen selecting best \\nnodes 407\\njobs 116–118\\ncreating CronJob 116–117\\noverview 117\\npods\\naway from each other with \\npod anti-affinity 474–476\\nto specific nodes 74–75\\nusing labels to constrain\\n73–75\\nusing selectors to \\nconstrain 73–75\\nusing multiple 321\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 621,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n589\\nscheduling (continued)\\nusing node affinity to attract \\npods to nodes 462–468\\ncomparing node affinity to \\nnode selectors 462–463\\nexamining default node \\nlabels 462–463\\nprioritizing nodes when sched-\\nuling pods 465–468\\nspecifying hard node affinity \\nrules 463–465\\nusing taints during 461–462\\nusing taints to repel pods from \\nnodes 457–462\\nadding custom taints to \\nnode 460\\ntaints, overview 458–460\\nusing taints 461–462\\nusing tolerations during\\n461–462\\nusing tolerations to repel pods \\nfrom nodes 457–462\\nadding tolerations to \\npods 460–461\\ntolerations, overview\\n458–460\\nusing tolerations 461–462\\nSee also scheduler\\nsecret volume 163\\nSecrets\\ncreating 216, 223\\ndefault token 214–215\\nexposing entries through envi-\\nronment variables 221–222\\nimage pull 351\\nmountable 350–351\\noverview 214\\nreading entries in pods 218\\nusing for binary data 217\\nusing in pods 218, 222\\nmodifying fortune-config \\nconfig map to enable \\nHTTPS 218–219\\nmounting fortune-https \\nSecret in pods 219–220\\nSecret volumes stored in \\nmemory 221\\nverifying Nginx is using cert \\nand key from Secret 221\\nusing newly created in client \\npods 525–526\\nusing to pass sensitive data to \\ncontainers 213, 223\\ndefault token Secrets\\n214–215\\nimage pull Secrets 222–223\\nversus ConfigMaps 217–218\\nSee also image pull secrets\\nsecuring\\ncluster nodes 375–403\\nconfiguring container secu-\\nrity contexts 380–389\\nisolating pod networks\\n399–402\\nrestricting use of security-\\nrelated features in \\npods 389–399\\nusing host node namespaces \\nin pods 376–380\\nclusters with RBAC 353–373\\nbinding roles to service \\naccounts 359–360\\ndefault ClusterRole-\\nBindings 371–373\\ndefault ClusterRoles\\n371–373\\ngranting authorization \\npermissions 373\\nincluding service accounts \\nfrom other namespaces \\nin RoleBinding 361\\nRBAC authorization \\nplugins 353–354\\nRBAC resources 355–357\\nusing ClusterRole-\\nBindings 362–371\\nusing ClusterRoles 362–371\\nusing RoleBindings 358–359\\nusing Roles 358–359\\nnetworks 375–403\\nconfiguring container secu-\\nrity contexts 380–389\\nisolating pod networks\\n399–402\\nrestricting use of security-\\nrelated features in \\npods 389–399\\nusing host node namespaces \\nin pods 376–380\\nsecurity contexts\\nof containers 380–389\\nrunning pods without \\nspecifying 381\\nsetting options at pod level 387\\nsecurityContext property 380, \\n383, 385, 387\\nsecurityContext.capabilities \\nfield 394\\nsecurityContext.capabilities.drop \\nproperty 386\\nsecurityContext.readOnlyRoot-\\nFilesystem property 386\\nsecurityContext.runAsUser \\nproperty 381\\nsecurity-enhanced Linux. See \\nSELinux\\nsecurity-related features, restrict-\\ning use of in pods 389–399\\nassigning different PodSecurity-\\nPolicies to different \\ngroups 396–399\\nassigning different PodSecurity-\\nPolicies to different \\nusers 396–399\\nconfiguring allowed \\ncapabilities 394–395\\nconfiguring default \\ncapabilities 394–395\\nconfiguring disallowed \\ncapabilities 394–395\\nconstraining types of volumes \\npods can use 395\\nfsGroup policies 392–394\\nPodSecurityPolicy \\nresources 389–392\\nrunAsUser policies 392–394\\nsupplementalGroups \\npolicies 392–394\\nselecting start-up disks 541\\nselector property 106\\nselector.matchLabels 106\\nselectors\\nconstraining pod scheduling \\nwith 73–75\\ncategorizing worker nodes \\nwith labels 74\\nscheduling pods to specific \\nnodes 74–75\\ncreating endpoints resource \\nfor services without\\n133–134\\ncreating services without\\n132–133\\nSelectorSpreadPriority \\nfunction 468\\nself-healing 22–23\\nSELinux (security-enhanced \\nLinux)\\ndisabling 544\\noverview 380\\nsemantics, at-most-one 290\\nserver pods, allowing some pods \\nin namespaces to connect \\nto 400\\nservers 346–374\\nAPI, connecting to 503\\nauthentication of 346–353\\ngroups 346–348\\nservice accounts 348–353\\nusers 347–348\\nsecuring clusters with \\nRBAC 353–373\\nbinding roles to service \\naccounts 359–360\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 622,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n590\\nservers (continued)\\ndefault ClusterRoleBindings\\n371–373\\ndefault ClusterRoles 371–373\\ngranting authorization \\npermissions 373\\nincluding service accounts \\nfrom other namespaces \\nin RoleBinding 361\\nRBAC authorization \\nplugins 353–354\\nRBAC resources 355–357\\nusing ClusterRoleBindings\\n362–371\\nusing ClusterRoles 362–371\\nusing RoleBindings 358–359\\nusing Roles 358–359\\nusing custom service account \\ntokens to communicate \\nwith 352–353\\nservice accounts 348–349\\nassigning to pods 351–353\\nbinding Roles to 359–360\\ncreating 349–351\\ncreating for each pod 373\\ncustom\\ncreating pods that use\\n351–352\\nusing tokens to communi-\\ncate with API servers\\n352–353\\nimage pull Secrets 351\\nincluding from other name-\\nspaces in RoleBinding 361\\nmountable Secrets 350–351\\nServiceAccount resources\\n348–349\\ntying into authorizations 349\\nservice brokers 522–523\\nlisting available services in \\nclusters 523\\nregistering in Service \\nCatalog 522–523\\nService Catalog 519–527\\nbenefits of using 526–527\\nController Manager 521\\ndeprovisioning instances 526\\nOpenServiceBroker API\\n522–523\\nlisting available services in \\nclusters 523\\noverview 522\\noverview 520–521\\nprovisioning services 524–526\\nbinding service instances\\n525\\nprovisioning service \\ninstances 524–525\\nusing newly created Secret in \\nclient pods 525–526\\nregistering service brokers \\nin 522–523\\nservice brokers 522–523\\nService Catalog API server 521\\nunbinding instances 526\\nusing services 524–526\\nService controllers 324\\nservice endpoints\\nmanually configuring 132–134\\ncreating endpoints resource \\nfor services without \\nselectors 133–134\\ncreating services without \\nselectors 132–133\\noverview 131–132\\nService Proxy 327–328\\nServiceAccount resources 348–349\\nSERVICE_ACCOUNT \\nvariable 228\\nserviceAccountName \\nproperty 373\\nSERVICE_HOST variable 129\\nSERVICE_PORT variable 129\\nservice-reader role 359\\nservices 47, 120–158\\naccessing pods through 264\\naccessing through external \\nIP 46–47\\naccessing through Ingress 145\\naccessing pods through \\nIngress 145\\nconfiguring host in Ingress \\npointing to Ingress IP \\naddresses 145\\nhow Ingresses work 145\\nobtaining IP address of \\nIngress 145\\navailable in clusters, listing 523\\nbackend, connecting to 502\\nbinding instances 525\\ncluster-internal, connecting \\nthrough API servers 299\\nconfiguring for kubectl 41\\nconfiguring session affinity \\non 126\\nconnecting to, through load \\nbalancers 139–141\\ncreating 45, 122–128\\nremotely executing com-\\nmands in running \\ncontainers 124–126\\nthrough kubectl expose \\ncommand 123\\nthrough YAML \\ndescriptors 123\\nusing named ports 127–128\\ncreating endpoints resource \\nwithout selectors 133–134\\ncreating without selectors\\n132–133\\ndedicated, using for each pod \\ninstance 283–284\\ndiscovering 128–131\\nconnecting to service \\nthrough its FQDN 130\\npinging service IP 131\\nrunning shell in pod \\ncontainers 130–131\\nthrough DNS 129\\nthrough environment \\nvariables 128–129\\ndisruptions of, limiting during \\ncluster scale-down 454–456\\nexamining new 124\\nexamples of 121–122\\nexposing applications through, \\nusing YAML file 255\\nexposing externally through \\nIngress resources 142–149\\nbenefits of using 142–143\\nconfiguring Ingress to handle \\nTLS traffic 147–149\\ncreating Ingress resources 144\\nusing Ingress controllers\\n143–144\\nexposing externally using \\nRoutes 530\\nexposing multiple ports in \\nsame 126–127\\nexposing multiple through \\nsame Ingress 146–147\\nmapping different services to \\ndifferent hosts 147\\nmapping different services to \\ndifferent paths of same \\nhost 146\\nexposing Pet pods \\nthrough 298–299\\nexposing through external load \\nbalancers 138–141\\nconnecting to services \\nthrough load balancers\\n139–141\\ncreating LoadBalancer \\nservices 139\\nexposing to external \\nclients 134–142\\nexternal connections 141–142\\nusing NodePort services\\n135–138\\ngoverning\\ncreating 292–294\\noverview 285–287\\nimplementing 338–340\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 623,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n591\\nservices (continued)\\nlisting 46\\nlisting from pods 357\\nliving outside cluster, connect-\\ning to 131–134\\noverview 121–131\\nprovisioning 524–526\\nrequests hitting three pods 50\\nrunning applications through, \\nusing YAML file 255\\nsignaling when pods ready to \\naccept connections\\n149–153\\ntesting from within cluster 124\\ntroubleshooting 156\\nusing 524–526\\nwhy needed 48\\nSee also external services\\nsessionAffinity property 126\\nsessions, configuring affinity on \\nservices 126\\nset-context command 538\\nshare-type label 467\\nsharing\\ndata between containers with \\nvolumes 163–169\\nusing emptyDir volume\\n163–166\\nusing Git repository as start-\\ning point for volume\\n166–169\\nvolumes when containers \\nrunning as different \\nusers 387–389\\nshell forms, versus exec forms\\n193–194\\nshells\\nrunning in containers 130–131\\nrunning inside existing \\ncontainers 33\\nshutdown handlers, implement-\\ning in applications 490–491\\nshutdown logic, pods 153\\nshutdowns\\ncritical, replacing dedicated \\nshut-down procedure \\npods 491\\ndedicated pods, replacing criti-\\ncal shut-down procedures \\nwith 491\\nof pods 489–491\\nimplementing proper shut-\\ndown handler in \\napplications 490–491\\nreplacing critical shut-down \\nprocedures with dedi-\\ncated shut-down proce-\\ndure pods 491\\nspecifying termination grace \\nperiods 490\\nshutting down VMs 545\\nsidecar container 60, 168\\nSIG (Special Interest Group) 448\\nSIGKILL 89\\nsignaling, when pods ready to \\naccept connections 149–153\\nSIGTERM signal 487–491, 493, \\n495\\nsimulating node disconnection \\nfrom network 304–306\\nchecking node status as seen by \\nKubernetes master 305\\npods with unknown status\\n305–306\\nshutting down node network \\nadapters 304–305\\nsingle-node clusters, local 37–38\\nsleep-interval entry 221\\nslowing rolling updates 265\\nSNAT (Source Network Address \\nTranslation) 142\\nspec section 75\\nspec.containers.ports field 377\\nspecifications, referring to \\ncontainer-level metadata \\nin 233\\nspec.initContainers field 484\\nspec.replicas field 102–103\\nspec.template.spec.contain-\\ners.image attribute 303\\nspinning up, new pods 252–253\\nsplitting\\napplications into \\nmicroservices 3–4\\ninto multiple pods 59\\nSRV records 300\\nSSD (Solid-State Drive) 109\\nssd storage class 427\\nstartingDeadlineSeconds \\nfield 118\\nstart-up disks, selecting 541\\nstateful pods 308\\nexamining 294–295\\noverview 284\\nreplicating 281–284\\nproviding stable identity for \\npods 282–284\\nrunning multiple replicas \\nwith separate storage for \\neach 281–282\\nStatefulSet controllers 324\\nStatefulSet resources 280–308\\ncreating 294\\ncreating applications 290–291\\ncreating container images\\n290–291\\ndeploying applications \\nthrough 291–295\\ncreating governing \\nservices 292–294\\ncreating persistent \\nvolumes 291–292\\nexamining PersistentVolume-\\nClaims 295\\nexamining stateful pods\\n294–295\\ndiscovering peers in 299–304\\nclustered data store 303–304\\nimplementing peer discovery \\nthrough DNS 301–302\\nSRV records, overview 300\\nguarantees 289–290\\nat-most-one semantics 290\\nimplications of stable \\nidentity 289–290\\nimplications of stable \\nstorage 289–290\\nnode failures and 304–307\\ndeleting pods manually\\n306–307\\nsimulating node disconnec-\\ntion from network\\n304–306\\noverview 284–290\\nproviding stable dedicated \\nstorage to pets 287–289\\nproviding stable network \\nidentities 285–287\\nreplicating stateful pods\\n281–284\\nproviding stable identity for \\npods 282–284\\nrunning multiple replicas \\nwith separate storage for \\neach 281–282\\nscaling 287, 298\\nupdating 302–303\\nusing 290–295\\nvs. ReplicaSets 284–285\\nvs. ReplicationControllers\\n284–285\\nwith Pet pods 295–299\\ncommunicating with pods \\nthrough API servers\\n295–297\\nconnecting to cluster-inter-\\nnal services through API \\nservers 299\\ndeleting Pet pods 297–298\\nexposing Pet pods through \\nservices 298–299\\nstates, visualizing new 50\\nstatistics, of historical resource \\nconsumption 432–434\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 624,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n592\\nstatus\\nof nodes 305\\npods with unknown 305–306\\nstatus attribute 138\\nSTATUS column 183\\nStatus section 63\\nstatus.certificate field 148\\nstatus.qosClass field 419\\nstopping\\ncontainers 34–35\\npods 80, 82\\nstorage\\ndefining available types \\nthrough StorageClass \\nresources 185\\nproviding to pets 287–289\\ncreating PersistentVolume-\\nClaims 288\\ndeleting PersistentVolume-\\nClaims 288\\nreattaching Persistent-\\nVolumeClaims to new \\ninstances of same \\npod 289\\nusing pod templates with \\nvolume claim templates\\n288\\nrunning multiple replicas with \\nseparate storage for \\neach 281–282\\ncreating pods manually 281\\nusing multiple directories in \\nsame volume 282\\nusing one ReplicaSet per pod \\ninstance 281–282\\nstable, implications of\\n289–290\\nSee also persistent storage\\nstorage classes\\ncreating PersistentVolume-\\nClaims without specifying\\n188–189\\ndefault 188\\ndynamic provisioning without \\nspecifying 187–189\\nlisting 187–188\\nrequesting in PersistentVolume-\\nClaims 185–187\\ncreating PersistentVolume-\\nClaim definition \\nrequesting specific stor-\\nage class 185–186\\nexamining created Persistent-\\nVolumeClaims 186–187\\nexamining dynamically provi-\\nsioned Persistent-\\nVolumes 186–187\\nusing 187\\nstorage technologies\\ndecoupling pods from 176–184\\nbenefits of using claims 182\\nPersistentVolumeClaims\\n176–177, 179–181\\nPersistentVolumes 176–184\\nusing 175\\nstorage volumes 160\\nStorageClass resources, defining \\navailable storage types \\nthrough 185\\nstorageclass-fast-hostpath.yaml \\nfile 187\\nstorageClassName attribute\\n188–189\\nstoring\\nhistorical resource consump-\\ntion statistics 432–434\\nanalyzing resource usage with \\nGrafana 433–434\\nGrafana 432\\nInfluxDB 432\\nrunning Grafana in \\nclusters 433\\nrunning InfluxDB \\nclusters 433\\nusing information shown in \\ncharts 434\\nresources persistently 318\\nStringData field 218\\nsubPath property 210–211\\nsubsets, listing through label \\nselectors 71–72\\nlisting pods using label \\nselectors 71–72\\nusing multiple conditions in \\nlabel selectors 72\\nsupplementalGroups \\npolicies 392–394\\ndeploying pod with container \\nimage with an out-of-range \\nuser ID 393\\nusing MustRunAs rule 392–393\\nsupplementalGroups property\\n388–389\\nSwagger framework 248\\nsymlink 213\\nsyncHandler field 323\\nsysadmins, role in continuous \\ndelivery 7\\nsystem:authenticated group 348, \\n366\\nsystem:discovery cluster role\\n373\\nsystem:serviceaccounts: 348\\nsystem:unauthenticated \\ngroup 348, 366\\nSYS_TIME capability 394\\nT\\ntab completion, configuring for \\nkubectl 41–42\\ntagging images\\noverview 497–498\\nunder additional tags 35\\ntags 28\\ntaints\\ncustom, adding to nodes 460\\neffects of 459–460\\nof nodes, displaying 458\\noverview 458–462\\nrepelling pods from nodes \\nwith 457–462\\nusing during scheduling\\n461–462\\ntargeting containers 489\\nTCP packets 126\\nTCP Socket 86, 150\\ntemplates\\nfor Job resources 117\\npods\\nadding readiness probes \\nto 151–152\\nchanging 101\\neffect of changing 93\\nusing with volume claim \\ntemplates 288\\nvolume claim, using with pod \\ntemplates 288\\nTERM variable 33\\nterminating processes, providing \\ninformation about 498–500\\nTerminating scope 429\\ntermination grace periods, \\nspecifying 490\\nterminationMessagePath field 499\\nterminationMessagePolicy \\nfield 500\\ntesting services from within \\nclusters 124\\nThirdPartyResource objects 509\\ntime sharing. See CPU time sharing\\nTLS (Transport Layer \\nSecurity) 147–149\\ntmpfs filesystem 166\\nTOKEN variable 241\\ntokens, of custom service \\naccounts 352–353\\ntolerations\\nadding to pods 460–461\\nof pods, displaying 459\\noverview 458–462\\nrepelling pods from nodes \\nwith 457–462\\nusing during scheduling\\n461–462\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 625,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n593\\ntopologyKey 471–473\\ntriggering\\nrolling updates 265–267\\nscale-ups 445–446\\ntroubleshooting services 156\\nU\\nubuntu:latest image 164\\nUDP packets 126\\nunbinding instances 526\\nundoing rollouts 269\\nuniversal resource locators. See \\nURLs\\nunschedulable nodes 109\\nupdates. See rolling updates\\nupdating\\nannotations 232\\napplication config without \\nrestarting application\\n211–213\\napplications declaratively using \\nDeployment 261–278\\nblocking rollouts of bad \\nversions 274–278\\ncontrolling rate of \\nrollout 271–273\\ncreating Deployments\\n262–264\\npausing rollout process\\n273–274\\nrolling back \\ndeployments 268–270\\nupdating Deployments\\n264–268\\napplications running in \\npods 251–253\\ndeleting old pods 252–253\\nreplacing old pods 252\\nspinning up new pods\\n252–253\\nConfigMap 213\\nDeployment resource 264–268\\nDeployment strategies\\n264–265\\nslowing down rolling \\nupdates 265\\ntriggering rolling \\nupdates 265–267\\ndeployments with kubectl \\napply 276–277\\nfiles automatically 212–213\\nlabels 232\\nreplica count on scaled \\nresources 440\\nStatefulSets 302–303\\nURLs (universal resource \\nlocators) 365–367\\n--user argument 359\\nuser credentials\\nadding 536–537\\nmodifying 536–537\\ntying with clusters 537\\nUSER directive 381\\nusers 347–348\\nassigning PodSecurityPolicies \\nto 396–399\\ncreating PodSecurityPolicy \\nallowing privileged con-\\ntainers to be deployed\\n396–397\\nwith RBAC 397–398\\ncreating additional for \\nkubectl 398\\ncreating pods as 398–399\\nin kubeconfig files 536\\nin Red Hat OpenShift Con-\\ntainer platform 528\\nout-of-range IDs 393\\nrunning containers as \\nspecific 381–382\\nsharing volumes when contain-\\ners running as different \\nusers 387–389\\nusing kubectl with 537–538\\nUTS Namespace 11\\nV\\nV1 applications, creating 254\\nvalidating\\ncustom objects 517–518\\nresources 318\\nvalueFrom field 202\\nvalues property 107\\nvalues, referring to environment \\nvariables in 198\\nvariables. See environment vari-\\nables\\nVCS (Version Control \\nSystem) 505\\nVDI (VirtualBox Disk Image)\\n539\\nverifying identity of API \\nservers 240–241\\nversioning\\ncontainer images 28\\nresource manifests 504–505\\nversions\\ncreating 268\\ndeploying 269\\nof resources, federated 558\\nvertical autoscaling, of pods\\n451–452\\nautomatically configuring \\nresource requests 451\\nmodifying resource requests \\nwhile pod is running\\n451–452\\nveth pair 336–337\\nview ClusterRole, allowing read-\\nonly access to resources \\nwith 372\\nvisualizing new states 50\\nVMs (virtual machines) 8, 26\\ncloning 545–547\\nchanging hostname on 546\\nconfiguring name resolution \\nfor hosts 546–547\\ncomparing to containers 8–10\\ncomparing to Docker 14–15\\nconfiguring network adapters \\nfor 540\\ncreating 539\\ninstead of containers 555\\nshutting down 545\\nSee also Minikube VM\\nvolume claim templates, using \\nwith pod templates 288\\nVOLUME column 186\\nvolumeClaimTemplates 293\\nvolumes 159–177, 190\\naccessing files on worker node \\nfilesystems 169–170\\nexamining system pods that \\nuse hostPath \\nvolumes 170\\nhostPath volume 169–170\\nconstraining types of 395\\ndecoupling pods from underly-\\ning storage technologies\\n176–184\\nclaims 182\\nPersistentVolumeClaims\\n176–181\\nPersistentVolumes 176–184\\ndynamic provisioning of \\nPersistentVolumes 184–189\\ndefining available storage \\ntypes through Storage-\\nClass resources 185\\nrequesting storage class in \\nPersistentVolume-\\nClaims 185–187\\nwithout specifying storage \\nclass 187–189\\noverview 160–163\\navailable volumes types\\n162–163\\nexamples of 160–162\\nsharing data between containers \\nwith 163–169\\nsharing when containers run as \\ndifferent users 387–389\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 626,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'INDEX\\n594\\nvolumes (continued)\\nspecifications, referring to \\ncontainer-level metadata \\nin 233\\nusing AWS elastic block store 174\\nusing ConfigMap entries \\nin 207–208\\nusing emptyDir 163–166\\ncreating pods 164–165\\nin pods 163–164\\nseeing pods in action 165\\nspecifying medium for 166\\nusing Git repository as starting \\npoint for 166–169\\nfiles in sync with gitRepo \\nvolume 168\\ngitRepo volume 169\\nrunning web server pod serv-\\ning files from \\ncloned 167\\nsidecar containers 168\\nusing gitRepo volume with \\nprivate Git repositories\\n168\\nusing multiple directories in 282\\nusing NFS 175\\nusing persistent storage 171–175\\nunderlying storage 174–175\\nusing GCE Persistent Disk in \\npod volumes 171–174\\nusing to preserve data across \\ncontainer restarts 480–482\\nSee also gitRepo volume; per-\\nsistent volumes\\nvsphereVolume volume 163\\nW\\nwatch command 445\\nweb applications, accessing 45–47\\naccessing services through \\nexternal IP 46–47\\ncreating services 45\\nlisting services 46\\nweb browsers 140\\nweb server pods, serving files from \\ncloned Git repository 167\\nWebHook plugin 353\\nWebServer container 162\\nWebsite controllers 514–515\\nwebsite-crd.yaml file 511\\nworker nodes 18, 43\\naccessing files on 169–170\\nexamining system pods that \\nuse hostPath \\nvolumes 170\\nhostPath volume 169–170\\ncategorizing with labels 74\\ncomponents running on 310\\nconfiguring with \\nkubeadm 549–550\\nworker() method 323\\nWorkflow. See Deis Workflow plat-\\nform\\nX\\n-Xmx JVM option 416\\nY\\nYAML file format\\ncreating definitions 110\\ncreating descriptors for \\npods 63–65\\ncreating namespaces 78\\ncreating pods from \\ndescriptors 61–67\\nsending requests to pods\\n66–67\\nusing kubectl create to create \\npods 65\\nviewing application logs\\n65–66\\ncreating services through 123\\nexamining descriptors of exist-\\ning pods 61–63\\nexposing applications through \\nservices using 255\\nmanifests, writing 505–506\\nrunning applications through \\nservices using 255\\nrunning pods without writing \\nmanifest 155\\nyum package manager, adding \\nKubernetes repo files 544\\n \\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []},\n",
       " {'page': 627,\n",
       "  'img_cnt': 0,\n",
       "  'img_flag': 0,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'Kubernetes resources covered in the book (continued)\\n* Cluster-level resource (not namespaced)\\n** Also in other API versions; listed version is the one used in this book\\nResource (abbr.) [API version]\\nDescription\\nSection\\nScaling\\nHorizontalPodAutoscaler (hpa) \\n[autoscaling/v2beta1**]\\nAutomatically scales number of pod replicas \\nbased on CPU usage or another metric\\n15.1\\nPodDisruptionBudget (pdb) \\n[policy/v1beta1]\\nDefines the minimum number of pods that must \\nremain running when evacuating nodes\\n15.3.3\\nResources\\nLimitRange (limits) [v1]\\nDefines the min, max, default limits, and default \\nrequests for pods in a namespace\\n14.4\\nResourceQuota (quota) [v1]\\nDefines the amount of computational resources \\navailable to pods in the namespace\\n14.5\\nCluster state\\nNode* (no) [v1]\\nRepresents a Kubernetes worker node\\n2.2.2\\nCluster* [federation/v1beta1]\\nA Kubernetes cluster (used in cluster federation)\\nApp. D\\nComponentStatus* (cs) [v1]\\nStatus of a Control Plane component\\n11.1.1\\nEvent (ev) [v1]\\nA report of something that occurred in the cluster\\n11.2.3\\nSecurity\\nServiceAccount (sa) [v1]\\nAn account used by apps running in pods\\n12.1.2\\nRole [rbac.authorization.k8s.io/v1]\\nDefines which actions a subject may perform on \\nwhich resources (per namespace)\\n12.2.3\\nClusterRole* \\n[rbac.authorization.k8s.io/v1]\\nLike Role, but for cluster-level resources or to \\ngrant access to resources across all namespaces\\n12.2.4\\nRoleBinding \\n[rbac.authorization.k8s.io/v1]\\nDefines who can perform the actions defined in a \\nRole or ClusterRole (within a namespace) \\n12.2.3\\nClusterRoleBinding* \\n[rbac.authorization.k8s.io/v1]\\nLike RoleBinding, but across all namespaces\\n12.2.4\\nPodSecurityPolicy* (psp) \\n[extensions/v1beta1]\\nA cluster-level resource that defines which security-\\nsensitive features pods can use\\n13.3.1\\nNetworkPolicy (netpol) \\n[networking.k8s.io/v1]\\nIsolates the network between pods by specifying \\nwhich pods can connect to each other\\n13.4\\nCertificateSigningRequest* (csr) \\n[certificates.k8s.io/v1beta1]\\nA request for signing a public key certificate\\n5.4.4\\nExt.\\nCustomResourceDefinition* (crd) \\n[apiextensions.k8s.io/v1beta1]\\nDefines a custom resource, allowing users to cre-\\nate instances of the custom resource \\n18.1\\n \\n',\n",
       "  'tables_flag': 1,\n",
       "  'tables': [            Col0                     Resource (abbr.) [API version]  \\\n",
       "   0        Scaling  HorizontalPodAutoscaler (hpa)\\n[autoscaling/v2...   \n",
       "   1      Resources  LimitRange (limits) [v1]\\nResourceQuota (quota...   \n",
       "   2  Cluster state  Node* (no) [v1]\\nCluster* [federation/v1beta1]...   \n",
       "   3       Security  ServiceAccount (sa) [v1]\\nRole [rbac.authoriza...   \n",
       "   4           Ext.  CustomResourceDefinition* (crd)\\n[apiextension...   \n",
       "   \n",
       "                                            Description  \\\n",
       "   0  Automatically scales number of pod replicas\\nb...   \n",
       "   1  Defines the min, max, default limits, and defa...   \n",
       "   2  Represents a Kubernetes worker node\\nA Kuberne...   \n",
       "   3  An account used by apps running in pods\\nDefin...   \n",
       "   4  Defines a custom resource, allowing users to c...   \n",
       "   \n",
       "                                                Section  \n",
       "   0                                       15.1\\n15.3.3  \n",
       "   1                                         14.4\\n14.5  \n",
       "   2                      2.2.2\\nApp. D\\n11.1.1\\n11.2.3  \n",
       "   3  12.1.2\\n12.2.3\\n12.2.4\\n12.2.3\\n12.2.4\\n13.3.1...  \n",
       "   4                                               18.1  ]},\n",
       " {'page': 628,\n",
       "  'img_cnt': 4,\n",
       "  'img_flag': 1,\n",
       "  'img_npy_lst': [],\n",
       "  'text': 'Marko Lukša\\nK\\nubernetes is Greek for “helmsman,” your guide through \\nunknown waters. The Kubernetes container orchestra-\\ntion system safely manages the structure and ﬂ\\n ow of a \\ndistributed application, organizing containers and services for \\nmaximum efﬁ\\n ciency. Kubernetes serves as an operating system \\nfor your clusters, eliminating the need to factor the underlying \\nnetwork and server infrastructure into your designs.\\nKubernetes in Action teaches you to use Kubernetes to deploy \\ncontainer-based distributed applications. You’ll start with an \\noverview of Docker and Kubernetes before building your ﬁ\\n rst \\nKubernetes cluster. You’ll gradually expand your initial \\napplication, adding features and deepening your knowledge \\nof Kubernetes architecture and operation. As you navigate \\nthis comprehensive guide, you’ll explore high-value topics \\nlike monitoring, tuning, and scaling. \\nWhat’s Inside\\n● Kubernetes’ internals\\n● Deploying containers across a cluster\\n● Securing clusters\\n● Updating applications with zero downtime\\nWritten for intermediate software developers with little or no \\nfamiliarity with Docker or container orchestration systems.\\nMarko Lukša is an engineer at Red Hat working on Kubernetes \\nand OpenShift.\\nTo download their free eBook in PDF, ePub, and Kindle formats, \\nowners of this book should visit \\nwww.manning.com/books/kubernetes-in-action\\n$59.99 / Can $79.99  [INCLUDING eBOOK]\\nKubernetes IN ACTION\\nSOFTWARE DEVELOPMENT/OPERATIONS\\nM A N N I N G\\n“\\nAuthoritative and \\nexhaustive. In a hands-on \\nstyle, the author teaches how \\nto manage the complete \\nlifecycle of any distributed \\n and scalable application.”\\n \\n—Antonio Magnaghi, System1\\n“\\nThe best parts are the real-\\nworld examples. They don’t \\njust apply the concepts, \\nthey road test them.”\\n \\n—Paolo Antinori, Red Hat\\n“\\nAn in-depth discussion \\nof Kubernetes and related \\n technologies. A must-have!”\\n—Al Krinker, USPTO \\n“\\nThe full path to becoming \\na professional Kubernaut. \\n Fundamental reading.”\\n \\n—Csaba Sári\\nChimera Entertainment\\nSEE  INSERT\\n',\n",
       "  'tables_flag': 0,\n",
       "  'tables': []}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_dict_deserialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60212f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Definitions of Individual Enrichment Modules######\n",
    "\n",
    "def extract_entities_per_page(page_text):\n",
    "    \n",
    "    \n",
    "    parser = XMLOutputParser()\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker\\n\n",
    "            Machine Larning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study a document page related to a particular technology and extract \\n\n",
    "            the main entities or objects from the document.The entities or objects can be anything surrounding \\n\n",
    "            technology in which your expetise lies.By Objects or entities i mean it can be a programming Language,\\n\n",
    "            command,framework,database,process,cache,controller,qeueues,schedulers,errors,exceptions or any \\n\n",
    "            components related to technology. \\n\n",
    "            surrounding the technology.Please output in form of list of json where each extracted entity will be \\n\n",
    "            represented by a json along with description and category.\\n\n",
    "            The category name can be like \\n\n",
    "            hardware,software,network,application,database,process,thread,container etc.Mention the category name \\n\n",
    "            in third bracket. \\n\n",
    "            After extracting all the entities along with their description and category format as per rules below: \\n\n",
    "            1.For each entity create a json with 3 keys \"entity\",\"description\" and \"category\". \\n\n",
    "            2.Collate all the entity json into a list of json where each json represents an entity. \\n\n",
    "            Output the collated list of json. \\n\n",
    "            Do not output anything other than the list of json neither heading/decsription before the list of json\\n\n",
    "            nor any decsription/footer below the json \\n\n",
    "            Here is the document page: \\n\\n {context} \\n\\n\"\"\",\n",
    "            input_variables=[\"context\"],\n",
    "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    #Commented out By Kaustav 20/08/2024 as str output parser is giving some unwanted text #####\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Trying out XML Output parser by Kautsva added 20/08/2024####\n",
    "    #chain = prompt | llm | parser\n",
    "    \n",
    "    \n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"context\": page_text,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    \n",
    "    json_output=output.split('[')[1]\n",
    "    #page_output_json=json.loads(output)\n",
    "    \n",
    "    \n",
    "    #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "    \n",
    "    json_output='['+json_output\n",
    "    \n",
    "    \n",
    "    #page_output_json=json.loads(output)\n",
    "    #return page_output_json\n",
    "    return json_output\n",
    "    \n",
    "    #return page_output_json\n",
    "    #return output\n",
    "\n",
    "def entity_collector_per_page(entity_lst):\n",
    "\n",
    "    entities=[]\n",
    "    \n",
    "    for entity in entity_lst:\n",
    "        print(\"Entity:\")\n",
    "        print(entity)\n",
    "        entity_name=entity['entity']\n",
    "        entities.append(entity_name)\n",
    "    return list(set(entities))\n",
    "\n",
    "\n",
    "def extract_relationship_per_page(page_text,entity_lst):\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker\\n\n",
    "            Machine Learning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study a document page related to a particular technology and extract all \\n\n",
    "            the possible relations between the list of entities provided as input along with the context/document.\\n\n",
    "            While extracting relations carry out the following steps as per the below rules for each : \\n\n",
    "            1.Define each relation by means of a json having three keys source_entity,description,destination_entity.\n",
    "                a.source_entity should have that entity who is the one performing some action that is the subject. \\n\n",
    "                b.destination_entity should have the entity who is the object on which the source entity carries out \\n\n",
    "                action.\\n\n",
    "                c.Description of the relation will contain a brief summary about what action was being carried by the \\n\n",
    "                source_entity on the destination_entity. \\n\n",
    "            2.After all the relations have been extracted collate them into a list of json.\n",
    "            Out should only contain the list of json and no oter words or character or sentences. \\n\n",
    "            Here is the document page: \\n\\n {context} \\n\\n and the entities: \\n\\n {entities} \\n\\n\"\"\",\n",
    "            input_variables=[\"context\",\"entities\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"context\": page_text,\"entities\":entity_lst\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    \n",
    "    json_output=output.split('[')[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    json_output='['+json_output\n",
    "    \n",
    "    \n",
    "    #page_output_json=json.loads(output)\n",
    "    #return page_output_json\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d155a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "######This code enrcihes the PDF page by page and while doing so it fails after processing 20 to 50 pages####\n",
    "######currently we are manually tackling this problem by making the pdf dict list into aglobal variable######\n",
    "######After the llm fails we take the index of the last page done and save the pdf dict list uptil that ####\n",
    "#####Then as of now i am manually restarting the by loading the saved pdf dict list as the source pdf and ###\n",
    "#####Using the last done pdf page index +1 as the starting index.So (last Done page Index + 1) is the ######\n",
    "###starting index###\n",
    "####The maximum page has been given as 565 as that is the last page where the main content of the document###\n",
    "####resides.In case of the book Kubernetes in action it is 565.##############################################\n",
    "####If this is batch process then the last page can be manually seen and used as the variable else one can ####\n",
    "#####also go for the pdf enrichment of all the pages of the book#############################################\n",
    "\n",
    "###This manual thing needs to be automated by either langraph or by using agents framework##################\n",
    "\n",
    "def enrich_page(page_idx):\n",
    "    \n",
    "    print(\"Page Number\")\n",
    "    print(page_idx)\n",
    "    \n",
    "    page_text=document_dict_deserialized[page_idx]['text']\n",
    "    \n",
    "    ##Entity Extraction Enrichment\n",
    "    page_entity_lst_dict=extract_entities_per_page(page_text)\n",
    "    \n",
    "    print(\"Page Entity List Dictionary\")\n",
    "    print(page_entity_lst_dict)\n",
    "    \n",
    "    print(\"Page Entity List Dictionary Cleaned\")\n",
    "    #print(page_entity_lst_dict)\n",
    "    \n",
    "    page_entity_lst_json=json.loads(page_entity_lst_dict)\n",
    "    document_dict_deserialized[page_idx]['entities']=page_entity_lst_json\n",
    "    \n",
    "    ##Collate entity list per page into unique Entities\n",
    "    \n",
    "    entity_lst=entity_collector_per_page(page_entity_lst_json)\n",
    "    \n",
    "    ##Relationship Extraction Enrichment\n",
    "    page_relationship_lst_dict=extract_relationship_per_page(page_text,entity_lst)\n",
    "    \n",
    "    print(\"Page Relationship List Dictionary\")\n",
    "    print(page_entity_lst_dict)\n",
    "    \n",
    "    document_dict_deserialized[page_idx]['relationships']=page_relationship_lst_dict\n",
    "    print(\"Done for page \"+str(page_idx))\n",
    "    return page_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0a6784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Number\n",
      "565\n",
      "Page Entity List Dictionary\n",
      "[{\"entity\":\"kubectl\",\"description\":\"command line tool to interact with Kubernetes clusters\",\"category\":\"software\"},{\"entity\":\"kubeconfig file\",\"description\":\"configuration file used by kubectl to store cluster information\",\"category\":\"software\"},{\"entity\":\"clusters\",\"description\":\"list of available Kubernetes clusters in kubeconfig file\",\"category\":\"database\"},{\"entity\":\"users\",\"description\":\"list of users authorized to access Kubernetes clusters in kubeconfig file\",\"category\":\"database\"},{\"entity\":\"contexts\",\"description\":\"list of kubectl contexts, each representing a combination of cluster, user, and namespace\",\"category\":\"software\"},{\"entity\":\"cluster\",\"description\":\"Kubernetes cluster entity, containing information such as server URL and certificate authority\",\"category\":\"database\"},{\"entity\":\"context\",\"description\":\"kubectl context entity, defining a combination of cluster, user, and namespace\",\"category\":\"software\"},{\"entity\":\"namespace\",\"description\":\"logical grouping of resources within a Kubernetes cluster, can be specified in kubectl command or config file\",\"category\":\"database\"}]\n",
      "Page Entity List Dictionary Cleaned\n",
      "Entity:\n",
      "{'entity': 'kubectl', 'description': 'command line tool to interact with Kubernetes clusters', 'category': 'software'}\n",
      "Entity:\n",
      "{'entity': 'kubeconfig file', 'description': 'configuration file used by kubectl to store cluster information', 'category': 'software'}\n",
      "Entity:\n",
      "{'entity': 'clusters', 'description': 'list of available Kubernetes clusters in kubeconfig file', 'category': 'database'}\n",
      "Entity:\n",
      "{'entity': 'users', 'description': 'list of users authorized to access Kubernetes clusters in kubeconfig file', 'category': 'database'}\n",
      "Entity:\n",
      "{'entity': 'contexts', 'description': 'list of kubectl contexts, each representing a combination of cluster, user, and namespace', 'category': 'software'}\n",
      "Entity:\n",
      "{'entity': 'cluster', 'description': 'Kubernetes cluster entity, containing information such as server URL and certificate authority', 'category': 'database'}\n",
      "Entity:\n",
      "{'entity': 'context', 'description': 'kubectl context entity, defining a combination of cluster, user, and namespace', 'category': 'software'}\n",
      "Entity:\n",
      "{'entity': 'namespace', 'description': 'logical grouping of resources within a Kubernetes cluster, can be specified in kubectl command or config file', 'category': 'database'}\n",
      "Page Relationship List Dictionary\n",
      "[{\"entity\":\"kubectl\",\"description\":\"command line tool to interact with Kubernetes clusters\",\"category\":\"software\"},{\"entity\":\"kubeconfig file\",\"description\":\"configuration file used by kubectl to store cluster information\",\"category\":\"software\"},{\"entity\":\"clusters\",\"description\":\"list of available Kubernetes clusters in kubeconfig file\",\"category\":\"database\"},{\"entity\":\"users\",\"description\":\"list of users authorized to access Kubernetes clusters in kubeconfig file\",\"category\":\"database\"},{\"entity\":\"contexts\",\"description\":\"list of kubectl contexts, each representing a combination of cluster, user, and namespace\",\"category\":\"software\"},{\"entity\":\"cluster\",\"description\":\"Kubernetes cluster entity, containing information such as server URL and certificate authority\",\"category\":\"database\"},{\"entity\":\"context\",\"description\":\"kubectl context entity, defining a combination of cluster, user, and namespace\",\"category\":\"software\"},{\"entity\":\"namespace\",\"description\":\"logical grouping of resources within a Kubernetes cluster, can be specified in kubectl command or config file\",\"category\":\"database\"}]\n",
      "Done for page 565\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx in range(start_page_idx,end_page_index):\n",
    "   \n",
    "        enrich_page(idx)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c44d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Store data (serialize in a pickle) upto page 102\n",
    "with open('pdf_content_dict_page_final.pickle', 'wb') as handle:\n",
    "    pickle.dump(document_dict_deserialized, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5054dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "####The main contents of Kubernets In Action book is contained from page 87 to 565####\n",
    "####Thats why we will only take the sliced portion i.e. from page 87 to 565 of the pdf dictionary list#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc9ff2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialize)\n",
    "with open('pdf_content_dict_page_final.pickle', 'rb') as handle:\n",
    "    document_dict_deserialized = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize in a pickle) upto page 102\n",
    "with open('../pdf_enrichment/pdf_enriched_output/pdf_content_dict_page_stage1.pickle', 'wb') as handle:\n",
    "    pickle.dump(document_dict_deserialized, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_agent_poc",
   "language": "python",
   "name": "search_agent_poc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
