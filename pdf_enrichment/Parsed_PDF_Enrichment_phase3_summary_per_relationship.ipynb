{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8447b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Python venv: search_agent_poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4375a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This module will take the serialized dictionary got out of PDF Parsing ann try to extract\n",
    "##Semantic Knowldge like identifying \n",
    "## 1.Important Objects/Entities\n",
    "## 2.Deduplicate Entities\n",
    "## 3.Extracting Relations\n",
    "## 4.Extract the main Ideas/Topics around Each Page\n",
    "## 5.Link the different topics via diffrent entities/Objects\n",
    "## 6.Break down the document by pages instead of Chunks .\n",
    "## 7.If a page does not fit a chunk then chunk them extract information and then deduplicate the information across\n",
    "## the page.\n",
    "\n",
    "#Next Steps:\n",
    "## 5.Try to Seggregate the BigPDF on Sections.\n",
    "## 8.Try To Find Common Objects or ideas that link these sections.\n",
    "## 9.Try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15191ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "#rag_test_input_path='/home/matrix4284/MY_GEN_AI_PROJECTS/RAG/GraphRAG/graphrag-local-ollama/ragtest/input/'+file_name\n",
    "import os\n",
    "# importing shutil module\n",
    "import shutil\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import XMLOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01aaca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"llama3.1\"\n",
    "book_text=''\n",
    "page_text=''\n",
    "file_name='Kubernetes_in_action_text_only'\n",
    "extension='.txt'\n",
    "start_page_idx=0\n",
    "end_page_index=6\n",
    "#full_filename=file_name+'_'+str(page_idx)+extension\n",
    "#full_filename\n",
    "pdf_enrichment_output_dir='./pdf_enriched_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbcded8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Model for Prompt Tuning\n",
    "llm = ChatOllama(base_url=\"http://192.168.50.100:11434\",model=model_name,temperature=0.15)\n",
    "\n",
    "#embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\",model_kwargs = model_kwargs)\n",
    "\n",
    "##Define Vectorstore\n",
    "vectorstore = Chroma(embedding_function=embeddings, persist_directory=\"./chroma_kubernetes_in_action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a648bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialized data)\n",
    "#with open(pdf_enrichment_output_dir+'pdf_enriched_content_dict_stage2_phase2_final_fixed.pickle', 'rb') as handle:\n",
    "#    document_dict_deserialized = pickle.load(handle)\n",
    "    \n",
    "with open(pdf_enrichment_output_dir+'pdf_enriched_content_dict_phase2_relation_extraction_478_final.pickle', 'rb') as handle:\n",
    "    document_dict_deserialized = pickle.load(handle)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44433479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da4eb885",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_dict_deserialized_stage3=document_dict_deserialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20db8986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document_dict_deserialized_stage3[4]['relationships'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b004489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_dict_deserialized_stage3[460]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2f9e33f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 91)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:91\u001b[0;36m\u001b[0m\n\u001b[0;31m    {\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "##Definitions of Individual Enrichment Modules######\n",
    "\n",
    "def compose_summary_per_enity_relationships(relationships):\n",
    "    ####To Be started By Kaustav Mukherjee 23/08/2024########\n",
    "    \n",
    "    parser = XMLOutputParser()\n",
    "\n",
    "# Prompt\n",
    "\n",
    "\n",
    "    \n",
    "    print('Relationships From Input:')\n",
    "    print(relationships)\n",
    "               \n",
    "    ###Convert Relationships to string to load into json#######\n",
    "    #relationships_str=str(relationships)\n",
    "    \n",
    "    #print('Relationships String From Input:')\n",
    "    #print(relationships_str.replace(\"'\",'\"\"')\n",
    "    \n",
    "    #relationships=relationships.split('[')[1]\n",
    "    #relationships='['+relationships\n",
    "    \n",
    "    #relationships=relationships.split(']')[0]\n",
    "    #relationships=relationships+']'\n",
    "    \n",
    "    #print('Relationships after cleaing:')\n",
    "    #print(relationships)\n",
    "    \n",
    "    print('Relationships:')\n",
    "    print(str(relationships).replace(\"'\\\"\",'\"').replace'\\\"\\'','\"'))\n",
    "    \n",
    "    \n",
    "    relationships_json=json.loads()\n",
    "    \n",
    "    \n",
    "                                  \n",
    "    relationship_entity_sumary_map=[]\n",
    "    \n",
    "\n",
    "    #prompt = PromptTemplate(\n",
    "    #        template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker \\n\n",
    "    #        Machine Larning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "    #        You have to deeply study a set of highlights and a list of entities for a page using the below rules.\\n\n",
    "    #        1.For every highlight study it very deeply and extract one or more entities from each highlight\\n\n",
    "    #        that matches entities provided as a part of input.\\n\n",
    "    #        2.Once this is done represent each highlight along with the list of extracted entities in a json \\n\n",
    "    #        3.Collate all the individual json into a list of json and output the list.\\n\n",
    "    #        There is no need to mention any header statement before the list .\\n\n",
    "    #        Output the list of json and nothing else no headers no footers.\n",
    "    #        Here are the highlights: \\n\\n {context} \\n\\n and the entities: \\n\\n {entities} \\n\\n\"\"\",\n",
    "    #        input_variables=[\"context\",\"entities\"],\n",
    "    #        ###Introduced by Kaustav while experimenting with XMLParsers\n",
    "    #        #partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    #)\n",
    "    \n",
    "   \n",
    "        \n",
    "    for relation in relationships_json:\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker \\n\n",
    "            Machine Larning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study all the relationships for a page using the below rules.\\n\n",
    "            1.For every relation item try to get the source entity along with its description, \\n \n",
    "            destination entity along with description and relation description\\n\n",
    "            2.From the information of source entity ,destination entity  and \\n\n",
    "            description MUST come up with a sumary within 25 TO 50 words. \\n\n",
    "            3.Once this is done represent each summary in a json with tags named \\n\n",
    "            source,source_description,destination,destination_description,relation_description,summary_er.\\n\n",
    "            Output the json and nothing else no headers no footers.\\n\n",
    "            Here are the souce: \\n\\n {source}, the relation description: \\n\\n {relation} \\n\n",
    "            and the destination pod \\n\\n {destination} \\n\\n\"\"\",\n",
    "            input_variables=[\"source\",\"relation\",\"destination\"],)\n",
    "    \n",
    "    # Chain\n",
    "    #Commented out By Kaustav 20/08/2024 as str output parser is giving some unwanted text #####\n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        \n",
    "        print(\"source\")\n",
    "        print(relation['source_entity'])\n",
    "        print(\"destination\")\n",
    "        print(relation[\"destination_entity\"])\n",
    "        print(\"relation\")\n",
    "        print(relation[\"destination_entity\"])\n",
    "        \n",
    "        output = chain.invoke(\n",
    "        {\n",
    "                \"source\":relation['source_entity'],\n",
    "                \"relation\": relation['description'],\n",
    "                \"destination\": relation[\"destination_entity\"],\n",
    "        }\n",
    "        )\n",
    "        \n",
    "        #print('From LLM OUTPUT:')\n",
    "        #print(output)\n",
    "\n",
    "\n",
    "    #Trying out XML Output parser by Kautsva added 20/08/2024####\n",
    "    #chain = prompt | llm | parser\n",
    "    \n",
    "    ######Print highlights ###########\n",
    "    #print('HIghlights:')\n",
    "    #print(highlights)\n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "\n",
    "    \n",
    "    \n",
    "        json_output = output\n",
    "        #print('json_output from llm')\n",
    "        #print(json_output)\n",
    "        \n",
    "        if '[' in json_output:\n",
    "            json_output=json_output.split('[')[1]\n",
    "            json_output='['+json_output\n",
    "        elif '{' in json_output:\n",
    "            json_output=json_output.split('{')[1]\n",
    "            json_output='{'+json_output\n",
    "            json_output='['+json_output\n",
    "            \n",
    "        #page_output_json=json.loads(output)\n",
    "        \n",
    "    \n",
    "        #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "        #except:\n",
    "        #    json_output=json_output\n",
    "        #    cotinue\n",
    "        \n",
    "        #try:\n",
    "        if ']' in json_output:\n",
    "            json_output=reverse(json_output)\n",
    "            #print('Reversed JSON OUTPUT:')\n",
    "            #print(json_output)\n",
    "            json_output=json_output.split(']')[1]\n",
    "            json_output=reverse(json_output)\n",
    "            #json_output=json_output.rsplit(']')[-1]\n",
    "            #page_output_json=json.loads(output)\n",
    "            #print('JSON OUTPUT:')\n",
    "            #print(json_output)\n",
    "            json_output= json_output + ']'\n",
    "            #print('JSON OUTPUT:')\n",
    "            #print(json_output) \n",
    "        elif '}' in json_output:\n",
    "            json_output=json_output.split('}')[0]\n",
    "            json_output=json_output+'}'\n",
    "            json_output=json_output+']'\n",
    "        \n",
    "        \n",
    "        #print('cleaned json_output from llm')\n",
    "        #print(json_output)\n",
    "        \n",
    "        #except:\n",
    "        #    json_output=json_output\n",
    "        #    cotinue\n",
    "    \n",
    "        #print(\"JSON OUTPUT:\")\n",
    "        #print(json_output)\n",
    "        #page_output_json=json.loads(output)\n",
    "        #return page_output_json\n",
    "        relationship_entity_sumary_map.append(json_output) \n",
    "            \n",
    "    return relationship_entity_sumary_map \n",
    "    \n",
    "        #return page_output_json\n",
    "        #return output\n",
    "\n",
    "    \n",
    "# Function to reverse a string\n",
    "def reverse(string):\n",
    "    string = string[::-1]\n",
    "    return string\n",
    "    \n",
    "##Definitions of Individual Enrichment Modules######\n",
    "\n",
    "\n",
    "\n",
    "def entity_collector_per_page(entity_lst):\n",
    "\n",
    "    entities=[]\n",
    "    \n",
    "    for entity in entity_lst:\n",
    "        print(\"Entity:\")\n",
    "        print(entity)\n",
    "        entity_name=entity['entity']\n",
    "        entities.append(entity_name)\n",
    "    return list(set(entities))\n",
    "\n",
    "\n",
    "def enrich_page(page_idx):\n",
    "    \n",
    "    #print(\"Page Number\")\n",
    "    #print(page_idx)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################################Summary Extraction Enrichment###########################################\n",
    "    #page_text=document_dict_deserialized_stage3[page_idx]['text']\n",
    "    ##Summary Extraction Enrichment\n",
    "    #page_summary_txt=extract_summary_per_page(page_text)\n",
    "    \n",
    "    #print(\"Page Summary Text\")\n",
    "    #print(page_summary_txt)\n",
    "    \n",
    "    ######Add Summary as a part of pdf structured dictionary list in order summarization enrichment to data######### \n",
    "    #document_dict_deserialized_stage2[page_idx]['summary']=json.loads(page_summary_txt.strip())[\"summary\"]\n",
    "    \n",
    "    ################################################################################################################\n",
    "    \n",
    "    ######################################Highlight Extraction######################################################\n",
    "    \n",
    "    ##Summary Extraction Enrichment\n",
    "    #page_highlights_json_lst=extract_highlights_per_page('\"\"'+page_text+'\"\"')\n",
    "    \n",
    "    #print(\"Page Highlights\")\n",
    "    #print(page_highlights_json_lst)\n",
    "    \n",
    "    #page_highlights_json_lst=json.loads(page_highlights_json_lst)\n",
    "    \n",
    "    ######Add Summary as a part of pdf structured dictionary list in order summarization enrichment to data######### \n",
    "    #document_dict_deserialized_stage3[page_idx]['highlights']=page_highlights_json_lst\n",
    "    \n",
    "    ################################################################################################################\n",
    "    \n",
    "    \n",
    "    #################################Map Highlights with individual entities for each page#########################\n",
    "    relationships_=[]\n",
    "    \n",
    "    if document_dict_deserialized_stage3[page_idx]['relationships']:\n",
    "        relationships=document_dict_deserialized_stage3[page_idx]['relationships']\n",
    "    \n",
    "    \n",
    "    relationships=document_dict_deserialized_stage3[page_idx]['relationships']\n",
    "    \n",
    "     \n",
    " \n",
    "    \n",
    "    #for relationship in relationships:\n",
    "    #    relationships_summary_lst.append(hightlight['highlight'])\n",
    "    \n",
    "    \n",
    "    ##Highlight to highlight mapping Enrichment\n",
    "    entity_relation_summ_text=compose_summary_per_enity_relationships(relationships)\n",
    "    \n",
    "    if entity_relation_summ_text is not None:\n",
    "        \n",
    "        \n",
    "        #entity_relation_summ_json=json.loads(entity_relation_summ_text)\n",
    "        #print(\"Summarization of Entity,Relation,Tuple\")\n",
    "        #print(entity_relation_summ_text)\n",
    "        #print(type(entity_relation_summ_text))\n",
    "        #print(entity_relation_summ_json)\n",
    "    \n",
    "    ######Add Summary as a part of pdf structured dictionary list in order summarization enrichment to data######### \n",
    "        document_dict_deserialized_stage3[page_idx]['summary_rel']=entity_relation_summ_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5465a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationships From Input:\n",
      "[{'source_entity': '\"Network namespace\"', 'description': 'manages', 'destination_entity': '\"Linux namespaces\"'}, {'source_entity': '\"Network namespace\"', 'description': 'allocates', 'destination_entity': '\"Port space\"'}, {'source_entity': '\"UTS namespace\"', 'description': 'manages', 'destination_entity': '\"Linux namespaces\"'}, {'source_entity': '\"Kubernetes\"', 'description': 'orchestrates', 'destination_entity': '\"Pods\"'}, {'source_entity': '\"Docker\"', 'description': 'runs', 'destination_entity': '\"Containers\"'}, {'source_entity': '\"Kubernetes\"', 'description': 'manages', 'destination_entity': '\"Volume\"'}, {'source_entity': '\"PID namespace\"', 'description': 'manages', 'destination_entity': '\"Linux namespaces\"'}, {'source_entity': '\"IPC namespace\"', 'description': 'manages', 'destination_entity': '\"Linux namespaces\"'}]\n",
      "Relationships:\n",
      "[{'source_entity': \"Network namespace\"', 'description': 'manages', 'destination_entity': \"Linux namespaces\"'}, {'source_entity': \"Network namespace\"', 'description': 'allocates', 'destination_entity': \"Port space\"'}, {'source_entity': \"UTS namespace\"', 'description': 'manages', 'destination_entity': \"Linux namespaces\"'}, {'source_entity': \"Kubernetes\"', 'description': 'orchestrates', 'destination_entity': \"Pods\"'}, {'source_entity': \"Docker\"', 'description': 'runs', 'destination_entity': \"Containers\"'}, {'source_entity': \"Kubernetes\"', 'description': 'manages', 'destination_entity': \"Volume\"'}, {'source_entity': \"PID namespace\"', 'description': 'manages', 'destination_entity': \"Linux namespaces\"'}, {'source_entity': \"IPC namespace\"', 'description': 'manages', 'destination_entity': \"Linux namespaces\"'}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "loads() missing 1 required positional argument: 's'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_page_idx,end_page_index):\n\u001b[0;32m----> 3\u001b[0m         \u001b[43menrich_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone for page number:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(idx))\n",
      "Cell \u001b[0;32mIn[25], line 249\u001b[0m, in \u001b[0;36menrich_page\u001b[0;34m(page_idx)\u001b[0m\n\u001b[1;32m    239\u001b[0m relationships\u001b[38;5;241m=\u001b[39mdocument_dict_deserialized_stage3[page_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelationships\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m#for relationship in relationships:\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m#    relationships_summary_lst.append(hightlight['highlight'])\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m##Highlight to highlight mapping Enrichment\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m entity_relation_summ_text\u001b[38;5;241m=\u001b[39m\u001b[43mcompose_summary_per_enity_relationships\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelationships\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entity_relation_summ_text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     \n\u001b[1;32m    253\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m \n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m######Add Summary as a part of pdf structured dictionary list in order summarization enrichment to data######### \u001b[39;00m\n\u001b[1;32m    261\u001b[0m     document_dict_deserialized_stage3[page_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary_rel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mentity_relation_summ_text\n",
      "Cell \u001b[0;32mIn[25], line 34\u001b[0m, in \u001b[0;36mcompose_summary_per_enity_relationships\u001b[0;34m(relationships)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRelationships:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(relationships)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 34\u001b[0m relationships_json\u001b[38;5;241m=\u001b[39m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m relationship_entity_sumary_map\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#prompt = PromptTemplate(\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#        template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker \\n\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#        Machine Larning,Generative AI,Natural Language Understanding and Computer Vision.\\n\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#        #partial_variables={\"format_instructions\": parser.get_format_instructions()},\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: loads() missing 1 required positional argument: 's'"
     ]
    }
   ],
   "source": [
    "for idx in range(start_page_idx,end_page_index):\n",
    "   \n",
    "        enrich_page(idx)\n",
    "        print(\"Done for page number:\"+str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21185e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 253,\n",
       " 'img_cnt': 0,\n",
       " 'img_npy_lst': [],\n",
       " 'text': '221\\nUsing Secrets to pass sensitive data to containers\\nTESTING WHETHER NGINX IS USING THE CERT AND KEY FROM THE SECRET\\nOnce the pod is running, you can see if it’s serving HTTPS traffic by opening a port-\\nforward tunnel to the pod’s port 443 and using it to send a request to the server\\nwith curl: \\n$ kubectl port-forward fortune-https 8443:443 &\\nForwarding from 127.0.0.1:8443 -> 443\\nForwarding from [::1]:8443 -> 443\\n$ curl https://localhost:8443 -k\\nIf you configured the server properly, you should get a response. You can check the\\nserver’s certificate to see if it matches the one you generated earlier. This can also be\\ndone with curl by turning on verbose logging using the -v option, as shown in the fol-\\nlowing listing.\\n$ curl https://localhost:8443 -k -v\\n* About to connect() to localhost port 8443 (#0)\\n*   Trying ::1...\\n* Connected to localhost (::1) port 8443 (#0)\\n* Initializing NSS with certpath: sql:/etc/pki/nssdb\\n* skipping SSL peer certificate verification\\n* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\\n* Server certificate:\\n*   subject: CN=www.kubia-example.com          \\n*   start date: aug 16 18:43:13 2016 GMT       \\n*   expire date: aug 14 18:43:13 2026 GMT      \\n*   common name: www.kubia-example.com         \\n*   issuer: CN=www.kubia-example.com           \\nUNDERSTANDING SECRET VOLUMES ARE STORED IN MEMORY\\nYou successfully delivered your certificate and private key to your container by mount-\\ning a secret volume in its directory tree at /etc/nginx/certs. The secret volume uses\\nan in-memory filesystem (tmpfs) for the Secret files. You can see this if you list mounts\\nin the container:\\n$ kubectl exec fortune-https -c web-server -- mount | grep certs\\ntmpfs on /etc/nginx/certs type tmpfs (ro,relatime) \\nBecause tmpfs is used, the sensitive data stored in the Secret is never written to disk,\\nwhere it could be compromised. \\nEXPOSING A SECRET’S ENTRIES THROUGH ENVIRONMENT VARIABLES\\nInstead of using a volume, you could also have exposed individual entries from the\\nsecret as environment variables, the way you did with the sleep-interval entry from\\nthe ConfigMap. For example, if you wanted to expose the foo key from your Secret as\\nenvironment variable FOO_SECRET, you’d add the snippet from the following listing to\\nthe container definition.\\nListing 7.26\\nDisplaying the server certificate sent by Nginx\\nThe certificate \\nmatches the one you \\ncreated and stored \\nin the Secret.\\n \\n',\n",
       " 'tables': [],\n",
       " 'entities': [{'entity': 'Secrets',\n",
       "   'description': 'pass sensitive data to containers',\n",
       "   'category': 'software'},\n",
       "  {'entity': 'Nginx', 'description': 'web server', 'category': 'application'},\n",
       "  {'entity': 'kubectl',\n",
       "   'description': 'command-line tool for managing Kubernetes clusters',\n",
       "   'category': 'software'},\n",
       "  {'entity': 'port-forward',\n",
       "   'description': \"tunneling traffic from a local port to a pod's port\",\n",
       "   'category': 'process'},\n",
       "  {'entity': 'curl',\n",
       "   'description': 'command-line tool for transferring data with URLs',\n",
       "   'category': 'software'},\n",
       "  {'entity': 'HTTPS',\n",
       "   'description': 'secure communication protocol',\n",
       "   'category': 'protocol'},\n",
       "  {'entity': 'TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384',\n",
       "   'description': 'encryption algorithm',\n",
       "   'category': 'algorithm'},\n",
       "  {'entity': 'tmpfs',\n",
       "   'description': 'in-memory filesystem',\n",
       "   'category': 'filesystem'},\n",
       "  {'entity': 'Secret volume',\n",
       "   'description': \"mounting a Secret in a container's directory tree\",\n",
       "   'category': 'process'},\n",
       "  {'entity': 'ConfigMap',\n",
       "   'description': 'configuration data stored as key-value pairs',\n",
       "   'category': 'software'},\n",
       "  {'entity': 'environment variables',\n",
       "   'description': 'exposing individual entries from a Secret as environment variables',\n",
       "   'category': 'process'},\n",
       "  {'entity': 'FOO_SECRET',\n",
       "   'description': 'environment variable exposing the foo key from a Secret',\n",
       "   'category': 'variable'}],\n",
       " 'relationships': [{'source_entity': '\"kubectl\"',\n",
       "   'description': 'used to manage and deploy applications',\n",
       "   'destination_entity': '\"Nginx\"'},\n",
       "  {'source_entity': '\"environment variables\"',\n",
       "   'description': 'configured for Nginx deployment',\n",
       "   'destination_entity': '\"Nginx\"'},\n",
       "  {'source_entity': '\"HTTPS\"',\n",
       "   'description': 'enabled for secure communication',\n",
       "   'destination_entity': '\"Nginx\"'},\n",
       "  {'source_entity': '\"tmpfs\"',\n",
       "   'description': 'mounted as a temporary file system',\n",
       "   'destination_entity': '\"Secret volume\"'},\n",
       "  {'source_entity': '\"FOO_SECRET\"',\n",
       "   'description': 'stored in a secret volume',\n",
       "   'destination_entity': '\"Secret volume\"'},\n",
       "  {'source_entity': '\"port-forward\"',\n",
       "   'description': 'enabled for external access',\n",
       "   'destination_entity': '\"Nginx\"'},\n",
       "  {'source_entity': '\"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\"',\n",
       "   'description': 'configured as a TLS protocol',\n",
       "   'destination_entity': '\"HTTPS\"'},\n",
       "  {'source_entity': '\"Secrets\"',\n",
       "   'description': 'managed and stored securely',\n",
       "   'destination_entity': '\"FOO_SECRET\"'},\n",
       "  {'source_entity': '\"curl\"',\n",
       "   'description': 'used to test and verify',\n",
       "   'destination_entity': '\"Nginx\"'},\n",
       "  {'source_entity': '\"ConfigMap\"',\n",
       "   'description': 'configured for Nginx deployment',\n",
       "   'destination_entity': '\"Nginx\"'}]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_dict_deserialized_stage3[164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c44d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Store data (serialize in a pickle) upto page 102\n",
    "with open('./pdf_enriched_output/pdf_enriched_content_dict_stage2_phase3_final_163.pickle', 'wb') as handle:\n",
    "    pickle.dump(document_dict_deserialized_stage3, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2199ccda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_agent_poc",
   "language": "python",
   "name": "search_agent_poc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
