<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 2000px;
                 height: 800px;
                 background-color: #ffffff;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             
             #loadingBar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width: 2000px;
                 height: 800px;
                 background-color:rgba(200,200,200,0.8);
                 -webkit-transition: all 0.5s ease;
                 -moz-transition: all 0.5s ease;
                 -ms-transition: all 0.5s ease;
                 -o-transition: all 0.5s ease;
                 transition: all 0.5s ease;
                 opacity:1;
             }

             #bar {
                 position:absolute;
                 top:0px;
                 left:0px;
                 width:20px;
                 height:20px;
                 margin:auto auto auto auto;
                 border-radius:11px;
                 border:2px solid rgba(30,30,30,0.05);
                 background: rgb(0, 173, 246); /* Old browsers */
                 box-shadow: 2px 0px 4px rgba(0,0,0,0.4);
             }

             #border {
                 position:absolute;
                 top:10px;
                 left:10px;
                 width:500px;
                 height:23px;
                 margin:auto auto auto auto;
                 box-shadow: 0px 0px 4px rgba(0,0,0,0.2);
                 border-radius:10px;
             }

             #text {
                 position:absolute;
                 top:8px;
                 left:530px;
                 width:30px;
                 height:50px;
                 margin:auto auto auto auto;
                 font-size:22px;
                 color: #000000;
             }

             div.outerBorder {
                 position:relative;
                 top:400px;
                 width:600px;
                 height:44px;
                 margin:auto auto auto auto;
                 border:8px solid rgba(0,0,0,0.1);
                 background: rgb(252,252,252); /* Old browsers */
                 background: -moz-linear-gradient(top,  rgba(252,252,252,1) 0%, rgba(237,237,237,1) 100%); /* FF3.6+ */
                 background: -webkit-gradient(linear, left top, left bottom, color-stop(0%,rgba(252,252,252,1)), color-stop(100%,rgba(237,237,237,1))); /* Chrome,Safari4+ */
                 background: -webkit-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Chrome10+,Safari5.1+ */
                 background: -o-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* Opera 11.10+ */
                 background: -ms-linear-gradient(top,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* IE10+ */
                 background: linear-gradient(to bottom,  rgba(252,252,252,1) 0%,rgba(237,237,237,1) 100%); /* W3C */
                 filter: progid:DXImageTransform.Microsoft.gradient( startColorstr='#fcfcfc', endColorstr='#ededed',GradientType=0 ); /* IE6-9 */
                 border-radius:72px;
                 box-shadow: 0px 0px 10px rgba(0,0,0,0.2);
             }
             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
            <div id="loadingBar">
              <div class="outerBorder">
                <div id="text">0%</div>
                <div id="border">
                  <div id="bar"></div>
                </div>
              </div>
            </div>
        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"color": "red", "id": 1, "label": "1.  INTRODUCTION\n", "shape": "dot", "size": 10, "title": "1.  INTRODUCTION\n"}, {"color": "blue", "id": "Topic_2", "label": "IN  A  previous  paper1  the  entropy  and  redundancy  of  a  language  have\n", "shape": "dot", "size": 10, "title": "IN  A  previous  paper1  the  entropy  and  redundancy  of  a  language  have\n"}, {"color": "blue", "id": "Topic_3", "label": "=  ~Z  p(bi,j)  lo\u0026pQi.j)  +  Z p(^  log  #(6V)\n", "shape": "dot", "size": 10, "title": "=  ~Z  p(bi,j)  lo\u0026pQi.j)  +  Z p(^  log  #(6V)\n"}, {"color": "blue", "id": "Topic_4", "label": "1:\n", "shape": "dot", "size": 10, "title": "1:\n"}, {"color": "blue", "id": "Topic_5", "label": "in which: bi is a block of  _Y-1  letters  [(Ar-l)-gram]\nj  is  an  arbitrary  letter  following  hi\np(bi  ,  j)  is  the  probability  of  the  JV-gram  6\u00bb , j\npbi(j)  is  the  conditional  probability  of  letter j  after  the  block  b,,\nand  is  given  by  p(b{  ,  j)/p(bf).\nThe equation  (1)  can be interpreted as measuring the average uncertainty\n(conditional entropy)  of the next letter /\u0027 when the preceding  _V-1  letters are\nknown.  As  .V  is  increased,  /\u0027V  includes  longer  and  longer  range  statistics\nand  the  entropy,  77,  is  given  by  the  limiting  value  of  FN  as  N  \u2014  -\u003e ^  :\n", "shape": "dot", "size": 10, "title": "in which: bi is a block of  _Y-1  letters  [(Ar-l)-gram]\nj  is  an  arbitrary  letter  following  hi\np(bi  ,  j)  is  the  probability  of  the  JV-gram  6\u00bb , j\npbi(j)  is  the  conditional  probability  of  letter j  after  the  block  b,,\nand  is  given  by  p(b{  ,  j)/p(bf).\nThe equation  (1)  can be interpreted as measuring the average uncertainty\n(conditional entropy)  of the next letter /\u0027 when the preceding  _V-1  letters are\nknown.  As  .V  is  increased,  /\u0027V  includes  longer  and  longer  range  statistics\nand  the  entropy,  77,  is  given  by  the  limiting  value  of  FN  as  N  \u2014  -\u003e ^  :\n"}, {"color": "blue", "id": "Topic_6", "label": "H  =  Lim  TV . \n", "shape": "dot", "size": 10, "title": "H  =  Lim  TV . \n"}, {"color": "blue", "id": "Topic_7", "label": "(2)\nThe  T-gram  entropies  F.\\  for  small  values  of  N  can  be  calculated  from\nstandard  tables  of  letter,  digram  and  trigram  frequencies.2  If  spaces  and\npunctuation  are  ignored  we  have  a  twenty-six  letter  alphabet  and  Ff,  may\nbe taken  (by definition)  to  be  log-2 26,  or 4.7 bits  per letter.  7\u003c\\ involves letter\nfrequencies  and  is  given  by\n", "shape": "dot", "size": 10, "title": "(2)\nThe  T-gram  entropies  F.\\  for  small  values  of  N  can  be  calculated  from\nstandard  tables  of  letter,  digram  and  trigram  frequencies.2  If  spaces  and\npunctuation  are  ignored  we  have  a  twenty-six  letter  alphabet  and  Ff,  may\nbe taken  (by definition)  to  be  log-2 26,  or 4.7 bits  per letter.  7\u003c\\ involves letter\nfrequencies  and  is  given  by\n"}, {"color": "blue", "id": "Topic_8", "label": "Fi  =  \u2014 X)  p(i)  Iog2  p(i)  =  4.14  bits  per  letter. \nt\u2014 i\nThe  digram  approximation  7^  gives  the  result\n(3)\n", "shape": "dot", "size": 10, "title": "Fi  =  \u2014 X)  p(i)  Iog2  p(i)  =  4.14  bits  per  letter. \nt\u2014 i\nThe  digram  approximation  7^  gives  the  result\n(3)\n"}, {"color": "blue", "id": "Topic_9", "label": "=  7.70  -  4.14  =  3.56 bits per letter.\n", "shape": "dot", "size": 10, "title": "=  7.70  -  4.14  =  3.56 bits per letter.\n"}, {"color": "blue", "id": "Topic_10", "label": "52 \nTIIK  BELL  SYSTEM  TECHNICAL  JOURNAL,  JANUARY  1951\n", "shape": "dot", "size": 10, "title": "52 \nTIIK  BELL  SYSTEM  TECHNICAL  JOURNAL,  JANUARY  1951\n"}, {"color": "blue", "id": "Topic_11", "label": "The  trigram  entropy  is  given  by\nF-i  =  -  Z  p(i,j,k)  log,  p,-j(k)\nX p(i, j, k)  log, p(i, j,  k)\n(*, j)  log, p(i, j)\n(5)\n= 11.0 - 7,7 = 3.3\nIn  this  calculation  the  trigram  table2  used  did  not  take  into  account  tri-\ngrams bridging two words, such as WOW and OWO in TWO WORDS. To\ncompensate  partially  for  this  omission,  corrected  trigram  probabilities  p(it\nj, k) were obtained from the probabilities p\u0027(i,j, k) of the table by the follow-\ning  rough  formula:\n, J,\n= ~ p\u0027(i,j, k) + ~  r(f)p(j, k)\nwhere r(i)  is  the probability  of letter i as the terminal letter of  a word and\ns(k)  is  the  probability  of  k  as  an  initial  letter.  Thus  the  trigrams  within\nwords (an average of 2.5 per word) are counted according to the table; the\nbridging  trigrams  (one  of  each  type  per  word)  are  counted  approximately\nby assuming independence of the terminal letter of one word and the initial\ndigram in the next or vice versa. Because of the approximations involved\nhere,  and  also  because  of  the  fact  that  the  sampling  error  in  identifying\nprobability with  sample frequency is more serious,  the value  of  F;i  is  less\nreliable than the previous numbers.\nSince tables of .Y-gram frequencies were not available for ,Y  \u003e  3,  F^ ,  Fj ,\netc.  could not be calculated  in  the  same way. However,  word  frequencies\nhave been  tabulated3  and can  be used  to obtain  a  further  approximation.\nFigure  1  is  a  plot  on  log-log  paper  of  the  probabilities  of  words  against\nfrequency  rank. The most  frequent  English  word  \"the\"  has  a probability\n.071  and this is plotted against 1. The next most frequent word \"of\" has a\nprobability  of  .034  and  is  plotted  against  2,  etc.  Using  logarithmic  scales\nboth  for  probability  and  rank,  the  curve  is  approximately  a  straight  line\nwith slope \u2014 1 ; thus, if pn is the probability of the rath most frequent word,\nwe have, roughly\nZipf4 has pointed out that this type of formula, pn  =  k/n, gives a rather good\napproximation  to  the  word probabilities  in  many  different  languages.  The\n3 G.  De\\vcy,  \"Relative  Frequency  of  English  Speech  Sounds,\"  Harvard  University\n", "shape": "dot", "size": 10, "title": "The  trigram  entropy  is  given  by\nF-i  =  -  Z  p(i,j,k)  log,  p,-j(k)\nX p(i, j, k)  log, p(i, j,  k)\n(*, j)  log, p(i, j)\n(5)\n= 11.0 - 7,7 = 3.3\nIn  this  calculation  the  trigram  table2  used  did  not  take  into  account  tri-\ngrams bridging two words, such as WOW and OWO in TWO WORDS. To\ncompensate  partially  for  this  omission,  corrected  trigram  probabilities  p(it\nj, k) were obtained from the probabilities p\u0027(i,j, k) of the table by the follow-\ning  rough  formula:\n, J,\n= ~ p\u0027(i,j, k) + ~  r(f)p(j, k)\nwhere r(i)  is  the probability  of letter i as the terminal letter of  a word and\ns(k)  is  the  probability  of  k  as  an  initial  letter.  Thus  the  trigrams  within\nwords (an average of 2.5 per word) are counted according to the table; the\nbridging  trigrams  (one  of  each  type  per  word)  are  counted  approximately\nby assuming independence of the terminal letter of one word and the initial\ndigram in the next or vice versa. Because of the approximations involved\nhere,  and  also  because  of  the  fact  that  the  sampling  error  in  identifying\nprobability with  sample frequency is more serious,  the value  of  F;i  is  less\nreliable than the previous numbers.\nSince tables of .Y-gram frequencies were not available for ,Y  \u003e  3,  F^ ,  Fj ,\netc.  could not be calculated  in  the  same way. However,  word  frequencies\nhave been  tabulated3  and can  be used  to obtain  a  further  approximation.\nFigure  1  is  a  plot  on  log-log  paper  of  the  probabilities  of  words  against\nfrequency  rank. The most  frequent  English  word  \"the\"  has  a probability\n.071  and this is plotted against 1. The next most frequent word \"of\" has a\nprobability  of  .034  and  is  plotted  against  2,  etc.  Using  logarithmic  scales\nboth  for  probability  and  rank,  the  curve  is  approximately  a  straight  line\nwith slope \u2014 1 ; thus, if pn is the probability of the rath most frequent word,\nwe have, roughly\nZipf4 has pointed out that this type of formula, pn  =  k/n, gives a rather good\napproximation  to  the  word probabilities  in  many  different  languages.  The\n3 G.  De\\vcy,  \"Relative  Frequency  of  English  Speech  Sounds,\"  Harvard  University\n"}, {"color": "blue", "id": "Topic_12", "label": "Press,  1923.\nPress,  1949.\n", "shape": "dot", "size": 10, "title": "Press,  1923.\nPress,  1949.\n"}, {"color": "blue", "id": "Topic_13", "label": "t ke unity,  while 2^ .!/\u00ab  is  infinite.  If we assume  (in  the absence of any\nh  tier estimate)  that  the formula  pn  =  .\\/n  holds out  to  the  n  at  which  the\n", "shape": "dot", "size": 10, "title": "t ke unity,  while 2^ .!/\u00ab  is  infinite.  If we assume  (in  the absence of any\nh  tier estimate)  that  the formula  pn  =  .\\/n  holds out  to  the  n  at  which  the\n"}, {"color": "blue", "id": "Topic_14", "label": "0.01\n0.001\n", "shape": "dot", "size": 10, "title": "0.01\n0.001\n"}, {"color": "blue", "id": "Topic_15", "label": "0.00001\n-SAY\nX\n", "shape": "dot", "size": 10, "title": "0.00001\n-SAY\nX\n"}, {"color": "blue", "id": "Topic_16", "label": "Fig.  1\u2014Relative  frequency against  rank  for  English  words.\n", "shape": "dot", "size": 10, "title": "Fig.  1\u2014Relative  frequency against  rank  for  English  words.\n"}, {"color": "blue", "id": "Topic_17", "label": "total  probability  is  unity,  and  that  pn  \u2014  0  for  larger  n,  we  find  that  the\ncritical  n  is  the word  of rank  8,727.  The entropy  is  then:\n", "shape": "dot", "size": 10, "title": "total  probability  is  unity,  and  that  pn  \u2014  0  for  larger  n,  we  find  that  the\ncritical  n  is  the word  of rank  8,727.  The entropy  is  then:\n"}, {"color": "blue", "id": "Topic_18", "label": "\u2014 /!  pn  Iog2  pn  =  11.82  bits  per  word, \ni\n(7)\nor 11,82/4.5  =  2.62  bits per letter since  the average word  length  in  English\nis 4.5  letters.  One  might  be  tempted  to  identify  this  value  with  F4.B,  but\nactually  the  ordinate  of  the  /*\u0027\u003e\u2022  curve  at  N  =  4.5  will  be  above  this  value.\nThe reason  is  that F4 or  F;,  involves  groups of four or five letters regardless\nof word division.  A  word  is  a  cohesive  group  of letters  with  strong  internal\n", "shape": "dot", "size": 10, "title": "\u2014 /!  pn  Iog2  pn  =  11.82  bits  per  word, \ni\n(7)\nor 11,82/4.5  =  2.62  bits per letter since  the average word  length  in  English\nis 4.5  letters.  One  might  be  tempted  to  identify  this  value  with  F4.B,  but\nactually  the  ordinate  of  the  /*\u0027\u003e\u2022  curve  at  N  =  4.5  will  be  above  this  value.\nThe reason  is  that F4 or  F;,  involves  groups of four or five letters regardless\nof word division.  A  word  is  a  cohesive  group  of letters  with  strong  internal\n"}, {"color": "blue", "id": "Topic_19", "label": "statistical  influences,  and  consequently  the  ]V-grams  within  words  are \nrestricted  than  those which bridge  words.  The  effect  of  this  is  that we  hav(\nobtained,  in  2.62  bits per  letter,  an  estimate  which  corresponds  more  nearly\nto,  say,  F!,  or  F$.\nA  similar  set  of  calculations  was  carried  out  including  the  space  as  aj\nadditional  letter,  giving  a  27  letter  alphabet.  The  results  of  both  26-  anj\n27-letter  calculations  are  summarized  below:\n.\n", "shape": "dot", "size": 10, "title": "statistical  influences,  and  consequently  the  ]V-grams  within  words  are \nrestricted  than  those which bridge  words.  The  effect  of  this  is  that we  hav(\nobtained,  in  2.62  bits per  letter,  an  estimate  which  corresponds  more  nearly\nto,  say,  F!,  or  F$.\nA  similar  set  of  calculations  was  carried  out  including  the  space  as  aj\nadditional  letter,  giving  a  27  letter  alphabet.  The  results  of  both  26-  anj\n27-letter  calculations  are  summarized  below:\n.\n"}, {"color": "blue", "id": "Topic_20", "label": "3.56\n3.32\n", "shape": "dot", "size": 10, "title": "3.56\n3.32\n"}, {"color": "blue", "id": "Topic_21", "label": "The estimate of 2.3 for 7;\ns, alluded to above, was found by several methods\none  of  which  is  the  extrapolation  of  the  26-letter  series  above  out  to  that\npoint.  Since  the  space  symbol  is  almost  completely  redundant  when  se-\nquences  of  one  or more  words are  involved,  the values  of  FN  in  the  27-letter\ncase will be \u2014- or .818 of FN for the 26-letter alphabet when  TV is reasonably\n4.5\n", "shape": "dot", "size": 10, "title": "The estimate of 2.3 for 7;\ns, alluded to above, was found by several methods\none  of  which  is  the  extrapolation  of  the  26-letter  series  above  out  to  that\npoint.  Since  the  space  symbol  is  almost  completely  redundant  when  se-\nquences  of  one  or more  words are  involved,  the values  of  FN  in  the  27-letter\ncase will be \u2014- or .818 of FN for the 26-letter alphabet when  TV is reasonably\n4.5\n"}, {"color": "blue", "id": "Topic_22", "label": "large.\n3.  PREDICTION  OF  ENGLISH\nThe  new  method  of  estimating  entropy  exploits  the  fact  that  anyone\nspeaking  a  language  possesses,  implicitly,  an  enormous  knowledge  of  the\nstatistics  of  the  language.  Familiarity  with  the  words,  idioms,  cliches and\ngrammar enables him to fill in missing or incorrect letters in proof-reading,\nor to complete an unfinished phrase in conversation. An experimental  demon-\nstration of  the extent to which  English  is predictable can be given as  follows;\nSelect a  short passage unfamiliar  to  the person who  is  to do  the  predicting,\nHe is then asked  to guess the first letter in the passage.  If the guess is  correct\nhe  is  so  informed,  and  proceeds  to  guess  the  second  letter.  If  not,  he  is  tolc\nthe  correct  first  letter  and  proceeds  to  his  next  guess.  This  is  continuec\nthrough  the text.  As the experiment progresses,  the subject writes down th\ncorrect  text up  to  the current point for use in predicting future letters.  Th\nresult  of  a  typical  experiment  of  this  type  is  given  below.  Spaces  were in-\ncluded  as  an  additional letter,  making  a  27  letter  alphabet.  The  first  line  i-\nthe  original  text;  the  second  line  contains  a  dash  for  each  letter  correctly\nguessed.  In  the  case  of  incorrect  guesses  the  correct  letter  is  copied  in th\nsecond  line.\nROD \n(1) THE ROOM WAS NOT VERY LIGHT A SMALL OBLONG \nNOT-? \n(2) \n(1) READING LAMP ON THE DESK SHED GLOW ON\n(2) REA \nSHED-GLO--0--\nD \n(1) POLISHED WOOD BUT LESS ON THE SHABBY RED CARPET\n(2) P-L-S \n0---BU--L-S-0 \nRE-C\nSH \nSM \nOBL\n0 \n1 \n..,\ntotal of  129  letters,  89  or  69%  were  guessed  correctly.  The  errors,  as\n.,  , e  expected,  occur  most  frequently  at  the  beginning  of  words  and\nW\u003cil  hies where the line of thought has more possibility of branching out.  It\nS^\u0027  ht be thought that the second  line in  (8),  which we will call the reduced\ncontains much less information than the first. Actually,  both  lines con-\n.\u0027  ..  game  information  in  the  sense  that  it  is  possible,  at  least  in  prin-\nto recover the first line from  the second.  To accomplish  this we need\n\u2022  1 \nidentical  twin  of  the  individual  who  produced  the  sequence.  The  twin\n,  kO must be mathematically, not just biologically identical)  will respond in\n,  same way  when  faced  with  the  same  problem.  Suppose,  no\\v,  we  have\ni \nthe reduced  text of  (8).  We  ask  the  twin  to  guess  the  passage.  At  each\noint we will know whether his guess is correct, since he is guessing the same\nthe first twin and  the presence  of a  dash  in  the  reduced  text  corresponds\nto a correct guess. The letters he guesses wrong are also available,  so that at\neach  stage  he  can  be  supplied  with  precisely  the  same  information  the  first\ntwin  had  available.\n", "shape": "dot", "size": 10, "title": "large.\n3.  PREDICTION  OF  ENGLISH\nThe  new  method  of  estimating  entropy  exploits  the  fact  that  anyone\nspeaking  a  language  possesses,  implicitly,  an  enormous  knowledge  of  the\nstatistics  of  the  language.  Familiarity  with  the  words,  idioms,  cliches and\ngrammar enables him to fill in missing or incorrect letters in proof-reading,\nor to complete an unfinished phrase in conversation. An experimental  demon-\nstration of  the extent to which  English  is predictable can be given as  follows;\nSelect a  short passage unfamiliar  to  the person who  is  to do  the  predicting,\nHe is then asked  to guess the first letter in the passage.  If the guess is  correct\nhe  is  so  informed,  and  proceeds  to  guess  the  second  letter.  If  not,  he  is  tolc\nthe  correct  first  letter  and  proceeds  to  his  next  guess.  This  is  continuec\nthrough  the text.  As the experiment progresses,  the subject writes down th\ncorrect  text up  to  the current point for use in predicting future letters.  Th\nresult  of  a  typical  experiment  of  this  type  is  given  below.  Spaces  were in-\ncluded  as  an  additional letter,  making  a  27  letter  alphabet.  The  first  line  i-\nthe  original  text;  the  second  line  contains  a  dash  for  each  letter  correctly\nguessed.  In  the  case  of  incorrect  guesses  the  correct  letter  is  copied  in th\nsecond  line.\nROD \n(1) THE ROOM WAS NOT VERY LIGHT A SMALL OBLONG \nNOT-? \n(2) \n(1) READING LAMP ON THE DESK SHED GLOW ON\n(2) REA \nSHED-GLO--0--\nD \n(1) POLISHED WOOD BUT LESS ON THE SHABBY RED CARPET\n(2) P-L-S \n0---BU--L-S-0 \nRE-C\nSH \nSM \nOBL\n0 \n1 \n..,\ntotal of  129  letters,  89  or  69%  were  guessed  correctly.  The  errors,  as\n.,  , e  expected,  occur  most  frequently  at  the  beginning  of  words  and\nW\u003cil  hies where the line of thought has more possibility of branching out.  It\nS^\u0027  ht be thought that the second  line in  (8),  which we will call the reduced\ncontains much less information than the first. Actually,  both  lines con-\n.\u0027  ..  game  information  in  the  sense  that  it  is  possible,  at  least  in  prin-\nto recover the first line from  the second.  To accomplish  this we need\n\u2022  1 \nidentical  twin  of  the  individual  who  produced  the  sequence.  The  twin\n,  kO must be mathematically, not just biologically identical)  will respond in\n,  same way  when  faced  with  the  same  problem.  Suppose,  no\\v,  we  have\ni \nthe reduced  text of  (8).  We  ask  the  twin  to  guess  the  passage.  At  each\noint we will know whether his guess is correct, since he is guessing the same\nthe first twin and  the presence  of a  dash  in  the  reduced  text  corresponds\nto a correct guess. The letters he guesses wrong are also available,  so that at\neach  stage  he  can  be  supplied  with  precisely  the  same  information  the  first\ntwin  had  available.\n"}, {"color": "blue", "id": "Topic_23", "label": "~~\n14-\n", "shape": "dot", "size": 10, "title": "~~\n14-\n"}, {"color": "blue", "id": "Topic_24", "label": "COMPARISON\nCOMPARISON\n", "shape": "dot", "size": 10, "title": "COMPARISON\nCOMPARISON\n"}, {"color": "blue", "id": "Topic_25", "label": "~  *\nPREDICTOR 4\n", "shape": "dot", "size": 10, "title": "~  *\nPREDICTOR 4\n"}, {"color": "blue", "id": "Topic_26", "label": "\u2014 *\u2022\n", "shape": "dot", "size": 10, "title": "\u2014 *\u2022\n"}, {"color": "blue", "id": "Topic_27", "label": "^L\n", "shape": "dot", "size": 10, "title": "^L\n"}, {"color": "blue", "id": "Topic_28", "label": "The  need  for  an  identical  twin  in  this  conceptual  experiment  can  be\neliminated  as  follows.  In  general,  good  prediction  does  not  require  knowl-\nedge of more  than N preceding letters of text, with N fairly small. There are\nonly a  finite  number  of  possible  sequences  of  N  letters.  We  could  ask  the\nsubject to guess the next letter for each of these possible .V-grams. The com-\nplete  list  of  these  predictions  could  then  be  used  both  for  obtaining  the\nreduced text from the original and for the inverse reconstruction process.\nTo  put  this  another  way,  the  reduced  text  can  be  considered  to  be  an\nencoded form of the original, the result of passing the original  text through\na  reversible  transducer.  In  fact,  a  communication  system  could  be  con-\nstructed  in  which  only  the  reduced  text  is  transmitted  from  one  point  to\nthe other.  This  could  be  set  up  as  shown  in  Fig.  2,  with  two  identical  pre-\ndiction devices.\nAn  extension  of  the  above  experiment  yields  further  information  con-\ncerning the predictability of English. As before,  the subject knows the  text\nup to the current point and  is asked  to guess  the  next letter.  If he  is wrong,\nhe is  told  so  and  asked  to  guess  again.  This  is  continued  until  he  finds  the\ncorrect  letter.  A  typical  result  with  this  experiment  is  shown  below.  The\n", "shape": "dot", "size": 10, "title": "The  need  for  an  identical  twin  in  this  conceptual  experiment  can  be\neliminated  as  follows.  In  general,  good  prediction  does  not  require  knowl-\nedge of more  than N preceding letters of text, with N fairly small. There are\nonly a  finite  number  of  possible  sequences  of  N  letters.  We  could  ask  the\nsubject to guess the next letter for each of these possible .V-grams. The com-\nplete  list  of  these  predictions  could  then  be  used  both  for  obtaining  the\nreduced text from the original and for the inverse reconstruction process.\nTo  put  this  another  way,  the  reduced  text  can  be  considered  to  be  an\nencoded form of the original, the result of passing the original  text through\na  reversible  transducer.  In  fact,  a  communication  system  could  be  con-\nstructed  in  which  only  the  reduced  text  is  transmitted  from  one  point  to\nthe other.  This  could  be  set  up  as  shown  in  Fig.  2,  with  two  identical  pre-\ndiction devices.\nAn  extension  of  the  above  experiment  yields  further  information  con-\ncerning the predictability of English. As before,  the subject knows the  text\nup to the current point and  is asked  to guess  the  next letter.  If he  is wrong,\nhe is  told  so  and  asked  to  guess  again.  This  is  continued  until  he  finds  the\ncorrect  letter.  A  typical  result  with  this  experiment  is  shown  below.  The\n"}, {"color": "blue", "id": "Topic_29", "label": "57\n", "shape": "dot", "size": 10, "title": "57\n"}, {"color": "blue", "id": "Topic_30", "label": "first  line  is  the  original text and  the numbers in  the second line  indicate  thcr\nguess at which  the correct letter was obtained.\n", "shape": "dot", "size": 10, "title": "first  line  is  the  original text and  the numbers in  the second line  indicate  thcr\nguess at which  the correct letter was obtained.\n"}, {"color": "blue", "id": "Topic_31", "label": "(1)  T H E RE \n(2) 1 1 1 5 1 1 2 1 1 2 11 151 17 1 1 1 2 1 3 2 1 2 2 7 1 1 1 1 4 1 11\nA  M O T O R C YC\nR E V E R SE \nIS \nNO \nON \nLE \nA\n1 l 3j\n(1)  F R I E ND  OF  M I NE \n( 2 ) 8 6 1 3 1 1 1 1 1 1 1 1 1 1 1 6 2 1 1 1 1 1 1 2 1 1 1 1 11\nT H IS  O UT\nF O U ND \n(1)  R A T H ER \n(2) 4 1 1 1 1 11 11 5 1 1 1 1 1 1 1 1 1 1 1 6 1 1 1 1 1 1 1 1 1 1 1 11 \nD R A M A T I C A L LY \nO T H ER\nT HE \n", "shape": "dot", "size": 10, "title": "(1)  T H E RE \n(2) 1 1 1 5 1 1 2 1 1 2 11 151 17 1 1 1 2 1 3 2 1 2 2 7 1 1 1 1 4 1 11\nA  M O T O R C YC\nR E V E R SE \nIS \nNO \nON \nLE \nA\n1 l 3j\n(1)  F R I E ND  OF  M I NE \n( 2 ) 8 6 1 3 1 1 1 1 1 1 1 1 1 1 1 6 2 1 1 1 1 1 1 2 1 1 1 1 11\nT H IS  O UT\nF O U ND \n(1)  R A T H ER \n(2) 4 1 1 1 1 11 11 5 1 1 1 1 1 1 1 1 1 1 1 6 1 1 1 1 1 1 1 1 1 1 1 11 \nD R A M A T I C A L LY \nO T H ER\nT HE \n"}, {"color": "blue", "id": "Topic_32", "label": "D AY\n", "shape": "dot", "size": 10, "title": "D AY\n"}, {"color": "blue", "id": "Topic_33", "label": "(9)\n", "shape": "dot", "size": 10, "title": "(9)\n"}, {"color": "blue", "id": "Topic_34", "label": "Out of  102  symbols the  subject guessed right on  the first guess  79  times\non the second guess 8 times, on the third guess 3 times, the fourth and fifth\nguesses 2 each and only eight times required more than five guesses. Results\nof this order are typical of prediction by a good subject with ordinary literary\nEnglish.  Newspaper  writing,  scientific  work  and  poetry  generally  lead to\nsomewhat  poorer  scores.\nThe  reduced  text  in  this  case  also  contains  the  same  information  as the\noriginal. Again utilizing the identical twin we ask him at each stage to  guess\nas many  times  as  the  number  given  in  the  reduced  text  and  recover  in  this\nway  the  original.  To  eliminate  the  human  element  here  we  must  ask  out\nsubject,  for  each  possible  .V-gram  of  text,  to  guess  the  most  probable  next\nletter,  the  second  most probable  next  letter, etc.  This  set  of  data  can  then\nserve  both  for  prediction  and  recovery.\nJust as before,  the reduced text can be considered an encoded version of\nthe  original.  The  original  language,  with  an  alphabet  of  27  symbols,  A,\nB,  \u2022 \u2022 \u2022  , Z, space, has been translated into a new language with the alphabet\n1,  2,  \u2022 \u2022 \u2022  ,  27. The translating has been such that the symbol 1  now has an\nextremely  high  frequency.  The  symbols  2,  3,  4  have  successively  smaller\nfrequencies and the final symbols 20, 21, \u2022 \u2022 \u2022 , 27 occur very rarely. Thus the\ntranslating has simplified to a considerable extent the nature of the statisti-\ncal structure involved. The redundancy which  originally appeared in com-\nplicated constraints among groups of letters, has, by the translating process,\nbeen made explicit to a large extent in the very unequal probabilities of the\nnew  symbols. It is this,  as will appear later, which  enables one to estimate\nthe entropy from these experiments.\nIn  order  to  determine  how  predictability  depends  on  the  number  .V oi\npreceding  letters  known  to  the  subject,  a  more  involved  experiment  was\ncarried out.  One  hundred  samples  of  English  text  were  selected  at  random\nfrom a book,  each fifteen letters  in length.  The subject was required to  gues\u003c\nthe  text,  letter  by  letter,  for  each  sample  as  in  the  preceding  experiment.\nThus one hundred samples were obtained in which the subject had available\n0,  1,  2,  3,  \u2022  \u2022  \u2022  ,  14  preceding  letters.  To  aid  in  prediction  the  subject  made\nsuch use as he wished of various statistical tables,  letter,  digram and  trigrair.\na table of  the frequencies  of  initial  letters  in  words,  a  list  of  the  fre-\njes of common words and a dictionary. The samples in  this experiment\nfrom  \"Jefferson  the  Virginian\"  by  Dumas  Maloiie.  These  results,  to-\nrt,  r with a similar test in which  100 letters were known to the subject, are\nmarized  in Table  I. The  column  corresponds  to  the  number  of preceding\ntters known  to  the  subject plus  one;  the  row  is  the  number  of  the  guess.\nThe entry in column N at row 5 is the number of times the subject guessed\nthe right letter at the Si\\\\ guess when (N-\\)  letters were known.  For example,\n", "shape": "dot", "size": 10, "title": "Out of  102  symbols the  subject guessed right on  the first guess  79  times\non the second guess 8 times, on the third guess 3 times, the fourth and fifth\nguesses 2 each and only eight times required more than five guesses. Results\nof this order are typical of prediction by a good subject with ordinary literary\nEnglish.  Newspaper  writing,  scientific  work  and  poetry  generally  lead to\nsomewhat  poorer  scores.\nThe  reduced  text  in  this  case  also  contains  the  same  information  as the\noriginal. Again utilizing the identical twin we ask him at each stage to  guess\nas many  times  as  the  number  given  in  the  reduced  text  and  recover  in  this\nway  the  original.  To  eliminate  the  human  element  here  we  must  ask  out\nsubject,  for  each  possible  .V-gram  of  text,  to  guess  the  most  probable  next\nletter,  the  second  most probable  next  letter, etc.  This  set  of  data  can  then\nserve  both  for  prediction  and  recovery.\nJust as before,  the reduced text can be considered an encoded version of\nthe  original.  The  original  language,  with  an  alphabet  of  27  symbols,  A,\nB,  \u2022 \u2022 \u2022  , Z, space, has been translated into a new language with the alphabet\n1,  2,  \u2022 \u2022 \u2022  ,  27. The translating has been such that the symbol 1  now has an\nextremely  high  frequency.  The  symbols  2,  3,  4  have  successively  smaller\nfrequencies and the final symbols 20, 21, \u2022 \u2022 \u2022 , 27 occur very rarely. Thus the\ntranslating has simplified to a considerable extent the nature of the statisti-\ncal structure involved. The redundancy which  originally appeared in com-\nplicated constraints among groups of letters, has, by the translating process,\nbeen made explicit to a large extent in the very unequal probabilities of the\nnew  symbols. It is this,  as will appear later, which  enables one to estimate\nthe entropy from these experiments.\nIn  order  to  determine  how  predictability  depends  on  the  number  .V oi\npreceding  letters  known  to  the  subject,  a  more  involved  experiment  was\ncarried out.  One  hundred  samples  of  English  text  were  selected  at  random\nfrom a book,  each fifteen letters  in length.  The subject was required to  gues\u003c\nthe  text,  letter  by  letter,  for  each  sample  as  in  the  preceding  experiment.\nThus one hundred samples were obtained in which the subject had available\n0,  1,  2,  3,  \u2022  \u2022  \u2022  ,  14  preceding  letters.  To  aid  in  prediction  the  subject  made\nsuch use as he wished of various statistical tables,  letter,  digram and  trigrair.\na table of  the frequencies  of  initial  letters  in  words,  a  list  of  the  fre-\njes of common words and a dictionary. The samples in  this experiment\nfrom  \"Jefferson  the  Virginian\"  by  Dumas  Maloiie.  These  results,  to-\nrt,  r with a similar test in which  100 letters were known to the subject, are\nmarized  in Table  I. The  column  corresponds  to  the  number  of preceding\ntters known  to  the  subject plus  one;  the  row  is  the  number  of  the  guess.\nThe entry in column N at row 5 is the number of times the subject guessed\nthe right letter at the Si\\\\ guess when (N-\\)  letters were known.  For example,\n"}, {"color": "blue", "id": "Topic_35", "label": "the entry  19 in column 6, row 2, means that with five letters known thi cor\nrect letter was obtained on  the  second guess nineteen  times out of  the hun\ndred.  The  first  two  columns  of  this  table  were  not  obtained  by  the  experi-\nmental  procedure  outlined  above  but  were  calculated  directly  from  the\nknown letter and digram  frequencies.  Thus with no known letters  the most\nprobable  symbol  is  the  space  (probability  .182);  the  next  guess,  if  this  is\nwrong,  should  be  E  (probability  .107),  etc.  These  probabilities  are  the\nfrequencies with which the right guess would occur at the first, second,  etc.,\ntrials  with  best prediction.  Similarly,  a  simple  calculation  from  the  digram\ntable gives  the  entries  in  column  1  when  the  subject  uses  the  table  to  best\n", "shape": "dot", "size": 10, "title": "the entry  19 in column 6, row 2, means that with five letters known thi cor\nrect letter was obtained on  the  second guess nineteen  times out of  the hun\ndred.  The  first  two  columns  of  this  table  were  not  obtained  by  the  experi-\nmental  procedure  outlined  above  but  were  calculated  directly  from  the\nknown letter and digram  frequencies.  Thus with no known letters  the most\nprobable  symbol  is  the  space  (probability  .182);  the  next  guess,  if  this  is\nwrong,  should  be  E  (probability  .107),  etc.  These  probabilities  are  the\nfrequencies with which the right guess would occur at the first, second,  etc.,\ntrials  with  best prediction.  Similarly,  a  simple  calculation  from  the  digram\ntable gives  the  entries  in  column  1  when  the  subject  uses  the  table  to  best\n"}, {"color": "blue", "id": "Topic_36", "label": "advantage. Since the frequency tables are determined from long samples 0(\nEnglish, these two columns are subject to less sampling error than the others\nIt will be seen that the prediction gradually improves, apart from son,\nstatistical fluctuation, with  increasing  knowledge  of  the past  as  indicate^\nby  the  larger  numbers  of  correct  first  guesses  and  the  smaller  numbers  oj\nhigh  rank  guesses.\nOne  experiment  was  carried  out  with  \"reverse\"  prediction,  in which  th(\nsubject  guessed  the  letter  preceding  those  already  known.  Although  the\ntask is subjectively much more difficult, the scores were only slightly poorer\nThus,  with  two  101  letter  samples  from  the  same  source,  the  subject  ob.\ntained the following results:\n", "shape": "dot", "size": 10, "title": "advantage. Since the frequency tables are determined from long samples 0(\nEnglish, these two columns are subject to less sampling error than the others\nIt will be seen that the prediction gradually improves, apart from son,\nstatistical fluctuation, with  increasing  knowledge  of  the past  as  indicate^\nby  the  larger  numbers  of  correct  first  guesses  and  the  smaller  numbers  oj\nhigh  rank  guesses.\nOne  experiment  was  carried  out  with  \"reverse\"  prediction,  in which  th(\nsubject  guessed  the  letter  preceding  those  already  known.  Although  the\ntask is subjectively much more difficult, the scores were only slightly poorer\nThus,  with  two  101  letter  samples  from  the  same  source,  the  subject  ob.\ntained the following results:\n"}, {"color": "blue", "id": "Topic_37", "label": "Incidentally,  the  .V-grarn  entropy  FN  for  a  reversed language is equal  te\nthat for the forward language as may  be  seen from  the second form  in  equa.\ntion  (1). Both terms have  the same value in the  forward and reversed  cases,\n4.  IDEAL  .V-GRAM  PREDICTION\nThe data of Table I can be used to obtain upper and lower bounds to th\nAT-gram  entropies  FN  \u2022  In  order  to  do  this,  it  is  necessary  first  to  develop\nsome  general  results  concerning  the  best  possible  prediction  of  a  languagt\nwhen the preceding N letters are known. There will be for the language a set\nof conditional probabilities/)^  , \u00bb , , \u2022 \u2022 \u2022,  *y_i (j). This is the probability whet\nthe  (N-\\)  gram ii ,  iz ,  \u2022 \u2022 \u2022  , in-i occurs  that  the  next  letter will be j. Th\nbest guess for the  next letter,  when this  (Ar-l)  gram  is known  to have  oc-\ncurred, will be that letter having the highest conditional probability. The\nsecond guess  should  be  that with  the  second  highest probability,  etc. A\nmachine  or person guessing  in  the best way  would guess letters  in  the  ordei\nof  decreasing  conditional  probability.  Thus  the  process  of  reducing  a  ter\nwith  such  an  ideal  predictor  consists  of  a  mapping  of  the  letters  into  tb:\nnumbers  from  1  to  27  in  such  a  way  that  the  most  probable  next  lette:\n[conditional  on  the  known  preceding  (A7-l)  gram]  is  mapped  into  1,  etc\nThe frequency of 1\u0027s in  the reduced  text will  then be given by\nwhere the sum is taken over all (N-\\) grams ii , i2 ,  \u2022 \u2022 \u2022  , in-\\ the j being tin\none  which  maximizes  p  for  that  particular  (N-\\)  gram.  Similarly,  the  fn\nquency  of  2\u0027s,  q% ,  is  given  by  the  same  formula  with j  chosen  to  be  tha\nletter having  the  second highest value of p,  etc.\nOn  the  basis  of  A7-grams,  a  different  set  of  probabilities  for  the  symbol\n", "shape": "dot", "size": 10, "title": "Incidentally,  the  .V-grarn  entropy  FN  for  a  reversed language is equal  te\nthat for the forward language as may  be  seen from  the second form  in  equa.\ntion  (1). Both terms have  the same value in the  forward and reversed  cases,\n4.  IDEAL  .V-GRAM  PREDICTION\nThe data of Table I can be used to obtain upper and lower bounds to th\nAT-gram  entropies  FN  \u2022  In  order  to  do  this,  it  is  necessary  first  to  develop\nsome  general  results  concerning  the  best  possible  prediction  of  a  languagt\nwhen the preceding N letters are known. There will be for the language a set\nof conditional probabilities/)^  , \u00bb , , \u2022 \u2022 \u2022,  *y_i (j). This is the probability whet\nthe  (N-\\)  gram ii ,  iz ,  \u2022 \u2022 \u2022  , in-i occurs  that  the  next  letter will be j. Th\nbest guess for the  next letter,  when this  (Ar-l)  gram  is known  to have  oc-\ncurred, will be that letter having the highest conditional probability. The\nsecond guess  should  be  that with  the  second  highest probability,  etc. A\nmachine  or person guessing  in  the best way  would guess letters  in  the  ordei\nof  decreasing  conditional  probability.  Thus  the  process  of  reducing  a  ter\nwith  such  an  ideal  predictor  consists  of  a  mapping  of  the  letters  into  tb:\nnumbers  from  1  to  27  in  such  a  way  that  the  most  probable  next  lette:\n[conditional  on  the  known  preceding  (A7-l)  gram]  is  mapped  into  1,  etc\nThe frequency of 1\u0027s in  the reduced  text will  then be given by\nwhere the sum is taken over all (N-\\) grams ii , i2 ,  \u2022 \u2022 \u2022  , in-\\ the j being tin\none  which  maximizes  p  for  that  particular  (N-\\)  gram.  Similarly,  the  fn\nquency  of  2\u0027s,  q% ,  is  given  by  the  same  formula  with j  chosen  to  be  tha\nletter having  the  second highest value of p,  etc.\nOn  the  basis  of  A7-grams,  a  different  set  of  probabilities  for  the  symbol\n"}, {"color": "blue", "id": "Topic_38", "label": "\u2022  the reduced text,  q\"+1,  q%+i,  ...  ,  g\u00a3r+1,  would normally result.  Since this\nm  H\u0027  tion is on the basis of a greater knowledge of the past,  one would  ex-\nt  the  probabilities  of  low  numbers  to  be  greater,  and  in  fact  one  can\nprove the following inequalities:\ns\n\u00bb-i\ns \nE N+1  -^  V^  w\nqt  \u003e  L,  qt\ni-i \n", "shape": "dot", "size": 10, "title": "\u2022  the reduced text,  q\"+1,  q%+i,  ...  ,  g\u00a3r+1,  would normally result.  Since this\nm  H\u0027  tion is on the basis of a greater knowledge of the past,  one would  ex-\nt  the  probabilities  of  low  numbers  to  be  greater,  and  in  fact  one  can\nprove the following inequalities:\ns\n\u00bb-i\ns \nE N+1  -^  V^  w\nqt  \u003e  L,  qt\ni-i \n"}, {"color": "blue", "id": "Topic_39", "label": "S  =  i, 2,  \u2022 \u2022 \u2022  .\n", "shape": "dot", "size": 10, "title": "S  =  i, 2,  \u2022 \u2022 \u2022  .\n"}, {"color": "blue", "id": "Topic_40", "label": "(ii)\n", "shape": "dot", "size": 10, "title": "(ii)\n"}, {"color": "blue", "id": "Topic_41", "label": "E \u003e E \u003c?r\u0027\n", "shape": "dot", "size": 10, "title": "E \u003e E \u003c?r\u0027\n"}, {"color": "blue", "id": "Topic_42", "label": "3  ivT^JTl \ni Ki  |p\"U.~ \ni \n__  I  I  ?^4\u003ei  t  t  I \n", "shape": "dot", "size": 10, "title": "3  ivT^JTl \ni Ki  |p\"U.~ \ni \n__  I  I  ?^4\u003ei  t  t  I \n"}, {"color": "blue", "id": "Topic_43", "label": "I \n1 1 1 11  \\\\\\\nli\n", "shape": "dot", "size": 10, "title": "I \n1 1 1 11  \\\\\\\nli\n"}, {"color": "blue", "id": "Topic_44", "label": "i  \" MM  Tn \nI \nI \nI \n1 \n", "shape": "dot", "size": 10, "title": "i  \" MM  Tn \nI \nI \nI \n1 \n"}, {"color": "blue", "id": "Topic_45", "label": "i \nII \n", "shape": "dot", "size": 10, "title": "i \nII \n"}, {"color": "blue", "id": "Topic_46", "label": "i\n", "shape": "dot", "size": 10, "title": "i\n"}, {"color": "blue", "id": "Topic_47", "label": "iIIT\nII  \\u\n", "shape": "dot", "size": 10, "title": "iIIT\nII  \\u\n"}, {"color": "blue", "id": "Topic_48", "label": "0.15 \n10.15 cq2-q3)\npossible  (A7-l)  gram  there is a  set  of  possible  next  letters  each  with  equal\nprobability, while all other next letters have zero probability.\n", "shape": "dot", "size": 10, "title": "0.15 \n10.15 cq2-q3)\npossible  (A7-l)  gram  there is a  set  of  possible  next  letters  each  with  equal\nprobability, while all other next letters have zero probability.\n"}, {"color": "blue", "id": "Topic_49", "label": "It will now  be  shown  that  the upper  and lower bounds  for  FK  given  by\nle | \n\u2014\u0027  (17) are monotonic decreasing functions of N. This is true of the upper bound\nI\n, 0.025 q,.\nsince the qt+l majorize the ql and any equalizing flow in a set of probabilities\nincreases the entropy. To prove that the lower bound is also monotonic de-\n", "shape": "dot", "size": 10, "title": "It will now  be  shown  that  the upper  and lower bounds  for  FK  given  by\nle | \n\u2014\u0027  (17) are monotonic decreasing functions of N. This is true of the upper bound\nI\n, 0.025 q,.\nsince the qt+l majorize the ql and any equalizing flow in a set of probabilities\nincreases the entropy. To prove that the lower bound is also monotonic de-\n"}, {"color": "blue", "id": "Topic_50", "label": ". \n,,  creasing  we  will  show  that  the  quantity\nThe  qi  as  we  have  said  are  obtained  from  the p(ii, ...  ,  i.v)  by  arrangii\nRe\neach row of the table in decreasing order of magnitude and adding vertical!\nThus the  q^ are the sum of  a set of monotonic decreasing distributions, E\nLOtt \ne  tht  K increased by an equalizing flow among the qi. Suppose a flow occurs from\nplace each of these distributions by its rectangular decomposition. Each c\nid  al  ?i to  g,-+1,  the  first  decreased  by  Ag  and  the  latter  increased  by  the  same\nis  replaced  then  (in  general)  by  27  rectangular distributions;  the  qt are  \u0027\nial tt amount. Then three terms in the sum change and the change in U is given by\nsum of 27 x 27W rectangular distributions, of from 1 to 27 elements, and\nstarting  at  the left column.  The  entropy  for  this  set is less  than or  equa\u0027\nthat  of  the  original set  of  distributions  since  a  termwise  addition  of  twt\nmore  distributions  always  increases  entropy.  This  is  actually  an  applicaf\n", "shape": "dot", "size": 10, "title": ". \n,,  creasing  we  will  show  that  the  quantity\nThe  qi  as  we  have  said  are  obtained  from  the p(ii, ...  ,  i.v)  by  arrangii\nRe\neach row of the table in decreasing order of magnitude and adding vertical!\nThus the  q^ are the sum of  a set of monotonic decreasing distributions, E\nLOtt \ne  tht  K increased by an equalizing flow among the qi. Suppose a flow occurs from\nplace each of these distributions by its rectangular decomposition. Each c\nid  al  ?i to  g,-+1,  the  first  decreased  by  Ag  and  the  latter  increased  by  the  same\nis  replaced  then  (in  general)  by  27  rectangular distributions;  the  qt are  \u0027\nial tt amount. Then three terms in the sum change and the change in U is given by\nsum of 27 x 27W rectangular distributions, of from 1 to 27 elements, and\nstarting  at  the left column.  The  entropy  for  this  set is less  than or  equa\u0027\nthat  of  the  original set  of  distributions  since  a  termwise  addition  of  twt\nmore  distributions  always  increases  entropy.  This  is  actually  an  applicaf\n"}, {"color": "blue", "id": "Topic_51", "label": "f/  =  X)  i(qi  -  9\u003e+i)  log  i\n(20)\n*\n", "shape": "dot", "size": 10, "title": "f/  =  X)  i(qi  -  9\u003e+i)  log  i\n(20)\n*\n"}, {"color": "blue", "id": "Topic_52", "label": "The  term  in  brackets  has  the  form  \u2014f(x  -  1)  +  2f(x)  -  f(x  +  1)  where\u0027\nf(x)  =  x log x. Now/(a;) is a function which is concave upward for positive x\nsince/\" (x)  =  \\/x \u003e  0. The bracketed term is twice the difference between the\nordinate of the curve at x  =  i and the ordinate of the midpoint of the chord\njoining i \u2014  1  and  i +  1,  and consequently is negative.  Since A\u00a7 also is nega.\ntive,  the change in  U brought about by the flow is positive. An even simpler  \u0027\ncalculation shows that this is also true for a flow from  q\u00b1 to  q^ or from  g2e to\n\u003c?27  (where  only  two  terms  of  the  sum  are affected).  It  follows that  the  lower\nbound  based  on  the  TV-gram  prediction  frequencies  \u003c?;  is  greater  than  or\nequal  to  that  calculated  from  the  N +  1  gram  frequencies  ql+1.\n6.  EXPERIMENTAL  BOUNDS  FOR  ENGLISH\nWorking from the data of Table I, the upper and lower bounds were calcu-\nlated from  relations  (17).  The  data were first smoothed  somewhat to  over-\ncome  the  worst  sampling  fluctuations.  The  low  numbers  in  this  table  are\nthe  least  reliable  and  these  were  averaged  together  in  groups.  Thus,  in\ncolumn  4,  the  47,  18  and  14  were  not  changed  but  the  remaining  group\ntotaling 21 was divided uniformly over the rows from 4 to 20. The upper and\nlower bounds given by  (17)  were then  calculated for each  column giving  the\nfollowing results:\n", "shape": "dot", "size": 10, "title": "The  term  in  brackets  has  the  form  \u2014f(x  -  1)  +  2f(x)  -  f(x  +  1)  where\u0027\nf(x)  =  x log x. Now/(a;) is a function which is concave upward for positive x\nsince/\" (x)  =  \\/x \u003e  0. The bracketed term is twice the difference between the\nordinate of the curve at x  =  i and the ordinate of the midpoint of the chord\njoining i \u2014  1  and  i +  1,  and consequently is negative.  Since A\u00a7 also is nega.\ntive,  the change in  U brought about by the flow is positive. An even simpler  \u0027\ncalculation shows that this is also true for a flow from  q\u00b1 to  q^ or from  g2e to\n\u003c?27  (where  only  two  terms  of  the  sum  are affected).  It  follows that  the  lower\nbound  based  on  the  TV-gram  prediction  frequencies  \u003c?;  is  greater  than  or\nequal  to  that  calculated  from  the  N +  1  gram  frequencies  ql+1.\n6.  EXPERIMENTAL  BOUNDS  FOR  ENGLISH\nWorking from the data of Table I, the upper and lower bounds were calcu-\nlated from  relations  (17).  The  data were first smoothed  somewhat to  over-\ncome  the  worst  sampling  fluctuations.  The  low  numbers  in  this  table  are\nthe  least  reliable  and  these  were  averaged  together  in  groups.  Thus,  in\ncolumn  4,  the  47,  18  and  14  were  not  changed  but  the  remaining  group\ntotaling 21 was divided uniformly over the rows from 4 to 20. The upper and\nlower bounds given by  (17)  were then  calculated for each  column giving  the\nfollowing results:\n"}, {"color": "blue", "id": "Topic_53", "label": "Upper \nLower \n", "shape": "dot", "size": 10, "title": "Upper \nLower \n"}, {"color": "blue", "id": "Topic_54", "label": ". 9 1 .2 \n", "shape": "dot", "size": 10, "title": ". 9 1 .2 \n"}, {"color": "blue", "id": "Topic_55", "label": ".6\n", "shape": "dot", "size": 10, "title": ".6\n"}, {"color": "green", "id": "Topic_2_content", "label": "Topic_2_content", "shape": "dot", "size": 10, "title": "been  defined.  The  entropy  is  a  statistical  parameter  which  measures,\nin  a  certain  sense,  how  much  information  is  produced  on  the  average  for\neach letter of a text in the language. If the language is translated into binary\ndigits (0 or 1) in the most efficient way, the entropy // is the average number\nof binary digits required per letter of the original language.  The redundancy,\non the other hand, measures the amount of constraint imposed on a text in\nthe  language  due  to  its  statistical  structure,  e.g.,  in  English  the  high  fre-\nquency of  the  letter \u00a3,  the strong  tendency of H to  follow  T or of  V  to  follow\nQ.  It  was  estimated  that  when  statistical  effects  extending  over  not  more\nthan  eight  letters  are  considered  the  entropy  is  roughly  2.3  bits  per  letter,\nthe  redundancy  about  50 per  cent.\nSince  then  a  new  method  has  been  found  for  estimating  these  quantities,\nwhich is more sensitive and takes account of long range statistics, influences\nextending  over phrases,  sentences,  etc.  This  method  is  based  on  a  study  of\nthe predictability of English;  how well can the next letter of a  text be pre-\ndicted when the preceding N letters are known. The results  of some experi-\nments  in  prediction  will  be  given,  and  a  theoretical  analysis  of  some  of  the\nproperties of ideal prediction.  By combining the experimental and  theoreti-\ncal results it is possible  to estimate upper and  lower bounds for the entropy\nand  redundancy.  From  this  analysis  it  appears  that,  in  ordinary  literary\nEnglish,  the  long  range  statistical  effects  (up  to  100  letters)  reduce  the\nentropy  to  something  of  the  order of  one  bit per  letter,  with  a  corresponding\nredundancy  of  roughly  75%.  The  redundancy  may  be  still  higher  when\nstructure extending over paragraphs, chapters, etc.  is included. However,  as\nthe lengths  involved  are increased,  the parameters  in  question become more\n1 C. E.  Shannon,  \"A Mathematical Theory of Communication,\" Bell System  Technical\nJournal,  v.  27,  pp.  379-423,  623-656, July,  October,  1948.\n50\nPREDICTION  A ND  ENTROPY  OF  PRINTED  ENGLISH \n51\ntic  and  uncertain,  and  they  depend  more  critically  on  the  type  of  text\ninvolved.\n2.  ENTROPY  CALCULATION  FROM  TIIK  STATISTICS  OF  ENGLISH\nOne method of calculating  the entropy // is by a  series of approximations\n7?  Fi ,  F-2 \u003e  \u0027 \u0027 \u0027 \u0027  which  successively  take more  and more  of  the  statistics\nf  the  language  into  account  and  approach  77  as  a  limit.  Fx  may  be  called\nthe Y-grani entropy; it measures the amount of information or entropy due\nto statistics extending over A\" adjacent letters of text. F.v is given by1\nFN  =\n-Tfp(bi,j) \nV)\n\\0gzpbi(j)\n,  ,\n"}, {"color": "green", "id": "Topic_3_content", "label": "Topic_3_content", "shape": "dot", "size": 10, "title": "i.j\n"}, {"color": "green", "id": "Topic_4_content", "label": "Topic_4_content", "shape": "dot", "size": 10, "title": ""}, {"color": "green", "id": "Topic_5_content", "label": "Topic_5_content", "shape": "dot", "size": 10, "title": ""}, {"color": "green", "id": "Topic_6_content", "label": "Topic_6_content", "shape": "dot", "size": 10, "title": "JV-\u00bboo\n"}, {"color": "green", "id": "Topic_7_content", "label": "Topic_7_content", "shape": "dot", "size": 10, "title": "26\n"}, {"color": "green", "id": "Topic_8_content", "label": "Topic_8_content", "shape": "dot", "size": 10, "title": "(4)\n"}, {"color": "green", "id": "Topic_9_content", "label": "Topic_9_content", "shape": "dot", "size": 10, "title": "2 Fletcher  Pratt,  \"Secret  and  Urgent,\"  Blue  Ribbon  Books,  1942.\n"}, {"color": "green", "id": "Topic_10_content", "label": "Topic_10_content", "shape": "dot", "size": 10, "title": ""}, {"color": "green", "id": "Topic_11_content", "label": "Topic_11_content", "shape": "dot", "size": 10, "title": "1 G.  K.  Zipf,  \"Human  Behavior  and  the  Principle  of  Least  Effort,\"  Acldison-Wesley\n"}, {"color": "green", "id": "Topic_12_content", "label": "Topic_12_content", "shape": "dot", "size": 10, "title": "PREDICTION  AND  KNTROPY  OF  PRINTED  ENGLISH\n53\nuja  (6)  clearly  cannot  hold  indefinitely  since  the  total  probability  ~2pn\n"}, {"color": "green", "id": "Topic_13_content", "label": "Topic_13_content", "shape": "dot", "size": 10, "title": "0.1\n"}, {"color": "green", "id": "Topic_14_content", "label": "Topic_14_content", "shape": "dot", "size": 10, "title": "0.0001\n"}, {"color": "green", "id": "Topic_15_content", "label": "Topic_15_content", "shape": "dot", "size": 10, "title": "REALLY\nX-QUALITY\n2 \n4  6  8 10 \n20 \n40  60  10O  200  400 \n1OOO  2000  4000  10,000\n"}, {"color": "green", "id": "Topic_16_content", "label": "Topic_16_content", "shape": "dot", "size": 10, "title": "WORD  ORDER\n"}, {"color": "green", "id": "Topic_17_content", "label": "Topic_17_content", "shape": "dot", "size": 10, "title": "8727\n"}, {"color": "green", "id": "Topic_18_content", "label": "Topic_18_content", "shape": "dot", "size": 10, "title": "54 \nTHE  BELL  SYSTEM  TECHNICAL  JOURNAL,  JANUARY  1951\nPREDICTION  AND  ENTROPY  OF  PRINTED  ENGLISH\n55\n"}, {"color": "green", "id": "Topic_19_content", "label": "Topic_19_content", "shape": "dot", "size": 10, "title": "26 letter\n27 letter\nFa\n4.70\n4.76\nPi\n4.14\n4.03\n"}, {"color": "green", "id": "Topic_20_content", "label": "Topic_20_content", "shape": "dot", "size": 10, "title": "F,\n3.3\n3.1\nfword\n2 . 62\n2.14\n"}, {"color": "green", "id": "Topic_21_content", "label": "Topic_21_content", "shape": "dot", "size": 10, "title": "O .O\n"}, {"color": "green", "id": "Topic_22_content", "label": "Topic_22_content", "shape": "dot", "size": 10, "title": "U^!XT\n"}, {"color": "green", "id": "Topic_23_content", "label": "Topic_23_content", "shape": "dot", "size": 10, "title": ""}, {"color": "green", "id": "Topic_24_content", "label": "Topic_24_content", "shape": "dot", "size": 10, "title": ""}, {"color": "green", "id": "Topic_25_content", "label": "Topic_25_content", "shape": "dot", "size": 10, "title": "REDUCED  TEXT\n*\u2022\n"}, {"color": "green", "id": "Topic_26_content", "label": "Topic_26_content", "shape": "dot", "size": 10, "title": "PREDICTOR\n"}, {"color": "green", "id": "Topic_27_content", "label": "Topic_27_content", "shape": "dot", "size": 10, "title": "Fig.  2\u2014Communication  system  using  reduced  text.\nTEXT\n1  ~*4\n"}, {"color": "green", "id": "Topic_28_content", "label": "Topic_28_content", "shape": "dot", "size": 10, "title": "56\nTHE  BELT.  SYSTEM  TECHNICAL  JOURNAL,  JANUARY  1951\nPREDICTION  AND  ENTROPY  OF  PRINTED  ENGLISH\n"}, {"color": "green", "id": "Topic_29_content", "label": "Topic_29_content", "shape": "dot", "size": 10, "title": ""}, {"color": "green", "id": "Topic_30_content", "label": "Topic_30_content", "shape": "dot", "size": 10, "title": "R\n"}, {"color": "green", "id": "Topic_31_content", "label": "Topic_31_content", "shape": "dot", "size": 10, "title": ""}, {"color": "green", "id": "Topic_32_content", "label": "Topic_32_content", "shape": "dot", "size": 10, "title": ""}, {"color": "green", "id": "Topic_33_content", "label": "Topic_33_content", "shape": "dot", "size": 10, "title": ""}, {"color": "green", "id": "Topic_34_content", "label": "Topic_34_content", "shape": "dot", "size": 10, "title": "TABLE  I\n3\n36\n20\n12\n7\n1\n4\n3\n2\n4\n2\n2\n4\n1\n1\n1\n4\n5\n51\n13\n8\n4\n3\n2\n2\n1\n5\n3\n2\n2\n1\n2\n1\n47\n18\n14\n3\n1\n5\n3\n2\n1\n2\n1\n1\n1\n1\n6\n58\n19\n5\n1\n4\n3\n2\n1\n1\n1\n1\n1\n1\n1\n1\n7\n48\n17\n3\n4\n3\n2\n8\n2\n4\n3\n1\n1\n1\n1\n1\n1\n8\n66\n15\n5\n4\n6\n1\n1\n1\n1\n10\n67\n10\n4\n4\n6\n1\n1\n1\n1\n3\n1\n1\n\u00bb\n66\n13\n9\n4\n1\n1\n1\n2\n1\n1\n1\nu\n62\n9\n7\n5\n5\n4\n1\n1\n1\n2\n1\n1\n1\n12\n13\n14\n15\n100\n58\n14\n7\n6\n2\n2\n4\n2\n1\n1\n1\n1\n1\n66\n9\n4\n4\n3\n3\n1\n2\n2\n1\n1\n1\n1\n2\n72\n6\n9\n3\n4\n1\n1\n2\n2\n80\n7\n3\n4\n2\n1\n1\n1\n1\n60\n18\n5\n5\n1\n4\n3\n1\n1\n1\n1\n2\n29.2\n14.8\n10.0\n8.6\n7.1\n5.5\n4.5\n3.6\n3.0\n2.6\n2 .2\n1.9\n1.5\n1.2\n1.0\n.9\n.7\n.5\n.4\n.3\n.2\n.1\n.1\n.0\ni\n18.2\n10.7\n\u0027  8.6\n6.7\n6.5\n5.8\n5.6\n5.2\n5.0\n4.3\n3.1\n2.8\n2.4\n2.3\n2.1\n2.0\n1.6\n1.6\n1.6\n1.3\n1.2\n.8\n.3\n.1\n.1\n.1\n.1\nJ\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n"}, {"color": "green", "id": "Topic_35_content", "label": "Topic_35_content", "shape": "dot", "size": 10, "title": "58 \nTHE  BELL  SYSTEM  TECHNICAL  JOURNAL,  JANUARY  1951\nV\n"}, {"color": "green", "id": "Topic_36_content", "label": "Topic_36_content", "shape": "dot", "size": 10, "title": "No.  of  guess \nForward \nReverse \n.................. \n................... \n1 \n70 \n66 \n2 \n10 \n7 \n3 \n7 \n4 \n4 \n2 \n4 \n5 \n2 \n6 \n6 \n3 \n2 \n7 \n3 \n1 \n8 \n0 \n2 \n\u003e8\n4\n9\n"}, {"color": "green", "id": "Topic_37_content", "label": "Topic_37_content", "shape": "dot", "size": 10, "title": "PREDICTION  AND  ENTROPY  OF  PRINTED  ENGLISH \n59\n"}, {"color": "green", "id": "Topic_38_content", "label": "Topic_38_content", "shape": "dot", "size": 10, "title": "0 10\n"}, {"color": "green", "id": "Topic_39_content", "label": "Topic_39_content", "shape": "dot", "size": 10, "title": "fl1\\\n"}, {"color": "green", "id": "Topic_40_content", "label": "Topic_40_content", "shape": "dot", "size": 10, "title": "Th\u0027s means  that  the  probability  of  being  right  in  the  first  S  guesses  when\nthe  preceding  N  letters  are  known  is  greater  than  or  equal  to  that  when\nlv  C/V-1)  are  known,  for  all  S.  To  prove  this,  imagine  the  probabilities\n\u2022bd  ii, \" \u0027  , i\" \u003e \u2022?\u0027) arrang\u00a3d in a table with j running horizontally and all\nthe  TV-grams  vertically.  The  table  will  therefore  have  27  columns  and  27N\nrows  The term on the left of  (11)  is the sum  of the S largest entries in  each\nrow  summed over all the rows. The right-hand member of (11)  is also a sum\nof entries from this table in which S entries are taken from each row but not\nnecessarily  the  S  largest.  This  follows  from  the  fact  that  the  right-hand\nmember  would  be  calculated  from  a  similar  table  with  (TV-1)  grams  rather\nthan TV-grams listed vertically.  Each row in the  TV-1  gram  table is the sum\nof 27 rows of the TV-gram table, since:\n27\np ( kj is,  \u2022 \u2022\u2022  , iy,j)  =  IL  P(ii ,i\"z,  \u0027\u0027 \u2022  ,  itr,j).\n.1=1\n(12)\nThe sum of the S largest entries in a row of the TV-1  gram table will equal\nthe sum  of  the  27S selected  entries from  the  corresponding  27  rows of  the\nTV-gram  table only  if  the  latter  fall into  S columns.  For  the equality  in  (11)\nto hold for a particular S,  this must be true of every row of the TV-1  gram\ntable. In this case, the first letter of the TV-gram does not affect the set of the\nS most probable  choices  for  the  next  letter,  although  the  ordering  within\nthe set may be affected. However,  if the equality in  (11)  holds for all S,  it\nfollows that the ordering as well will be unaffected by  the first letter of the\nJV-gram. The reduced text obtained from an ideal .V-l  gram predictor is then\nidentical with  that obtained from an  ideal N-gram predictor.\nSince the partial  sums\nQs  =  E  ql\ni-l\n5  =  1 , 2 , - -. \n(13)\nare monotonic  increasing  functions  of N,  \u003c 1  for all  N,  they must  all  ap-\nproach  limits  as  N  \u2014^  \u00ab\u003e.  Their  first  differences  must  therefore  approach\nlimits as  TV \u2014\u003e  =o ,  i.e.,  the  \u003c/j  approach limits,  q\u2122 . These may be interpreted\nas the relative frequency of correct first, second,  \u2022 \u2022 \u2022  ,  guesses with knowl-\nedge  of  the  entire  (infinite)  past  history  of  the  text.\n60 \nTHE  BELL  SYSTEM  TECHNICAL  JOURNAL,  JANUARY  1951\nThe  ideal  A7-gram predictor can  be  considered,  as has been pointed  out,  t0\nbe a transducer which operates on the language translating it into a sequence\nof numbers running from 1 to 27. As such it has the following two properties\u0027\n1.  The  output  symbol  is  a  function  of  the  present  input  (the  predicted\nnext letter when we think of it as a predicting device)  and the  prece\u003cJ.\ning  (A7-!)  letters.\n2.  It is instantaneously reversible. The original input can be recovered by\na  suitable  operation  on  the  reduced  text  without  loss  of  time.  In  fact\nthe  inverse  operation  also  operates  on  only  the  (N-\\)  preceding  sym,\nbols of the reduced text together with the present output.\nThe  above  proof  that  the  frequencies  of  output  symbols  with  an  N.\\\ngram  predictor  satisfy  the  inequalities:\n"}, {"color": "green", "id": "Topic_41_content", "label": "Topic_41_content", "shape": "dot", "size": 10, "title": "5  =  1,  2,\n\u2022  \u2022  \u2022  ,  27\n(14)\ncan  be  applied  to  any  transducer  having  the  two  properties  listed  above,\nIn  fact  we  can imagine  again  an array with  the  various  (A7-l)  grams  listed\nvertically and  the present  input letter horizontally.  Since the present output\nis a function  of  only  these  quantities  there will be a definite output symbol\nwhich may be entered at  the corresponding intersection of row and column,\nFurthermore,  the  instantaneous  reversibility  requires  that  no  two  entries\nin the same row be the same. Otherwise,  there would be ambiguity  between\nthe  two  or  more  possible  present  input  letters  when  reversing  the  transla.\ntion.  The  total  probability  of  the  5  most  probable  symbols  in  the  output,\ns\nsay ^ri, will be the sum of the probabilities for 5 entries in each row, summed\ni\nover the rows,  and  consequently is certainly not greater than the sum of the\n5  largest  entries  in  each  row.  Thus we will have\nZ \u003c??\u003e!\u003e,\u2022 \ni \ni\n5  =  1,  2 , . - ., 27 \n(15)\nIn other words ideal prediction as denned above enjoys a preferred  position\namong  all  translating  operations  that  may  be  applied  to  a  language  am\nwhich satisfy the two properties above. Roughly speaking, ideal prediction\ncollapses  the  probabilities  of  various  symbols  to  a  small  group  more  than\nany  other  translating  operation  involving  the  same  number  of letters  whict\nis  instantaneously  reversible.\nSets  of  numbers  satisfying  the  inequalities  (15)  have  been  studied  by\nMuirhead  in  connection  with  the  theory  of  algebraic  inequalities.6  If  (15\nholds when the q* and r{ are arranged in decreasing order of magnitude, ant\n6 Hardy, Littlewood  and  Polya,  \"Inequalities,\"  Cambridge  University Press,  1934.\nPREDICTION  AND  ENTROPY  OF  PRINTED  ENGLISH\n61\n27 \n\u00a3,\n.  53of  =  2-*ri,  (this is true  here  since  the  total  probability  in  each\nis  1), then  the  first  set, gf , is said to majorizs the second  set, rt.  It  is\n\\vn that the majorizing property is  equivalent  to either of  the following\nproperties:\n1  The  r\u003e  can be  obtained  from  the  qt  by  a  finite  series  of  \"flows.\"  By  a\nflow is understood a transfer of probability from a larger q to a smaller\none,  as  heat flows from  hotter  to  cooler  bodies  but  not  in  the  reverse\ndirection.\n2  The  TI  can  be  obtained  from  the  \u003c?j  by  a  generalized  \"averaging\"\noperation.  There  exists  a  set  of  non-negative  real  numbers,  a a ,  with\n(16)\n5.  ENTROPY  BOUNDS  FROM  PREDICTION  FREQUENCIES\nIf we know the frequencies  of  symbols  in  the  reduced  text with  the  ideal\njV-gram predictor,  \u003c?\u00bb\u2022  ,  it  is possible  to  set  both  upper  and  lower  bounds  to\nthe  A^gram  entropy,  FN ,  of  the  original  language.  These  bounds  are  as\nfollows:\ni(q\"  -  ?i+i)  log  i  \u003c  FN  \u003c  \u2014  ]C  q\"  log  q\".\n(17)\n=1\nThe upper  bound  follows  immediately  from  the  fact  that  the  maximum\npossible entropy in a  language with letter frequencies  q\"  is  \u2014 ^  ql  log ql.\nThus  the  entropy  per  symbol  of  the  reduced  text  is  not  greater  than  this.\nThe  JV-gram  entropy  of  the  reduced  text  is  equal  to  that  for  the  original\nlanguage, as may be seen by an  inspection of  the definition  (1)  of  FX . The\nsums involved will contain precisely, the same terms although, perhaps, in a\ndifferent order. This upper  bound  is clearly valid,  whether or  not  the pre-\ndiction is ideal.\nThe lower bound is more difficult to establish. It is necessary to show that\nwith any selection of Ar-gram probabilities p(ii,  i%, ...  ,  iN),  we will have\n27\nYs  i(q*i  \u2014  9?+i)  log  i  \u003c  X\ni i, \u2022  \u2022  \u2022, ijv\n\u2022\u20141\np(i,  \u0027  \u2022  \u2022 \nif/)  log  pi,  \u2022  \u2022  \u2022 \niff-i(iff)\n(18)\nThe  left-hand  member  of  the  inequality  can  be  interpreted  as  follows:\nImagine the q* arranged as a sequence of lines of decreasing height  (Fig. 3).\nThe actual q* can be considered as the sum of a set of rectangular distribu-\ntions as shown. The left member of (18) is the entropy of this set of distribu-\ntions.  Thus,  the  i\u0027h  rectangular  distribution  has  a  total  probability  of\nPREDICTION  AND  ENTROPY  OF  PRINTED  ENGLISH\n63\n62 \nTHE  BELL  SYSTEM  TECHNICAL  JOURNAL,  JANUARY  1951\ni(qi  \u2014  q*i+\\).  The  entropy  of  the  distribution  is  log  i.  The  total  entropy L* \u0027\nthen \neneral theorem  that Hv(x)  \u003c  H(x)  for any  chance variables x and y.\nuality  holds  only  if  the  distributions  being  added  are  proportional.\ne  may  add  the  different  components  of  the  same  width  without\ning the entropy  (since in this case  the distributions  are proportional).\nsuit is that  we  have  arrived at  the  rectangular  decomposition of  the\nhv  a  series  of  processes  which  decrease  or  leave  constant  the  entropy,\n1\nThe  problem,  then,  is  to  show  that  any  system  of  probabilities  p(ii,  ... \n\u0027*\u0027  ting  with  the  original  ^-gram  probabilities.  Consequently  the  entropy\nIN),  with  best  prediction  frequencies  g;  has  an  entropy  FN  greater  than  Q,  q\n, j. e original system FN is greater  than or equal to  that of  the  rectangular\ns\nequal  to  that  of  this  rectangular  system,  derived  from  the  same  set  of  q. \n,Composition  of  the qt.  This  proves  the  desired  result.\n\u00a3\n4-1\n2  i(q\"  \u2014  9m) log i.\nI  *\nf\nI \n\u00bb\u0027  c\n~~*\n0.60\nORIGINAL  DISTRIBUTION \n1\n(\n1\nTt will be noted that the lower bound is definitely less than FN unless each\nOf the  table  has a  rectangular  distribution.  This  requires  that  for  each\n0.20\n10.05 \n10.05 \n0.025 \n0.025 \n0.025 \n0.025\n1i \nla \n13 \n1* \nIs \n\u003cU \n\u003cl7 \n1e\no.4o (q,-qa)\nRECTANGULAR  DECOMPOSITION \n~~\\  \\^ -UPPER  BOUND\n"}, {"color": "green", "id": "Topic_42_content", "label": "Topic_42_content", "shape": "dot", "size": 10, "title": ""}, {"color": "green", "id": "Topic_43_content", "label": "Topic_43_content", "shape": "dot", "size": 10, "title": "1 \nLOWER  BOUND\u2014\u2022J^f*\u003e~~ \n"}, {"color": "green", "id": "Topic_44_content", "label": "Topic_44_content", "shape": "dot", "size": 10, "title": "I \nI \nI \nI \nI \nI \n. \nj\nY \n9 \nI \nI\nI  I\nA \n"}, {"color": "green", "id": "Topic_45_content", "label": "Topic_45_content", "shape": "dot", "size": 10, "title": "A\n"}, {"color": "green", "id": "Topic_46_content", "label": "Topic_46_content", "shape": "dot", "size": 10, "title": ""}, {"color": "green", "id": "Topic_47_content", "label": "Topic_47_content", "shape": "dot", "size": 10, "title": "1\nZ \n\u00b00\n10 \nNUMBER  OF  LETTERS\nFig. 4\u2014Upper and lower experimental bounds for the entropy of 27-letter English.\n14  15  \"  100\n6  78\n12  13 \n11 \n4 \n5 \n9\n3\n"}, {"color": "green", "id": "Topic_48_content", "label": "Topic_48_content", "shape": "dot", "size": 10, "title": "10.025 \n10.025 \nI \n, \nI \n. \nIfcf-oS \n, \u00ab f c 1 \u00bb\\ \n, \n, \n"}, {"color": "green", "id": "Topic_49_content", "label": "Topic_49_content", "shape": "dot", "size": 10, "title": "Fig.  3\u2014Rectangular  decomposition  of  a  monotonic  distribution.\n"}, {"color": "green", "id": "Topic_50_content", "label": "Topic_50_content", "shape": "dot", "size": 10, "title": "lVO\u00b0:  Atf = [ - ( \u00ab-  1) log (i -  1) +  2t log i -  (i + 1) log (i +  l)]Ag  (21)\n:atiot\n"}, {"color": "green", "id": "Topic_51_content", "label": "Topic_51_content", "shape": "dot", "size": 10, "title": "64\nTHE  BELL  SYSTEM  TECHNICAL  JOURNAL,  JANUARY  1951\n"}, {"color": "green", "id": "Topic_52_content", "label": "Topic_52_content", "shape": "dot", "size": 10, "title": "Column \n"}, {"color": "green", "id": "Topic_53_content", "label": "Topic_53_content", "shape": "dot", "size": 10, "title": "1 \n3 \n2 \n8 \n4.03  3.42  3.0  2.6  2 .7  2.2  2.8  1.8  1.9  2.1  2.2  2.3  2.1  1.7  2.1  13\n3.19  2.50  2 . 1 1 . 7 1 . 7 1 . 3 1 . 8 1 . 0 1 . 0 1 . 0 1 . 3 1 . 3 1 .2 \n"}, {"color": "green", "id": "Topic_54_content", "label": "Topic_54_content", "shape": "dot", "size": 10, "title": "13 \n10 \n12 \n11 \n14 \n15 \n4 \n5 \n7 \n9 \n6 \n"}, {"color": "green", "id": "Topic_55_content", "label": "Topic_55_content", "shape": "dot", "size": 10, "title": "100\n"}]);
                  edges = new vis.DataSet([{"from": 1, "to": "Topic_2", "width": 1}, {"from": 1, "to": "Topic_3", "width": 1}, {"from": 1, "to": "Topic_4", "width": 1}, {"from": 1, "to": "Topic_5", "width": 1}, {"from": 1, "to": "Topic_6", "width": 1}, {"from": 1, "to": "Topic_7", "width": 1}, {"from": 1, "to": "Topic_8", "width": 1}, {"from": 1, "to": "Topic_9", "width": 1}, {"from": 1, "to": "Topic_10", "width": 1}, {"from": 1, "to": "Topic_11", "width": 1}, {"from": 1, "to": "Topic_12", "width": 1}, {"from": 1, "to": "Topic_13", "width": 1}, {"from": 1, "to": "Topic_14", "width": 1}, {"from": 1, "to": "Topic_15", "width": 1}, {"from": 1, "to": "Topic_16", "width": 1}, {"from": 1, "to": "Topic_17", "width": 1}, {"from": 1, "to": "Topic_18", "width": 1}, {"from": 1, "to": "Topic_19", "width": 1}, {"from": 1, "to": "Topic_20", "width": 1}, {"from": 1, "to": "Topic_21", "width": 1}, {"from": 1, "to": "Topic_22", "width": 1}, {"from": 1, "to": "Topic_23", "width": 1}, {"from": 1, "to": "Topic_24", "width": 1}, {"from": 1, "to": "Topic_25", "width": 1}, {"from": 1, "to": "Topic_26", "width": 1}, {"from": 1, "to": "Topic_27", "width": 1}, {"from": 1, "to": "Topic_28", "width": 1}, {"from": 1, "to": "Topic_29", "width": 1}, {"from": 1, "to": "Topic_30", "width": 1}, {"from": 1, "to": "Topic_31", "width": 1}, {"from": 1, "to": "Topic_32", "width": 1}, {"from": 1, "to": "Topic_33", "width": 1}, {"from": 1, "to": "Topic_34", "width": 1}, {"from": 1, "to": "Topic_35", "width": 1}, {"from": 1, "to": "Topic_36", "width": 1}, {"from": 1, "to": "Topic_37", "width": 1}, {"from": 1, "to": "Topic_38", "width": 1}, {"from": 1, "to": "Topic_39", "width": 1}, {"from": 1, "to": "Topic_40", "width": 1}, {"from": 1, "to": "Topic_41", "width": 1}, {"from": 1, "to": "Topic_42", "width": 1}, {"from": 1, "to": "Topic_43", "width": 1}, {"from": 1, "to": "Topic_44", "width": 1}, {"from": 1, "to": "Topic_45", "width": 1}, {"from": 1, "to": "Topic_46", "width": 1}, {"from": 1, "to": "Topic_47", "width": 1}, {"from": 1, "to": "Topic_48", "width": 1}, {"from": 1, "to": "Topic_49", "width": 1}, {"from": 1, "to": "Topic_50", "width": 1}, {"from": 1, "to": "Topic_51", "width": 1}, {"from": 1, "to": "Topic_52", "width": 1}, {"from": 1, "to": "Topic_53", "width": 1}, {"from": 1, "to": "Topic_54", "width": 1}, {"from": 1, "to": "Topic_55", "width": 1}, {"from": "Topic_2", "to": "Topic_2_content", "width": 1}, {"from": "Topic_3", "to": "Topic_3_content", "width": 1}, {"from": "Topic_4", "to": "Topic_4_content", "width": 1}, {"from": "Topic_5", "to": "Topic_5_content", "width": 1}, {"from": "Topic_6", "to": "Topic_6_content", "width": 1}, {"from": "Topic_7", "to": "Topic_7_content", "width": 1}, {"from": "Topic_8", "to": "Topic_8_content", "width": 1}, {"from": "Topic_9", "to": "Topic_9_content", "width": 1}, {"from": "Topic_10", "to": "Topic_10_content", "width": 1}, {"from": "Topic_11", "to": "Topic_11_content", "width": 1}, {"from": "Topic_12", "to": "Topic_12_content", "width": 1}, {"from": "Topic_13", "to": "Topic_13_content", "width": 1}, {"from": "Topic_14", "to": "Topic_14_content", "width": 1}, {"from": "Topic_15", "to": "Topic_15_content", "width": 1}, {"from": "Topic_16", "to": "Topic_16_content", "width": 1}, {"from": "Topic_17", "to": "Topic_17_content", "width": 1}, {"from": "Topic_18", "to": "Topic_18_content", "width": 1}, {"from": "Topic_19", "to": "Topic_19_content", "width": 1}, {"from": "Topic_20", "to": "Topic_20_content", "width": 1}, {"from": "Topic_21", "to": "Topic_21_content", "width": 1}, {"from": "Topic_22", "to": "Topic_22_content", "width": 1}, {"from": "Topic_23", "to": "Topic_23_content", "width": 1}, {"from": "Topic_24", "to": "Topic_24_content", "width": 1}, {"from": "Topic_25", "to": "Topic_25_content", "width": 1}, {"from": "Topic_26", "to": "Topic_26_content", "width": 1}, {"from": "Topic_27", "to": "Topic_27_content", "width": 1}, {"from": "Topic_28", "to": "Topic_28_content", "width": 1}, {"from": "Topic_29", "to": "Topic_29_content", "width": 1}, {"from": "Topic_30", "to": "Topic_30_content", "width": 1}, {"from": "Topic_31", "to": "Topic_31_content", "width": 1}, {"from": "Topic_32", "to": "Topic_32_content", "width": 1}, {"from": "Topic_33", "to": "Topic_33_content", "width": 1}, {"from": "Topic_34", "to": "Topic_34_content", "width": 1}, {"from": "Topic_35", "to": "Topic_35_content", "width": 1}, {"from": "Topic_36", "to": "Topic_36_content", "width": 1}, {"from": "Topic_37", "to": "Topic_37_content", "width": 1}, {"from": "Topic_38", "to": "Topic_38_content", "width": 1}, {"from": "Topic_39", "to": "Topic_39_content", "width": 1}, {"from": "Topic_40", "to": "Topic_40_content", "width": 1}, {"from": "Topic_41", "to": "Topic_41_content", "width": 1}, {"from": "Topic_42", "to": "Topic_42_content", "width": 1}, {"from": "Topic_43", "to": "Topic_43_content", "width": 1}, {"from": "Topic_44", "to": "Topic_44_content", "width": 1}, {"from": "Topic_45", "to": "Topic_45_content", "width": 1}, {"from": "Topic_46", "to": "Topic_46_content", "width": 1}, {"from": "Topic_47", "to": "Topic_47_content", "width": 1}, {"from": "Topic_48", "to": "Topic_48_content", "width": 1}, {"from": "Topic_49", "to": "Topic_49_content", "width": 1}, {"from": "Topic_50", "to": "Topic_50_content", "width": 1}, {"from": "Topic_51", "to": "Topic_51_content", "width": 1}, {"from": "Topic_52", "to": "Topic_52_content", "width": 1}, {"from": "Topic_53", "to": "Topic_53_content", "width": 1}, {"from": "Topic_54", "to": "Topic_54_content", "width": 1}, {"from": "Topic_55", "to": "Topic_55_content", "width": 1}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {
    "configure": {
        "enabled": false
    },
    "edges": {
        "color": {
            "inherit": true
        },
        "smooth": {
            "enabled": true,
            "type": "dynamic"
        }
    },
    "interaction": {
        "dragNodes": true,
        "hideEdgesOnDrag": false,
        "hideNodesOnDrag": false
    },
    "physics": {
        "enabled": true,
        "stabilization": {
            "enabled": true,
            "fit": true,
            "iterations": 1000,
            "onlyDynamicEdges": false,
            "updateInterval": 50
        }
    }
};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  
                      network.on("stabilizationProgress", function(params) {
                          document.getElementById('loadingBar').removeAttribute("style");
                          var maxWidth = 496;
                          var minWidth = 20;
                          var widthFactor = params.iterations/params.total;
                          var width = Math.max(minWidth,maxWidth * widthFactor);
                          document.getElementById('bar').style.width = width + 'px';
                          document.getElementById('text').innerHTML = Math.round(widthFactor*100) + '%';
                      });
                      network.once("stabilizationIterationsDone", function() {
                          document.getElementById('text').innerHTML = '100%';
                          document.getElementById('bar').style.width = '496px';
                          document.getElementById('loadingBar').style.opacity = 0;
                          // really clean the dom element
                          setTimeout(function () {document.getElementById('loadingBar').style.display = 'none';}, 500);
                      });
                  

                  return network;

              }
              drawGraph();
        </script>
    </body>
</html>