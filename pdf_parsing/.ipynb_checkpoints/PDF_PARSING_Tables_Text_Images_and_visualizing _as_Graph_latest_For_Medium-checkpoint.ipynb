{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95cc4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf # imports the pymupdf library\n",
    "import pandas as pd \n",
    "import fitz  # import PyMuPDF\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ce03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###The Next Section is totally Decicated on Parsing of PDF to extract the text,images,tables.####\n",
    "###After the PDF is parsed it is transformed to Dictionary Object for the storage of the parsed content.####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f322a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(item, title=\"\"):\n",
    "    \"\"\"Display a pixmap.\n",
    "\n",
    "    Just to display Pixmap image of \"item\" - ignore the man behind the curtain.\n",
    "\n",
    "    Args:\n",
    "        item: any PyMuPDF object having a \"get_pixmap\" method.\n",
    "        title: a string to be used as image title\n",
    "\n",
    "    Generates an RGB Pixmap from item using a constant DPI and using matplotlib\n",
    "    to show it inline of the notebook.\n",
    "    \"\"\"\n",
    "    DPI = 300  # use this resolution\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # %matplotlib inline\n",
    "    pix = item.get_pixmap(dpi=DPI)\n",
    "    img = np.ndarray([pix.h, pix.w, 3], dtype=np.uint8, buffer=pix.samples_mv)\n",
    "    plt.figure(dpi=DPI)  # set the figure's DPI\n",
    "    plt.title(title)  # set title of image\n",
    "    _ = plt.imshow(img, extent=(0, pix.w * 72 / DPI, pix.h * 72 / DPI, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318fefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images_per_page(doc,doc_name,page,page_index,DPI):\n",
    "    image_list = page.get_images()\n",
    "    # print the number of images found on the page\n",
    "    if image_list:\n",
    "        print(f\"Found {len(image_list)} images on page {page_index}\")\n",
    "    else:\n",
    "        print(\"No images found on page\", page_index)\n",
    "\n",
    "    for image_index, img in enumerate(image_list, start=1): # enumerate the image list\n",
    "        xref = img[0] # get the XREF of the image\n",
    "        pix = pymupdf.Pixmap(doc, xref) # create a Pixmap\n",
    "        #pix = pymupdf.Pixmap(doc.extract_image(xref)[\"image\"])\n",
    "        #mask = pymupdf.Pixmap(doc.extract_image(smask)[\"image\"])\n",
    "        #pix = pymupdf.Pixmap(pix1, mask)\n",
    "\n",
    "        ### Commented Out Section for showing images################################\n",
    "        #if pix.n - pix.alpha > 3: # CMYK: convert to RGB first\n",
    "        #    pix = pymupdf.Pixmap(pymupdf.csRGB, pix)            \n",
    "        # %matplotlib inline\n",
    "        #pix = item.get_pixmap(dpi=DPI)\n",
    "        #img = np.ndarray([pix.h, pix.w, 3], dtype=np.uint8, buffer=pix.samples_mv)\n",
    "        #plt.figure(dpi=DPI)  # set the figure's DPI\n",
    "        #plt.title(title)  # set title of image\n",
    "        #_ = plt.imshow(img, extent=(0, pix.w * 72 / DPI, pix.h * 72 / DPI, 0))\n",
    "        \n",
    "        #############################################################################\n",
    "            \n",
    "        print('Image:')\n",
    "        print(type(img))\n",
    "            \n",
    "        pix.save(\"./Parsed_PDF_Output/\"+doc_name+\"/\"+\"page_\"+str(page_index+1)+\"/image_%s.jpg\" % (image_index)) # save the image as png\n",
    "        pix = None\n",
    "        \n",
    "    return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d321ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images_per_page(doc,doc_name,page_index,download):\n",
    "    print(\"IN extract_images_per_page\")\n",
    "    page_image_dict={}\n",
    "    page_number=page_index+1\n",
    "    page = doc[page_index] # get the page by index\n",
    "    \n",
    "    ####Get Images along with the metadata of it in the following order:\n",
    "    #(xref, smask, width, height, bpc, colorspace, alt. colorspace, name, filter, referencer)\n",
    "    #xref (int) is the image object number\n",
    "    #smask (int) is the object number of its soft-mask image\n",
    "    #width and height (ints) are the image dimensions\n",
    "    #bpc (int) denotes the number of bits per component (normally 8)\n",
    "    #colorspace (str) a string naming the colorspace (like DeviceRGB)\n",
    "    #alt. colorspace (str) is any alternate colorspace depending on the value of colorspace\n",
    "    #name (str) is the symbolic name by which the image is referenced\n",
    "    #filter (str) is the decode filter of the image (Adobe PDF References, pp. 22).\n",
    "    #referencer (int) the xref of the referencer. Zero if directly referenced by the page. Only present if full=True.\n",
    "    \n",
    "    image_list = page.get_images(full=True) #full=True as it will give the if any other pages are referencing\n",
    "                                            #the image.\n",
    "    \n",
    "    #for image in image_list:\n",
    "    #    xref, smask, width, height, bpc, colorspace, alt_colorspace, name, filter, referencer=image\n",
    "    #    print(\"width\")\n",
    "    #    print(width)\n",
    "    ###########################################################################################################\n",
    "    \n",
    "    img_cnt=len(image_list)\n",
    "    npy_img_lst=[]\n",
    "    DPI=150\n",
    "    title=\"\"\n",
    "    ###########Extraction Of Images in Numpy Format############\n",
    "    \n",
    "    for image_index, img in enumerate(image_list, start=1): # enumerate the image list\n",
    "        \n",
    "            img_meta_dict={}\n",
    "            \n",
    "            xref = img[0] # get the XREF of the image\n",
    "            \n",
    "            smask= img[1] # Get Object number of the Soft Mask of the Image\n",
    "            \n",
    "            width = img[2]\n",
    "            print(\"width\")\n",
    "            print(width)\n",
    "            \n",
    "            height = img[3]\n",
    "            print(\"height\")\n",
    "            print(height)\n",
    "            \n",
    "            num_bits = img[4] # Nuber of bits that is being used to represent the smallest component of the image\n",
    "            colorspace = img[5] #colorspace of the image\n",
    "            alt_colorspace = img[6] #colorspace of the image\n",
    "            sym_name = img[7] #Symbolic name of the image\n",
    "            img_filter = img[8] #decode filter of the image (Adobe PDF References, pp. 22)\n",
    "            img_ref = img[9] #xref of the referencer. Zero if directly referenced by the page. \n",
    "                             #Only present if full=True.\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            img_meta_dict[\"img_obj_num\"]=xref\n",
    "            img_meta_dict[\"smask_obj_num\"]=smask\n",
    "            img_meta_dict[\"width\"]=width\n",
    "            img_meta_dict[\"height\"]=height\n",
    "            img_meta_dict[\"num_bits\"]=num_bits\n",
    "            img_meta_dict[\"colorspace\"]=colorspace\n",
    "            img_meta_dict[\"alt_colorspace\"]=alt_colorspace\n",
    "            img_meta_dict[\"sym_name\"]=sym_name\n",
    "            img_meta_dict[\"filter\"]=img_filter\n",
    "            img_meta_dict[\"referencer\"]=img_ref\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            pix = pymupdf.Pixmap(doc, xref) # create a Pixmap\n",
    "            \n",
    "            #pix = pymupdf.Pixmap(doc.extract_image(xref)[\"image\"])\n",
    "            #mask = pymupdf.Pixmap(doc.extract_image(smask)[\"image\"])\n",
    "            #pix = pymupdf.Pixmap(pix1, mask)\n",
    "            \n",
    "            #pix.set_alpha(premultiply=False)\n",
    "            \n",
    "            ### Commented Out Section for showing images#############################\n",
    "\n",
    "            if pix.n - pix.alpha > 3: # CMYK: convert to RGB first\n",
    "                pix = pymupdf.Pixmap(pymupdf.csRGB, pix)\n",
    "            # %matplotlib inline\n",
    "            #pix = item.get_pixmap(dpi=DPI)\n",
    "            \n",
    "            print(\"PIX BUFFER SIZE\")\n",
    "            print(len(pix.samples_mv))\n",
    "            \n",
    "            \n",
    "            image_size=(pix.h*pix.w*3)\n",
    "            print('Original IMG_BUFFER_SIZE')\n",
    "            print(image_size)\n",
    "            \n",
    "            print('Page Image Buffer Size')\n",
    "            print(pix.samples_mv)\n",
    "            \n",
    "            #pix.size()=image_size\n",
    "            \n",
    "            try:\n",
    "                img = np.ndarray([pix.h, pix.w, 3], dtype=np.uint8, buffer=pix.samples_mv)\n",
    "            except:\n",
    "                print('Image too large for Picture')\n",
    "            finally:\n",
    "                #img = np.ndarray([pix.h, pix.w, 3], dtype=np.uint8, buffer=pix.samples_mv)\n",
    "                continue\n",
    "            \n",
    "            #plt.figure(dpi=DPI)  # set the figure's DPI\n",
    "            #plt.title(title)  # set title of image\n",
    "            #_ = plt.imshow(img, extent=(0, pix.w * 18 / DPI, pix.h * 18 / DPI, 0))\n",
    "            \n",
    "            #############################################################################\n",
    "            \n",
    "            #print('Image:')\n",
    "            #print(type(img))\n",
    "            \n",
    "            #Encode the Image into Base64\n",
    "\n",
    "            #img_enc = base64.b64encode(img)\n",
    "            \n",
    "            #For Decoding use the following statement \n",
    "            \n",
    "            #decoded_image = base64.decodestring(img_enc)\n",
    "            \n",
    "            img_meta_dict[\"img_matrix\"]=img\n",
    "            \n",
    "            npy_img_lst.append(img_meta_dict)\n",
    "            \n",
    "            #pix.save(\"page_%s-image_%s.png\" % (page_index, image_index)) # save the image as png\n",
    "            #pix = None\n",
    "            \n",
    "    \n",
    "    ###########################################################\n",
    "    \n",
    "    page_image_dict['page']=page_number\n",
    "    #page_image_dict['img_lst']=image_list\n",
    "    page_image_dict['img_cnt']=len(image_list)\n",
    "    page_image_dict['img_npy_lst']=npy_img_lst\n",
    "    \n",
    "    if download:\n",
    "        download_images_per_page(doc,doc_name,page,page_index,DPI)\n",
    "    \n",
    "    return page_image_dict\n",
    "\n",
    "\n",
    "def extract_text_tables_images_per_page(doc,doc_name,doc_img,index,download):\n",
    "    print(\"IN extract_text_tables_images_per_page\")\n",
    "    page_dict={}\n",
    "    page_image_dict={}\n",
    "    tab_df_lst=[]\n",
    "    page = doc[index]\n",
    "    tabs = page.find_tables()  # detect the tables\n",
    "    \n",
    "    ##Extract Images From Pages############\n",
    "    \n",
    "    page_image_dict=extract_images_per_page(doc_img,doc_name,index,download)\n",
    "    \n",
    "    print(\"page_image_dict\")\n",
    "    print(page_image_dict)\n",
    "    \n",
    "    \n",
    "    page_dict['page']=page_image_dict['page']\n",
    "    #page_dict['img_lst']=page_image_dict['img_lst']\n",
    "\n",
    "    page_dict['img_cnt']=page_image_dict['img_cnt']\n",
    "    \n",
    "    if page_dict['img_cnt']==0:\n",
    "        page_dict['img_flag']=0\n",
    "    else:\n",
    "        page_dict['img_flag']=1\n",
    "    \n",
    "    page_dict['img_npy_lst']=page_image_dict['img_npy_lst']\n",
    "    \n",
    "    #######################################\n",
    "    \n",
    "    ############################Extract Text#######################################################\n",
    "    #Extract Text From each page\n",
    "    text = page.get_text()\n",
    "    page_dict['text']=text\n",
    "    \n",
    "    print(text)\n",
    "    ###############################################################################################\n",
    "    \n",
    "    #for i,tab in enumerate(tabs):  # iterate over all tables\n",
    "    #    for cell in tab.header.cells:\n",
    "    #       page.draw_rect(cell,color=fitz.pdfcolor[\"red\"],width=0.3)\n",
    "    #    page.draw_rect(tab.bbox,color=fitz.pdfcolor[\"green\"])\n",
    "    #    print(f\"Table {i} column names: {tab.header.names}, external: {tab.header.external}\")\n",
    "    \n",
    "    #show_image(page, f\"Table & Header BBoxes\")\n",
    "   \n",
    "    # choose the second table for conversion to a DataFrame\n",
    "    #tab = tabs[0]\n",
    "    #print(tabs)\n",
    "    \n",
    "    if tabs.tables == []:\n",
    "        page_dict['tables_flag']=0\n",
    "    else:\n",
    "        for tab in tabs:\n",
    "            df=pd.DataFrame()\n",
    "            df = tab.to_pandas()\n",
    "            tab_df_lst.append(df)\n",
    "        page_dict['tables_flag']=1\n",
    "    \n",
    "    page_dict['tables']=tab_df_lst\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(tab_df_lst)\n",
    "    #df = tab.to_pandas()\n",
    "    # show the DataFrame\n",
    "    return page_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e215785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpyencoder import NumpyEncoder\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "def extract_text_images_tables(doc_path,download=False):\n",
    "    \n",
    "    doc_per_page_tabs_lst=[]\n",
    "    doc = fitz.open(doc_path)\n",
    "    \n",
    "    num_pages=len(doc)\n",
    "    \n",
    "    doc_img = pymupdf.open(doc_path)\n",
    "    \n",
    "    doc_name=url.split('/')[-1]\n",
    "    \n",
    "    doc_type=url.split('.')[-1]\n",
    "    \n",
    "    doc_name_wo_type=url.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    print(doc_name_wo_type)\n",
    "    \n",
    "    isExist = os.path.exists('./Parsed_PDF_Output/'+doc_name_wo_type)\n",
    "    \n",
    "    if isExist:\n",
    "        shutil.rmtree('./Parsed_PDF_Output/'+doc_name_wo_type)\n",
    "        \n",
    "    os.mkdir('./Parsed_PDF_Output/'+doc_name_wo_type)\n",
    "    \n",
    "    for i in range(1,num_pages):\n",
    "        #page_image_dict={}\n",
    "        \n",
    "        os.mkdir('./Parsed_PDF_Output/'+doc_name_wo_type+'/page_'+str(i+1))\n",
    "        \n",
    "        tab_df_lst=extract_text_tables_images_per_page(doc,doc_name_wo_type,doc_img,i,download)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if len(tab_df_lst) == 0:\n",
    "            print(\"Do Nothing Here\")\n",
    "        else: \n",
    "            doc_per_page_tabs_lst.append(tab_df_lst)\n",
    "        \n",
    "    \n",
    "    \n",
    "    document_dictionary={\"name\":doc_name,\"type\":doc_type, \"data\": doc_per_page_tabs_lst}\n",
    "\n",
    "    if download:\n",
    "        #Save the List of MetaData to pickle in disk.\n",
    "        with open(\"./Parsed_PDF_Output/\"+doc_name_wo_type+'/'+doc_name+'.pkl', 'wb') as f:\n",
    "            pickle.dump(document_dictionary, f)\n",
    "    \n",
    "    return doc_per_page_tabs_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac81a21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d29ae6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kubernetes_in_Action\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 2, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "Kubernetes resources covered in the book\n",
      "* Cluster-level resource (not namespaced)\n",
      "** Also in other API versions; listed version is the one used in this book\n",
      "(continues on inside back cover)\n",
      "Resource (abbr.) [API version]\n",
      "Description\n",
      "Section\n",
      "Namespace* (ns) [v1]\n",
      "Enables organizing resources into non-overlapping \n",
      "groups (for example, per tenant)\n",
      "3.7\n",
      "Deploying workloads\n",
      "Pod (po) [v1]\n",
      "The basic deployable unit containing one or more \n",
      "processes in co-located containers\n",
      "3.1\n",
      "ReplicaSet (rs) [apps/v1beta2**]\n",
      "Keeps one or more pod replicas running\n",
      "4.3\n",
      "ReplicationController (rc) [v1]\n",
      "The older, less-powerful equivalent of a \n",
      "ReplicaSet\n",
      "4.2\n",
      "Job [batch/v1]\n",
      "Runs pods that perform a completable task\n",
      "4.5\n",
      "CronJob [batch/v1beta1]\n",
      "Runs a scheduled job once or periodically\n",
      "4.6\n",
      "DaemonSet (ds) [apps/v1beta2**]\n",
      "Runs one pod replica per node (on all nodes or \n",
      "only on those matching a node selector)\n",
      "4.4\n",
      "StatefulSet (sts) [apps/v1beta1**]\n",
      "Runs stateful pods with a stable identity\n",
      "10.2\n",
      "Deployment (deploy) [apps/v1beta1**]\n",
      "Declarative deployment and updates of pods\n",
      "9.3\n",
      "Services\n",
      "Service (svc) [v1]\n",
      "Exposes one or more pods at a single and stable \n",
      "IP address and port pair\n",
      "5.1\n",
      "Endpoints (ep) [v1]\n",
      "Defines which pods (or other servers) are \n",
      "exposed through a service\n",
      "5.2.1\n",
      "Ingress (ing) [extensions/v1beta1]\n",
      "Exposes one or more services to external clients \n",
      "through a single externally reachable IP address\n",
      "5.4\n",
      "Config\n",
      "ConfigMap (cm) [v1]\n",
      "A key-value map for storing non-sensitive config \n",
      "options for apps and exposing it to them\n",
      "7.4\n",
      "Secret [v1]\n",
      "Like a ConfigMap, but for sensitive data\n",
      "7.5\n",
      "Storage\n",
      "PersistentVolume* (pv) [v1]\n",
      "Points to persistent storage that can be mounted \n",
      "into a pod through a PersistentVolumeClaim\n",
      "6.5\n",
      "PersistentVolumeClaim (pvc) [v1]\n",
      "A request for and claim to a PersistentVolume\n",
      "6.5\n",
      "StorageClass* (sc) [storage.k8s.io/v1]\n",
      "Defines the type of dynamically-provisioned stor-\n",
      "age claimable in a PersistentVolumeClaim\n",
      "6.6\n",
      " \n",
      "www.allitebooks.com\n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 3, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "Kubernetes in Action\n",
      " \n",
      "www.allitebooks.com\n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 4, 'img_cnt': 0, 'img_npy_lst': []}\n",
      " \n",
      "www.allitebooks.com\n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 5, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "Kubernetes\n",
      "in Action\n",
      "MARKO LUKŠA\n",
      "M A N N I N G\n",
      "SHELTER ISLAND\n",
      " \n",
      "www.allitebooks.com\n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 6, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "For online information and ordering of this and other Manning books, please visit\n",
      "www.manning.com. The publisher offers discounts on this book when ordered in quantity. \n",
      "For more information, please contact\n",
      "Special Sales Department\n",
      "Manning Publications Co.\n",
      "20 Baldwin Road\n",
      "PO Box 761\n",
      "Shelter Island, NY 11964\n",
      "Email: orders@manning.com\n",
      "©2018 by Manning Publications Co. All rights reserved.\n",
      "No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \n",
      "any form or by means electronic, mechanical, photocopying, or otherwise, without prior written \n",
      "permission of the publisher.\n",
      "Many of the designations used by manufacturers and sellers to distinguish their products are \n",
      "claimed as trademarks. Where those designations appear in the book, and Manning \n",
      "Publications was aware of a trademark claim, the designations have been printed in initial caps \n",
      "or all caps.\n",
      "Recognizing the importance of preserving what has been written, it is Manning’s policy to have \n",
      "the books we publish printed on acid-free paper, and we exert our best efforts to that end. \n",
      "Recognizing also our responsibility to conserve the resources of our planet, Manning books\n",
      "are printed on paper that is at least 15 percent recycled and processed without the use of \n",
      "elemental chlorine.\n",
      "Manning Publications Co.\n",
      "Development editor: Elesha Hyde\n",
      "20 Baldwin Road\n",
      "Review editor: Aleksandar Dragosavljevic\n",
      "´\n",
      "PO Box 761\n",
      "Technical development editor: Jeanne Boyarsky\n",
      "Shelter Island, NY 11964\n",
      "Project editor: Kevin Sullivan\n",
      "Copyeditor: Katie Petito\n",
      "Proofreader: Melody Dolab\n",
      "Technical proofreader: Antonio Magnaghi\n",
      "Illustrator: Chuck Larson\n",
      "Typesetter: Dennis Dalinnik\n",
      "Cover designer: Marija Tudor\n",
      "ISBN: 9781617293726\n",
      "Printed in the United States of America\n",
      "1 2 3 4 5 6 7 8 9 10 – EBM – 22 21 20 19 18 17\n",
      " \n",
      "www.allitebooks.com\n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 7, 'img_cnt': 0, 'img_npy_lst': []}\n",
      " To my parents, \n",
      "who have always put their children’s needs above their own\n",
      " \n",
      "www.allitebooks.com\n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 8, 'img_cnt': 0, 'img_npy_lst': []}\n",
      " \n",
      " \n",
      "www.allitebooks.com\n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 9, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "vii\n",
      "brief contents\n",
      "PART 1\n",
      "OVERVIEW\n",
      "1\n",
      "■\n",
      "Introducing Kubernetes\n",
      "1\n",
      "2\n",
      "■\n",
      "First steps with Docker and Kubernetes\n",
      "25\n",
      "PART 2\n",
      "CORE CONCEPTS\n",
      "3\n",
      "■\n",
      "Pods: running containers in Kubernetes\n",
      "55\n",
      "4\n",
      "■\n",
      "Replication and other controllers: deploying \n",
      "managed pods\n",
      "84\n",
      "5\n",
      "■\n",
      "Services: enabling clients to discover and talk \n",
      "to pods\n",
      "120\n",
      "6\n",
      "■\n",
      "Volumes: attaching disk storage to containers\n",
      "159\n",
      "7\n",
      "■\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "191\n",
      "8\n",
      "■\n",
      "Accessing pod metadata and other resources from \n",
      "applications\n",
      "225\n",
      "9\n",
      "■\n",
      "Deployments: updating applications declaratively \n",
      "250\n",
      "10\n",
      "■\n",
      "StatefulSets: deploying replicated stateful \n",
      "applications\n",
      "280\n",
      " \n",
      "www.allitebooks.com\n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 10, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "BRIEF CONTENTS\n",
      "viii\n",
      "PART 3\n",
      "BEYOND THE BASICS\n",
      "11\n",
      "■\n",
      "Understanding Kubernetes internals\n",
      "309\n",
      "12\n",
      "■\n",
      "Securing the Kubernetes API server\n",
      "346\n",
      "13\n",
      "■\n",
      "Securing cluster nodes and the network\n",
      "375\n",
      "14\n",
      "■\n",
      "Managing pods’ computational resources\n",
      "404\n",
      "15\n",
      "■\n",
      "Automatic scaling of pods and cluster nodes\n",
      "437\n",
      "16\n",
      "■\n",
      "Advanced scheduling\n",
      "457\n",
      "17\n",
      "■\n",
      "Best practices for developing apps\n",
      "477\n",
      "18\n",
      "■\n",
      "Extending Kubernetes\n",
      "508\n",
      " \n",
      "www.allitebooks.com\n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 11, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "ix\n",
      "contents\n",
      "preface\n",
      "xxi\n",
      "acknowledgments\n",
      "xxiii\n",
      "about this book\n",
      "xxv\n",
      "about the author\n",
      "xxix\n",
      "about the cover illustration\n",
      "xxx\n",
      "PART 1\n",
      "OVERVIEW\n",
      "1 \n",
      "Introducing Kubernetes\n",
      "1\n",
      "1.1\n",
      "Understanding the need for a system like Kubernetes\n",
      "2\n",
      "Moving from monolithic apps to microservices\n",
      "3\n",
      "■Providing a \n",
      "consistent environment to applications\n",
      "6\n",
      "■Moving to continuous \n",
      "delivery: DevOps and NoOps\n",
      "6\n",
      "1.2\n",
      "Introducing container technologies\n",
      "7\n",
      "Understanding what containers are\n",
      "8\n",
      "■Introducing the Docker \n",
      "container platform\n",
      "12\n",
      "■Introducing rkt—an alternative to Docker\n",
      "15\n",
      "1.3\n",
      "Introducing Kubernetes\n",
      "16\n",
      "Understanding its origins\n",
      "16\n",
      "■Looking at Kubernetes from the \n",
      "top of a mountain\n",
      "16\n",
      "■Understanding the architecture of a \n",
      "Kubernetes cluster\n",
      "18\n",
      "■Running an application in Kubernetes\n",
      "19\n",
      "Understanding the benefits of using Kubernetes\n",
      "21\n",
      "1.4\n",
      "Summary\n",
      "23\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 12, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "CONTENTS\n",
      "x\n",
      "2 \n",
      "First steps with Docker and Kubernetes\n",
      "25\n",
      "2.1\n",
      "Creating, running, and sharing a container image\n",
      "26\n",
      "Installing Docker and running a Hello World container\n",
      "26\n",
      "Creating a trivial Node.js app\n",
      "28\n",
      "■Creating a Dockerfile \n",
      "for the image\n",
      "29\n",
      "■Building the container image\n",
      "29\n",
      "Running the container image\n",
      "32\n",
      "■Exploring the inside \n",
      "of a running container\n",
      "33\n",
      "■Stopping and removing a \n",
      "container\n",
      "34\n",
      "■Pushing the image to an image registry\n",
      "35\n",
      "2.2\n",
      "Setting up a Kubernetes cluster\n",
      "36\n",
      "Running a local single-node Kubernetes cluster with Minikube\n",
      "37\n",
      "Using a hosted Kubernetes cluster with Google Kubernetes \n",
      "Engine\n",
      "38\n",
      "■Setting up an alias and command-line completion \n",
      "for kubectl\n",
      "41\n",
      "2.3\n",
      "Running your first app on Kubernetes\n",
      "42\n",
      "Deploying your Node.js app\n",
      "42\n",
      "■Accessing your web \n",
      "application\n",
      "45\n",
      "■The logical parts of your system\n",
      "47\n",
      "Horizontally scaling the application\n",
      "48\n",
      "■Examining what \n",
      "nodes your app is running on\n",
      "51\n",
      "■Introducing the \n",
      "Kubernetes dashboard\n",
      "52\n",
      "2.4\n",
      "Summary\n",
      "53\n",
      "PART 2\n",
      "CORE CONCEPTS\n",
      "3 \n",
      "Pods: running containers in Kubernetes\n",
      "55\n",
      "3.1\n",
      "Introducing pods\n",
      "56\n",
      "Understanding why we need pods\n",
      "56\n",
      "■Understanding pods\n",
      "57\n",
      "Organizing containers across pods properly\n",
      "58\n",
      "3.2\n",
      "Creating pods from YAML or JSON descriptors\n",
      "61\n",
      "Examining a YAML descriptor of an existing pod\n",
      "61\n",
      "■Creating a \n",
      "simple YAML descriptor for a pod\n",
      "63\n",
      "■Using kubectl create to \n",
      "create the pod\n",
      "65\n",
      "■Viewing application logs\n",
      "65\n",
      "■Sending \n",
      "requests to the pod\n",
      "66\n",
      "3.3\n",
      "Organizing pods with labels\n",
      "67\n",
      "Introducing labels\n",
      "68\n",
      "■Specifying labels when creating a pod\n",
      "69\n",
      "Modifying labels of existing pods\n",
      "70\n",
      "3.4\n",
      "Listing subsets of pods through label selectors\n",
      "71\n",
      "Listing pods using a label selector\n",
      "71\n",
      "■Using multiple conditions \n",
      "in a label selector\n",
      "72\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 13, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "CONTENTS\n",
      "xi\n",
      "3.5\n",
      "Using labels and selectors to constrain pod \n",
      "scheduling\n",
      "73\n",
      "Using labels for categorizing worker nodes\n",
      "74\n",
      "■Scheduling pods to \n",
      "specific nodes\n",
      "74\n",
      "■Scheduling to one specific node\n",
      "75\n",
      "3.6\n",
      "Annotating pods\n",
      "75\n",
      "Looking up an object’s annotations\n",
      "75\n",
      "■Adding and modifying \n",
      "annotations\n",
      "76\n",
      "3.7\n",
      "Using namespaces to group resources\n",
      "76\n",
      "Understanding the need for namespaces\n",
      "77\n",
      "■Discovering other \n",
      "namespaces and their pods\n",
      "77\n",
      "■Creating a namespace\n",
      "78\n",
      "Managing objects in other namespaces\n",
      "79\n",
      "■Understanding \n",
      "the isolation provided by namespaces\n",
      "79\n",
      "3.8\n",
      "Stopping and removing pods\n",
      "80\n",
      "Deleting a pod by name\n",
      "80\n",
      "■Deleting pods using label \n",
      "selectors\n",
      "80\n",
      "■Deleting pods by deleting the whole \n",
      "namespace\n",
      "80\n",
      "■Deleting all pods in a namespace, \n",
      "while keeping the namespace\n",
      "81\n",
      "■Deleting (almost) \n",
      "all resources in a namespace\n",
      "82\n",
      "3.9\n",
      "Summary\n",
      "82\n",
      "4 \n",
      "Replication and other controllers: deploying managed pods\n",
      "84\n",
      "4.1\n",
      "Keeping pods healthy\n",
      "85\n",
      "Introducing liveness probes\n",
      "85\n",
      "■Creating an HTTP-based \n",
      "liveness probe\n",
      "86\n",
      "■Seeing a liveness probe in action\n",
      "87\n",
      "Configuring additional properties of the liveness probe\n",
      "88\n",
      "Creating effective liveness probes\n",
      "89\n",
      "4.2\n",
      "Introducing ReplicationControllers\n",
      "90\n",
      "The operation of a ReplicationController\n",
      "91\n",
      "■Creating a \n",
      "ReplicationController\n",
      "93\n",
      "■Seeing the ReplicationController \n",
      "in action\n",
      "94\n",
      "■Moving pods in and out of the scope of a \n",
      "ReplicationController\n",
      "98\n",
      "■Changing the pod template\n",
      "101\n",
      "Horizontally scaling pods\n",
      "102\n",
      "■Deleting a \n",
      "ReplicationController\n",
      "103\n",
      "4.3\n",
      "Using ReplicaSets instead of ReplicationControllers\n",
      "104\n",
      "Comparing a ReplicaSet to a ReplicationController\n",
      "105\n",
      "Defining a ReplicaSet\n",
      "105\n",
      "■Creating and examining a \n",
      "ReplicaSet\n",
      "106\n",
      "■Using the ReplicaSet’s more expressive \n",
      "label selectors\n",
      "107\n",
      "■Wrapping up ReplicaSets\n",
      "108\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 14, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "CONTENTS\n",
      "xii\n",
      "4.4\n",
      "Running exactly one pod on each node with \n",
      "DaemonSets\n",
      "108\n",
      "Using a DaemonSet to run a pod on every node\n",
      "109\n",
      "Using a DaemonSet to run pods only on certain nodes\n",
      "109\n",
      "4.5\n",
      "Running pods that perform a single completable \n",
      "task\n",
      "112\n",
      "Introducing the Job resource\n",
      "112\n",
      "■Defining a Job resource\n",
      "113\n",
      "Seeing a Job run a pod\n",
      "114\n",
      "■Running multiple pod instances \n",
      "in a Job\n",
      "114\n",
      "■Limiting the time allowed for a Job pod to \n",
      "complete\n",
      "116\n",
      "4.6\n",
      "Scheduling Jobs to run periodically or once \n",
      "in the future\n",
      "116\n",
      "Creating a CronJob\n",
      "116\n",
      "■Understanding how scheduled \n",
      "jobs are run\n",
      "117\n",
      "4.7\n",
      "Summary\n",
      "118\n",
      "5 \n",
      "Services: enabling clients to discover and talk to pods\n",
      "120\n",
      "5.1\n",
      "Introducing services\n",
      "121\n",
      "Creating services\n",
      "122\n",
      "■Discovering services\n",
      "128\n",
      "5.2\n",
      "Connecting to services living outside the cluster\n",
      "131\n",
      "Introducing service endpoints\n",
      "131\n",
      "■Manually configuring \n",
      "service endpoints\n",
      "132\n",
      "■Creating an alias for an external \n",
      "service\n",
      "134\n",
      "5.3\n",
      "Exposing services to external clients\n",
      "134\n",
      "Using a NodePort service\n",
      "135\n",
      "■Exposing a service through \n",
      "an external load balancer\n",
      "138\n",
      "■Understanding the peculiarities \n",
      "of external connections\n",
      "141\n",
      "5.4\n",
      "Exposing services externally through an Ingress \n",
      "resource\n",
      "142\n",
      "Creating an Ingress resource\n",
      "144\n",
      "■Accessing the service \n",
      "through the Ingress\n",
      "145\n",
      "■Exposing multiple services \n",
      "through the same Ingress\n",
      "146\n",
      "■Configuring Ingress to \n",
      "handle TLS traffic\n",
      "147\n",
      "5.5\n",
      "Signaling when a pod is ready to accept connections\n",
      "149\n",
      "Introducing readiness probes\n",
      "149\n",
      "■Adding a readiness probe \n",
      "to a pod\n",
      "151\n",
      "■Understanding what real-world readiness \n",
      "probes should do\n",
      "153\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 15, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "CONTENTS\n",
      "xiii\n",
      "5.6\n",
      "Using a headless service for discovering individual \n",
      "pods\n",
      "154\n",
      "Creating a headless service\n",
      "154\n",
      "■Discovering pods \n",
      "through DNS\n",
      "155\n",
      "■Discovering all pods—even those \n",
      "that aren’t ready\n",
      "156\n",
      "5.7\n",
      "Troubleshooting services\n",
      "156\n",
      "5.8\n",
      "Summary\n",
      "157\n",
      "6 \n",
      "Volumes: attaching disk storage to containers\n",
      "159\n",
      "6.1\n",
      "Introducing volumes\n",
      "160\n",
      "Explaining volumes in an example\n",
      "160\n",
      "■Introducing available \n",
      "volume types\n",
      "162\n",
      "6.2\n",
      "Using volumes to share data between containers\n",
      "163\n",
      "Using an emptyDir volume\n",
      "163\n",
      "■Using a Git repository as the \n",
      "starting point for a volume\n",
      "166\n",
      "6.3\n",
      "Accessing files on the worker node’s filesystem\n",
      "169\n",
      "Introducing the hostPath volume\n",
      "169\n",
      "■Examining system pods \n",
      "that use hostPath volumes\n",
      "170\n",
      "6.4\n",
      "Using persistent storage\n",
      "171\n",
      "Using a GCE Persistent Disk in a pod volume\n",
      "171\n",
      "■Using other \n",
      "types of volumes with underlying persistent storage\n",
      "174\n",
      "6.5\n",
      "Decoupling pods from the underlying storage \n",
      "technology\n",
      "176\n",
      "Introducing PersistentVolumes and PersistentVolumeClaims\n",
      "176\n",
      "Creating a PersistentVolume\n",
      "177\n",
      "■Claiming a PersistentVolume \n",
      "by creating a PersistentVolumeClaim\n",
      "179\n",
      "■Using a \n",
      "PersistentVolumeClaim in a pod\n",
      "181\n",
      "■Understanding the \n",
      "benefits of using PersistentVolumes and claims\n",
      "182\n",
      "■Recycling \n",
      "PersistentVolumes\n",
      "183\n",
      "6.6\n",
      "Dynamic provisioning of PersistentVolumes\n",
      "184\n",
      "Defining the available storage types through StorageClass \n",
      "resources\n",
      "185\n",
      "■Requesting the storage class in a \n",
      "PersistentVolumeClaim\n",
      "185\n",
      "■Dynamic provisioning \n",
      "without specifying a storage class\n",
      "187\n",
      "6.7\n",
      "Summary\n",
      "190\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 16, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "CONTENTS\n",
      "xiv\n",
      "7 \n",
      "ConfigMaps and Secrets: configuring applications\n",
      "191\n",
      "7.1\n",
      "Configuring containerized applications\n",
      "191\n",
      "7.2\n",
      "Passing command-line arguments to containers\n",
      "192\n",
      "Defining the command and arguments in Docker\n",
      "193\n",
      "Overriding the command and arguments in Kubernetes\n",
      "195\n",
      "7.3\n",
      "Setting environment variables for a container\n",
      "196\n",
      "Specifying environment variables in a container definition\n",
      "197\n",
      "Referring to other environment variables in a variable’s value\n",
      "198\n",
      "Understanding the drawback of hardcoding environment \n",
      "variables\n",
      "198\n",
      "7.4\n",
      "Decoupling configuration with a ConfigMap\n",
      "198\n",
      "Introducing ConfigMaps\n",
      "198\n",
      "■Creating a ConfigMap\n",
      "200\n",
      "Passing a ConfigMap entry to a container as an environment \n",
      "variable\n",
      "202\n",
      "■Passing all entries of a ConfigMap as environment \n",
      "variables at once\n",
      "204\n",
      "■Passing a ConfigMap entry as a \n",
      "command-line argument\n",
      "204\n",
      "■Using a configMap volume to \n",
      "expose ConfigMap entries as files\n",
      "205\n",
      "■Updating an app’s config \n",
      "without having to restart the app\n",
      "211\n",
      "7.5\n",
      "Using Secrets to pass sensitive data to containers\n",
      "213\n",
      "Introducing Secrets\n",
      "214\n",
      "■Introducing the default token \n",
      "Secret\n",
      "214\n",
      "■Creating a Secret\n",
      "216\n",
      "■Comparing ConfigMaps \n",
      "and Secrets\n",
      "217\n",
      "■Using the Secret in a pod\n",
      "218\n",
      "Understanding image pull Secrets\n",
      "222\n",
      "7.6\n",
      "Summary\n",
      "224\n",
      "8 \n",
      "Accessing pod metadata and other resources from \n",
      "applications\n",
      "225\n",
      "8.1\n",
      "Passing metadata through the Downward API\n",
      "226\n",
      "Understanding the available metadata\n",
      "226\n",
      "■Exposing metadata \n",
      "through environment variables\n",
      "227\n",
      "■Passing metadata through \n",
      "files in a downwardAPI volume\n",
      "230\n",
      "8.2\n",
      "Talking to the Kubernetes API server\n",
      "233\n",
      "Exploring the Kubernetes REST API\n",
      "234\n",
      "■Talking to the API \n",
      "server from within a pod\n",
      "238\n",
      "■Simplifying API server \n",
      "communication with ambassador containers\n",
      "243\n",
      "■Using client \n",
      "libraries to talk to the API server\n",
      "246\n",
      "8.3\n",
      "Summary\n",
      "249\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 17, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "CONTENTS\n",
      "xv\n",
      "9 \n",
      "Deployments: updating applications declaratively\n",
      "250\n",
      "9.1\n",
      "Updating applications running in pods\n",
      "251\n",
      "Deleting old pods and replacing them with new ones\n",
      "252\n",
      "Spinning up new pods and then deleting the old ones\n",
      "252\n",
      "9.2\n",
      "Performing an automatic rolling update with a \n",
      "ReplicationController\n",
      "254\n",
      "Running the initial version of the app\n",
      "254\n",
      "■Performing a rolling \n",
      "update with kubectl\n",
      "256\n",
      "■Understanding why kubectl rolling-\n",
      "update is now obsolete\n",
      "260\n",
      "9.3\n",
      "Using Deployments for updating apps declaratively\n",
      "261\n",
      "Creating a Deployment\n",
      "262\n",
      "■Updating a Deployment\n",
      "264\n",
      "Rolling back a deployment\n",
      "268\n",
      "■Controlling the rate of the \n",
      "rollout\n",
      "271\n",
      "■Pausing the rollout process\n",
      "273\n",
      "■Blocking \n",
      "rollouts of bad versions\n",
      "274\n",
      "9.4\n",
      "Summary\n",
      "279\n",
      "10 \n",
      "StatefulSets: deploying replicated stateful applications\n",
      "280\n",
      "10.1\n",
      "Replicating stateful pods\n",
      "281\n",
      "Running multiple replicas with separate storage for each\n",
      "281\n",
      "Providing a stable identity for each pod\n",
      "282\n",
      "10.2\n",
      "Understanding StatefulSets\n",
      "284\n",
      "Comparing StatefulSets with ReplicaSets\n",
      "284\n",
      "■Providing a \n",
      "stable network identity\n",
      "285\n",
      "■Providing stable dedicated storage \n",
      "to each stateful instance\n",
      "287\n",
      "■Understanding StatefulSet \n",
      "guarantees\n",
      "289\n",
      "10.3\n",
      "Using a StatefulSet\n",
      "290\n",
      "Creating the app and container image\n",
      "290\n",
      "■Deploying the app \n",
      "through a StatefulSet\n",
      "291\n",
      "■Playing with your pods\n",
      "295\n",
      "10.4\n",
      "Discovering peers in a StatefulSet\n",
      "299\n",
      "Implementing peer discovery through DNS\n",
      "301\n",
      "■Updating a \n",
      "StatefulSet\n",
      "302\n",
      "■Trying out your clustered data store\n",
      "303\n",
      "10.5\n",
      "Understanding how StatefulSets deal with node \n",
      "failures\n",
      "304\n",
      "Simulating a node’s disconnection from the network\n",
      "304\n",
      "Deleting the pod manually\n",
      "306\n",
      "10.6\n",
      "Summary\n",
      "307\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 18, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "CONTENTS\n",
      "xvi\n",
      "PART 3\n",
      "BEYOND THE BASICS\n",
      "11 \n",
      "Understanding Kubernetes internals\n",
      "309\n",
      "11.1\n",
      "Understanding the architecture\n",
      "310\n",
      "The distributed nature of Kubernetes components\n",
      "310\n",
      "How Kubernetes uses etcd\n",
      "312\n",
      "■What the API server does\n",
      "316\n",
      "Understanding how the API server notifies clients of resource \n",
      "changes\n",
      "318\n",
      "■Understanding the Scheduler\n",
      "319\n",
      "Introducing the controllers running in the Controller Manager\n",
      "321\n",
      "What the Kubelet does\n",
      "326\n",
      "■The role of the Kubernetes Service \n",
      "Proxy\n",
      "327\n",
      "■Introducing Kubernetes add-ons\n",
      "328\n",
      "■Bringing it \n",
      "all together\n",
      "330\n",
      "11.2\n",
      "How controllers cooperate\n",
      "330\n",
      "Understanding which components are involved\n",
      "330\n",
      "■The chain \n",
      "of events\n",
      "331\n",
      "■Observing cluster events\n",
      "332\n",
      "11.3\n",
      "Understanding what a running pod is\n",
      "333\n",
      "11.4\n",
      "Inter-pod networking\n",
      "335\n",
      "What the network must be like\n",
      "335\n",
      "■Diving deeper into \n",
      "how networking works\n",
      "336\n",
      "■Introducing the Container \n",
      "Network Interface\n",
      "338\n",
      "11.5\n",
      "How services are implemented\n",
      "338\n",
      "Introducing the kube-proxy\n",
      "339\n",
      "■How kube-proxy uses iptables\n",
      "339\n",
      "11.6\n",
      "Running highly available clusters\n",
      "341\n",
      "Making your apps highly available\n",
      "341\n",
      "■Making Kubernetes \n",
      "Control Plane components highly available\n",
      "342\n",
      "11.7\n",
      "Summary\n",
      "345\n",
      "12 \n",
      "Securing the Kubernetes API server\n",
      "346\n",
      "12.1\n",
      "Understanding authentication\n",
      "346\n",
      "Users and groups\n",
      "347\n",
      "■Introducing ServiceAccounts\n",
      "348\n",
      "Creating ServiceAccounts\n",
      "349\n",
      "■Assigning a ServiceAccount \n",
      "to a pod\n",
      "351\n",
      "12.2\n",
      "Securing the cluster with role-based access control\n",
      "353\n",
      "Introducing the RBAC authorization plugin\n",
      "353\n",
      "■Introducing \n",
      "RBAC resources\n",
      "355\n",
      "■Using Roles and RoleBindings\n",
      "358\n",
      "Using ClusterRoles and ClusterRoleBindings\n",
      "362\n",
      "Understanding default ClusterRoles and ClusterRoleBindings\n",
      "371\n",
      "Granting authorization permissions wisely\n",
      "373\n",
      "12.3\n",
      "Summary\n",
      "373\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 19, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "CONTENTS\n",
      "xvii\n",
      "13 \n",
      "Securing cluster nodes and the network\n",
      "375\n",
      "13.1\n",
      "Using the host node’s namespaces in a pod\n",
      "376\n",
      "Using the node’s network namespace in a pod\n",
      "376\n",
      "■Binding to \n",
      "a host port without using the host’s network namespace\n",
      "377\n",
      "Using the node’s PID and IPC namespaces\n",
      "379\n",
      "13.2\n",
      "Configuring the container’s security context\n",
      "380\n",
      "Running a container as a specific user\n",
      "381\n",
      "■Preventing a \n",
      "container from running as root\n",
      "382\n",
      "■Running pods in \n",
      "privileged mode\n",
      "382\n",
      "■Adding individual kernel capabilities \n",
      "to a container\n",
      "384\n",
      "■Dropping capabilities from a container\n",
      "385\n",
      "Preventing processes from writing to the container’s filesystem\n",
      "386\n",
      "Sharing volumes when containers run as different users\n",
      "387\n",
      "13.3\n",
      "Restricting the use of security-related features \n",
      "in pods\n",
      "389\n",
      "Introducing the PodSecurityPolicy resource\n",
      "389\n",
      "■Understanding \n",
      "runAsUser, fsGroup, and supplementalGroups policies\n",
      "392\n",
      "Configuring allowed, default, and disallowed capabilities\n",
      "394\n",
      "Constraining the types of volumes pods can use\n",
      "395\n",
      "■Assigning \n",
      "different PodSecurityPolicies to different users and groups\n",
      "396\n",
      "13.4\n",
      "Isolating the pod network\n",
      "399\n",
      "Enabling network isolation in a namespace\n",
      "399\n",
      "■Allowing \n",
      "only some pods in the namespace to connect to a server pod\n",
      "400\n",
      "Isolating the network between Kubernetes namespaces\n",
      "401\n",
      "Isolating using CIDR notation\n",
      "402\n",
      "■Limiting the outbound \n",
      "traffic of a set of pods\n",
      "403\n",
      "13.5\n",
      "Summary\n",
      "403\n",
      "14 \n",
      "Managing pods’ computational resources\n",
      "404\n",
      "14.1\n",
      "Requesting resources for a pod’s containers\n",
      "405\n",
      "Creating pods with resource requests\n",
      "405\n",
      "■Understanding how \n",
      "resource requests affect scheduling\n",
      "406\n",
      "■Understanding how CPU \n",
      "requests affect CPU time sharing\n",
      "411\n",
      "■Defining and requesting \n",
      "custom resources\n",
      "411\n",
      "14.2\n",
      "Limiting resources available to a container\n",
      "412\n",
      "Setting a hard limit for the amount of resources a container \n",
      "can use\n",
      "412\n",
      "■Exceeding the limits\n",
      "414\n",
      "■Understanding \n",
      "how apps in containers see limits\n",
      "415\n",
      "14.3\n",
      "Understanding pod QoS classes\n",
      "417\n",
      "Defining the QoS class for a pod\n",
      "417\n",
      "■Understanding which \n",
      "process gets killed when memory is low\n",
      "420\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 20, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "CONTENTS\n",
      "xviii\n",
      "14.4\n",
      "Setting default requests and limits for pods per \n",
      "namespace\n",
      "421\n",
      "Introducing the LimitRange resource\n",
      "421\n",
      "■Creating a \n",
      "LimitRange object\n",
      "422\n",
      "■Enforcing the limits\n",
      "423\n",
      "Applying default resource requests and limits\n",
      "424\n",
      "14.5\n",
      "Limiting the total resources available in \n",
      "a namespace\n",
      "425\n",
      "Introducing the ResourceQuota object\n",
      "425\n",
      "■Specifying a quota \n",
      "for persistent storage\n",
      "427\n",
      "■Limiting the number of objects that can \n",
      "be created\n",
      "427\n",
      "■Specifying quotas for specific pod states and/or \n",
      "QoS classes\n",
      "429\n",
      "14.6\n",
      "Monitoring pod resource usage\n",
      "430\n",
      "Collecting and retrieving actual resource usages\n",
      "430\n",
      "■Storing \n",
      "and analyzing historical resource consumption statistics\n",
      "432\n",
      "14.7\n",
      "Summary\n",
      "435\n",
      "15 \n",
      "Automatic scaling of pods and cluster nodes\n",
      "437\n",
      "15.1\n",
      "Horizontal pod autoscaling\n",
      "438\n",
      "Understanding the autoscaling process\n",
      "438\n",
      "■Scaling based \n",
      "on CPU utilization\n",
      "441\n",
      "■Scaling based on memory \n",
      "consumption\n",
      "448\n",
      "■Scaling based on other and custom \n",
      "metrics\n",
      "448\n",
      "■Determining which metrics are appropriate for \n",
      "autoscaling\n",
      "450\n",
      "■Scaling down to zero replicas\n",
      "450\n",
      "15.2\n",
      "Vertical pod autoscaling\n",
      "451\n",
      "Automatically configuring resource requests\n",
      "451\n",
      "■Modifying \n",
      "resource requests while a pod is running\n",
      "451\n",
      "15.3\n",
      "Horizontal scaling of cluster nodes\n",
      "452\n",
      "Introducing the Cluster Autoscaler\n",
      "452\n",
      "■Enabling the Cluster \n",
      "Autoscaler\n",
      "454\n",
      "■Limiting service disruption during cluster \n",
      "scale-down\n",
      "454\n",
      "15.4\n",
      "Summary\n",
      "456\n",
      "16 \n",
      "Advanced scheduling\n",
      "457\n",
      "16.1\n",
      "Using taints and tolerations to repel pods from certain \n",
      "nodes\n",
      "457\n",
      "Introducing taints and tolerations\n",
      "458\n",
      "■Adding custom taints to \n",
      "a node\n",
      "460\n",
      "■Adding tolerations to pods\n",
      "460\n",
      "■Understanding \n",
      "what taints and tolerations can be used for\n",
      "461\n",
      " \n",
      "www.allitebooks.com\n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 21, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "CONTENTS\n",
      "xix\n",
      "16.2\n",
      "Using node affinity to attract pods to certain nodes\n",
      "462\n",
      "Specifying hard node affinity rules\n",
      "463\n",
      "■Prioritizing nodes when \n",
      "scheduling a pod\n",
      "465\n",
      "16.3\n",
      "Co-locating pods with pod affinity and anti-affinity\n",
      "468\n",
      "Using inter-pod affinity to deploy pods on the same node\n",
      "468\n",
      "Deploying pods in the same rack, availability zone, or geographic \n",
      "region\n",
      "471\n",
      "■Expressing pod affinity preferences instead of hard \n",
      "requirements\n",
      "472\n",
      "■Scheduling pods away from each other with \n",
      "pod anti-affinity\n",
      "474\n",
      "16.4\n",
      "Summary\n",
      "476\n",
      "17 \n",
      "Best practices for developing apps\n",
      "477\n",
      "17.1\n",
      "Bringing everything together\n",
      "478\n",
      "17.2\n",
      "Understanding the pod’s lifecycle\n",
      "479\n",
      "Applications must expect to be killed and relocated\n",
      "479\n",
      "Rescheduling of dead or partially dead pods\n",
      "482\n",
      "■Starting \n",
      "pods in a specific order\n",
      "483\n",
      "■Adding lifecycle hooks\n",
      "485\n",
      "Understanding pod shutdown\n",
      "489\n",
      "17.3\n",
      "Ensuring all client requests are handled properly\n",
      "492\n",
      "Preventing broken client connections when a pod is starting up\n",
      "492\n",
      "Preventing broken connections during pod shut-down\n",
      "493\n",
      "17.4\n",
      "Making your apps easy to run and manage in \n",
      "Kubernetes\n",
      "497\n",
      "Making manageable container images\n",
      "497\n",
      "■Properly \n",
      "tagging your images and using imagePullPolicy wisely\n",
      "497\n",
      "Using multi-dimensional instead of single-dimensional labels\n",
      "498\n",
      "Describing each resource through annotations\n",
      "498\n",
      "■Providing \n",
      "information on why the process terminated\n",
      "498\n",
      "■Handling \n",
      "application logs\n",
      "500\n",
      "17.5\n",
      "Best practices for development and testing\n",
      "502\n",
      "Running apps outside of Kubernetes during development\n",
      "502\n",
      "Using Minikube in development\n",
      "503\n",
      "■Versioning and auto-\n",
      "deploying resource manifests\n",
      "504\n",
      "■Introducing Ksonnet as an \n",
      "alternative to writing YAML/JSON manifests\n",
      "505\n",
      "■Employing \n",
      "Continuous Integration and Continuous Delivery (CI/CD)\n",
      "506\n",
      "17.6\n",
      "Summary\n",
      "506\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 22, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "CONTENTS\n",
      "xx\n",
      "18 \n",
      "Extending Kubernetes\n",
      "508\n",
      "18.1\n",
      "Defining custom API objects\n",
      "508\n",
      "Introducing CustomResourceDefinitions\n",
      "509\n",
      "■Automating \n",
      "custom resources with custom controllers\n",
      "513\n",
      "■Validating \n",
      "custom objects\n",
      "517\n",
      "■Providing a custom API server for your \n",
      "custom objects\n",
      "518\n",
      "18.2\n",
      "Extending Kubernetes with the Kubernetes Service \n",
      "Catalog\n",
      "519\n",
      "Introducing the Service Catalog\n",
      "520\n",
      "■Introducing the \n",
      "Service Catalog API server and Controller Manager\n",
      "521\n",
      "Introducing Service Brokers and the OpenServiceBroker API\n",
      "522\n",
      "Provisioning and using a service\n",
      "524\n",
      "■Unbinding and \n",
      "deprovisioning\n",
      "526\n",
      "■Understanding what the Service \n",
      "Catalog brings\n",
      "526\n",
      "18.3\n",
      "Platforms built on top of Kubernetes\n",
      "527\n",
      "Red Hat OpenShift Container Platform\n",
      "527\n",
      "■Deis Workflow \n",
      "and Helm\n",
      "530\n",
      "18.4\n",
      "Summary\n",
      "533\n",
      "appendix A\n",
      "Using kubectl with multiple clusters\n",
      "534\n",
      "appendix B\n",
      "Setting up a multi-node cluster with kubeadm\n",
      "539\n",
      "appendix C\n",
      "Using other container runtimes\n",
      "552\n",
      "appendix D\n",
      "Cluster Federation\n",
      "556\n",
      "index 561\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 23, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "xxi\n",
      "preface\n",
      "After working at Red Hat for a few years, in late 2014 I was assigned to a newly-\n",
      "established team called Cloud Enablement. Our task was to bring the company’s\n",
      "range of middleware products to the OpenShift Container Platform, which was then\n",
      "being developed on top of Kubernetes. At that time, Kubernetes was still in its\n",
      "infancy—version 1.0 hadn’t even been released yet.\n",
      " Our team had to get to know the ins and outs of Kubernetes quickly to set a proper\n",
      "direction for our software and take advantage of everything Kubernetes had to offer.\n",
      "When faced with a problem, it was hard for us to tell if we were doing things wrong or\n",
      "merely hitting one of the early Kubernetes bugs. \n",
      " Both Kubernetes and my understanding of it have come a long way since then.\n",
      "When I first started using it, most people hadn’t even heard of Kubernetes. Now, virtu-\n",
      "ally every software engineer knows about it, and it has become one of the fastest-\n",
      "growing and most-widely-adopted ways of running applications in both the cloud and\n",
      "on-premises datacenters. \n",
      " In my first month of dealing with Kubernetes, I wrote a two-part blog post about\n",
      "how to run a JBoss WildFly application server cluster in OpenShift/Kubernetes. At the\n",
      "time, I never could have imagined that a simple blog post would ultimately lead the\n",
      "people at Manning to contact me about whether I would like to write a book about\n",
      "Kubernetes. Of course, I couldn’t say no to such an offer, even though I was sure\n",
      "they’d approached other people as well and would ultimately pick someone else.\n",
      " And yet, here we are. After more than a year and a half of writing and researching,\n",
      "the book is done. It’s been an awesome journey. Writing a book about a technology is\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 24, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "PREFACE\n",
      "xxii\n",
      "absolutely the best way to get to know it in much greater detail than you’d learn as just\n",
      "a user. As my knowledge of Kubernetes has expanded during the process and Kuber-\n",
      "netes itself has evolved, I’ve constantly gone back to previous chapters I’ve written and\n",
      "added additional information. I’m a perfectionist, so I’ll never really be absolutely sat-\n",
      "isfied with the book, but I’m happy to hear that a lot of readers of the Manning Early\n",
      "Access Program (MEAP) have found it to be a great guide to Kubernetes.\n",
      " My aim is to get the reader to understand the technology itself and teach them\n",
      "how to use the tooling to effectively and efficiently develop and deploy apps to Kuber-\n",
      "netes clusters. In the book, I don’t put much emphasis on how to actually set up and\n",
      "maintain a proper highly available Kubernetes cluster, but the last part should give\n",
      "readers a very solid understanding of what such a cluster consists of and should allow\n",
      "them to easily comprehend additional resources that deal with this subject.\n",
      " I hope you’ll enjoy reading it, and that it teaches you how to get the most out of\n",
      "the awesome system that is Kubernetes.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 25, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "xxiii\n",
      "acknowledgments\n",
      "Before I started writing this book, I had no clue how many people would be involved\n",
      "in bringing it from a rough manuscript to a published piece of work. This means\n",
      "there are a lot of people to thank.\n",
      " First, I’d like to thank Erin Twohey for approaching me about writing this book,\n",
      "and Michael Stephens from Manning, who had full confidence in my ability to write it\n",
      "from day one. His words of encouragement early on really motivated me and kept me\n",
      "motivated throughout the last year and a half. \n",
      " I would also like to thank my initial development editor Andrew Warren, who\n",
      "helped me get my first chapter out the door, and Elesha Hyde, who took over from\n",
      "Andrew and worked with me all the way to the last chapter. Thank you for bearing\n",
      "with me, even though I’m a difficult person to deal with, as I tend to drop off the\n",
      "radar fairly regularly. \n",
      " I would also like to thank Jeanne Boyarsky, who was the first reviewer to read and\n",
      "comment on my chapters while I was writing them. Jeanne and Elesha were instrumen-\n",
      "tal in making the book as nice as it hopefully is. Without their comments, the book\n",
      "could never have received such good reviews from external reviewers and readers.\n",
      " I’d like to thank my technical proofreader, Antonio Magnaghi, and of course all\n",
      "my external reviewers: Al Krinker, Alessandro Campeis, Alexander Myltsev, Csaba Sari,\n",
      "David DiMaria, Elias Rangel, Erisk Zelenka, Fabrizio Cucci, Jared Duncan, Keith\n",
      "Donaldson, Michael Bright, Paolo Antinori, Peter Perlepes, and Tiklu Ganguly. Their\n",
      "positive comments kept me going at times when I worried my writing was utterly awful\n",
      "and completely useless. On the other hand, their constructive criticism helped improve\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 26, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "ACKNOWLEDGMENTS\n",
      "xxiv\n",
      "sections that I’d quickly thrown together without enough effort. Thank you for point-\n",
      "ing out the hard-to-understand sections and suggesting ways of improving the book.\n",
      "Also, thank you for asking the right questions, which made me realize I was wrong\n",
      "about two or three things in the initial versions of the manuscript.\n",
      " I also need to thank readers who bought the early version of the book through\n",
      "Manning’s MEAP program and voiced their comments in the online forum or reached\n",
      "out to me directly—especially Vimal Kansal, Paolo Patierno, and Roland Huß, who\n",
      "noticed quite a few inconsistencies and other mistakes. And I would like to thank\n",
      "everyone at Manning who has been involved in getting this book published. Before I\n",
      "finish, I also need to thank my colleague and high school friend Aleš Justin, who\n",
      "brought me to Red Hat, and my wonderful colleagues from the Cloud Enablement\n",
      "team. If I hadn’t been at Red Hat or in the team, I wouldn’t have been the one to write\n",
      "this book.\n",
      " Lastly, I would like to thank my wife and my son, who were way too understanding\n",
      "and supportive over the last 18 months, while I was locked in my office instead of\n",
      "spending time with them.\n",
      " Thank you all!\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 27, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "xxv\n",
      "about this book\n",
      "Kubernetes in Action aims to make you a proficient user of Kubernetes. It teaches you\n",
      "virtually all the concepts you need to understand to effectively develop and run appli-\n",
      "cations in a Kubernetes environment. \n",
      " Before diving into Kubernetes, the book gives an overview of container technolo-\n",
      "gies like Docker, including how to build containers, so that even readers who haven’t\n",
      "used these technologies before can get up and running.  It then slowly guides you\n",
      "through most of what you need to know about Kubernetes—from basic concepts to\n",
      "things hidden below the surface.\n",
      "Who should read this book\n",
      "The book focuses primarily on application developers, but it also provides an overview\n",
      "of managing applications from the operational perspective. It’s meant for anyone\n",
      "interested in running and managing containerized applications on more than just a\n",
      "single server.\n",
      " Both beginner and advanced software engineers who want to learn about con-\n",
      "tainer technologies and orchestrating multiple related containers at scale will gain the\n",
      "expertise necessary to develop, containerize, and run their applications in a Kuberne-\n",
      "tes environment. \n",
      " No previous exposure to either container technologies or Kubernetes is required.\n",
      "The book explains the subject matter in a progressively detailed manner, and doesn’t\n",
      "use any application source code that would be too hard for non-expert developers to\n",
      "understand. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 28, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "ABOUT THIS BOOK\n",
      "xxvi\n",
      " Readers, however, should have at least a basic knowledge of programming, com-\n",
      "puter networking, and running basic commands in Linux, and an understanding of\n",
      "well-known computer protocols like HTTP. \n",
      "How this book is organized: a roadmap\n",
      "This book has three parts that cover 18 chapters.\n",
      " Part 1 gives a short introduction to Docker and Kubernetes, how to set up a Kuber-\n",
      "netes cluster, and how to run a simple application in it. It contains two chapters:\n",
      "■\n",
      "Chapter 1 explains what Kubernetes is, how it came to be, and how it helps to\n",
      "solve today’s problems of managing applications at scale.\n",
      "■\n",
      "Chapter 2 is a hands-on tutorial on how to build a container image and run it in\n",
      "a Kubernetes cluster. It also explains how to run a local single-node Kubernetes\n",
      "cluster and a proper multi-node cluster in the cloud.\n",
      "Part 2 introduces the key concepts you must understand to run applications in Kuber-\n",
      "netes. The chapters are as follows:\n",
      "■\n",
      "Chapter 3 introduces the fundamental building block in Kubernetes—the pod—\n",
      "and explains how to organize pods and other Kubernetes objects through labels. \n",
      "■\n",
      "Chapter 4 teaches you how Kubernetes keeps applications healthy by automati-\n",
      "cally restarting containers. It also shows how to properly run managed pods,\n",
      "horizontally scale them, make them resistant to failures of cluster nodes, and\n",
      "run them at a predefined time in the future or periodically.\n",
      "■\n",
      "Chapter 5 shows how pods can expose the service they provide to clients run-\n",
      "ning both inside and outside the cluster. It also shows how pods running in the\n",
      "cluster can discover and access services, regardless of whether they live in or out\n",
      "of the cluster. \n",
      "■\n",
      "Chapter 6 explains how multiple containers running in the same pod can share\n",
      "files and how you can manage persistent storage and make it accessible to pods. \n",
      "■\n",
      "Chapter 7 shows how to pass configuration data and sensitive information like\n",
      "credentials to apps running inside pods.\n",
      "■\n",
      "Chapter 8 describes how applications can get information about the Kuberne-\n",
      "tes environment they’re running in and how they can talk to Kubernetes to\n",
      "alter the state of the cluster.\n",
      "■\n",
      "Chapter 9 introduces the concept of a Deployment and explains the proper way\n",
      "of running and updating applications in a Kubernetes environment.\n",
      "■\n",
      "Chapter 10 introduces a dedicated way of running stateful applications, which\n",
      "usually require a stable identity and state.\n",
      "Part 3 dives deep into the internals of a Kubernetes cluster, introduces some addi-\n",
      "tional concepts, and reviews everything you’ve learned in the first two parts from a\n",
      "higher perspective. This is the last group of chapters:\n",
      "■\n",
      "Chapter 11 goes beneath the surface of Kubernetes and explains all the compo-\n",
      "nents that make up a Kubernetes cluster and what each of them does. It also\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 29, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "ABOUT THIS BOOK\n",
      "xxvii\n",
      "explains how pods communicate through the network and how services per-\n",
      "form load balancing across multiple pods.\n",
      "■\n",
      "Chapter 12 explains how to secure your Kubernetes API server, and by exten-\n",
      "sion the cluster, using authentication and authorization. \n",
      "■\n",
      "Chapter 13 teaches you how pods can access the node’s resources and how a\n",
      "cluster administrator can prevent pods from doing that.\n",
      "■\n",
      "Chapter 14 dives into constraining the computational resources each applica-\n",
      "tion is allowed to consume, configuring the applications’ Quality of Service\n",
      "guarantees, and monitoring the resource usage of individual applications. It\n",
      "also teaches you how to prevent users from consuming too many resources.\n",
      "■\n",
      "Chapter 15 discusses how Kubernetes can be configured to automatically scale\n",
      "the number of running replicas of your application, and how it can also increase\n",
      "the size of your cluster when your current number of cluster nodes can’t accept\n",
      "any additional applications. \n",
      "■\n",
      "Chapter 16 shows how to ensure pods are scheduled only to certain nodes or\n",
      "how to prevent them from being scheduled to others. It also shows how to make\n",
      "sure pods are scheduled together or how to prevent that from happening.\n",
      "■\n",
      "Chapter 17 teaches you how you should develop your applications to make them\n",
      "good citizens of your cluster. It also gives you a few pointers on how to set up your\n",
      "development and testing workflows to reduce friction during development.\n",
      "■\n",
      "Chapter 18 shows you how you can extend Kubernetes with your own custom\n",
      "objects and how others have done it and created enterprise-class application\n",
      "platforms.\n",
      "As you progress through these chapters, you’ll not only learn about the individual\n",
      "Kubernetes building blocks, but also progressively improve your knowledge of using\n",
      "the kubectl command-line tool.\n",
      "About the code\n",
      "While this book doesn’t contain a lot of actual source code, it does contain a lot of\n",
      "manifests of Kubernetes resources in YAML format and shell commands along with\n",
      "their outputs. All of this is formatted in a fixed-width font like this to separate it\n",
      "from ordinary text. \n",
      " Shell commands are mostly in bold, to clearly separate them from their output, but\n",
      "sometimes only the most important parts of the command or parts of the command’s\n",
      "output are in bold for emphasis. In most cases, the command output has been reformat-\n",
      "ted to make it fit into the limited space in the book. Also, because the Kubernetes CLI\n",
      "tool kubectl is constantly evolving, newer versions may print out more information\n",
      "than what’s shown in the book. Don’t be confused if they don’t match exactly. \n",
      " Listings sometimes include a line-continuation marker (➥) to show that a line of\n",
      "text wraps to the next line. They also include annotations, which highlight and explain\n",
      "the most important parts. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 30, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "ABOUT THIS BOOK\n",
      "xxviii\n",
      " Within text paragraphs, some very common elements such as Pod, Replication-\n",
      "Controller, ReplicaSet, DaemonSet, and so forth are set in regular font to avoid over-\n",
      "proliferation of code font and help readability. In some places, “Pod” is capitalized\n",
      "to refer to the Pod resource, and lowercased to refer to the actual group of running\n",
      "containers.\n",
      " All the samples in the book have been tested with Kubernetes version 1.8 running\n",
      "in Google Kubernetes Engine and in a local cluster run with Minikube. The complete\n",
      "source code and YAML manifests can be found at https:/\n",
      "/github.com/luksa/kubernetes-\n",
      "in-action or downloaded from the publisher’s website at www.manning.com/books/\n",
      "kubernetes-in-action.\n",
      "Book forum\n",
      "Purchase of Kubernetes in Action includes free access to a private web forum run by\n",
      "Manning Publications where you can make comments about the book, ask technical\n",
      "questions, and receive help from the author and from other users. To access the\n",
      "forum, go to https:/\n",
      "/forums.manning.com/forums/kubernetes-in-action. You can also\n",
      "learn more about Manning’s forums and the rules of conduct at https:/\n",
      "/forums\n",
      ".manning.com/forums/about.\n",
      " Manning’s commitment to our readers is to provide a venue where a meaningful\n",
      "dialogue between individual readers and between readers and the author can take\n",
      "place. It is not a commitment to any specific amount of participation on the part of\n",
      "the author, whose contribution to the forum remains voluntary (and unpaid). We sug-\n",
      "gest you try asking the author some challenging questions lest his interest stray! The\n",
      "forum and the archives of previous discussions will be accessible from the publisher’s\n",
      "website as long as the book is in print.\n",
      "Other online resources\n",
      "You can find a wide range of additional Kubernetes resources at the following locations:\n",
      "■\n",
      "The Kubernetes website at https:/\n",
      "/kubernetes.io\n",
      "■\n",
      "The Kubernetes Blog, which regularly posts interesting info (http:/\n",
      "/blog.kuber-\n",
      "netes.io)\n",
      "■\n",
      "The Kubernetes community’s Slack channel at http:/\n",
      "/slack.k8s.io\n",
      "■\n",
      "The Kubernetes and Cloud Native Computing Foundation’s YouTube channels:\n",
      "– https:/\n",
      "/www.youtube.com/channel/UCZ2bu0qutTOM0tHYa_jkIwg \n",
      "– https:/\n",
      "/www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA \n",
      "To gain a deeper understanding of individual topics or even to help contribute to\n",
      "Kubernetes, you can also check out any of the Kubernetes Special Interest Groups (SIGs)\n",
      "at https:/\n",
      "/github.com/kubernetes/kubernetes/wiki/Special-Interest-Groups-(SIGs).\n",
      " And, finally, as Kubernetes is open source, there’s a wealth of information available\n",
      "in the Kubernetes source code itself. You’ll find it at https:/\n",
      "/github.com/kubernetes/\n",
      "kubernetes and related repositories. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "375\n",
      "height\n",
      "513\n",
      "PIX BUFFER SIZE\n",
      "577125\n",
      "Original IMG_BUFFER_SIZE\n",
      "577125\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014374d00>\n",
      "page_image_dict\n",
      "{'page': 31, 'img_cnt': 1, 'img_npy_lst': []}\n",
      "xxix\n",
      "about the author\n",
      "Marko Lukša is a software engineer with more than 20 years of\n",
      "professional experience developing everything from simple\n",
      "web applications to full ERP systems, frameworks, and middle-\n",
      "ware software. He took his first steps in programming back in\n",
      "1985, at the age of six, on a second-hand ZX Spectrum com-\n",
      "puter his father had bought for him. In primary school, he was\n",
      "the national champion in the Logo programming competition\n",
      "and attended summer coding camps, where he learned to pro-\n",
      "gram in Pascal. Since then, he has developed software in a\n",
      "wide range of programming languages.\n",
      "   In high school, he started building dynamic websites when\n",
      "the web was still relatively young. He then moved on to developing software for the\n",
      "healthcare and telecommunications industries at a local company, while studying\n",
      "computer science at the University of Ljubljana, Slovenia. Eventually, he ended up\n",
      "working for Red Hat, initially developing an open source implementation of the Goo-\n",
      "gle App Engine API, which utilized Red Hat’s JBoss middleware products underneath.\n",
      "He also worked in or contributed to projects like CDI/Weld, Infinispan/JBoss Data-\n",
      "Grid, and others.\n",
      " Since late 2014, he has been part of Red Hat’s Cloud Enablement team, where his\n",
      "responsibilities include staying up-to-date on new developments in Kubernetes and\n",
      "related technologies and ensuring the company’s middleware software utilizes the fea-\n",
      "tures of Kubernetes and OpenShift to their full potential.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 32, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "xxx\n",
      "about the cover illustration\n",
      "The figure on the cover of Kubernetes in Action is a “Member of the Divan,” the Turkish\n",
      "Council of State or governing body. The illustration is taken from a collection of cos-\n",
      "tumes of the Ottoman Empire published on January 1, 1802, by William Miller of Old\n",
      "Bond Street, London. The title page is missing from the collection and we have been\n",
      "unable to track it down to date. The book’s table of contents identifies the figures in\n",
      "both English and French, and each illustration bears the names of two artists who\n",
      "worked on it, both of whom would no doubt be surprised to find their art gracing the\n",
      "front cover of a computer programming book ... 200 years later.\n",
      " The collection was purchased by a Manning editor at an antiquarian flea market in\n",
      "the “Garage” on West 26th Street in Manhattan. The seller was an American based in\n",
      "Ankara, Turkey, and the transaction took place just as he was packing up his stand for\n",
      "the day. The Manning editor didn’t have on his person the substantial amount of cash\n",
      "that was required for the purchase, and a credit card and check were both politely\n",
      "turned down. With the seller flying back to Ankara that evening, the situation was get-\n",
      "ting hopeless. What was the solution? It turned out to be nothing more than an old-\n",
      "fashioned verbal agreement sealed with a handshake. The seller proposed that the\n",
      "money be transferred to him by wire, and the editor walked out with the bank infor-\n",
      "mation on a piece of paper and the portfolio of images under his arm. Needless to say,\n",
      "we transferred the funds the next day, and we remain grateful and impressed by this\n",
      "unknown person’s trust in one of us. It recalls something that might have happened a\n",
      "long time ago. We at Manning celebrate the inventiveness, the initiative, and, yes, the\n",
      "fun of the computer business with book covers based on the rich diversity of regional\n",
      "life of two centuries ago‚ brought back to life by the pictures from this collection.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 33, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "1\n",
      "Introducing Kubernetes\n",
      "Years ago, most software applications were big monoliths, running either as a single\n",
      "process or as a small number of processes spread across a handful of servers. These\n",
      "legacy systems are still widespread today. They have slow release cycles and are\n",
      "updated relatively infrequently. At the end of every release cycle, developers pack-\n",
      "age up the whole system and hand it over to the ops team, who then deploys and\n",
      "monitors it. In case of hardware failures, the ops team manually migrates it to the\n",
      "remaining healthy servers. \n",
      " Today, these big monolithic legacy applications are slowly being broken down\n",
      "into smaller, independently running components called microservices. Because\n",
      "This chapter covers\n",
      "Understanding how software development and \n",
      "deployment has changed over recent years\n",
      "Isolating applications and reducing environment \n",
      "differences using containers\n",
      "Understanding how containers and Docker are \n",
      "used by Kubernetes\n",
      "Making developers’ and sysadmins’ jobs easier \n",
      "with Kubernetes\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 34, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "2\n",
      "CHAPTER 1\n",
      "Introducing Kubernetes\n",
      "microservices are decoupled from each other, they can be developed, deployed, updated,\n",
      "and scaled individually. This enables you to change components quickly and as often as\n",
      "necessary to keep up with today’s rapidly changing business requirements. \n",
      " But with bigger numbers of deployable components and increasingly larger data-\n",
      "centers, it becomes increasingly difficult to configure, manage, and keep the whole\n",
      "system running smoothly. It’s much harder to figure out where to put each of those\n",
      "components to achieve high resource utilization and thereby keep the hardware costs\n",
      "down. Doing all this manually is hard work. We need automation, which includes\n",
      "automatic scheduling of those components to our servers, automatic configuration,\n",
      "supervision, and failure-handling. This is where Kubernetes comes in.\n",
      " Kubernetes enables developers to deploy their applications themselves and as\n",
      "often as they want, without requiring any assistance from the operations (ops) team.\n",
      "But Kubernetes doesn’t benefit only developers. It also helps the ops team by automat-\n",
      "ically monitoring and rescheduling those apps in the event of a hardware failure. The\n",
      "focus for system administrators (sysadmins) shifts from supervising individual apps to\n",
      "mostly supervising and managing Kubernetes and the rest of the infrastructure, while\n",
      "Kubernetes itself takes care of the apps. \n",
      "NOTE\n",
      "Kubernetes is Greek for pilot or helmsman (the person holding the\n",
      "ship’s steering wheel). People pronounce Kubernetes in a few different ways.\n",
      "Many pronounce it as Koo-ber-nay-tace, while others pronounce it more like\n",
      "Koo-ber-netties. No matter which form you use, people will understand what\n",
      "you mean.\n",
      "Kubernetes abstracts away the hardware infrastructure and exposes your whole data-\n",
      "center as a single enormous computational resource. It allows you to deploy and run\n",
      "your software components without having to know about the actual servers under-\n",
      "neath. When deploying a multi-component application through Kubernetes, it selects\n",
      "a server for each component, deploys it, and enables it to easily find and communi-\n",
      "cate with all the other components of your application. \n",
      " This makes Kubernetes great for most on-premises datacenters, but where it starts\n",
      "to shine is when it’s used in the largest datacenters, such as the ones built and oper-\n",
      "ated by cloud providers. Kubernetes allows them to offer developers a simple platform\n",
      "for deploying and running any type of application, while not requiring the cloud pro-\n",
      "vider’s own sysadmins to know anything about the tens of thousands of apps running\n",
      "on their hardware. \n",
      " With more and more big companies accepting the Kubernetes model as the best\n",
      "way to run apps, it’s becoming the standard way of running distributed apps both in\n",
      "the cloud, as well as on local on-premises infrastructure. \n",
      "1.1\n",
      "Understanding the need for a system like Kubernetes \n",
      "Before you start getting to know Kubernetes in detail, let’s take a quick look at how\n",
      "the development and deployment of applications has changed in recent years. This\n",
      "change is both a consequence of splitting big monolithic apps into smaller microservices\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 35, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "3\n",
      "Understanding the need for a system like Kubernetes\n",
      "and of the changes in the infrastructure that runs those apps. Understanding these\n",
      "changes will help you better see the benefits of using Kubernetes and container tech-\n",
      "nologies such as Docker.\n",
      "1.1.1\n",
      "Moving from monolithic apps to microservices\n",
      "Monolithic applications consist of components that are all tightly coupled together and\n",
      "have to be developed, deployed, and managed as one entity, because they all run as a sin-\n",
      "gle OS process. Changes to one part of the application require a redeployment of the\n",
      "whole application, and over time the lack of hard boundaries between the parts results\n",
      "in the increase of complexity and consequential deterioration of the quality of the whole\n",
      "system because of the unconstrained growth of inter-dependencies between these parts. \n",
      " Running a monolithic application usually requires a small number of powerful\n",
      "servers that can provide enough resources for running the application. To deal with\n",
      "increasing loads on the system, you then either have to vertically scale the servers (also\n",
      "known as scaling up) by adding more CPUs, memory, and other server components,\n",
      "or scale the whole system horizontally, by setting up additional servers and running\n",
      "multiple copies (or replicas) of an application (scaling out). While scaling up usually\n",
      "doesn’t require any changes to the app, it gets expensive relatively quickly and in prac-\n",
      "tice always has an upper limit. Scaling out, on the other hand, is relatively cheap hard-\n",
      "ware-wise, but may require big changes in the application code and isn’t always\n",
      "possible—certain parts of an application are extremely hard or next to impossible to\n",
      "scale horizontally (relational databases, for example). If any part of a monolithic\n",
      "application isn’t scalable, the whole application becomes unscalable, unless you can\n",
      "split up the monolith somehow.\n",
      "SPLITTING APPS INTO MICROSERVICES\n",
      "These and other problems have forced us to start splitting complex monolithic appli-\n",
      "cations into smaller independently deployable components called microservices. Each\n",
      "microservice runs as an independent process (see figure 1.1) and communicates with\n",
      "other microservices through simple, well-defined interfaces (APIs).\n",
      "Server 1\n",
      "Monolithic application\n",
      "Single process\n",
      "Server 1\n",
      "Process 1.1\n",
      "Process 1.2\n",
      "Microservices-based application\n",
      "Server 2\n",
      "Process 2.1\n",
      "Process 2.2\n",
      "Figure 1.1\n",
      "Components inside a monolithic application vs. standalone microservices\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 36, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "4\n",
      "CHAPTER 1\n",
      "Introducing Kubernetes\n",
      "Microservices communicate through synchronous protocols such as HTTP, over which\n",
      "they usually expose RESTful (REpresentational State Transfer) APIs, or through asyn-\n",
      "chronous protocols such as AMQP (Advanced Message Queueing Protocol). These\n",
      "protocols are simple, well understood by most developers, and not tied to any specific\n",
      "programming language. Each microservice can be written in the language that’s most\n",
      "appropriate for implementing that specific microservice.\n",
      " Because each microservice is a standalone process with a relatively static external\n",
      "API, it’s possible to develop and deploy each microservice separately. A change to one\n",
      "of them doesn’t require changes or redeployment of any other service, provided that\n",
      "the API doesn’t change or changes only in a backward-compatible way. \n",
      "SCALING MICROSERVICES\n",
      "Scaling microservices, unlike monolithic systems, where you need to scale the system as\n",
      "a whole, is done on a per-service basis, which means you have the option of scaling only\n",
      "those services that require more resources, while leaving others at their original scale.\n",
      "Figure 1.2 shows an example. Certain components are replicated and run as multiple\n",
      "processes deployed on different servers, while others run as a single application process.\n",
      "When a monolithic application can’t be scaled out because one of its parts is unscal-\n",
      "able, splitting the app into microservices allows you to horizontally scale the parts that\n",
      "allow scaling out, and scale the parts that don’t, vertically instead of horizontally.\n",
      "Server 1\n",
      "Process 1.1\n",
      "Process 1.2\n",
      "Process 1.3\n",
      "Server 2\n",
      "Process 2.1\n",
      "Process 2.2\n",
      "Server 3\n",
      "Process 3.1\n",
      "Process 3.2\n",
      "Process 3.3\n",
      "Server 4\n",
      "Process 4.1\n",
      "Process 4.2\n",
      "Process 2.3\n",
      "Single instance\n",
      "(possibly not scalable)\n",
      "Three instances of\n",
      "the same component\n",
      "Figure 1.2\n",
      "Each microservice can be scaled individually.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 37, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "5\n",
      "Understanding the need for a system like Kubernetes\n",
      "DEPLOYING MICROSERVICES\n",
      "As always, microservices also have drawbacks. When your system consists of only a\n",
      "small number of deployable components, managing those components is easy. It’s\n",
      "trivial to decide where to deploy each component, because there aren’t that many\n",
      "choices. When the number of those components increases, deployment-related deci-\n",
      "sions become increasingly difficult because not only does the number of deployment\n",
      "combinations increase, but the number of inter-dependencies between the compo-\n",
      "nents increases by an even greater factor. \n",
      " Microservices perform their work together as a team, so they need to find and talk\n",
      "to each other. When deploying them, someone or something needs to configure all of\n",
      "them properly to enable them to work together as a single system. With increasing\n",
      "numbers of microservices, this becomes tedious and error-prone, especially when you\n",
      "consider what the ops/sysadmin teams need to do when a server fails. \n",
      " Microservices also bring other problems, such as making it hard to debug and trace\n",
      "execution calls, because they span multiple processes and machines. Luckily, these\n",
      "problems are now being addressed with distributed tracing systems such as Zipkin. \n",
      "UNDERSTANDING THE DIVERGENCE OF ENVIRONMENT REQUIREMENTS\n",
      "As I’ve already mentioned, components in a microservices architecture aren’t only\n",
      "deployed independently, but are also developed that way. Because of their indepen-\n",
      "dence and the fact that it’s common to have separate teams developing each compo-\n",
      "nent, nothing impedes each team from using different libraries and replacing them\n",
      "whenever the need arises. The divergence of dependencies between application com-\n",
      "ponents, like the one shown in figure 1.3, where applications require different ver-\n",
      "sions of the same libraries, is inevitable.\n",
      "Server running a monolithic app\n",
      "Monolithic app\n",
      "Library B\n",
      "v2.4\n",
      "Library C\n",
      "v1.1\n",
      "Library A\n",
      "v1.0\n",
      "Library Y\n",
      "v3.2\n",
      "Library X\n",
      "v1.4\n",
      "Server running multiple apps\n",
      "Library B\n",
      "v2.4\n",
      "Library C\n",
      "v1.1\n",
      "Library C\n",
      "v2.0\n",
      "Library A\n",
      "v1.0\n",
      "Library A\n",
      "v2.2\n",
      "Library Y\n",
      "v4.0\n",
      "Library Y\n",
      "v3.2\n",
      "Library X\n",
      "v2.3\n",
      "Library X\n",
      "v1.4\n",
      "App 1\n",
      "App 2\n",
      "App 3\n",
      "App 4\n",
      "Requires libraries\n",
      "Requires libraries\n",
      "Figure 1.3\n",
      "Multiple applications running on the same host may have conflicting dependencies.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 38, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "6\n",
      "CHAPTER 1\n",
      "Introducing Kubernetes\n",
      "Deploying dynamically linked applications that require different versions of shared\n",
      "libraries, and/or require other environment specifics, can quickly become a night-\n",
      "mare for the ops team who deploys and manages them on production servers. The\n",
      "bigger the number of components you need to deploy on the same host, the harder it\n",
      "will be to manage all their dependencies to satisfy all their requirements. \n",
      "1.1.2\n",
      "Providing a consistent environment to applications\n",
      "Regardless of how many individual components you’re developing and deploying,\n",
      "one of the biggest problems that developers and operations teams always have to deal\n",
      "with is the differences in the environments they run their apps in. Not only is there a\n",
      "huge difference between development and production environments, differences\n",
      "even exist between individual production machines. Another unavoidable fact is that\n",
      "the environment of a single production machine will change over time. \n",
      " These differences range from hardware to the operating system to the libraries\n",
      "that are available on each machine. Production environments are managed by the\n",
      "operations team, while developers often take care of their development laptops on\n",
      "their own. The difference is how much these two groups of people know about sys-\n",
      "tem administration, and this understandably leads to relatively big differences\n",
      "between those two systems, not to mention that system administrators give much more\n",
      "emphasis on keeping the system up to date with the latest security patches, while a lot\n",
      "of developers don’t care about that as much. \n",
      " Also, production systems can run applications from multiple developers or devel-\n",
      "opment teams, which isn’t necessarily true for developers’ computers. A production\n",
      "system must provide the proper environment to all applications it hosts, even though\n",
      "they may require different, even conflicting, versions of libraries.\n",
      " To reduce the number of problems that only show up in production, it would be\n",
      "ideal if applications could run in the exact same environment during development\n",
      "and in production so they have the exact same operating system, libraries, system con-\n",
      "figuration, networking environment, and everything else. You also don’t want this\n",
      "environment to change too much over time, if at all. Also, if possible, you want the\n",
      "ability to add applications to the same server without affecting any of the existing\n",
      "applications on that server. \n",
      "1.1.3\n",
      "Moving to continuous delivery: DevOps and NoOps\n",
      "In the last few years, we’ve also seen a shift in the whole application development pro-\n",
      "cess and how applications are taken care of in production. In the past, the develop-\n",
      "ment team’s job was to create the application and hand it off to the operations team,\n",
      "who then deployed it, tended to it, and kept it running. But now, organizations are\n",
      "realizing it’s better to have the same team that develops the application also take part\n",
      "in deploying it and taking care of it over its whole lifetime. This means the developer,\n",
      "QA, and operations teams now need to collaborate throughout the whole process.\n",
      "This practice is called DevOps.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 39, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "7\n",
      "Introducing container technologies\n",
      "UNDERSTANDING THE BENEFITS\n",
      "Having the developers more involved in running the application in production leads\n",
      "to them having a better understanding of both the users’ needs and issues and the\n",
      "problems faced by the ops team while maintaining the app. Application developers\n",
      "are now also much more inclined to give users the app earlier and then use their feed-\n",
      "back to steer further development of the app. \n",
      " To release newer versions of applications more often, you need to streamline the\n",
      "deployment process. Ideally, you want developers to deploy the applications them-\n",
      "selves without having to wait for the ops people. But deploying an application often\n",
      "requires an understanding of the underlying infrastructure and the organization of\n",
      "the hardware in the datacenter. Developers don’t always know those details and, most\n",
      "of the time, don’t even want to know about them. \n",
      "LETTING DEVELOPERS AND SYSADMINS DO WHAT THEY DO BEST\n",
      "Even though developers and system administrators both work toward achieving the\n",
      "same goal of running a successful software application as a service to its customers, they\n",
      "have different individual goals and motivating factors. Developers love creating new fea-\n",
      "tures and improving the user experience. They don’t normally want to be the ones mak-\n",
      "ing sure that the underlying operating system is up to date with all the security patches\n",
      "and things like that. They prefer to leave that up to the system administrators. \n",
      " The ops team is in charge of the production deployments and the hardware infra-\n",
      "structure they run on. They care about system security, utilization, and other aspects\n",
      "that aren’t a high priority for developers. The ops people don’t want to deal with the\n",
      "implicit interdependencies of all the application components and don’t want to think\n",
      "about how changes to either the underlying operating system or the infrastructure\n",
      "can affect the operation of the application as a whole, but they must.\n",
      " Ideally, you want the developers to deploy applications themselves without know-\n",
      "ing anything about the hardware infrastructure and without dealing with the ops\n",
      "team. This is referred to as NoOps. Obviously, you still need someone to take care of\n",
      "the hardware infrastructure, but ideally, without having to deal with peculiarities of\n",
      "each application running on it. \n",
      " As you’ll see, Kubernetes enables us to achieve all of this. By abstracting away the\n",
      "actual hardware and exposing it as a single platform for deploying and running apps,\n",
      "it allows developers to configure and deploy their applications without any help from\n",
      "the sysadmins and allows the sysadmins to focus on keeping the underlying infrastruc-\n",
      "ture up and running, while not having to know anything about the actual applications\n",
      "running on top of it.\n",
      "1.2\n",
      "Introducing container technologies\n",
      "In section 1.1 I presented a non-comprehensive list of problems facing today’s devel-\n",
      "opment and ops teams. While you have many ways of dealing with them, this book will\n",
      "focus on how they’re solved with Kubernetes. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 40, 'img_cnt': 0, 'img_npy_lst': []}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "CHAPTER 1\n",
      "Introducing Kubernetes\n",
      " Kubernetes uses Linux container technologies to provide isolation of running\n",
      "applications, so before we dig into Kubernetes itself, you need to become familiar\n",
      "with the basics of containers to understand what Kubernetes does itself, and what it\n",
      "offloads to container technologies like Docker or rkt (pronounced “rock-it”).\n",
      "1.2.1\n",
      "Understanding what containers are\n",
      "In section 1.1.1 we saw how different software components running on the same\n",
      "machine will require different, possibly conflicting, versions of dependent libraries or\n",
      "have other different environment requirements in general. \n",
      " When an application is composed of only smaller numbers of large components,\n",
      "it’s completely acceptable to give a dedicated Virtual Machine (VM) to each compo-\n",
      "nent and isolate their environments by providing each of them with their own operat-\n",
      "ing system instance. But when these components start getting smaller and their\n",
      "numbers start to grow, you can’t give each of them their own VM if you don’t want to\n",
      "waste hardware resources and keep your hardware costs down. But it’s not only about\n",
      "wasting hardware resources. Because each VM usually needs to be configured and\n",
      "managed individually, rising numbers of VMs also lead to wasting human resources,\n",
      "because they increase the system administrators’ workload considerably.\n",
      "ISOLATING COMPONENTS WITH LINUX CONTAINER TECHNOLOGIES\n",
      "Instead of using virtual machines to isolate the environments of each microservice (or\n",
      "software processes in general), developers are turning to Linux container technolo-\n",
      "gies. They allow you to run multiple services on the same host machine, while not only\n",
      "exposing a different environment to each of them, but also isolating them from each\n",
      "other, similarly to VMs, but with much less overhead.\n",
      " A process running in a container runs inside the host’s operating system, like all\n",
      "the other processes (unlike VMs, where processes run in separate operating sys-\n",
      "tems). But the process in the container is still isolated from other processes. To the\n",
      "process itself, it looks like it’s the only one running on the machine and in its oper-\n",
      "ating system. \n",
      "COMPARING VIRTUAL MACHINES TO CONTAINERS\n",
      "Compared to VMs, containers are much more lightweight, which allows you to run\n",
      "higher numbers of software components on the same hardware, mainly because each\n",
      "VM needs to run its own set of system processes, which requires additional compute\n",
      "resources in addition to those consumed by the component’s own process. A con-\n",
      "tainer, on the other hand, is nothing more than a single isolated process running in\n",
      "the host OS, consuming only the resources that the app consumes and without the\n",
      "overhead of any additional processes. \n",
      " Because of the overhead of VMs, you often end up grouping multiple applications\n",
      "into each VM because you don’t have enough resources to dedicate a whole VM to\n",
      "each app. When using containers, you can (and should) have one container for each\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 41, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "9\n",
      "Introducing container technologies\n",
      "application, as shown in figure 1.4. The end-result is that you can fit many more appli-\n",
      "cations on the same bare-metal machine.\n",
      "When you run three VMs on a host, you have three completely separate operating sys-\n",
      "tems running on and sharing the same bare-metal hardware. Underneath those VMs\n",
      "is the host’s OS and a hypervisor, which divides the physical hardware resources into\n",
      "smaller sets of virtual resources that can be used by the operating system inside each\n",
      "VM. Applications running inside those VMs perform system calls to the guest OS’ ker-\n",
      "nel in the VM, and the kernel then performs x86 instructions on the host’s physical\n",
      "CPU through the hypervisor. \n",
      "NOTE\n",
      "Two types of hypervisors exist. Type 1 hypervisors don’t use a host OS,\n",
      "while Type 2 do.\n",
      "Containers, on the other hand, all perform system calls on the exact same kernel run-\n",
      "ning in the host OS. This single kernel is the only one performing x86 instructions on\n",
      "the host’s CPU. The CPU doesn’t need to do any kind of virtualization the way it does\n",
      "with VMs (see figure 1.5).\n",
      " The main benefit of virtual machines is the full isolation they provide, because\n",
      "each VM runs its own Linux kernel, while containers all call out to the same kernel,\n",
      "which can clearly pose a security risk. If you have a limited amount of hardware\n",
      "resources, VMs may only be an option when you have a small number of processes that\n",
      "Apps running in three VMs\n",
      "(on a single machine)\n",
      "Bare-metal machine\n",
      "VM 1\n",
      "VM 2\n",
      "VM 3\n",
      "App A\n",
      "App B\n",
      "App C\n",
      "App D\n",
      "App E\n",
      "App F\n",
      "Guest OS\n",
      "Guest OS\n",
      "Guest OS\n",
      "Bare-metal machine\n",
      "Host OS\n",
      "Hypervisor\n",
      "Apps running in\n",
      "isolated containers\n",
      "Container 1\n",
      "Container 2\n",
      "Container 3\n",
      "App A\n",
      "App B\n",
      "App C\n",
      "Container 4\n",
      "Container 5\n",
      "Container 6\n",
      "App D\n",
      "App E\n",
      "App F\n",
      "Container 7\n",
      "Container 8\n",
      "Container 9\n",
      "App ...\n",
      "App ...\n",
      "App ...\n",
      "Host OS\n",
      "Figure 1.4\n",
      "Using VMs to isolate groups of applications vs. isolating individual apps with containers\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 42, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "10\n",
      "CHAPTER 1\n",
      "Introducing Kubernetes\n",
      "you want to isolate. To run greater numbers of isolated processes on the same\n",
      "machine, containers are a much better choice because of their low overhead. Remem-\n",
      "ber, each VM runs its own set of system services, while containers don’t, because they\n",
      "all run in the same OS. That also means that to run a container, nothing needs to be\n",
      "booted up, as is the case in VMs. A process run in a container starts up immediately.\n",
      "Apps running in multiple VMs\n",
      "VM 1\n",
      "App\n",
      "A\n",
      "App\n",
      "B\n",
      "Kernel\n",
      "Virtual CPU\n",
      "Hypervisor\n",
      "Physical CPU\n",
      "Kernel\n",
      "Physical CPU\n",
      "VM 2\n",
      "App\n",
      "D\n",
      "Kernel\n",
      "Virtual CPU\n",
      "App\n",
      "C\n",
      "App\n",
      "E\n",
      "VM 3\n",
      "App\n",
      "F\n",
      "Kernel\n",
      "Virtual CPU\n",
      "Apps running in isolated containers\n",
      "Container\n",
      "A\n",
      "Container\n",
      "B\n",
      "Container\n",
      "C\n",
      "Container\n",
      "D\n",
      "Container\n",
      "E\n",
      "Container\n",
      "F\n",
      "App\n",
      "A\n",
      "App\n",
      "B\n",
      "App\n",
      "D\n",
      "App\n",
      "E\n",
      "App\n",
      "F\n",
      "App\n",
      "C\n",
      "Figure 1.5\n",
      "The difference between \n",
      "how apps in VMs use the CPU vs. how \n",
      "they use them in containers\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 43, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "11\n",
      "Introducing container technologies\n",
      "INTRODUCING THE MECHANISMS THAT MAKE CONTAINER ISOLATION POSSIBLE \n",
      "By this point, you’re probably wondering how exactly containers can isolate processes\n",
      "if they’re running on the same operating system. Two mechanisms make this possible.\n",
      "The first one, Linux Namespaces, makes sure each process sees its own personal view of\n",
      "the system (files, processes, network interfaces, hostname, and so on). The second\n",
      "one is Linux Control Groups (cgroups), which limit the amount of resources the process\n",
      "can consume (CPU, memory, network bandwidth, and so on).\n",
      "ISOLATING PROCESSES WITH LINUX NAMESPACES\n",
      "By default, each Linux system initially has one single namespace. All system resources,\n",
      "such as filesystems, process IDs, user IDs, network interfaces, and others, belong to the\n",
      "single namespace. But you can create additional namespaces and organize resources\n",
      "across them. When running a process, you run it inside one of those namespaces. The\n",
      "process will only see resources that are inside the same namespace. Well, multiple\n",
      "kinds of namespaces exist, so a process doesn’t belong to one namespace, but to one\n",
      "namespace of each kind. \n",
      " The following kinds of namespaces exist:\n",
      "Mount (mnt)\n",
      "Process ID (pid)\n",
      "Network (net)\n",
      "Inter-process communication (ipc)\n",
      "UTS\n",
      "User ID (user)\n",
      "Each namespace kind is used to isolate a certain group of resources. For example, the\n",
      "UTS namespace determines what hostname and domain name the process running\n",
      "inside that namespace sees. By assigning two different UTS namespaces to a pair of\n",
      "processes, you can make them see different local hostnames. In other words, to the\n",
      "two processes, it will appear as though they are running on two different machines (at\n",
      "least as far as the hostname is concerned). \n",
      " Likewise, what Network namespace a process belongs to determines which net-\n",
      "work interfaces the application running inside the process sees. Each network inter-\n",
      "face belongs to exactly one namespace, but can be moved from one namespace to\n",
      "another. Each container uses its own Network namespace, and therefore each con-\n",
      "tainer sees its own set of network interfaces.\n",
      " This should give you a basic idea of how namespaces are used to isolate applica-\n",
      "tions running in containers from each other. \n",
      "LIMITING RESOURCES AVAILABLE TO A PROCESS\n",
      "The other half of container isolation deals with limiting the amount of system\n",
      "resources a container can consume. This is achieved with cgroups, a Linux kernel fea-\n",
      "ture that limits the resource usage of a process (or a group of processes). A process\n",
      "can’t use more than the configured amount of CPU, memory, network bandwidth,\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 44, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "12\n",
      "CHAPTER 1\n",
      "Introducing Kubernetes\n",
      "and so on. This way, processes cannot hog resources reserved for other processes,\n",
      "which is similar to when each process runs on a separate machine.\n",
      "1.2.2\n",
      "Introducing the Docker container platform\n",
      "While container technologies have been around for a long time, they’ve become\n",
      "more widely known with the rise of the Docker container platform. Docker was the\n",
      "first container system that made containers easily portable across different machines.\n",
      "It simplified the process of packaging up not only the application but also all its\n",
      "libraries and other dependencies, even the whole OS file system, into a simple, por-\n",
      "table package that can be used to provision the application to any other machine\n",
      "running Docker. \n",
      " When you run an application packaged with Docker, it sees the exact filesystem\n",
      "contents that you’ve bundled with it. It sees the same files whether it’s running on\n",
      "your development machine or a production machine, even if it the production server\n",
      "is running a completely different Linux OS. The application won’t see anything from\n",
      "the server it’s running on, so it doesn’t matter if the server has a completely different\n",
      "set of installed libraries compared to your development machine. \n",
      " For example, if you’ve packaged up your application with the files of the whole\n",
      "Red Hat Enterprise Linux (RHEL) operating system, the application will believe it’s\n",
      "running inside RHEL, both when you run it on your development computer that runs\n",
      "Fedora and when you run it on a server running Debian or some other Linux distribu-\n",
      "tion. Only the kernel may be different.\n",
      " This is similar to creating a VM image by installing an operating system into a VM,\n",
      "installing the app inside it, and then distributing the whole VM image around and\n",
      "running it. Docker achieves the same effect, but instead of using VMs to achieve app\n",
      "isolation, it uses Linux container technologies mentioned in the previous section to\n",
      "provide (almost) the same level of isolation that VMs do. Instead of using big mono-\n",
      "lithic VM images, it uses container images, which are usually smaller.\n",
      " A big difference between Docker-based container images and VM images is that\n",
      "container images are composed of layers, which can be shared and reused across mul-\n",
      "tiple images. This means only certain layers of an image need to be downloaded if the\n",
      "other layers were already downloaded previously when running a different container\n",
      "image that also contains the same layers.\n",
      "UNDERSTANDING DOCKER CONCEPTS\n",
      "Docker is a platform for packaging, distributing, and running applications. As we’ve\n",
      "already stated, it allows you to package your application together with its whole envi-\n",
      "ronment. This can be either a few libraries that the app requires or even all the files\n",
      "that are usually available on the filesystem of an installed operating system. Docker\n",
      "makes it possible to transfer this package to a central repository from which it can\n",
      "then be transferred to any computer running Docker and executed there (for the\n",
      "most part, but not always, as we’ll soon explain).\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 45, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "13\n",
      "Introducing container technologies\n",
      " Three main concepts in Docker comprise this scenario:\n",
      "Images—A Docker-based container image is something you package your appli-\n",
      "cation and its environment into. It contains the filesystem that will be available\n",
      "to the application and other metadata, such as the path to the executable that\n",
      "should be executed when the image is run. \n",
      "Registries—A Docker Registry is a repository that stores your Docker images and\n",
      "facilitates easy sharing of those images between different people and comput-\n",
      "ers. When you build your image, you can either run it on the computer you’ve\n",
      "built it on, or you can push (upload) the image to a registry and then pull\n",
      "(download) it on another computer and run it there. Certain registries are pub-\n",
      "lic, allowing anyone to pull images from it, while others are private, only accessi-\n",
      "ble to certain people or machines.\n",
      "Containers—A Docker-based container is a regular Linux container created from\n",
      "a Docker-based container image. A running container is a process running on\n",
      "the host running Docker, but it’s completely isolated from both the host and all\n",
      "other processes running on it. The process is also resource-constrained, mean-\n",
      "ing it can only access and use the amount of resources (CPU, RAM, and so on)\n",
      "that are allocated to it. \n",
      "BUILDING, DISTRIBUTING, AND RUNNING A DOCKER IMAGE\n",
      "Figure 1.6 shows all three concepts and how they relate to each other. The developer\n",
      "first builds an image and then pushes it to a registry. The image is thus available to\n",
      "anyone who can access the registry. They can then pull the image to any other\n",
      "machine running Docker and run the image. Docker creates an isolated container\n",
      "based on the image and runs the binary executable specified as part of the image.\n",
      "Docker\n",
      "Image\n",
      "Container\n",
      "Image registry\n",
      "Image\n",
      "Docker\n",
      "Image\n",
      "Development machine\n",
      "Production machine\n",
      "1. Developer tells\n",
      "Docker to build\n",
      "and push image\n",
      "2. Docker\n",
      "builds image\n",
      "4. Developer tells\n",
      "Docker on production\n",
      "machine to run image\n",
      "3. Docker\n",
      "pushes image\n",
      "to registry\n",
      "5. Docker pulls\n",
      "image from\n",
      "registry\n",
      "6. Docker runs\n",
      "container from\n",
      "image\n",
      "Developer\n",
      "Figure 1.6\n",
      "Docker images, registries, and containers\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 46, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "14\n",
      "CHAPTER 1\n",
      "Introducing Kubernetes\n",
      "COMPARING VIRTUAL MACHINES AND DOCKER CONTAINERS\n",
      "I’ve explained how Linux containers are generally like virtual machines, but much\n",
      "more lightweight. Now let’s look at how Docker containers specifically compare to vir-\n",
      "tual machines (and how Docker images compare to VM images). Figure 1.7 again shows\n",
      "the same six applications running both in VMs and as Docker containers.\n",
      "You’ll notice that apps A and B have access to the same binaries and libraries both\n",
      "when running in a VM and when running as two separate containers. In the VM, this\n",
      "is obvious, because both apps see the same filesystem (that of the VM). But we said\n",
      "Host running multiple VMs\n",
      "VM 1\n",
      "App\n",
      "A\n",
      "App\n",
      "B\n",
      "Binaries and\n",
      "libraries\n",
      "(Filesystem)\n",
      "Guest OS kernel\n",
      "Hypervisor\n",
      "Host OS\n",
      "Host OS\n",
      "VM 2\n",
      "App\n",
      "D\n",
      "Guest OS kernel\n",
      "App\n",
      "C\n",
      "App\n",
      "E\n",
      "VM 3\n",
      "App\n",
      "F\n",
      "Guest OS kernel\n",
      "Host running multiple Docker containers\n",
      "Container 1\n",
      "Container 2\n",
      "Container 3\n",
      "Container 4\n",
      "Container 5\n",
      "Container 6\n",
      "App\n",
      "D\n",
      "App\n",
      "E\n",
      "App\n",
      "F\n",
      "App\n",
      "C\n",
      "App\n",
      "A\n",
      "App\n",
      "B\n",
      "Binaries and\n",
      "libraries\n",
      "(Filesystem)\n",
      "Binaries and\n",
      "libraries\n",
      "(Filesystem)\n",
      "Binaries and\n",
      "libraries\n",
      "(Filesystem)\n",
      "Binaries and\n",
      "libraries\n",
      "(Filesystem)\n",
      "Binaries and\n",
      "libraries\n",
      "(Filesystem)\n",
      "Docker\n",
      "Figure 1.7\n",
      "Running six apps on \n",
      "three VMs vs. running them in \n",
      "Docker containers\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 47, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "15\n",
      "Introducing container technologies\n",
      "that each container has its own isolated filesystem. How can both app A and app B\n",
      "share the same files?\n",
      "UNDERSTANDING IMAGE LAYERS\n",
      "I’ve already said that Docker images are composed of layers. Different images can con-\n",
      "tain the exact same layers because every Docker image is built on top of another\n",
      "image and two different images can both use the same parent image as their base.\n",
      "This speeds up the distribution of images across the network, because layers that have\n",
      "already been transferred as part of the first image don’t need to be transferred again\n",
      "when transferring the other image. \n",
      " But layers don’t only make distribution more efficient, they also help reduce the\n",
      "storage footprint of images. Each layer is only stored once. Two containers created\n",
      "from two images based on the same base layers can therefore read the same files, but\n",
      "if one of them writes over those files, the other one doesn’t see those changes. There-\n",
      "fore, even if they share files, they’re still isolated from each other. This works because\n",
      "container image layers are read-only. When a container is run, a new writable layer is\n",
      "created on top of the layers in the image. When the process in the container writes to\n",
      "a file located in one of the underlying layers, a copy of the whole file is created in the\n",
      "top-most layer and the process writes to the copy. \n",
      "UNDERSTANDING THE PORTABILITY LIMITATIONS OF CONTAINER IMAGES\n",
      "In theory, a container image can be run on any Linux machine running Docker, but\n",
      "one small caveat exists—one related to the fact that all containers running on a host use\n",
      "the host’s Linux kernel. If a containerized application requires a specific kernel version,\n",
      "it may not work on every machine. If a machine runs a different version of the Linux\n",
      "kernel or doesn’t have the same kernel modules available, the app can’t run on it.\n",
      " While containers are much more lightweight compared to VMs, they impose cer-\n",
      "tain constraints on the apps running inside them. VMs have no such constraints,\n",
      "because each VM runs its own kernel. \n",
      " And it’s not only about the kernel. It should also be clear that a containerized app\n",
      "built for a specific hardware architecture can only run on other machines that have\n",
      "the same architecture. You can’t containerize an application built for the x86 architec-\n",
      "ture and expect it to run on an ARM-based machine because it also runs Docker. You\n",
      "still need a VM for that.\n",
      "1.2.3\n",
      "Introducing rkt—an alternative to Docker\n",
      "Docker was the first container platform that made containers mainstream. I hope I’ve\n",
      "made it clear that Docker itself doesn’t provide process isolation. The actual isolation\n",
      "of containers is done at the Linux kernel level using kernel features such as Linux\n",
      "Namespaces and cgroups. Docker only makes it easy to use those features.\n",
      " After the success of Docker, the Open Container Initiative (OCI) was born to cre-\n",
      "ate open industry standards around container formats and runtime. Docker is part\n",
      "of that initiative, as is rkt (pronounced “rock-it”), which is another Linux container\n",
      "engine. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 48, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "16\n",
      "CHAPTER 1\n",
      "Introducing Kubernetes\n",
      " Like Docker, rkt is a platform for running containers. It puts a strong emphasis on\n",
      "security, composability, and conforming to open standards. It uses the OCI container\n",
      "image format and can even run regular Docker container images. \n",
      " This book focuses on using Docker as the container runtime for Kubernetes,\n",
      "because it was initially the only one supported by Kubernetes. Recently, Kubernetes\n",
      "has also started supporting rkt, as well as others, as the container runtime. \n",
      " The reason I mention rkt at this point is so you don’t make the mistake of thinking\n",
      "Kubernetes is a container orchestration system made specifically for Docker-based\n",
      "containers. In fact, over the course of this book, you’ll realize that the essence of\n",
      "Kubernetes isn’t orchestrating containers. It’s much more. Containers happen to be\n",
      "the best way to run apps on different cluster nodes. With that in mind, let’s finally dive\n",
      "into the core of what this book is all about—Kubernetes.\n",
      "1.3\n",
      "Introducing Kubernetes\n",
      "We’ve already shown that as the number of deployable application components in\n",
      "your system grows, it becomes harder to manage them all. Google was probably the\n",
      "first company that realized it needed a much better way of deploying and managing\n",
      "their software components and their infrastructure to scale globally. It’s one of only a\n",
      "few companies in the world that runs hundreds of thousands of servers and has had to\n",
      "deal with managing deployments on such a massive scale. This has forced them to\n",
      "develop solutions for making the development and deployment of thousands of soft-\n",
      "ware components manageable and cost-efficient.\n",
      "1.3.1\n",
      "Understanding its origins\n",
      "Through the years, Google developed an internal system called Borg (and later a new\n",
      "system called Omega), that helped both application developers and system administra-\n",
      "tors manage those thousands of applications and services. In addition to simplifying\n",
      "the development and management, it also helped them achieve a much higher utiliza-\n",
      "tion of their infrastructure, which is important when your organization is that large.\n",
      "When you run hundreds of thousands of machines, even tiny improvements in utiliza-\n",
      "tion mean savings in the millions of dollars, so the incentives for developing such a\n",
      "system are clear.\n",
      " After having kept Borg and Omega secret for a whole decade, in 2014 Google\n",
      "introduced Kubernetes, an open-source system based on the experience gained\n",
      "through Borg, Omega, and other internal Google systems. \n",
      "1.3.2\n",
      "Looking at Kubernetes from the top of a mountain\n",
      "Kubernetes is a software system that allows you to easily deploy and manage container-\n",
      "ized applications on top of it. It relies on the features of Linux containers to run het-\n",
      "erogeneous applications without having to know any internal details of these\n",
      "applications and without having to manually deploy these applications on each host.\n",
      "Because these apps run in containers, they don’t affect other apps running on the\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 49, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "17\n",
      "Introducing Kubernetes\n",
      "same server, which is critical when you run applications for completely different orga-\n",
      "nizations on the same hardware. This is of paramount importance for cloud provid-\n",
      "ers, because they strive for the best possible utilization of their hardware while still\n",
      "having to maintain complete isolation of hosted applications.\n",
      " Kubernetes enables you to run your software applications on thousands of com-\n",
      "puter nodes as if all those nodes were a single, enormous computer. It abstracts away\n",
      "the underlying infrastructure and, by doing so, simplifies development, deployment,\n",
      "and management for both development and the operations teams. \n",
      " Deploying applications through Kubernetes is always the same, whether your clus-\n",
      "ter contains only a couple of nodes or thousands of them. The size of the cluster\n",
      "makes no difference at all. Additional cluster nodes simply represent an additional\n",
      "amount of resources available to deployed apps.\n",
      "UNDERSTANDING THE CORE OF WHAT KUBERNETES DOES\n",
      "Figure 1.8 shows the simplest possible view of a Kubernetes system. The system is com-\n",
      "posed of a master node and any number of worker nodes. When the developer sub-\n",
      "mits a list of apps to the master, Kubernetes deploys them to the cluster of worker\n",
      "nodes. What node a component lands on doesn’t (and shouldn’t) matter—neither to\n",
      "the developer nor to the system administrator.\n",
      "The developer can specify that certain apps must run together and Kubernetes will\n",
      "deploy them on the same worker node. Others will be spread around the cluster, but\n",
      "they can talk to each other in the same way, regardless of where they’re deployed.\n",
      "HELPING DEVELOPERS FOCUS ON THE CORE APP FEATURES\n",
      "Kubernetes can be thought of as an operating system for the cluster. It relieves appli-\n",
      "cation developers from having to implement certain infrastructure-related services\n",
      "into their apps; instead they rely on Kubernetes to provide these services. This includes\n",
      "things such as service discovery, scaling, load-balancing, self-healing, and even leader\n",
      "Kubernetes\n",
      "master\n",
      "Tens or thousands of worker nodes exposed\n",
      "as a single deployment platform\n",
      "1x\n",
      "App descriptor\n",
      "5x\n",
      "2x\n",
      "Developer\n",
      "Figure 1.8\n",
      "Kubernetes exposes the whole datacenter as a single deployment platform.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 50, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "18\n",
      "CHAPTER 1\n",
      "Introducing Kubernetes\n",
      "election. Application developers can therefore focus on implementing the actual fea-\n",
      "tures of the applications and not waste time figuring out how to integrate them with\n",
      "the infrastructure.\n",
      "HELPING OPS TEAMS ACHIEVE BETTER RESOURCE UTILIZATION\n",
      "Kubernetes will run your containerized app somewhere in the cluster, provide infor-\n",
      "mation to its components on how to find each other, and keep all of them running.\n",
      "Because your application doesn’t care which node it’s running on, Kubernetes can\n",
      "relocate the app at any time, and by mixing and matching apps, achieve far better\n",
      "resource utilization than is possible with manual scheduling.\n",
      "1.3.3\n",
      "Understanding the architecture of a Kubernetes cluster\n",
      "We’ve seen a bird’s-eye view of Kubernetes’ architecture. Now let’s take a closer look at\n",
      "what a Kubernetes cluster is composed of. At the hardware level, a Kubernetes cluster\n",
      "is composed of many nodes, which can be split into two types: \n",
      "The master node, which hosts the Kubernetes Control Plane that controls and man-\n",
      "ages the whole Kubernetes system\n",
      "Worker nodes that run the actual applications you deploy\n",
      "Figure 1.9 shows the components running on these two sets of nodes. I’ll explain\n",
      "them next.\n",
      "THE CONTROL PLANE\n",
      "The Control Plane is what controls the cluster and makes it function. It consists of\n",
      "multiple components that can run on a single master node or be split across multiple\n",
      "nodes and replicated to ensure high availability. These components are\n",
      "The Kubernetes API Server, which you and the other Control Plane components\n",
      "communicate with\n",
      "Control Plane (master)\n",
      "etcd\n",
      "API server\n",
      "kube-proxy\n",
      "Worker node(s)\n",
      "Kubelet\n",
      "Container Runtime\n",
      "Scheduler\n",
      "Controller\n",
      "Manager\n",
      "Figure 1.9\n",
      "The components that make up a Kubernetes cluster\n",
      " \n",
      "www.allitebooks.com\n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 51, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "19\n",
      "Introducing Kubernetes\n",
      "The Scheduler, which schedules your apps (assigns a worker node to each deploy-\n",
      "able component of your application) \n",
      "The Controller Manager, which performs cluster-level functions, such as repli-\n",
      "cating components, keeping track of worker nodes, handling node failures,\n",
      "and so on\n",
      "etcd, a reliable distributed data store that persistently stores the cluster\n",
      "configuration.\n",
      "The components of the Control Plane hold and control the state of the cluster, but\n",
      "they don’t run your applications. This is done by the (worker) nodes.\n",
      "THE NODES\n",
      "The worker nodes are the machines that run your containerized applications. The\n",
      "task of running, monitoring, and providing services to your applications is done by\n",
      "the following components:\n",
      "Docker, rkt, or another container runtime, which runs your containers\n",
      "The Kubelet, which talks to the API server and manages containers on its node\n",
      "The Kubernetes Service Proxy (kube-proxy), which load-balances network traffic\n",
      "between application components\n",
      "We’ll explain all these components in detail in chapter 11. I’m not a fan of explaining\n",
      "how things work before first explaining what something does and teaching people to\n",
      "use it. It’s like learning to drive a car. You don’t want to know what’s under the hood.\n",
      "You first want to learn how to drive it from point A to point B. Only after you learn\n",
      "how to do that do you become interested in how a car makes that possible. After all,\n",
      "knowing what’s under the hood may someday help you get the car moving again after\n",
      "it breaks down and leaves you stranded at the side of the road.\n",
      "1.3.4\n",
      "Running an application in Kubernetes\n",
      "To run an application in Kubernetes, you first need to package it up into one or more\n",
      "container images, push those images to an image registry, and then post a description\n",
      "of your app to the Kubernetes API server. \n",
      " The description includes information such as the container image or images that\n",
      "contain your application components, how those components are related to each\n",
      "other, and which ones need to be run co-located (together on the same node) and\n",
      "which don’t. For each component, you can also specify how many copies (or replicas)\n",
      "you want to run. Additionally, the description also includes which of those compo-\n",
      "nents provide a service to either internal or external clients and should be exposed\n",
      "through a single IP address and made discoverable to the other components. \n",
      "UNDERSTANDING HOW THE DESCRIPTION RESULTS IN A RUNNING CONTAINER\n",
      "When the API server processes your app’s description, the Scheduler schedules the\n",
      "specified groups of containers onto the available worker nodes based on computa-\n",
      "tional resources required by each group and the unallocated resources on each node\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 52, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "20\n",
      "CHAPTER 1\n",
      "Introducing Kubernetes\n",
      "at that moment. The Kubelet on those nodes then instructs the Container Runtime\n",
      "(Docker, for example) to pull the required container images and run the containers. \n",
      " Examine figure 1.10 to gain a better understanding of how applications are\n",
      "deployed in Kubernetes. The app descriptor lists four containers, grouped into three\n",
      "sets (these sets are called pods; we’ll explain what they are in chapter 3). The first two\n",
      "pods each contain only a single container, whereas the last one contains two. That\n",
      "means both containers need to run co-located and shouldn’t be isolated from each\n",
      "other. Next to each pod, you also see a number representing the number of replicas\n",
      "of each pod that need to run in parallel. After submitting the descriptor to Kuberne-\n",
      "tes, it will schedule the specified number of replicas of each pod to the available\n",
      "worker nodes. The Kubelets on the nodes will then tell Docker to pull the container\n",
      "images from the image registry and run the containers.\n",
      "KEEPING THE CONTAINERS RUNNING\n",
      "Once the application is running, Kubernetes continuously makes sure that the deployed\n",
      "state of the application always matches the description you provided. For example, if\n",
      "1x\n",
      "App descriptor\n",
      "Legend:\n",
      "Container image\n",
      "Multiple containers\n",
      "running “together”\n",
      "(not fully isolated)\n",
      "5x\n",
      "2x\n",
      "Control Plane\n",
      "(master)\n",
      "Image registry\n",
      "Worker nodes\n",
      "...\n",
      "kube-proxy\n",
      "Docker\n",
      "Kubelet\n",
      "kube-proxy\n",
      "Docker\n",
      "Kubelet\n",
      "Container\n",
      "...\n",
      "kube-proxy\n",
      "Docker\n",
      "Kubelet\n",
      "kube-proxy\n",
      "Docker\n",
      "Kubelet\n",
      "...\n",
      "kube-proxy\n",
      "Docker\n",
      "Kubelet\n",
      "kube-proxy\n",
      "Docker\n",
      "Kubelet\n",
      "Figure 1.10\n",
      "A basic overview of the Kubernetes architecture and an application running on top of it\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 53, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "21\n",
      "Introducing Kubernetes\n",
      "you specify that you always want five instances of a web server running, Kubernetes will\n",
      "always keep exactly five instances running. If one of those instances stops working\n",
      "properly, like when its process crashes or when it stops responding, Kubernetes will\n",
      "restart it automatically. \n",
      " Similarly, if a whole worker node dies or becomes inaccessible, Kubernetes will\n",
      "select new nodes for all the containers that were running on the node and run them\n",
      "on the newly selected nodes.\n",
      "SCALING THE NUMBER OF COPIES\n",
      "While the application is running, you can decide you want to increase or decrease the\n",
      "number of copies, and Kubernetes will spin up additional ones or stop the excess\n",
      "ones, respectively. You can even leave the job of deciding the optimal number of cop-\n",
      "ies to Kubernetes. It can automatically keep adjusting the number, based on real-time\n",
      "metrics, such as CPU load, memory consumption, queries per second, or any other\n",
      "metric your app exposes. \n",
      "HITTING A MOVING TARGET\n",
      "We’ve said that Kubernetes may need to move your containers around the cluster.\n",
      "This can occur when the node they were running on has failed or because they were\n",
      "evicted from a node to make room for other containers. If the container is providing a\n",
      "service to external clients or other containers running in the cluster, how can they use\n",
      "the container properly if it’s constantly moving around the cluster? And how can cli-\n",
      "ents connect to containers providing a service when those containers are replicated\n",
      "and spread across the whole cluster?\n",
      " To allow clients to easily find containers that provide a specific service, you can tell\n",
      "Kubernetes which containers provide the same service and Kubernetes will expose all\n",
      "of them at a single static IP address and expose that address to all applications run-\n",
      "ning in the cluster. This is done through environment variables, but clients can also\n",
      "look up the service IP through good old DNS. The kube-proxy will make sure connec-\n",
      "tions to the service are load balanced across all the containers that provide the service.\n",
      "The IP address of the service stays constant, so clients can always connect to its con-\n",
      "tainers, even when they’re moved around the cluster.\n",
      "1.3.5\n",
      "Understanding the benefits of using Kubernetes\n",
      "If you have Kubernetes deployed on all your servers, the ops team doesn’t need to\n",
      "deal with deploying your apps anymore. Because a containerized application already\n",
      "contains all it needs to run, the system administrators don’t need to install anything to\n",
      "deploy and run the app. On any node where Kubernetes is deployed, Kubernetes can\n",
      "run the app immediately without any help from the sysadmins. \n",
      "SIMPLIFYING APPLICATION DEPLOYMENT\n",
      "Because Kubernetes exposes all its worker nodes as a single deployment platform,\n",
      "application developers can start deploying applications on their own and don’t need\n",
      "to know anything about the servers that make up the cluster. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 54, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "22\n",
      "CHAPTER 1\n",
      "Introducing Kubernetes\n",
      " In essence, all the nodes are now a single bunch of computational resources that\n",
      "are waiting for applications to consume them. A developer doesn’t usually care what\n",
      "kind of server the application is running on, as long as the server can provide the\n",
      "application with adequate system resources. \n",
      " Certain cases do exist where the developer does care what kind of hardware the\n",
      "application should run on. If the nodes are heterogeneous, you’ll find cases when you\n",
      "want certain apps to run on nodes with certain capabilities and run other apps on oth-\n",
      "ers. For example, one of your apps may require being run on a system with SSDs\n",
      "instead of HDDs, while other apps run fine on HDDs. In such cases, you obviously\n",
      "want to ensure that particular app is always scheduled to a node with an SSD.\n",
      " Without using Kubernetes, the sysadmin would select one specific node that has an\n",
      "SSD and deploy the app there. But when using Kubernetes, instead of selecting a spe-\n",
      "cific node where your app should be run, it’s more appropriate to tell Kubernetes to\n",
      "only choose among nodes with an SSD. You’ll learn how to do that in chapter 3.\n",
      "ACHIEVING BETTER UTILIZATION OF HARDWARE\n",
      "By setting up Kubernetes on your servers and using it to run your apps instead of run-\n",
      "ning them manually, you’ve decoupled your app from the infrastructure. When you\n",
      "tell Kubernetes to run your application, you’re letting it choose the most appropriate\n",
      "node to run your application on based on the description of the application’s\n",
      "resource requirements and the available resources on each node. \n",
      " By using containers and not tying the app down to a specific node in your cluster,\n",
      "you’re allowing the app to freely move around the cluster at any time, so the different\n",
      "app components running on the cluster can be mixed and matched to be packed\n",
      "tightly onto the cluster nodes. This ensures the node’s hardware resources are utilized\n",
      "as best as possible.\n",
      " The ability to move applications around the cluster at any time allows Kubernetes\n",
      "to utilize the infrastructure much better than what you can achieve manually. Humans\n",
      "aren’t good at finding optimal combinations, especially when the number of all possi-\n",
      "ble options is huge, such as when you have many application components and many\n",
      "server nodes they can be deployed on. Computers can obviously perform this work\n",
      "much better and faster than humans. \n",
      "HEALTH CHECKING AND SELF-HEALING\n",
      "Having a system that allows moving an application across the cluster at any time is also\n",
      "valuable in the event of server failures. As your cluster size increases, you’ll deal with\n",
      "failing computer components ever more frequently. \n",
      " Kubernetes monitors your app components and the nodes they run on and auto-\n",
      "matically reschedules them to other nodes in the event of a node failure. This frees\n",
      "the ops team from having to migrate app components manually and allows the team\n",
      "to immediately focus on fixing the node itself and returning it to the pool of available\n",
      "hardware resources instead of focusing on relocating the app.\n",
      " If your infrastructure has enough spare resources to allow normal system opera-\n",
      "tion even without the failed node, the ops team doesn’t even need to react to the failure\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 55, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "23\n",
      "Summary\n",
      "immediately, such as at 3 a.m. They can sleep tight and deal with the failed node\n",
      "during regular work hours.\n",
      "AUTOMATIC SCALING\n",
      "Using Kubernetes to manage your deployed applications also means the ops team\n",
      "doesn’t need to constantly monitor the load of individual applications to react to sud-\n",
      "den load spikes. As previously mentioned, Kubernetes can be told to monitor the\n",
      "resources used by each application and to keep adjusting the number of running\n",
      "instances of each application. \n",
      " If Kubernetes is running on cloud infrastructure, where adding additional nodes is\n",
      "as easy as requesting them through the cloud provider’s API, Kubernetes can even\n",
      "automatically scale the whole cluster size up or down based on the needs of the\n",
      "deployed applications.\n",
      "SIMPLIFYING APPLICATION DEVELOPMENT\n",
      "The features described in the previous section mostly benefit the operations team. But\n",
      "what about the developers? Does Kubernetes bring anything to their table? It defi-\n",
      "nitely does.\n",
      " If you turn back to the fact that apps run in the same environment both during\n",
      "development and in production, this has a big effect on when bugs are discovered. We\n",
      "all agree the sooner you discover a bug, the easier it is to fix it, and fixing it requires\n",
      "less work. It’s the developers who do the fixing, so this means less work for them. \n",
      " Then there’s the fact that developers don’t need to implement features that they\n",
      "would usually implement. This includes discovery of services and/or peers in a clustered\n",
      "application. Kubernetes does this instead of the app. Usually, the app only needs to look\n",
      "up certain environment variables or perform a DNS lookup. If that’s not enough, the\n",
      "application can query the Kubernetes API server directly to get that and/or other infor-\n",
      "mation. Querying the Kubernetes API server like that can even save developers from\n",
      "having to implement complicated mechanisms such as leader election.\n",
      " As a final example of what Kubernetes brings to the table, you also need to con-\n",
      "sider the increase in confidence developers will feel knowing that when a new version\n",
      "of their app is going to be rolled out, Kubernetes can automatically detect if the new\n",
      "version is bad and stop its rollout immediately. This increase in confidence usually\n",
      "accelerates the continuous delivery of apps, which benefits the whole organization.\n",
      "1.4\n",
      "Summary\n",
      "In this introductory chapter, you’ve seen how applications have changed in recent\n",
      "years and how they can now be harder to deploy and manage. We’ve introduced\n",
      "Kubernetes and shown how it, together with Docker and other container platforms,\n",
      "helps deploy and manage applications and the infrastructure they run on. You’ve\n",
      "learned that\n",
      "Monolithic apps are easier to deploy, but harder to maintain over time and\n",
      "sometimes impossible to scale.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 56, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "24\n",
      "CHAPTER 1\n",
      "Introducing Kubernetes\n",
      "Microservices-based application architectures allow easier development of each\n",
      "component, but are harder to deploy and configure to work as a single system.\n",
      "Linux containers provide much the same benefits as virtual machines, but are\n",
      "far more lightweight and allow for much better hardware utilization.\n",
      "Docker improved on existing Linux container technologies by allowing easier and\n",
      "faster provisioning of containerized apps together with their OS environments.\n",
      "Kubernetes exposes the whole datacenter as a single computational resource\n",
      "for running applications.\n",
      "Developers can deploy apps through Kubernetes without assistance from\n",
      "sysadmins.\n",
      "Sysadmins can sleep better by having Kubernetes deal with failed nodes auto-\n",
      "matically.\n",
      "In the next chapter, you’ll get your hands dirty by building an app and running it in\n",
      "Docker and then in Kubernetes.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 57, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "25\n",
      "First steps with Docker\n",
      "and Kubernetes\n",
      "Before you start learning about Kubernetes concepts in detail, let’s see how to cre-\n",
      "ate a simple application, package it into a container image, and run it in a managed\n",
      "Kubernetes cluster (in Google Kubernetes Engine) or in a local single-node cluster.\n",
      "This should give you a slightly better overview of the whole Kubernetes system and\n",
      "will make it easier to follow the next few chapters, where we’ll go over the basic\n",
      "building blocks and concepts in Kubernetes.\n",
      "This chapter covers\n",
      "Creating, running, and sharing a container image \n",
      "with Docker\n",
      "Running a single-node Kubernetes cluster locally\n",
      "Setting up a Kubernetes cluster on Google \n",
      "Kubernetes Engine\n",
      "Setting up and using the kubectl command-line \n",
      "client\n",
      "Deploying an app on Kubernetes and scaling it \n",
      "horizontally\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 58, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "26\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "2.1\n",
      "Creating, running, and sharing a container image\n",
      "As you’ve already learned in the previous chapter, running applications in Kubernetes\n",
      "requires them to be packaged into container images. We’ll do a basic introduction to\n",
      "using Docker in case you haven’t used it yet. In the next few sections you’ll\n",
      "1\n",
      "Install Docker and run your first “Hello world” container \n",
      "2\n",
      "Create a trivial Node.js app that you’ll later deploy in Kubernetes\n",
      "3\n",
      "Package the app into a container image so you can then run it as an isolated\n",
      "container\n",
      "4\n",
      "Run a container based on the image\n",
      "5\n",
      "Push the image to Docker Hub so that anyone anywhere can run it\n",
      "2.1.1\n",
      "Installing Docker and running a Hello World container\n",
      "First, you’ll need to install Docker on your Linux machine. If you don’t use Linux,\n",
      "you’ll need to start a Linux virtual machine (VM) and run Docker inside that VM. If\n",
      "you’re using a Mac or Windows and install Docker per instructions, Docker will set up\n",
      "a VM for you and run the Docker daemon inside that VM. The Docker client execut-\n",
      "able will be available on your host OS, and will communicate with the daemon inside\n",
      "the VM. \n",
      " To install Docker, follow the instructions at http:/\n",
      "/docs.docker.com/engine/\n",
      "installation/ for your specific operating system. After completing the installation, you\n",
      "can use the Docker client executable to run various Docker commands. For example,\n",
      "you could try pulling and running an existing image from Docker Hub, the public\n",
      "Docker registry, which contains ready-to-use container images for many well-known\n",
      "software packages. One of them is the busybox image, which you’ll use to run a simple\n",
      "echo \"Hello world\" command. \n",
      "RUNNING A HELLO WORLD CONTAINER\n",
      "If you’re unfamiliar with busybox, it’s a single executable that combines many of the\n",
      "standard UNIX command-line tools, such as echo, ls, gzip, and so on. Instead of the\n",
      "busybox image, you could also use any other full-fledged OS container image such as\n",
      "Fedora, Ubuntu, or other similar images, as long as it includes the echo executable.\n",
      " How do you run the busybox image? You don’t need to download or install any-\n",
      "thing. Use the docker run command and specify what image to download and run\n",
      "and (optionally) what command to execute, as shown in the following listing.\n",
      "$ docker run busybox echo \"Hello world\"\n",
      "Unable to find image 'busybox:latest' locally\n",
      "latest: Pulling from docker.io/busybox\n",
      "9a163e0b8d13: Pull complete \n",
      "fef924a0204a: Pull complete\n",
      "Digest: sha256:97473e34e311e6c1b3f61f2a721d038d1e5eef17d98d1353a513007cf46ca6bd\n",
      "Status: Downloaded newer image for docker.io/busybox:latest\n",
      "Hello world\n",
      "Listing 2.1\n",
      "Running a Hello world container with Docker\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 59, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "27\n",
      "Creating, running, and sharing a container image\n",
      "This doesn’t look that impressive, but when you consider that the whole “app” was\n",
      "downloaded and executed with a single command, without you having to install that\n",
      "app or anything else, you’ll agree it’s awesome. In your case, the app was a single execut-\n",
      "able (busybox), but it might as well have been an incredibly complex app with many\n",
      "dependencies. The whole process of setting up and running the app would have been\n",
      "exactly the same. What’s also important is that the app was executed inside a container,\n",
      "completely isolated from all the other processes running on your machine.\n",
      "UNDERSTANDING WHAT HAPPENS BEHIND THE SCENES\n",
      "Figure 2.1 shows exactly what happened when you performed the docker run com-\n",
      "mand. First, Docker checked to see if the busybox:latest image was already present\n",
      "on your local machine. It wasn’t, so Docker pulled it from the Docker Hub registry at\n",
      "http:/\n",
      "/docker.io. After the image was downloaded to your machine, Docker created a\n",
      "container from that image and ran the command inside it. The echo command\n",
      "printed the text to STDOUT and then the process terminated and the container\n",
      "stopped.\n",
      "RUNNING OTHER IMAGES\n",
      "Running other existing container images is much the same as how you ran the busybox\n",
      "image. In fact, it’s often even simpler, because you usually don’t need to specify what\n",
      "command to execute, the way you did in the example (echo \"Hello world\"). The\n",
      "command that should be executed is usually baked into the image itself, but you can\n",
      "override it if you want. After searching or browsing through the publicly available\n",
      "images on http:/\n",
      "/hub.docker.com or another public registry, you tell Docker to run\n",
      "the image like this:\n",
      "$ docker run <image>\n",
      "Figure 2.1\n",
      "Running echo “Hello world” in a container based on the busybox container image\n",
      "Local machine\n",
      "Docker Hub\n",
      "1. docker run busybox\n",
      "echo \"Hello world\"\n",
      "3. Docker pulls\n",
      "busybox image\n",
      "from registry (if not\n",
      "available locally)\n",
      "2. Docker checks if busybox\n",
      "image is already stored locally\n",
      "4. Docker runs\n",
      "echo \"Hello world\"\n",
      "in isolated container\n",
      "busybox\n",
      "Docker\n",
      "busybox\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 60, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "28\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "VERSIONING CONTAINER IMAGES\n",
      "All software packages get updated, so more than a single version of a package usually\n",
      "exists. Docker supports having multiple versions or variants of the same image under\n",
      "the same name. Each variant must have a unique tag. When referring to images with-\n",
      "out explicitly specifying the tag, Docker will assume you’re referring to the so-called\n",
      "latest tag. To run a different version of the image, you may specify the tag along with\n",
      "the image name like this:\n",
      "$ docker run <image>:<tag>\n",
      "2.1.2\n",
      "Creating a trivial Node.js app\n",
      "Now that you have a working Docker setup, you’re going to create an app. You’ll build\n",
      "a trivial Node.js web application and package it into a container image. The applica-\n",
      "tion will accept HTTP requests and respond with the hostname of the machine it’s\n",
      "running in. This way, you’ll see that an app running inside a container sees its own\n",
      "hostname and not that of the host machine, even though it’s running on the host like\n",
      "any other process. This will be useful later, when you deploy the app on Kubernetes\n",
      "and scale it out (scale it horizontally; that is, run multiple instances of the app). You’ll\n",
      "see your HTTP requests hitting different instances of the app.\n",
      " Your app will consist of a single file called app.js with the contents shown in the fol-\n",
      "lowing listing.\n",
      "const http = require('http');\n",
      "const os = require('os');\n",
      "console.log(\"Kubia server starting...\");\n",
      "var handler = function(request, response) {\n",
      "  console.log(\"Received request from \" + request.connection.remoteAddress);\n",
      "  response.writeHead(200);\n",
      "  response.end(\"You've hit \" + os.hostname() + \"\\n\");\n",
      "};\n",
      "var www = http.createServer(handler);\n",
      "www.listen(8080);\n",
      "It should be clear what this code does. It starts up an HTTP server on port 8080. The\n",
      "server responds with an HTTP response status code 200 OK and the text \"You’ve hit\n",
      "<hostname>\" to every request. The request handler also logs the client’s IP address to\n",
      "the standard output, which you’ll need later.\n",
      "NOTE\n",
      "The returned hostname is the server’s actual hostname, not the one\n",
      "the client sends in the HTTP request’s Host header.\n",
      "You could now download and install Node.js and test your app directly, but this isn’t\n",
      "necessary, because you’ll use Docker to package the app into a container image and\n",
      "Listing 2.2\n",
      "A simple Node.js app: app.js\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 61, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "29\n",
      "Creating, running, and sharing a container image\n",
      "enable it to be run anywhere without having to download or install anything (except\n",
      "Docker, which does need to be installed on the machine you want to run the image on).\n",
      "2.1.3\n",
      "Creating a Dockerfile for the image\n",
      "To package your app into an image, you first need to create a file called Dockerfile,\n",
      "which will contain a list of instructions that Docker will perform when building the\n",
      "image. The Dockerfile needs to be in the same directory as the app.js file and should\n",
      "contain the commands shown in the following listing.\n",
      "FROM node:7\n",
      "ADD app.js /app.js\n",
      "ENTRYPOINT [\"node\", \"app.js\"]\n",
      "The FROM line defines the container image you’ll use as a starting point (the base\n",
      "image you’re building on top of). In your case, you’re using the node container image,\n",
      "tag 7. In the second line, you’re adding your app.js file from your local directory into\n",
      "the root directory in the image, under the same name (app.js). Finally, in the third\n",
      "line, you’re defining what command should be executed when somebody runs the\n",
      "image. In your case, the command is node app.js.\n",
      "2.1.4\n",
      "Building the container image\n",
      "Now that you have your Dockerfile and the app.js file, you have everything you need\n",
      "to build your image. To build it, run the following Docker command:\n",
      "$ docker build -t kubia .\n",
      "Figure 2.2 shows what happens during the build process. You’re telling Docker to\n",
      "build an image called kubia based on the contents of the current directory (note the\n",
      "dot at the end of the build command). Docker will look for the Dockerfile in the direc-\n",
      "tory and build the image based on the instructions in the file.\n",
      "Listing 2.3\n",
      "A Dockerfile for building a container image for your app\n",
      "Choosing a base image\n",
      "You may wonder why we chose this specific image as your base. Because your app\n",
      "is a Node.js app, you need your image to contain the node binary executable to run\n",
      "the app. You could have used any image that contains that binary, or you could have\n",
      "even used a Linux distro base image such as fedora or ubuntu and installed\n",
      "Node.js into the container at image build time. But because the node image is made\n",
      "specifically for running Node.js apps, and includes everything you need to run your\n",
      "app, you’ll use that as the base image.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 62, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "30\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "UNDERSTANDING HOW AN IMAGE IS BUILT\n",
      "The build process isn’t performed by the Docker client. Instead, the contents of the\n",
      "whole directory are uploaded to the Docker daemon and the image is built there.\n",
      "The client and daemon don’t need to be on the same machine at all. If you’re using\n",
      "Docker on a non-Linux OS, the client is on your host OS, but the daemon runs\n",
      "inside a VM. Because all the files in the build directory are uploaded to the daemon,\n",
      "if it contains many large files and the daemon isn’t running locally, the upload may\n",
      "take longer. \n",
      "TIP\n",
      "Don’t include any unnecessary files in the build directory, because they’ll\n",
      "slow down the build process—especially when the Docker daemon is on a\n",
      "remote machine. \n",
      "During the build process, Docker will first pull the base image (node:7) from the pub-\n",
      "lic image repository (Docker Hub), unless the image has already been pulled and is\n",
      "stored on your machine. \n",
      "UNDERSTANDING IMAGE LAYERS\n",
      "An image isn’t a single, big, binary blob, but is composed of multiple layers, which you\n",
      "may have already noticed when running the busybox example (there were multiple\n",
      "Pull complete lines—one for each layer). Different images may share several layers,\n",
      "Figure 2.2\n",
      "Building a new container image from a Dockerfile\n",
      "Local machine\n",
      "Docker Hub\n",
      "1. docker build\n",
      "kubia .\n",
      "3. Docker pulls image\n",
      "node:7.0 if it isn’t\n",
      "stored locally yet\n",
      "4. Build new\n",
      "image\n",
      "2. Docker client uploads\n",
      "directory contents to daemon\n",
      "Dockerﬁle\n",
      "Docker client\n",
      "Docker daemon\n",
      "app.js\n",
      "node:7.0\n",
      "node:7.0\n",
      "kubia:latest\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 63, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "31\n",
      "Creating, running, and sharing a container image\n",
      "which makes storing and transferring images much more efficient. For example, if\n",
      "you create multiple images based on the same base image (such as node:7 in the exam-\n",
      "ple), all the layers comprising the base image will be stored only once. Also, when pull-\n",
      "ing an image, Docker will download each layer individually. Several layers may already\n",
      "be stored on your machine, so Docker will only download those that aren’t.\n",
      " You may think that each Dockerfile creates only a single new layer, but that’s not\n",
      "the case. When building an image, a new layer is created for each individual command\n",
      "in the Dockerfile. During the build of your image, after pulling all the layers of the base\n",
      "image, Docker will create a new layer on top of them and add the app.js file into it.\n",
      "Then it will create yet another layer that will specify the command that should be exe-\n",
      "cuted when the image is run. This last layer will then be tagged as kubia:latest. This is\n",
      "shown in figure 2.3, which also shows how a different image called other:latest would\n",
      "use the same layers of the Node.js image as your own image does.\n",
      "When the build process completes, you have a new image stored locally. You can see it\n",
      "by telling Docker to list all locally stored images, as shown in the following listing.\n",
      "$ docker images\n",
      "REPOSITORY   TAG      IMAGE ID           CREATED             VIRTUAL SIZE\n",
      "kubia        latest   d30ecc7419e7       1 minute ago        637.1 MB\n",
      "...\n",
      "COMPARING BUILDING IMAGES WITH A DOCKERFILE VS. MANUALLY\n",
      "Dockerfiles are the usual way of building container images with Docker, but you could\n",
      "also build the image manually by running a container from an existing image, execut-\n",
      "ing commands in the container, exiting the container, and committing the final state\n",
      "as a new image. This is exactly what happens when you build from a Dockerfile, but\n",
      "it’s performed automatically and is repeatable, which allows you to make changes to\n",
      "Listing 2.4\n",
      "Listing locally stored images\n",
      "Figure 2.3\n",
      "Container images are composed of layers that can be shared among different images.\n",
      "RUN curl ...\n",
      "CMD node\n",
      "ADD app.js/app.js\n",
      "...\n",
      "CMD node app.js\n",
      "kubia:latest image\n",
      "...\n",
      "...\n",
      "RUN apt-get ...\n",
      "other:latest image\n",
      "...\n",
      "node:0.12 image\n",
      "buildpack-deps:jessie image\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 64, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "32\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "the Dockerfile and rebuild the image any time, without having to manually retype all\n",
      "the commands again.\n",
      "2.1.5\n",
      "Running the container image\n",
      "You can now run your image with the following command:\n",
      "$ docker run --name kubia-container -p 8080:8080 -d kubia\n",
      "This tells Docker to run a new container called kubia-container from the kubia\n",
      "image. The container will be detached from the console (-d flag), which means it will\n",
      "run in the background. Port 8080 on the local machine will be mapped to port 8080\n",
      "inside the container (-p 8080:8080 option), so you can access the app through\n",
      "http:/\n",
      "/localhost:8080. \n",
      " If you’re not running the Docker daemon on your local machine (if you’re using a\n",
      "Mac or Windows, the daemon is running inside a VM), you’ll need to use the host-\n",
      "name or IP of the VM running the daemon instead of localhost. You can look it up\n",
      "through the DOCKER_HOST environment variable.\n",
      "ACCESSING YOUR APP\n",
      "Now try to access your application at http:/\n",
      "/localhost:8080 (be sure to replace local-\n",
      "host with the hostname or IP of the Docker host if necessary): \n",
      "$ curl localhost:8080\n",
      "You’ve hit 44d76963e8e1   \n",
      "That’s the response from your app. Your tiny application is now running inside a con-\n",
      "tainer, isolated from everything else. As you can see, it’s returning 44d76963e8e1 as its\n",
      "hostname, and not the actual hostname of your host machine. The hexadecimal num-\n",
      "ber is the ID of the Docker container. \n",
      "LISTING ALL RUNNING CONTAINERS\n",
      "Let’s list all running containers in the following listing, so you can examine the list\n",
      "(I’ve edited the output to make it more readable—imagine the last two lines as the\n",
      "continuation of the first two).\n",
      "$ docker ps\n",
      "CONTAINER ID  IMAGE         COMMAND               CREATED        ...\n",
      "44d76963e8e1  kubia:latest  \"/bin/sh -c 'node ap  6 minutes ago  ...\n",
      "...  STATUS              PORTS                    NAMES\n",
      "...  Up 6 minutes        0.0.0.0:8080->8080/tcp   kubia-container\n",
      "A single container is running. For each container, Docker prints out its ID and name,\n",
      "the image used to run the container, and the command that’s executing inside the\n",
      "container. \n",
      "Listing 2.5\n",
      "Listing running containers\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 65, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "33\n",
      "Creating, running, and sharing a container image\n",
      "GETTING ADDITIONAL INFORMATION ABOUT A CONTAINER\n",
      "The docker ps command only shows the most basic information about the containers.\n",
      "To see additional information, you can use docker inspect:\n",
      "$ docker inspect kubia-container\n",
      "Docker will print out a long JSON containing low-level information about the con-\n",
      "tainer. \n",
      "2.1.6\n",
      "Exploring the inside of a running container\n",
      "What if you want to see what the environment is like inside the container? Because\n",
      "multiple processes can run inside the same container, you can always run an addi-\n",
      "tional process in it to see what’s inside. You can even run a shell, provided that the\n",
      "shell’s binary executable is available in the image. \n",
      "RUNNING A SHELL INSIDE AN EXISTING CONTAINER\n",
      "The Node.js image on which you’ve based your image contains the bash shell, so you\n",
      "can run the shell inside the container like this:\n",
      "$ docker exec -it kubia-container bash\n",
      "This will run bash inside the existing kubia-container container. The bash process\n",
      "will have the same Linux namespaces as the main container process. This allows you\n",
      "to explore the container from within and see how Node.js and your app see the system\n",
      "when running inside the container. The -it option is shorthand for two options: \n",
      "\n",
      "-i, which makes sure STDIN is kept open. You need this for entering com-\n",
      "mands into the shell. \n",
      "\n",
      "-t, which allocates a pseudo terminal (TTY).\n",
      "You need both if you want the use the shell like you’re used to. (If you leave out the\n",
      "first one, you can’t type any commands, and if you leave out the second one, the com-\n",
      "mand prompt won’t be displayed and some commands will complain about the TERM\n",
      "variable not being set.)\n",
      "EXPLORING THE CONTAINER FROM WITHIN\n",
      "Let’s see how to use the shell in the following listing to see the processes running in\n",
      "the container.\n",
      "root@44d76963e8e1:/# ps aux\n",
      "USER  PID %CPU %MEM    VSZ   RSS TTY STAT START TIME COMMAND\n",
      "root    1  0.0  0.1 676380 16504 ?   Sl   12:31 0:00 node app.js\n",
      "root   10  0.0  0.0  20216  1924 ?   Ss   12:31 0:00 bash\n",
      "root   19  0.0  0.0  17492  1136 ?   R+   12:38 0:00 ps aux\n",
      "You see only three processes. You don’t see any other processes from the host OS. \n",
      "Listing 2.6\n",
      "Listing processes from inside a container\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 66, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "34\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "UNDERSTANDING THAT PROCESSES IN A CONTAINER RUN IN THE HOST OPERATING SYSTEM\n",
      "If you now open another terminal and list the processes on the host OS itself, you will,\n",
      "among all other host processes, also see the processes running in the container, as\n",
      "shown in listing 2.7. \n",
      "NOTE\n",
      "If you’re using a Mac or Windows, you’ll need to log into the VM where\n",
      "the Docker daemon is running to see these processes.\n",
      "$ ps aux | grep app.js\n",
      "USER  PID %CPU %MEM    VSZ   RSS TTY STAT START TIME COMMAND\n",
      "root  382  0.0  0.1 676380 16504 ?   Sl   12:31 0:00 node app.js\n",
      "This proves that processes running in the container are running in the host OS. If you\n",
      "have a keen eye, you may have noticed that the processes have different IDs inside the\n",
      "container vs. on the host. The container is using its own PID Linux namespace and\n",
      "has a completely isolated process tree, with its own sequence of numbers. \n",
      "THE CONTAINER’S FILESYSTEM IS ALSO ISOLATED\n",
      "Like having an isolated process tree, each container also has an isolated filesystem.\n",
      "Listing the contents of the root directory inside the container will only show the files\n",
      "in the container and will include all the files that are in the image plus any files that\n",
      "are created while the container is running (log files and similar), as shown in the fol-\n",
      "lowing listing.\n",
      "root@44d76963e8e1:/# ls /\n",
      "app.js  boot  etc   lib    media  opt   root  sbin  sys  usr\n",
      "bin     dev   home  lib64  mnt    proc  run   srv   tmp  var\n",
      "It contains the app.js file and other system directories that are part of the node:7 base\n",
      "image you’re using. To exit the container, you exit the shell by running the exit com-\n",
      "mand and you’ll be returned to your host machine (like logging out of an ssh session,\n",
      "for example).\n",
      "TIP\n",
      "Entering a running container like this is useful when debugging an app\n",
      "running in a container. When something’s wrong, the first thing you’ll want\n",
      "to explore is the actual state of the system your application sees. Keep in mind\n",
      "that an application will not only see its own unique filesystem, but also pro-\n",
      "cesses, users, hostname, and network interfaces.\n",
      "2.1.7\n",
      "Stopping and removing a container\n",
      "To stop your app, you tell Docker to stop the kubia-container container:\n",
      "$ docker stop kubia-container\n",
      "Listing 2.7\n",
      "A container’s processes run in the host OS\n",
      "Listing 2.8\n",
      "A container has its own complete filesystem\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 67, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "35\n",
      "Creating, running, and sharing a container image\n",
      "This will stop the main process running in the container and consequently stop the\n",
      "container, because no other processes are running inside the container. The con-\n",
      "tainer itself still exists and you can see it with docker ps -a. The -a option prints out\n",
      "all the containers, those running and those that have been stopped. To truly remove a\n",
      "container, you need to remove it with the docker rm command:\n",
      "$ docker rm kubia-container\n",
      "This deletes the container. All its contents are removed and it can’t be started again.\n",
      "2.1.8\n",
      "Pushing the image to an image registry\n",
      "The image you’ve built has so far only been available on your local machine. To allow\n",
      "you to run it on any other machine, you need to push the image to an external image\n",
      "registry. For the sake of simplicity, you won’t set up a private image registry and will\n",
      "instead push the image to Docker Hub (http:/\n",
      "/hub.docker.com), which is one of the\n",
      "publicly available registries. Other widely used such registries are Quay.io and the\n",
      "Google Container Registry.\n",
      " Before you do that, you need to re-tag your image according to Docker Hub’s\n",
      "rules. Docker Hub will allow you to push an image if the image’s repository name\n",
      "starts with your Docker Hub ID. You create your Docker Hub ID by registering at\n",
      "http:/\n",
      "/hub.docker.com. I’ll use my own ID (luksa) in the following examples. Please\n",
      "change every occurrence with your own ID.\n",
      "TAGGING AN IMAGE UNDER AN ADDITIONAL TAG\n",
      "Once you know your ID, you’re ready to rename your image, currently tagged as\n",
      "kubia, to luksa/kubia (replace luksa with your own Docker Hub ID):\n",
      "$ docker tag kubia luksa/kubia\n",
      "This doesn’t rename the tag; it creates an additional tag for the same image. You can\n",
      "confirm this by listing the images stored on your system with the docker images com-\n",
      "mand, as shown in the following listing.\n",
      "$ docker images | head\n",
      "REPOSITORY        TAG      IMAGE ID        CREATED             VIRTUAL SIZE\n",
      "luksa/kubia       latest   d30ecc7419e7    About an hour ago   654.5 MB\n",
      "kubia             latest   d30ecc7419e7    About an hour ago   654.5 MB\n",
      "docker.io/node    7.0      04c0ca2a8dad    2 days ago          654.5 MB\n",
      "...\n",
      "As you can see, both kubia and luksa/kubia point to the same image ID, so they’re in\n",
      "fact one single image with two tags. \n",
      "Listing 2.9\n",
      "A container image can have multiple tags\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 68, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "36\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "PUSHING THE IMAGE TO DOCKER HUB\n",
      "Before you can push the image to Docker Hub, you need to log in under your user ID\n",
      "with the docker login command. Once you’re logged in, you can finally push the\n",
      "yourid/kubia image to Docker Hub like this:\n",
      "$ docker push luksa/kubia\n",
      "RUNNING THE IMAGE ON A DIFFERENT MACHINE\n",
      "After the push to Docker Hub is complete, the image will be available to everyone.\n",
      "You can now run the image on any machine running Docker by executing the follow-\n",
      "ing command:\n",
      "$ docker run -p 8080:8080 -d luksa/kubia\n",
      "It doesn’t get much simpler than that. And the best thing about this is that your appli-\n",
      "cation will have the exact same environment every time and everywhere it’s run. If it\n",
      "ran fine on your machine, it should run as well on every other Linux machine. No\n",
      "need to worry about whether the host machine has Node.js installed or not. In fact,\n",
      "even if it does, your app won’t use it, because it will use the one installed inside the\n",
      "image.\n",
      "2.2\n",
      "Setting up a Kubernetes cluster\n",
      "Now that you have your app packaged inside a container image and made available\n",
      "through Docker Hub, you can deploy it in a Kubernetes cluster instead of running it\n",
      "in Docker directly. But first, you need to set up the cluster itself. \n",
      " Setting up a full-fledged, multi-node Kubernetes cluster isn’t a simple task, espe-\n",
      "cially if you’re not well-versed in Linux and networking administration. A proper\n",
      "Kubernetes install spans multiple physical or virtual machines and requires the net-\n",
      "working to be set up properly, so that all the containers running inside the Kuberne-\n",
      "tes cluster can connect to each other through the same flat networking space. \n",
      " A long list of methods exists for installing a Kubernetes cluster. These methods are\n",
      "described in detail in the documentation at http:/\n",
      "/kubernetes.io. We’re not going to\n",
      "list all of them here, because the list keeps evolving, but Kubernetes can be run on\n",
      "your local development machine, your own organization’s cluster of machines, on\n",
      "cloud providers providing virtual machines (Google Compute Engine, Amazon EC2,\n",
      "Microsoft Azure, and so on), or by using a managed Kubernetes cluster such as Goo-\n",
      "gle Kubernetes Engine (previously known as Google Container Engine). \n",
      " In this chapter, we’ll cover two simple options for getting your hands on a running\n",
      "Kubernetes cluster. You’ll see how to run a single-node Kubernetes cluster on your\n",
      "local machine and how to get access to a hosted cluster running on Google Kuberne-\n",
      "tes Engine (GKE). \n",
      " A third option, which covers installing a cluster with the kubeadm tool, is explained\n",
      "in appendix B. The instructions there show you how to set up a three-node Kubernetes\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 69, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "37\n",
      "Setting up a Kubernetes cluster\n",
      "cluster using virtual machines, but I suggest you try it only after reading the first 11\n",
      "chapters of the book.\n",
      " Another option is to install Kubernetes on Amazon’s AWS (Amazon Web Services).\n",
      "For this, you can look at the kops tool, which is built on top of kubeadm mentioned in\n",
      "the previous paragraph, and is available at http:/\n",
      "/github.com/kubernetes/kops. It\n",
      "helps you deploy production-grade, highly available Kubernetes clusters on AWS and\n",
      "will eventually support other platforms as well (Google Kubernetes Engine, VMware,\n",
      "vSphere, and so on).\n",
      "2.2.1\n",
      "Running a local single-node Kubernetes cluster with Minikube\n",
      "The simplest and quickest path to a fully functioning Kubernetes cluster is by using\n",
      "Minikube. Minikube is a tool that sets up a single-node cluster that’s great for both\n",
      "testing Kubernetes and developing apps locally. \n",
      " Although we can’t show certain Kubernetes features related to managing apps on\n",
      "multiple nodes, the single-node cluster should be enough for exploring most topics\n",
      "discussed in this book. \n",
      "INSTALLING MINIKUBE\n",
      "Minikube is a single binary that needs to be downloaded and put onto your path. It’s\n",
      "available for OSX, Linux, and Windows. To install it, the best place to start is to go to\n",
      "the Minikube repository on GitHub (http:/\n",
      "/github.com/kubernetes/minikube) and\n",
      "follow the instructions there.\n",
      " For example, on OSX and Linux, Minikube can be downloaded and set up with a\n",
      "single command. For OSX, this is what the command looks like:\n",
      "$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/\n",
      "➥ v0.23.0/minikube-darwin-amd64 && chmod +x minikube && sudo mv minikube \n",
      "➥ /usr/local/bin/\n",
      "On Linux, you download a different release (replace “darwin” with “linux” in the\n",
      "URL). On Windows, you can download the file manually, rename it to minikube.exe,\n",
      "and put it onto your path. Minikube runs Kubernetes inside a VM run through either\n",
      "VirtualBox or KVM, so you also need to install one of them before you can start the\n",
      "Minikube cluster.\n",
      "STARTING A KUBERNETES CLUSTER WITH MINIKUBE\n",
      "Once you have Minikube installed locally, you can immediately start up the Kuberne-\n",
      "tes cluster with the command in the following listing.\n",
      "$ minikube start\n",
      "Starting local Kubernetes cluster...\n",
      "Starting VM...\n",
      "SSH-ing files into VM...\n",
      "...\n",
      "Kubectl is now configured to use the cluster. \n",
      "Listing 2.10\n",
      "Starting a Minikube virtual machine\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 70, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "38\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "Starting the cluster takes more than a minute, so don’t interrupt the command before\n",
      "it completes. \n",
      "INSTALLING THE KUBERNETES CLIENT (KUBECTL)\n",
      "To interact with Kubernetes, you also need the kubectl CLI client. Again, all you need\n",
      "to do is download it and put it on your path. The latest stable release for OSX, for\n",
      "example, can be downloaded and installed with the following command:\n",
      "$ curl -LO https://storage.googleapis.com/kubernetes-release/release\n",
      "➥  /$(curl -s https://storage.googleapis.com/kubernetes-release/release\n",
      "➥  /stable.txt)/bin/darwin/amd64/kubectl \n",
      "➥  && chmod +x kubectl \n",
      "➥  && sudo mv kubectl /usr/local/bin/\n",
      "To download kubectl for Linux or Windows, replace darwin in the URL with either\n",
      "linux or windows.\n",
      "NOTE\n",
      "If you’ll be using multiple Kubernetes clusters (for example, both\n",
      "Minikube and GKE), refer to appendix A for information on how to set up\n",
      "and switch between different kubectl contexts.\n",
      "CHECKING TO SEE THE CLUSTER IS UP AND KUBECTL CAN TALK TO IT\n",
      "To verify your cluster is working, you can use the kubectl cluster-info command\n",
      "shown in the following listing.\n",
      "$ kubectl cluster-info\n",
      "Kubernetes master is running at https://192.168.99.100:8443\n",
      "KubeDNS is running at https://192.168.99.100:8443/api/v1/proxy/...\n",
      "kubernetes-dashboard is running at https://192.168.99.100:8443/api/v1/...\n",
      "This shows the cluster is up. It shows the URLs of the various Kubernetes components,\n",
      "including the API server and the web console. \n",
      "TIP\n",
      "You can run minikube ssh to log into the Minikube VM and explore it\n",
      "from the inside. For example, you may want to see what processes are run-\n",
      "ning on the node.\n",
      "2.2.2\n",
      "Using a hosted Kubernetes cluster with Google Kubernetes Engine\n",
      "If you want to explore a full-fledged multi-node Kubernetes cluster instead, you can\n",
      "use a managed Google Kubernetes Engine (GKE) cluster. This way, you don’t need to\n",
      "manually set up all the cluster nodes and networking, which is usually too much for\n",
      "someone making their first steps with Kubernetes. Using a managed solution such as\n",
      "GKE makes sure you don’t end up with a misconfigured, non-working, or partially work-\n",
      "ing cluster.\n",
      "Listing 2.11\n",
      "Displaying cluster information\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 71, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "39\n",
      "Setting up a Kubernetes cluster\n",
      "SETTING UP A GOOGLE CLOUD PROJECT AND DOWNLOADING THE NECESSARY CLIENT BINARIES\n",
      "Before you can set up a new Kubernetes cluster, you need to set up your GKE environ-\n",
      "ment. Because the process may change, I’m not listing the exact instructions here. To\n",
      "get started, please follow the instructions at https:/\n",
      "/cloud.google.com/container-\n",
      "engine/docs/before-you-begin.\n",
      " Roughly, the whole procedure includes\n",
      "1\n",
      "Signing up for a Google account, in the unlikely case you don’t have one\n",
      "already.\n",
      "2\n",
      "Creating a project in the Google Cloud Platform Console. \n",
      "3\n",
      "Enabling billing. This does require your credit card info, but Google provides a\n",
      "12-month free trial. And they’re nice enough to not start charging automati-\n",
      "cally after the free trial is over.)\n",
      "4\n",
      "Enabling the Kubernetes Engine API.\n",
      "5\n",
      "Downloading and installing Google Cloud SDK. (This includes the gcloud\n",
      "command-line tool, which you’ll need to create a Kubernetes cluster.)\n",
      "6\n",
      "Installing the kubectl command-line tool with gcloud components install\n",
      "kubectl.\n",
      "NOTE\n",
      "Certain operations (the one in step 2, for example) may take a few\n",
      "minutes to complete, so relax and grab a coffee in the meantime.\n",
      "CREATING A KUBERNETES CLUSTER WITH THREE NODES\n",
      "After completing the installation, you can create a Kubernetes cluster with three\n",
      "worker nodes using the command shown in the following listing.\n",
      "$ gcloud container clusters create kubia --num-nodes 3 \n",
      "➥ --machine-type f1-micro\n",
      "Creating cluster kubia...done.\n",
      "Created [https://container.googleapis.com/v1/projects/kubia1-\n",
      "1227/zones/europe-west1-d/clusters/kubia].\n",
      "kubeconfig entry generated for kubia.\n",
      "NAME   ZONE   MST_VER MASTER_IP     TYPE     NODE_VER NUM_NODES STATUS\n",
      "kubia  eu-w1d 1.5.3   104.155.92.30 f1-micro 1.5.3    3         RUNNING\n",
      "You should now have a running Kubernetes cluster with three worker nodes as shown\n",
      "in figure 2.4. You’re using three nodes to help better demonstrate features that apply\n",
      "to multiple nodes. You can use a smaller number of nodes, if you want. \n",
      "GETTING AN OVERVIEW OF YOUR CLUSTER\n",
      "To give you a basic idea of what your cluster looks like and how to interact with it, see\n",
      "figure 2.4. Each node runs Docker, the Kubelet and the kube-proxy. You’ll interact\n",
      "with the cluster through the kubectl command line client, which issues REST requests\n",
      "to the Kubernetes API server running on the master node.\n",
      "Listing 2.12\n",
      "Creating a three-node cluster in GKE\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 72, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "40\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "CHECKING IF THE CLUSTER IS UP BY LISTING CLUSTER NODES\n",
      "You’ll use the kubectl command now to list all the nodes in your cluster, as shown in\n",
      "the following listing.\n",
      "$ kubectl get nodes\n",
      "NAME                      STATUS  AGE  VERSION\n",
      "gke-kubia-85f6-node-0rrx  Ready   1m    v1.5.3\n",
      "gke-kubia-85f6-node-heo1  Ready   1m    v1.5.3\n",
      "gke-kubia-85f6-node-vs9f  Ready   1m    v1.5.3\n",
      "The kubectl get command can list all kinds of Kubernetes objects. You’ll use it con-\n",
      "stantly, but it usually shows only the most basic information for the listed objects. \n",
      "TIP\n",
      "You can log into one of the nodes with gcloud compute ssh <node-name>\n",
      "to explore what’s running on the node.\n",
      "Listing 2.13\n",
      "Listing cluster nodes with kubectl\n",
      "Figure 2.4\n",
      "How you’re interacting with your three-node Kubernetes cluster \n",
      "Local dev machine\n",
      "REST call\n",
      "kubectl\n",
      "REST API server\n",
      "Master node\n",
      "(IP 104.155.92.30)\n",
      "Docker\n",
      "Kubelet\n",
      "kube-proxy\n",
      "gke-kubia-85f6-node-heo1\n",
      "Docker\n",
      "Kubelet\n",
      "kube-proxy\n",
      "gke-kubia-85f6-node-vs9f\n",
      "Docker\n",
      "Worker nodes\n",
      "Kubernetes cluster\n",
      "Kubelet\n",
      "kube-proxy\n",
      "gke-kubia-85f6-node-0rrx\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 73, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "41\n",
      "Setting up a Kubernetes cluster\n",
      "RETRIEVING ADDITIONAL DETAILS OF AN OBJECT\n",
      "To see more detailed information about an object, you can use the kubectl describe\n",
      "command, which shows much more: \n",
      "$ kubectl describe node gke-kubia-85f6-node-0rrx\n",
      "I’m omitting the actual output of the describe command, because it’s fairly wide and\n",
      "would be completely unreadable here in the book. The output shows the node’s sta-\n",
      "tus, its CPU and memory data, system information, containers running on the node,\n",
      "and much more.\n",
      " In the previous kubectl describe example, you specified the name of the node\n",
      "explicitly, but you could also have performed a simple kubectl describe node without\n",
      "typing the node’s name and it would print out a detailed description of all the nodes.\n",
      "TIP\n",
      "Running the describe and get commands without specifying the name\n",
      "of the object comes in handy when only one object of a given type exists, so\n",
      "you don’t waste time typing or copy/pasting the object’s name.\n",
      "While we’re talking about reducing keystrokes, let me give you additional advice on\n",
      "how to make working with kubectl much easier, before we move on to running your\n",
      "first app in Kubernetes.\n",
      "2.2.3\n",
      "Setting up an alias and command-line completion for kubectl \n",
      "You’ll use kubectl often. You’ll soon realize that having to type the full command\n",
      "every time is a real pain. Before you continue, take a minute to make your life easier\n",
      "by setting up an alias and tab completion for kubectl.\n",
      "CREATING AN ALIAS\n",
      "Throughout the book, I’ll always be using the full name of the kubectl executable,\n",
      "but you may want to add a short alias such as k, so you won’t have to type kubectl\n",
      "every time. If you haven’t used aliases yet, here’s how you define one. Add the follow-\n",
      "ing line to your ~/.bashrc or equivalent file:\n",
      "alias k=kubectl\n",
      "NOTE\n",
      "You may already have the k executable if you used gcloud to set up the\n",
      "cluster.\n",
      "CONFIGURING TAB COMPLETION FOR KUBECTL\n",
      "Even with a short alias such as k, you’ll still need to type way more than you’d like. Luck-\n",
      "ily, the kubectl command can also output shell completion code for both the bash and\n",
      "zsh shell. It doesn’t enable tab completion of only command names, but also of the\n",
      "actual object names. For example, instead of having to write the whole node name in\n",
      "the previous example, all you’d need to type is\n",
      "$ kubectl desc<TAB> no<TAB> gke-ku<TAB>\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 74, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "42\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "To enable tab completion in bash, you’ll first need to install a package called bash-\n",
      "completion and then run the following command (you’ll probably also want to add it\n",
      "to ~/.bashrc or equivalent):\n",
      "$ source <(kubectl completion bash)\n",
      "But there’s one caveat. When you run the preceding command, tab completion will\n",
      "only work when you use the full kubectl name (it won’t work when you use the k\n",
      "alias). To fix this, you need to transform the output of the kubectl completion com-\n",
      "mand a bit:\n",
      "$ source <(kubectl completion bash | sed s/kubectl/k/g)\n",
      "NOTE\n",
      "Unfortunately, as I’m writing this, shell completion doesn’t work for\n",
      "aliases on MacOS. You’ll have to use the full kubectl command name if you\n",
      "want completion to work.\n",
      "Now you’re all set to start interacting with your cluster without having to type too\n",
      "much. You can finally run your first app on Kubernetes.\n",
      "2.3\n",
      "Running your first app on Kubernetes\n",
      "Because this may be your first time, you’ll use the simplest possible way of running an\n",
      "app on Kubernetes. Usually, you’d prepare a JSON or YAML manifest, containing a\n",
      "description of all the components you want to deploy, but because we haven’t talked\n",
      "about the types of components you can create in Kubernetes yet, you’ll use a simple\n",
      "one-line command to get something running.\n",
      "2.3.1\n",
      "Deploying your Node.js app\n",
      "The simplest way to deploy your app is to use the kubectl run command, which will\n",
      "create all the necessary components without having to deal with JSON or YAML. This\n",
      "way, we don’t need to dive into the structure of each object yet. Try to run the image\n",
      "you created and pushed to Docker Hub earlier. Here’s how to run it in Kubernetes:\n",
      "$ kubectl run kubia --image=luksa/kubia --port=8080 --generator=run/v1\n",
      "replicationcontroller \"kubia\" created\n",
      "The --image=luksa/kubia part obviously specifies the container image you want to\n",
      "run, and the --port=8080 option tells Kubernetes that your app is listening on port\n",
      "8080. The last flag (--generator) does require an explanation, though. Usually, you\n",
      "won’t use it, but you’re using it here so Kubernetes creates a ReplicationController\n",
      "instead of a Deployment. You’ll learn what ReplicationControllers are later in the chap-\n",
      "ter, but we won’t talk about Deployments until chapter 9. That’s why I don’t want\n",
      "kubectl to create a Deployment yet.\n",
      " As the previous command’s output shows, a ReplicationController called kubia\n",
      "has been created. As already mentioned, we’ll see what that is later in the chapter. For\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 75, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "43\n",
      "Running your first app on Kubernetes\n",
      "now, let’s start from the bottom and focus on the container you created (you can\n",
      "assume a container has been created, because you specified a container image in the\n",
      "run command).\n",
      "INTRODUCING PODS\n",
      "You may be wondering if you can see your container in a list showing all the running\n",
      "containers. Maybe something such as kubectl get containers? Well, that’s not exactly\n",
      "how Kubernetes works. It doesn’t deal with individual containers directly. Instead, it\n",
      "uses the concept of multiple co-located containers. This group of containers is called\n",
      "a Pod. \n",
      " A pod is a group of one or more tightly related containers that will always run\n",
      "together on the same worker node and in the same Linux namespace(s). Each pod\n",
      "is like a separate logical machine with its own IP, hostname, processes, and so on,\n",
      "running a single application. The application can be a single process, running in a\n",
      "single container, or it can be a main application process and additional supporting\n",
      "processes, each running in its own container. All the containers in a pod will appear\n",
      "to be running on the same logical machine, whereas containers in other pods, even\n",
      "if they’re running on the same worker node, will appear to be running on a differ-\n",
      "ent one. \n",
      " To better understand the relationship between containers, pods, and nodes, exam-\n",
      "ine figure 2.5. As you can see, each pod has its own IP and contains one or more con-\n",
      "tainers, each running an application process. Pods are spread out across different\n",
      "worker nodes.\n",
      "LISTING PODS\n",
      "Because you can’t list individual containers, since they’re not standalone Kubernetes\n",
      "objects, can you list pods instead? Yes, you can. Let’s see how to tell kubectl to list\n",
      "pods in the following listing.\n",
      "Figure 2.5\n",
      "The relationship between containers, pods, and physical worker nodes\n",
      "Worker node 1\n",
      "Pod 2\n",
      "IP: 10.1.0.2\n",
      "Container 1\n",
      "Container 2\n",
      "Pod 3\n",
      "IP: 10.1.0.3\n",
      "Container 1\n",
      "Pod 1\n",
      "IP: 10.1.0.1\n",
      "Container\n",
      "Worker node 2\n",
      "Pod 5\n",
      "IP: 10.1.1.2\n",
      "Container 1\n",
      "Pod 6\n",
      "IP: 10.1.1.3\n",
      "Container 1\n",
      "Pod 4\n",
      "IP: 10.1.1.1\n",
      "Container\n",
      "Container 2\n",
      "Container 2\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 76, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "44\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "$ kubectl get pods\n",
      "NAME          READY     STATUS    RESTARTS   AGE\n",
      "kubia-4jfyf   0/1       Pending   0          1m\n",
      "This is your pod. Its status is still Pending and the pod’s single container is shown as\n",
      "not ready yet (this is what the 0/1 in the READY column means). The reason why the\n",
      "pod isn’t running yet is because the worker node the pod has been assigned to is\n",
      "downloading the container image before it can run it. When the download is finished,\n",
      "the pod’s container will be created and then the pod will transition to the Running\n",
      "state, as shown in the following listing.\n",
      "$ kubectl get pods\n",
      "NAME          READY     STATUS    RESTARTS   AGE\n",
      "kubia-4jfyf   1/1       Running   0          5m\n",
      "To see more information about the pod, you can also use the kubectl describe pod\n",
      "command, like you did earlier for one of the worker nodes. If the pod stays stuck in\n",
      "the Pending status, it might be that Kubernetes can’t pull the image from the registry.\n",
      "If you’re using your own image, make sure it’s marked as public on Docker Hub. To\n",
      "make sure the image can be pulled successfully, try pulling the image manually with\n",
      "the docker pull command on another machine. \n",
      "UNDERSTANDING WHAT HAPPENED BEHIND THE SCENES\n",
      "To help you visualize what transpired, look at figure 2.6. It shows both steps you had to\n",
      "perform to get a container image running inside Kubernetes. First, you built the\n",
      "image and pushed it to Docker Hub. This was necessary because building the image\n",
      "on your local machine only makes it available on your local machine, but you needed\n",
      "to make it accessible to the Docker daemons running on your worker nodes.\n",
      " When you ran the kubectl command, it created a new ReplicationController\n",
      "object in the cluster by sending a REST HTTP request to the Kubernetes API server.\n",
      "The ReplicationController then created a new pod, which was then scheduled to one\n",
      "of the worker nodes by the Scheduler. The Kubelet on that node saw that the pod was\n",
      "scheduled to it and instructed Docker to pull the specified image from the registry\n",
      "because the image wasn’t available locally. After downloading the image, Docker cre-\n",
      "ated and ran the container. \n",
      " The other two nodes are displayed to show context. They didn’t play any role in\n",
      "the process, because the pod wasn’t scheduled to them.\n",
      "DEFINITION\n",
      "The term scheduling means assigning the pod to a node. The\n",
      "pod is run immediately, not at a time in the future as the term might lead you\n",
      "to believe.\n",
      "Listing 2.14\n",
      "Listing pods\n",
      "Listing 2.15\n",
      "Listing pods again to see if the pod’s status has changed\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 77, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "45\n",
      "Running your first app on Kubernetes\n",
      "2.3.2\n",
      "Accessing your web application\n",
      "With your pod running, how do you access it? We mentioned that each pod gets its\n",
      "own IP address, but this address is internal to the cluster and isn’t accessible from\n",
      "outside of it. To make the pod accessible from the outside, you’ll expose it through a\n",
      "Service object. You’ll create a special service of type LoadBalancer, because if you cre-\n",
      "ate a regular service (a ClusterIP service), like the pod, it would also only be accessi-\n",
      "ble from inside the cluster. By creating a LoadBalancer-type service, an external load\n",
      "balancer will be created and you can connect to the pod through the load balancer’s\n",
      "public IP. \n",
      "CREATING A SERVICE OBJECT\n",
      "To create the service, you’ll tell Kubernetes to expose the ReplicationController you\n",
      "created earlier:\n",
      "$ kubectl expose rc kubia --type=LoadBalancer --name kubia-http\n",
      "service \"kubia-http\" exposed\n",
      "Figure 2.6\n",
      "Running the luksa/kubia container image in Kubernetes\n",
      "Local dev\n",
      "machine\n",
      "kubectl\n",
      "REST API server\n",
      "Scheduler\n",
      "Master node(s)\n",
      "Docker\n",
      "Kubelet\n",
      "gke-kubia-85f6-node-0rrx\n",
      "Docker\n",
      "Kubelet\n",
      "gke-kubia-85f6-node-heo1\n",
      "Docker\n",
      "Kubelet\n",
      "gke-kubia-85f6-node-vs9f\n",
      "Docker Hub\n",
      "3. kubectl run kubia\n",
      "--image=luksa/kubia\n",
      "--port=8080\n",
      "4.\n",
      "issues\n",
      "kubectl\n",
      "REST call\n",
      "5. Pod created\n",
      "and scheduled\n",
      "to a worker node\n",
      "7. Kubelet\n",
      "instructs\n",
      "Docker\n",
      "to run the\n",
      "image\n",
      "8. Docker pulls\n",
      "and runs\n",
      "luksa/kubia\n",
      "6. Kubelet\n",
      "is notiﬁed\n",
      "1. docker push\n",
      "luksa/kubia\n",
      "2. Image\n",
      "luksa/kubia\n",
      "is pushed to\n",
      "Docker Hub\n",
      "Docker\n",
      "pod kubia-4jfyf\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 78, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "46\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "NOTE\n",
      "We’re using the abbreviation rc instead of replicationcontroller.\n",
      "Most resource types have an abbreviation like this so you don’t have to type\n",
      "the full name (for example, po for pods, svc for services, and so on).\n",
      "LISTING SERVICES\n",
      "The expose command’s output mentions a service called kubia-http. Services are\n",
      "objects like Pods and Nodes, so you can see the newly created Service object by run-\n",
      "ning the kubectl get services command, as shown in the following listing.\n",
      "$ kubectl get services\n",
      "NAME         CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\n",
      "kubernetes   10.3.240.1     <none>        443/TCP         34m\n",
      "kubia-http   10.3.246.185   <pending>     8080:31348/TCP  4s\n",
      "The list shows two services. Ignore the kubernetes service for now and take a close\n",
      "look at the kubia-http service you created. It doesn’t have an external IP address yet,\n",
      "because it takes time for the load balancer to be created by the cloud infrastructure\n",
      "Kubernetes is running on. Once the load balancer is up, the external IP address of the\n",
      "service should be displayed. Let’s wait a while and list the services again, as shown in\n",
      "the following listing.\n",
      "$ kubectl get svc\n",
      "NAME         CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\n",
      "kubernetes   10.3.240.1     <none>        443/TCP         35m\n",
      "kubia-http   10.3.246.185   104.155.74.57 8080:31348/TCP  1m\n",
      "Aha, there’s the external IP. Your application is now accessible at http:/\n",
      "/104.155.74\n",
      ".57:8080 from anywhere in the world. \n",
      "NOTE\n",
      "Minikube doesn’t support LoadBalancer services, so the service will\n",
      "never get an external IP. But you can access the service anyway through its\n",
      "external port. How to do that is described in the next section’s tip.\n",
      "ACCESSING YOUR SERVICE THROUGH ITS EXTERNAL IP\n",
      "You can now send requests to your pod through the service’s external IP and port:\n",
      "$ curl 104.155.74.57:8080\n",
      "You’ve hit kubia-4jfyf \n",
      "Woohoo! Your app is now running somewhere in your three-node Kubernetes cluster\n",
      "(or a single-node cluster if you’re using Minikube). If you don’t count the steps\n",
      "required to set up the whole cluster, all it took was two simple commands to get your\n",
      "app running and to make it accessible to users across the world.\n",
      "Listing 2.16\n",
      "Listing Services\n",
      "Listing 2.17\n",
      "Listing services again to see if an external IP has been assigned\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 79, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "47\n",
      "Running your first app on Kubernetes\n",
      "TIP\n",
      "When using Minikube, you can get the IP and port through which you\n",
      "can access the service by running minikube service kubia-http.\n",
      "If you look closely, you’ll see that the app is reporting the name of the pod as its host-\n",
      "name. As already mentioned, each pod behaves like a separate independent machine\n",
      "with its own IP address and hostname. Even though the application is running in\n",
      "the worker node’s operating system, to the app it appears as though it’s running on\n",
      "a separate machine dedicated to the app itself—no other processes are running\n",
      "alongside it.\n",
      "2.3.3\n",
      "The logical parts of your system\n",
      "Until now, I’ve mostly explained the actual physical components of your system. You\n",
      "have three worker nodes, which are VMs running Docker and the Kubelet, and you\n",
      "have a master node that controls the whole system. Honestly, we don’t know if a single\n",
      "master node is hosting all the individual components of the Kubernetes Control Plane\n",
      "or if they’re split across multiple nodes. It doesn’t really matter, because you’re only\n",
      "interacting with the API server, which is accessible at a single endpoint.\n",
      " Besides this physical view of the system, there’s also a separate, logical view of it.\n",
      "I’ve already mentioned Pods, ReplicationControllers, and Services. All of them will be\n",
      "explained in the next few chapters, but let’s quickly look at how they fit together and\n",
      "what roles they play in your little setup.\n",
      "UNDERSTANDING HOW THE REPLICATIONCONTROLLER, THE POD, AND THE SERVICE FIT TOGETHER\n",
      "As I’ve already explained, you’re not creating and working with containers directly.\n",
      "Instead, the basic building block in Kubernetes is the pod. But, you didn’t really cre-\n",
      "ate any pods either, at least not directly. By running the kubectl run command you\n",
      "created a ReplicationController, and this ReplicationController is what created the\n",
      "actual Pod object. To make that pod accessible from outside the cluster, you told\n",
      "Kubernetes to expose all the pods managed by that ReplicationController as a single\n",
      "Service. A rough picture of all three elements is presented in figure 2.7.\n",
      "Figure 2.7\n",
      "Your system consists of a ReplicationController, a Pod, and a Service.\n",
      "Pod: kubia-4jfyf\n",
      "IP: 10.1.0.1\n",
      "Container\n",
      "Port\n",
      "8080\n",
      "Service: kubia-http\n",
      "Internal IP: 10.3.246.185\n",
      "External IP: 104.155.74.57\n",
      "ReplicationController: kubia\n",
      "Replicas: 1\n",
      "Port\n",
      "8080\n",
      "Incoming\n",
      "request\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 80, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "48\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "UNDERSTANDING THE POD AND ITS CONTAINER\n",
      "The main and most important component in your system is the pod. It contains only a\n",
      "single container, but generally a pod can contain as many containers as you want.\n",
      "Inside the container is your Node.js process, which is bound to port 8080 and is wait-\n",
      "ing for HTTP requests. The pod has its own unique private IP address and hostname. \n",
      "UNDERSTANDING THE ROLE OF THE REPLICATIONCONTROLLER\n",
      "The next component is the kubia ReplicationController. It makes sure there’s always\n",
      "exactly one instance of your pod running. Generally, ReplicationControllers are used\n",
      "to replicate pods (that is, create multiple copies of a pod) and keep them running. In\n",
      "your case, you didn’t specify how many pod replicas you want, so the Replication-\n",
      "Controller created a single one. If your pod were to disappear for any reason, the\n",
      "ReplicationController would create a new pod to replace the missing one. \n",
      "UNDERSTANDING WHY YOU NEED A SERVICE\n",
      "The third component of your system is the kubia-http service. To understand why\n",
      "you need services, you need to learn a key detail about pods. They’re ephemeral. A\n",
      "pod may disappear at any time—because the node it’s running on has failed, because\n",
      "someone deleted the pod, or because the pod was evicted from an otherwise healthy\n",
      "node. When any of those occurs, a missing pod is replaced with a new one by the\n",
      "ReplicationController, as described previously. This new pod gets a different IP\n",
      "address from the pod it’s replacing. This is where services come in—to solve the prob-\n",
      "lem of ever-changing pod IP addresses, as well as exposing multiple pods at a single\n",
      "constant IP and port pair. \n",
      " When a service is created, it gets a static IP, which never changes during the lifetime of\n",
      "the service. Instead of connecting to pods directly, clients should connect to the service\n",
      "through its constant IP address. The service makes sure one of the pods receives the con-\n",
      "nection, regardless of where the pod is currently running (and what its IP address is). \n",
      " Services represent a static location for a group of one or more pods that all provide\n",
      "the same service. Requests coming to the IP and port of the service will be forwarded\n",
      "to the IP and port of one of the pods belonging to the service at that moment.  \n",
      "2.3.4\n",
      "Horizontally scaling the application\n",
      "You now have a running application, monitored and kept running by a Replication-\n",
      "Controller and exposed to the world through a service. Now let’s make additional\n",
      "magic happen. \n",
      " One of the main benefits of using Kubernetes is the simplicity with which you can\n",
      "scale your deployments. Let’s see how easy it is to scale up the number of pods. You’ll\n",
      "increase the number of running instances to three. \n",
      " Your pod is managed by a ReplicationController. Let’s see it with the kubectl get\n",
      "command:\n",
      "$ kubectl get replicationcontrollers\n",
      "NAME        DESIRED    CURRENT   AGE\n",
      "kubia       1          1         17m\n",
      " \n",
      "www.allitebooks.com\n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 81, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "49\n",
      "Running your first app on Kubernetes\n",
      "The list shows a single ReplicationController called kubia. The DESIRED column\n",
      "shows the number of pod replicas you want the ReplicationController to keep,\n",
      "whereas the CURRENT column shows the actual number of pods currently running. In\n",
      "your case, you wanted to have a single replica of the pod running, and exactly one\n",
      "replica is currently running. \n",
      "INCREASING THE DESIRED REPLICA COUNT\n",
      "To scale up the number of replicas of your pod, you need to change the desired\n",
      "replica count on the ReplicationController like this:\n",
      "$ kubectl scale rc kubia --replicas=3\n",
      "replicationcontroller \"kubia\" scaled\n",
      "You’ve now told Kubernetes to make sure three instances of your pod are always run-\n",
      "ning. Notice that you didn’t instruct Kubernetes what action to take. You didn’t tell it\n",
      "to add two more pods. You only set the new desired number of instances and let\n",
      "Kubernetes determine what actions it needs to take to achieve the requested state. \n",
      " This is one of the most fundamental Kubernetes principles. Instead of telling\n",
      "Kubernetes exactly what actions it should perform, you’re only declaratively changing\n",
      "the desired state of the system and letting Kubernetes examine the current actual\n",
      "state and reconcile it with the desired state. This is true across all of Kubernetes.\n",
      "SEEING THE RESULTS OF THE SCALE-OUT\n",
      "Back to your replica count increase. Let’s list the ReplicationControllers again to see\n",
      "the updated replica count:\n",
      "$ kubectl get rc\n",
      "NAME        DESIRED    CURRENT   READY   AGE\n",
      "kubia       3          3         2       17m\n",
      "Because the actual number of pods has already been increased to three (as evident\n",
      "from the CURRENT column), listing all the pods should now show three pods instead\n",
      "of one:\n",
      "$ kubectl get pods\n",
      "NAME          READY     STATUS    RESTARTS   AGE\n",
      "kubia-hczji   1/1       Running   0          7s\n",
      "kubia-iq9y6   0/1       Pending   0          7s\n",
      "kubia-4jfyf   1/1       Running   0          18m\n",
      "Listing all the resource types with kubectl get\n",
      "You’ve been using the same basic kubectl get command to list things in your cluster.\n",
      "You’ve used this command to list Node, Pod, Service and ReplicationController\n",
      "objects. You can get a list of all the possible object types by invoking kubectl get\n",
      "without specifying the type. You can then use those types with various kubectl\n",
      "commands such as get, describe, and so on. The list also shows the abbreviations\n",
      "I mentioned earlier.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 82, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "50\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "As you can see, three pods exist instead of one. Two are already running, one is still\n",
      "pending, but should be ready in a few moments, as soon as the container image is\n",
      "downloaded and the container is started. \n",
      " As you can see, scaling an application is incredibly simple. Once your app is run-\n",
      "ning in production and a need to scale the app arises, you can add additional\n",
      "instances with a single command without having to install and run additional copies\n",
      "manually. \n",
      " Keep in mind that the app itself needs to support being scaled horizontally. Kuber-\n",
      "netes doesn’t magically make your app scalable; it only makes it trivial to scale the app\n",
      "up or down. \n",
      "SEEING REQUESTS HIT ALL THREE PODS WHEN HITTING THE SERVICE\n",
      "Because you now have multiple instances of your app running, let’s see what happens\n",
      "if you hit the service URL again. Will you always hit the same app instance or not?\n",
      "$ curl 104.155.74.57:8080\n",
      "You’ve hit kubia-hczji\n",
      "$ curl 104.155.74.57:8080\n",
      "You’ve hit kubia-iq9y6\n",
      "$ curl 104.155.74.57:8080\n",
      "You’ve hit kubia-iq9y6\n",
      "$ curl 104.155.74.57:8080\n",
      "You’ve hit kubia-4jfyf   \n",
      "Requests are hitting different pods randomly. This is what services in Kubernetes do\n",
      "when more than one pod instance backs them. They act as a load balancer standing in\n",
      "front of multiple pods. When there’s only one pod, services provide a static address\n",
      "for the single pod. Whether a service is backed by a single pod or a group of pods,\n",
      "those pods come and go as they’re moved around the cluster, which means their IP\n",
      "addresses change, but the service is always there at the same address. This makes it\n",
      "easy for clients to connect to the pods, regardless of how many exist and how often\n",
      "they change location.\n",
      "VISUALIZING THE NEW STATE OF YOUR SYSTEM\n",
      "Let’s visualize your system again to see what’s changed from before. Figure 2.8\n",
      "shows the new state of your system. You still have a single service and a single\n",
      "ReplicationController, but you now have three instances of your pod, all managed\n",
      "by the ReplicationController. The service no longer sends all requests to a single\n",
      "pod, but spreads them across all three pods as shown in the experiment with curl\n",
      "in the previous section.\n",
      " As an exercise, you can now try spinning up additional instances by increasing the\n",
      "ReplicationController’s replica count even further and then scaling back down.\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 83, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "51\n",
      "Running your first app on Kubernetes\n",
      "2.3.5\n",
      "Examining what nodes your app is running on \n",
      "You may be wondering what nodes your pods have been scheduled to. In the Kuber-\n",
      "netes world, what node a pod is running on isn’t that important, as long as it gets\n",
      "scheduled to a node that can provide the CPU and memory the pod needs to run\n",
      "properly. \n",
      " Regardless of the node they’re scheduled to, all the apps running inside contain-\n",
      "ers have the same type of OS environment. Each pod has its own IP and can talk to\n",
      "any other pod, regardless of whether that other pod is also running on the same node\n",
      "or on a different one. Each pod is provided with the requested amount of computa-\n",
      "tional resources, so whether those resources are provided by one node or another\n",
      "doesn’t make any difference. \n",
      "DISPLAYING THE POD IP AND THE POD’S NODE WHEN LISTING PODS\n",
      "If you’ve been paying close attention, you probably noticed that the kubectl get pods\n",
      "command doesn’t even show any information about the nodes the pods are scheduled\n",
      "to. This is because it’s usually not an important piece of information.\n",
      " But you can request additional columns to display using the -o wide option. When\n",
      "listing pods, this option shows the pod’s IP and the node the pod is running on:\n",
      "$ kubectl get pods -o wide\n",
      "NAME          READY   STATUS    RESTARTS   AGE   IP         NODE\n",
      "kubia-hczji   1/1     Running   0          7s    10.1.0.2   gke-kubia-85...\n",
      "Pod: kubia-4jfyf\n",
      "IP: 10.1.0.1\n",
      "Container\n",
      "Service: kubia-http\n",
      "Internal IP: 10.3.246.185\n",
      "External IP: 104.155.74.57\n",
      "Port\n",
      "8080\n",
      "Incoming\n",
      "request\n",
      "Pod: kubia-hczji\n",
      "IP: 10.1.0.2\n",
      "Container\n",
      "Pod: kubia-iq9y6\n",
      "IP: 10.1.0.3\n",
      "Container\n",
      "ReplicationController: kubia\n",
      "Replicas: 3\n",
      "Port\n",
      "8080\n",
      "Port\n",
      "8080\n",
      "Port\n",
      "8080\n",
      "Figure 2.8\n",
      "Three instances of a pod managed by the same ReplicationController and exposed \n",
      "through a single service IP and port.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 84, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "52\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "INSPECTING OTHER DETAILS OF A POD WITH KUBECTL DESCRIBE\n",
      "You can also see the node by using the kubectl describe command, which shows\n",
      "many other details of the pod, as shown in the following listing.\n",
      "$ kubectl describe pod kubia-hczji\n",
      "Name:        kubia-hczji\n",
      "Namespace:   default\n",
      "Node:        gke-kubia-85f6-node-vs9f/10.132.0.3    \n",
      "Start Time:  Fri, 29 Apr 2016 14:12:33 +0200\n",
      "Labels:      run=kubia\n",
      "Status:      Running\n",
      "IP:          10.1.0.2\n",
      "Controllers: ReplicationController/kubia\n",
      "Containers:  ...\n",
      "Conditions:\n",
      "  Type       Status\n",
      "  Ready      True \n",
      "Volumes: ...\n",
      "Events: ...\n",
      "This shows, among other things, the node the pod has been scheduled to, the time\n",
      "when it was started, the image(s) it’s running, and other useful information.\n",
      "2.3.6\n",
      "Introducing the Kubernetes dashboard\n",
      "Before we wrap up this initial hands-on chapter, let’s look at another way of exploring\n",
      "your Kubernetes cluster.\n",
      " Up to now, you’ve only been using the kubectl command-line tool. If you’re more\n",
      "into graphical web user interfaces, you’ll be glad to hear that Kubernetes also comes\n",
      "with a nice (but still evolving) web dashboard.\n",
      " The dashboard allows you to list all the Pods, ReplicationControllers, Services, and\n",
      "other objects deployed in your cluster, as well as to create, modify, and delete them.\n",
      "Figure 2.9 shows the dashboard.\n",
      " Although you won’t use the dashboard in this book, you can open it up any time to\n",
      "quickly see a graphical view of what’s deployed in your cluster after you create or mod-\n",
      "ify objects through kubectl.\n",
      "ACCESSING THE DASHBOARD WHEN RUNNING KUBERNETES IN GKE\n",
      "If you’re using Google Kubernetes Engine, you can find out the URL of the dash-\n",
      "board through the kubectl cluster-info command, which we already introduced:\n",
      "$ kubectl cluster-info | grep dashboard\n",
      "kubernetes-dashboard is running at https://104.155.108.191/api/v1/proxy/\n",
      "➥ namespaces/kube-system/services/kubernetes-dashboard\n",
      "Listing 2.18\n",
      "Describing a pod with kubectl describe\n",
      "Here’s the node the pod \n",
      "has been scheduled to.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "1700\n",
      "height\n",
      "932\n",
      "PIX BUFFER SIZE\n",
      "4753200\n",
      "Original IMG_BUFFER_SIZE\n",
      "4753200\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014012140>\n",
      "page_image_dict\n",
      "{'page': 85, 'img_cnt': 1, 'img_npy_lst': []}\n",
      "53\n",
      "Summary\n",
      "If you open this URL in a browser, you’re presented with a username and password\n",
      "prompt. You’ll find the username and password by running the following command:\n",
      "$ gcloud container clusters describe kubia | grep -E \"(username|password):\"\n",
      "  password: 32nENgreEJ632A12          \n",
      "  username: admin                     \n",
      "ACCESSING THE DASHBOARD WHEN USING MINIKUBE\n",
      "To open the dashboard in your browser when using Minikube to run your Kubernetes\n",
      "cluster, run the following command:\n",
      "$ minikube dashboard\n",
      "The dashboard will open in your default browser. Unlike with GKE, you won’t need to\n",
      "enter any credentials to access it.\n",
      "2.4\n",
      "Summary\n",
      "Hopefully, this initial hands-on chapter has shown you that Kubernetes isn’t a compli-\n",
      "cated platform to use, and you’re ready to learn in depth about all the things it can\n",
      "provide. After reading this chapter, you should now know how to\n",
      "Pull and run any publicly available container image\n",
      "Package your apps into container images and make them available to anyone by\n",
      "pushing the images to a remote image registry\n",
      "Figure 2.9\n",
      "Screenshot of the Kubernetes web-based dashboard\n",
      "The username and password \n",
      "for the dashboard\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 86, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "54\n",
      "CHAPTER 2\n",
      "First steps with Docker and Kubernetes\n",
      "Enter a running container and inspect its environment\n",
      "Set up a multi-node Kubernetes cluster on Google Kubernetes Engine\n",
      "Configure an alias and tab completion for the kubectl command-line tool\n",
      "List and inspect Nodes, Pods, Services, and ReplicationControllers in a Kuber-\n",
      "netes cluster\n",
      "Run a container in Kubernetes and make it accessible from outside the cluster\n",
      "Have a basic sense of how Pods, ReplicationControllers, and Services relate to\n",
      "one another\n",
      "Scale an app horizontally by changing the ReplicationController’s replica count\n",
      "Access the web-based Kubernetes dashboard on both Minikube and GKE \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 87, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "55\n",
      "Pods: running\n",
      "containers in Kubernetes\n",
      "The previous chapter should have given you a rough picture of the basic compo-\n",
      "nents you create in Kubernetes and at least an outline of what they do. Now, we’ll\n",
      "start reviewing all types of Kubernetes objects (or resources) in greater detail, so\n",
      "you’ll understand when, how, and why to use each of them. We’ll start with pods,\n",
      "because they’re the central, most important, concept in Kubernetes. Everything\n",
      "else either manages, exposes, or is used by pods. \n",
      "This chapter covers\n",
      "Creating, running, and stopping pods\n",
      "Organizing pods and other resources with labels\n",
      "Performing an operation on all pods with a \n",
      "specific label\n",
      "Using namespaces to split pods into non-\n",
      "overlapping groups\n",
      "Scheduling pods onto specific types of worker \n",
      "nodes\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 88, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "56\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "3.1\n",
      "Introducing pods\n",
      "You’ve already learned that a pod is a co-located group of containers and represents\n",
      "the basic building block in Kubernetes. Instead of deploying containers individually,\n",
      "you always deploy and operate on a pod of containers. We’re not implying that a pod\n",
      "always includes more than one container—it’s common for pods to contain only a sin-\n",
      "gle container. The key thing about pods is that when a pod does contain multiple con-\n",
      "tainers, all of them are always run on a single worker node—it never spans multiple\n",
      "worker nodes, as shown in figure 3.1.\n",
      "3.1.1\n",
      "Understanding why we need pods\n",
      "But why do we even need pods? Why can’t we use containers directly? Why would we\n",
      "even need to run multiple containers together? Can’t we put all our processes into a\n",
      "single container? We’ll answer those questions now.\n",
      "UNDERSTANDING WHY MULTIPLE CONTAINERS ARE BETTER THAN ONE CONTAINER RUNNING \n",
      "MULTIPLE PROCESSES\n",
      "Imagine an app consisting of multiple processes that either communicate through\n",
      "IPC (Inter-Process Communication) or through locally stored files, which requires\n",
      "them to run on the same machine. Because in Kubernetes you always run processes in\n",
      "containers and each container is much like an isolated machine, you may think it\n",
      "makes sense to run multiple processes in a single container, but you shouldn’t do that. \n",
      " Containers are designed to run only a single process per container (unless the\n",
      "process itself spawns child processes). If you run multiple unrelated processes in a\n",
      "single container, it is your responsibility to keep all those processes running, man-\n",
      "age their logs, and so on. For example, you’d have to include a mechanism for auto-\n",
      "matically restarting individual processes if they crash. Also, all those processes would\n",
      "log to the same standard output, so you’d have a hard time figuring out what pro-\n",
      "cess logged what. \n",
      "Node 1\n",
      "Pod 2\n",
      "IP: 10.1.0.2\n",
      "Container 1\n",
      "Container 2\n",
      "Pod 1\n",
      "IP: 10.1.0.1\n",
      "Container\n",
      "Node 2\n",
      "Pod 4\n",
      "IP: 10.1.1.2\n",
      "Container 1\n",
      "Pod 5\n",
      "IP: 10.1.1.3\n",
      "Container 1\n",
      "Container 2\n",
      "Pod 3\n",
      "Container 1\n",
      "Container 2\n",
      "Figure 3.1\n",
      "All containers of a pod run on the same node. A pod never spans two nodes.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 89, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "57\n",
      "Introducing pods\n",
      " Therefore, you need to run each process in its own container. That’s how Docker\n",
      "and Kubernetes are meant to be used. \n",
      "3.1.2\n",
      "Understanding pods\n",
      "Because you’re not supposed to group multiple processes into a single container, it’s\n",
      "obvious you need another higher-level construct that will allow you to bind containers\n",
      "together and manage them as a single unit. This is the reasoning behind pods. \n",
      " A pod of containers allows you to run closely related processes together and pro-\n",
      "vide them with (almost) the same environment as if they were all running in a single\n",
      "container, while keeping them somewhat isolated. This way, you get the best of both\n",
      "worlds. You can take advantage of all the features containers provide, while at the\n",
      "same time giving the processes the illusion of running together. \n",
      "UNDERSTANDING THE PARTIAL ISOLATION BETWEEN CONTAINERS OF THE SAME POD\n",
      "In the previous chapter, you learned that containers are completely isolated from\n",
      "each other, but now you see that you want to isolate groups of containers instead of\n",
      "individual ones. You want containers inside each group to share certain resources,\n",
      "although not all, so that they’re not fully isolated. Kubernetes achieves this by config-\n",
      "uring Docker to have all containers of a pod share the same set of Linux namespaces\n",
      "instead of each container having its own set. \n",
      " Because all containers of a pod run under the same Network and UTS namespaces\n",
      "(we’re talking about Linux namespaces here), they all share the same hostname and\n",
      "network interfaces. Similarly, all containers of a pod run under the same IPC namespace\n",
      "and can communicate through IPC. In the latest Kubernetes and Docker versions, they\n",
      "can also share the same PID namespace, but that feature isn’t enabled by default. \n",
      "NOTE\n",
      "When containers of the same pod use separate PID namespaces, you\n",
      "only see the container’s own processes when running ps aux in the container.\n",
      "But when it comes to the filesystem, things are a little different. Because most of the\n",
      "container’s filesystem comes from the container image, by default, the filesystem of\n",
      "each container is fully isolated from other containers. However, it’s possible to have\n",
      "them share file directories using a Kubernetes concept called a Volume, which we’ll\n",
      "talk about in chapter 6.\n",
      "UNDERSTANDING HOW CONTAINERS SHARE THE SAME IP AND PORT SPACE\n",
      "One thing to stress here is that because containers in a pod run in the same Network\n",
      "namespace, they share the same IP address and port space. This means processes run-\n",
      "ning in containers of the same pod need to take care not to bind to the same port\n",
      "numbers or they’ll run into port conflicts. But this only concerns containers in the\n",
      "same pod. Containers of different pods can never run into port conflicts, because\n",
      "each pod has a separate port space. All the containers in a pod also have the same\n",
      "loopback network interface, so a container can communicate with other containers in\n",
      "the same pod through localhost.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 90, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "58\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "INTRODUCING THE FLAT INTER-POD NETWORK\n",
      "All pods in a Kubernetes cluster reside in a single flat, shared, network-address space\n",
      "(shown in figure 3.2), which means every pod can access every other pod at the other\n",
      "pod’s IP address. No NAT (Network Address Translation) gateways exist between them.\n",
      "When two pods send network packets between each other, they’ll each see the actual\n",
      "IP address of the other as the source IP in the packet.\n",
      "Consequently, communication between pods is always simple. It doesn’t matter if two\n",
      "pods are scheduled onto a single or onto different worker nodes; in both cases the\n",
      "containers inside those pods can communicate with each other across the flat NAT-\n",
      "less network, much like computers on a local area network (LAN), regardless of the\n",
      "actual inter-node network topology. Like a computer on a LAN, each pod gets its own\n",
      "IP address and is accessible from all other pods through this network established spe-\n",
      "cifically for pods. This is usually achieved through an additional software-defined net-\n",
      "work layered on top of the actual network.\n",
      " To sum up what’s been covered in this section: pods are logical hosts and behave\n",
      "much like physical hosts or VMs in the non-container world. Processes running in the\n",
      "same pod are like processes running on the same physical or virtual machine, except\n",
      "that each process is encapsulated in a container. \n",
      "3.1.3\n",
      "Organizing containers across pods properly\n",
      "You should think of pods as separate machines, but where each one hosts only a cer-\n",
      "tain app. Unlike the old days, when we used to cram all sorts of apps onto the same\n",
      "host, we don’t do that with pods. Because pods are relatively lightweight, you can have\n",
      "as many as you need without incurring almost any overhead. Instead of stuffing every-\n",
      "thing into a single pod, you should organize apps into multiple pods, where each one\n",
      "contains only tightly related components or processes.\n",
      "Node 1\n",
      "Pod A\n",
      "IP: 10.1.1.6\n",
      "Container 1\n",
      "Container 2\n",
      "Pod B\n",
      "IP: 10.1.1.7\n",
      "Container 1\n",
      "Container 2\n",
      "Node 2\n",
      "Flat network\n",
      "Pod C\n",
      "IP: 10.1.2.5\n",
      "Container 1\n",
      "Container 2\n",
      "Pod D\n",
      "IP: 10.1.2.7\n",
      "Container 1\n",
      "Container 2\n",
      "Figure 3.2\n",
      "Each pod gets a routable IP address and all other pods see the pod under \n",
      "that IP address.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 91, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "59\n",
      "Introducing pods\n",
      " Having said that, do you think a multi-tier application consisting of a frontend\n",
      "application server and a backend database should be configured as a single pod or as\n",
      "two pods?\n",
      "SPLITTING MULTI-TIER APPS INTO MULTIPLE PODS\n",
      "Although nothing is stopping you from running both the frontend server and the\n",
      "database in a single pod with two containers, it isn’t the most appropriate way. We’ve\n",
      "said that all containers of the same pod always run co-located, but do the web server\n",
      "and the database really need to run on the same machine? The answer is obviously no,\n",
      "so you don’t want to put them into a single pod. But is it wrong to do so regardless? In\n",
      "a way, it is.\n",
      " If both the frontend and backend are in the same pod, then both will always be\n",
      "run on the same machine. If you have a two-node Kubernetes cluster and only this sin-\n",
      "gle pod, you’ll only be using a single worker node and not taking advantage of the\n",
      "computational resources (CPU and memory) you have at your disposal on the second\n",
      "node. Splitting the pod into two would allow Kubernetes to schedule the frontend to\n",
      "one node and the backend to the other node, thereby improving the utilization of\n",
      "your infrastructure.\n",
      "SPLITTING INTO MULTIPLE PODS TO ENABLE INDIVIDUAL SCALING\n",
      "Another reason why you shouldn’t put them both into a single pod is scaling. A pod is\n",
      "also the basic unit of scaling. Kubernetes can’t horizontally scale individual contain-\n",
      "ers; instead, it scales whole pods. If your pod consists of a frontend and a backend con-\n",
      "tainer, when you scale up the number of instances of the pod to, let’s say, two, you end\n",
      "up with two frontend containers and two backend containers. \n",
      " Usually, frontend components have completely different scaling requirements\n",
      "than the backends, so we tend to scale them individually. Not to mention the fact that\n",
      "backends such as databases are usually much harder to scale compared to (stateless)\n",
      "frontend web servers. If you need to scale a container individually, this is a clear indi-\n",
      "cation that it needs to be deployed in a separate pod. \n",
      "UNDERSTANDING WHEN TO USE MULTIPLE CONTAINERS IN A POD\n",
      "The main reason to put multiple containers into a single pod is when the application\n",
      "consists of one main process and one or more complementary processes, as shown in\n",
      "figure 3.3.\n",
      "Pod\n",
      "Main container\n",
      "Supporting\n",
      "container 1\n",
      "Supporting\n",
      "container 2\n",
      "Volume\n",
      "Figure 3.3\n",
      "Pods should contain tightly coupled \n",
      "containers, usually a main container and containers \n",
      "that support the main one.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 92, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "60\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "For example, the main container in a pod could be a web server that serves files from\n",
      "a certain file directory, while an additional container (a sidecar container) periodi-\n",
      "cally downloads content from an external source and stores it in the web server’s\n",
      "directory. In chapter 6 you’ll see that you need to use a Kubernetes Volume that you\n",
      "mount into both containers. \n",
      " Other examples of sidecar containers include log rotators and collectors, data pro-\n",
      "cessors, communication adapters, and others.\n",
      "DECIDING WHEN TO USE MULTIPLE CONTAINERS IN A POD\n",
      "To recap how containers should be grouped into pods—when deciding whether to\n",
      "put two containers into a single pod or into two separate pods, you always need to ask\n",
      "yourself the following questions:\n",
      "Do they need to be run together or can they run on different hosts?\n",
      "Do they represent a single whole or are they independent components?\n",
      "Must they be scaled together or individually? \n",
      "Basically, you should always gravitate toward running containers in separate pods,\n",
      "unless a specific reason requires them to be part of the same pod. Figure 3.4 will help\n",
      "you memorize this.\n",
      "Although pods can contain multiple containers, to keep things simple for now, you’ll\n",
      "only be dealing with single-container pods in this chapter. You’ll see how multiple\n",
      "containers are used in the same pod later, in chapter 6. \n",
      "Pod\n",
      "Frontend\n",
      "process\n",
      "Backend\n",
      "process\n",
      "Container\n",
      "Pod\n",
      "Frontend\n",
      "process\n",
      "Frontend\n",
      "container\n",
      "Frontend pod\n",
      "Frontend\n",
      "process\n",
      "Frontend\n",
      "container\n",
      "Backend pod\n",
      "Backend\n",
      "process\n",
      "Backend\n",
      "container\n",
      "Backend\n",
      "process\n",
      "Backend\n",
      "container\n",
      "Figure 3.4\n",
      "A container shouldn’t run multiple processes. A pod shouldn’t contain multiple \n",
      "containers if they don’t need to run on the same machine.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 93, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "61\n",
      "Creating pods from YAML or JSON descriptors\n",
      "3.2\n",
      "Creating pods from YAML or JSON descriptors\n",
      "Pods and other Kubernetes resources are usually created by posting a JSON or YAML\n",
      "manifest to the Kubernetes REST API endpoint. Also, you can use other, simpler ways\n",
      "of creating resources, such as the kubectl run command you used in the previous\n",
      "chapter, but they usually only allow you to configure a limited set of properties, not\n",
      "all. Additionally, defining all your Kubernetes objects from YAML files makes it possi-\n",
      "ble to store them in a version control system, with all the benefits it brings.\n",
      " To configure all aspects of each type of resource, you’ll need to know and under-\n",
      "stand the Kubernetes API object definitions. You’ll get to know most of them as you\n",
      "learn about each resource type throughout this book. We won’t explain every single\n",
      "property, so you should also refer to the Kubernetes API reference documentation at\n",
      "http:/\n",
      "/kubernetes.io/docs/reference/ when creating objects.\n",
      "3.2.1\n",
      "Examining a YAML descriptor of an existing pod\n",
      "You already have some existing pods you created in the previous chapter, so let’s look\n",
      "at what a YAML definition for one of those pods looks like. You’ll use the kubectl get\n",
      "command with the -o yaml option to get the whole YAML definition of the pod, as\n",
      "shown in the following listing.\n",
      "$ kubectl get po kubia-zxzij -o yaml\n",
      "apiVersion: v1                         \n",
      "kind: Pod                                       \n",
      "metadata:                                                 \n",
      "  annotations:                                            \n",
      "    kubernetes.io/created-by: ...                         \n",
      "  creationTimestamp: 2016-03-18T12:37:50Z                 \n",
      "  generateName: kubia-                                    \n",
      "  labels:                                                 \n",
      "    run: kubia                                            \n",
      "  name: kubia-zxzij                                       \n",
      "  namespace: default                                      \n",
      "  resourceVersion: \"294\"                                  \n",
      "  selfLink: /api/v1/namespaces/default/pods/kubia-zxzij   \n",
      "  uid: 3a564dc0-ed06-11e5-ba3b-42010af00004               \n",
      "spec:                                                   \n",
      "  containers:                                           \n",
      "  - image: luksa/kubia                                  \n",
      "    imagePullPolicy: IfNotPresent                       \n",
      "    name: kubia                                         \n",
      "    ports:                                              \n",
      "    - containerPort: 8080                               \n",
      "      protocol: TCP                                     \n",
      "    resources:                                          \n",
      "      requests:                                         \n",
      "        cpu: 100m                                       \n",
      "Listing 3.1\n",
      "Full YAML of a deployed pod\n",
      "Kubernetes API version used \n",
      "in this YAML descriptor\n",
      "Type of Kubernetes \n",
      "object/resource\n",
      "Pod metadata (name, \n",
      "labels, annotations, \n",
      "and so on)\n",
      "Pod specification/\n",
      "contents (list of \n",
      "pod’s containers, \n",
      "volumes, and so on)\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 94, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "62\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "    terminationMessagePath: /dev/termination-log      \n",
      "    volumeMounts:                                     \n",
      "    - mountPath: /var/run/secrets/k8s.io/servacc      \n",
      "      name: default-token-kvcqa                       \n",
      "      readOnly: true                                  \n",
      "  dnsPolicy: ClusterFirst                             \n",
      "  nodeName: gke-kubia-e8fe08b8-node-txje              \n",
      "  restartPolicy: Always                               \n",
      "  serviceAccount: default                             \n",
      "  serviceAccountName: default                         \n",
      "  terminationGracePeriodSeconds: 30                   \n",
      "  volumes:                                            \n",
      "  - name: default-token-kvcqa                         \n",
      "    secret:                                           \n",
      "      secretName: default-token-kvcqa                 \n",
      "status:                                                   \n",
      "  conditions:                                             \n",
      "  - lastProbeTime: null                                   \n",
      "    lastTransitionTime: null                              \n",
      "    status: \"True\"                                        \n",
      "    type: Ready                                           \n",
      "  containerStatuses:                                      \n",
      "  - containerID: docker://f0276994322d247ba...            \n",
      "    image: luksa/kubia                                    \n",
      "    imageID: docker://4c325bcc6b40c110226b89fe...         \n",
      "    lastState: {}                                         \n",
      "    name: kubia                                           \n",
      "    ready: true                                           \n",
      "    restartCount: 0                                       \n",
      "    state:                                                \n",
      "      running:                                            \n",
      "        startedAt: 2016-03-18T12:46:05Z                   \n",
      "  hostIP: 10.132.0.4                                      \n",
      "  phase: Running                                          \n",
      "  podIP: 10.0.2.3                                         \n",
      "  startTime: 2016-03-18T12:44:32Z                         \n",
      "I know this looks complicated, but it becomes simple once you understand the basics\n",
      "and know how to distinguish between the important parts and the minor details. Also,\n",
      "you can take comfort in the fact that when creating a new pod, the YAML you need to\n",
      "write is much shorter, as you’ll see later.\n",
      "INTRODUCING THE MAIN PARTS OF A POD DEFINITION\n",
      "The pod definition consists of a few parts. First, there’s the Kubernetes API version\n",
      "used in the YAML and the type of resource the YAML is describing. Then, three\n",
      "important sections are found in almost all Kubernetes resources:\n",
      "Metadata includes the name, namespace, labels, and other information about\n",
      "the pod.\n",
      "Spec contains the actual description of the pod’s contents, such as the pod’s con-\n",
      "tainers, volumes, and other data.\n",
      "Pod specification/\n",
      "contents (list of \n",
      "pod’s containers, \n",
      "volumes, and so on)\n",
      "Detailed status \n",
      "of the pod and \n",
      "its containers\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 95, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "63\n",
      "Creating pods from YAML or JSON descriptors\n",
      "Status contains the current information about the running pod, such as what\n",
      "condition the pod is in, the description and status of each container, and the\n",
      "pod’s internal IP and other basic info.\n",
      "Listing 3.1 showed a full description of a running pod, including its status. The status\n",
      "part contains read-only runtime data that shows the state of the resource at a given\n",
      "moment. When creating a new pod, you never need to provide the status part. \n",
      " The three parts described previously show the typical structure of a Kubernetes\n",
      "API object. As you’ll see throughout the book, all other objects have the same anat-\n",
      "omy. This makes understanding new objects relatively easy.\n",
      " Going through all the individual properties in the previous YAML doesn’t make\n",
      "much sense, so, instead, let’s see what the most basic YAML for creating a pod looks\n",
      "like. \n",
      "3.2.2\n",
      "Creating a simple YAML descriptor for a pod\n",
      "You’re going to create a file called kubia-manual.yaml (you can create it in any\n",
      "directory you want), or download the book’s code archive, where you’ll find the\n",
      "file inside the Chapter03 directory. The following listing shows the entire contents\n",
      "of the file.\n",
      "apiVersion: v1         \n",
      "kind: Pod                             \n",
      "metadata:     \n",
      "  name: kubia-manual         \n",
      "spec: \n",
      "  containers: \n",
      "  - image: luksa/kubia          \n",
      "    name: kubia         \n",
      "    ports: \n",
      "    - containerPort: 8080     \n",
      "      protocol: TCP\n",
      "I’m sure you’ll agree this is much simpler than the definition in listing 3.1. Let’s exam-\n",
      "ine this descriptor in detail. It conforms to the v1 version of the Kubernetes API. The\n",
      "type of resource you’re describing is a pod, with the name kubia-manual. The pod\n",
      "consists of a single container based on the luksa/kubia image. You’ve also given a\n",
      "name to the container and indicated that it’s listening on port 8080. \n",
      "SPECIFYING CONTAINER PORTS\n",
      "Specifying ports in the pod definition is purely informational. Omitting them has no\n",
      "effect on whether clients can connect to the pod through the port or not. If the con-\n",
      "Listing 3.2\n",
      "A basic pod manifest: kubia-manual.yaml\n",
      "Descriptor conforms\n",
      "to version v1 of\n",
      "Kubernetes API\n",
      "You’re \n",
      "describing a pod.\n",
      "The name \n",
      "of the pod\n",
      "Container image to create \n",
      "the container from\n",
      "Name of the container\n",
      "The port the app \n",
      "is listening on\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 96, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "64\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "tainer is accepting connections through a port bound to the 0.0.0.0 address, other\n",
      "pods can always connect to it, even if the port isn’t listed in the pod spec explicitly. But\n",
      "it makes sense to define the ports explicitly so that everyone using your cluster can\n",
      "quickly see what ports each pod exposes. Explicitly defining ports also allows you to\n",
      "assign a name to each port, which can come in handy, as you’ll see later in the book.\n",
      "Using kubectl explain to discover possible API object fields\n",
      "When preparing a manifest, you can either turn to the Kubernetes reference\n",
      "documentation at http:/\n",
      "/kubernetes.io/docs/api to see which attributes are\n",
      "supported by each API object, or you can use the kubectl explain command.\n",
      "For example, when creating a pod manifest from scratch, you can start by asking\n",
      "kubectl to explain pods:\n",
      "$ kubectl explain pods\n",
      "DESCRIPTION:\n",
      "Pod is a collection of containers that can run on a host. This resource \n",
      "is created by clients and scheduled onto hosts.\n",
      "FIELDS:\n",
      "   kind      <string>\n",
      "     Kind is a string value representing the REST resource this object\n",
      "     represents...\n",
      "   metadata  <Object>\n",
      "     Standard object's metadata...\n",
      "   spec      <Object>\n",
      "     Specification of the desired behavior of the pod...\n",
      "   status    <Object>\n",
      "     Most recently observed status of the pod. This data may not be up to\n",
      "     date...\n",
      "Kubectl prints out the explanation of the object and lists the attributes the object\n",
      "can contain. You can then drill deeper to find out more about each attribute. For\n",
      "example, you can examine the spec attribute like this:\n",
      "$ kubectl explain pod.spec\n",
      "RESOURCE: spec <Object>\n",
      "DESCRIPTION:\n",
      "    Specification of the desired behavior of the pod...\n",
      "    podSpec is a description of a pod.\n",
      "FIELDS:\n",
      "   hostPID   <boolean>\n",
      "     Use the host's pid namespace. Optional: Default to false.\n",
      "   ...\n",
      "   volumes   <[]Object>\n",
      "     List of volumes that can be mounted by containers belonging to the\n",
      "     pod.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 97, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "65\n",
      "Creating pods from YAML or JSON descriptors\n",
      "3.2.3\n",
      "Using kubectl create to create the pod\n",
      "To create the pod from your YAML file, use the kubectl create command:\n",
      "$ kubectl create -f kubia-manual.yaml\n",
      "pod \"kubia-manual\" created\n",
      "The kubectl create -f command is used for creating any resource (not only pods)\n",
      "from a YAML or JSON file. \n",
      "RETRIEVING THE WHOLE DEFINITION OF A RUNNING POD\n",
      "After creating the pod, you can ask Kubernetes for the full YAML of the pod. You’ll\n",
      "see it’s similar to the YAML you saw earlier. You’ll learn about the additional fields\n",
      "appearing in the returned definition in the next sections. Go ahead and use the fol-\n",
      "lowing command to see the full descriptor of the pod:\n",
      "$ kubectl get po kubia-manual -o yaml\n",
      "If you’re more into JSON, you can also tell kubectl to return JSON instead of YAML\n",
      "like this (this works even if you used YAML to create the pod):\n",
      "$ kubectl get po kubia-manual -o json\n",
      "SEEING YOUR NEWLY CREATED POD IN THE LIST OF PODS\n",
      "Your pod has been created, but how do you know if it’s running? Let’s list pods to see\n",
      "their statuses:\n",
      "$ kubectl get pods\n",
      "NAME            READY   STATUS    RESTARTS   AGE\n",
      "kubia-manual    1/1     Running   0          32s\n",
      "kubia-zxzij     1/1     Running   0          1d    \n",
      "There’s your kubia-manual pod. Its status shows that it’s running. If you’re like me,\n",
      "you’ll probably want to confirm that’s true by talking to the pod. You’ll do that in a\n",
      "minute. First, you’ll look at the app’s log to check for any errors.\n",
      "3.2.4\n",
      "Viewing application logs\n",
      "Your little Node.js application logs to the process’s standard output. Containerized\n",
      "applications usually log to the standard output and standard error stream instead of\n",
      "   Containers  <[]Object> -required-\n",
      "     List of containers belonging to the pod. Containers cannot currently\n",
      "     Be added or removed. There must be at least one container in a pod.\n",
      "     Cannot be updated. More info:\n",
      "     http://releases.k8s.io/release-1.4/docs/user-guide/containers.md\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 98, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "66\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "writing their logs to files. This is to allow users to view logs of different applications in\n",
      "a simple, standard way. \n",
      " The container runtime (Docker in your case) redirects those streams to files and\n",
      "allows you to get the container’s log by running\n",
      "$ docker logs <container id>\n",
      "You could use ssh to log into the node where your pod is running and retrieve its logs\n",
      "with docker logs, but Kubernetes provides an easier way. \n",
      "RETRIEVING A POD’S LOG WITH KUBECTL LOGS\n",
      "To see your pod’s log (more precisely, the container’s log) you run the following com-\n",
      "mand on your local machine (no need to ssh anywhere):\n",
      "$ kubectl logs kubia-manual\n",
      "Kubia server starting...\n",
      "You haven’t sent any web requests to your Node.js app, so the log only shows a single\n",
      "log statement about the server starting up. As you can see, retrieving logs of an appli-\n",
      "cation running in Kubernetes is incredibly simple if the pod only contains a single\n",
      "container. \n",
      "NOTE\n",
      "Container logs are automatically rotated daily and every time the log file\n",
      "reaches 10MB in size. The kubectl logs command only shows the log entries\n",
      "from the last rotation.\n",
      "SPECIFYING THE CONTAINER NAME WHEN GETTING LOGS OF A MULTI-CONTAINER POD\n",
      "If your pod includes multiple containers, you have to explicitly specify the container\n",
      "name by including the -c <container name> option when running kubectl logs. In\n",
      "your kubia-manual pod, you set the container’s name to kubia, so if additional con-\n",
      "tainers exist in the pod, you’d have to get its logs like this:\n",
      "$ kubectl logs kubia-manual -c kubia\n",
      "Kubia server starting...\n",
      "Note that you can only retrieve container logs of pods that are still in existence. When\n",
      "a pod is deleted, its logs are also deleted. To make a pod’s logs available even after the\n",
      "pod is deleted, you need to set up centralized, cluster-wide logging, which stores all\n",
      "the logs into a central store. Chapter 17 explains how centralized logging works.\n",
      "3.2.5\n",
      "Sending requests to the pod\n",
      "The pod is now running—at least that’s what kubectl get and your app’s log say. But\n",
      "how do you see it in action? In the previous chapter, you used the kubectl expose\n",
      "command to create a service to gain access to the pod externally. You’re not going to\n",
      "do that now, because a whole chapter is dedicated to services, and you have other ways\n",
      "of connecting to a pod for testing and debugging purposes. One of them is through\n",
      "port forwarding.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 99, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "67\n",
      "Organizing pods with labels\n",
      "FORWARDING A LOCAL NETWORK PORT TO A PORT IN THE POD\n",
      "When you want to talk to a specific pod without going through a service (for debug-\n",
      "ging or other reasons), Kubernetes allows you to configure port forwarding to the\n",
      "pod. This is done through the kubectl port-forward command. The following\n",
      "command will forward your machine’s local port 8888 to port 8080 of your kubia-\n",
      "manual pod:\n",
      "$ kubectl port-forward kubia-manual 8888:8080\n",
      "... Forwarding from 127.0.0.1:8888 -> 8080\n",
      "... Forwarding from [::1]:8888 -> 8080\n",
      "The port forwarder is running and you can now connect to your pod through the\n",
      "local port. \n",
      "CONNECTING TO THE POD THROUGH THE PORT FORWARDER\n",
      "In a different terminal, you can now use curl to send an HTTP request to your pod\n",
      "through the kubectl port-forward proxy running on localhost:8888:\n",
      "$ curl localhost:8888\n",
      "You’ve hit kubia-manual\n",
      "Figure 3.5 shows an overly simplified view of what happens when you send the request.\n",
      "In reality, a couple of additional components sit between the kubectl process and the\n",
      "pod, but they aren’t relevant right now.\n",
      "Using port forwarding like this is an effective way to test an individual pod. You’ll\n",
      "learn about other similar methods throughout the book. \n",
      "3.3\n",
      "Organizing pods with labels\n",
      "At this point, you have two pods running in your cluster. When deploying actual\n",
      "applications, most users will end up running many more pods. As the number of\n",
      "pods increases, the need for categorizing them into subsets becomes more and\n",
      "more evident.\n",
      " For example, with microservices architectures, the number of deployed microser-\n",
      "vices can easily exceed 20 or more. Those components will probably be replicated\n",
      "Kubernetes cluster\n",
      "Port\n",
      "8080\n",
      "Local machine\n",
      "kubectl\n",
      "port-forward\n",
      "process\n",
      "curl\n",
      "Port\n",
      "8888\n",
      "Pod:\n",
      "kubia-manual\n",
      "Figure 3.5\n",
      "A simplified view of what happens when you use curl with kubectl port-forward\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 100, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "68\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "(multiple copies of the same component will be deployed) and multiple versions or\n",
      "releases (stable, beta, canary, and so on) will run concurrently. This can lead to hun-\n",
      "dreds of pods in the system. Without a mechanism for organizing them, you end up\n",
      "with a big, incomprehensible mess, such as the one shown in figure 3.6. The figure\n",
      "shows pods of multiple microservices, with several running multiple replicas, and others\n",
      "running different releases of the same microservice.\n",
      "It’s evident you need a way of organizing them into smaller groups based on arbitrary\n",
      "criteria, so every developer and system administrator dealing with your system can eas-\n",
      "ily see which pod is which. And you’ll want to operate on every pod belonging to a cer-\n",
      "tain group with a single action instead of having to perform the action for each pod\n",
      "individually. \n",
      " Organizing pods and all other Kubernetes objects is done through labels.\n",
      "3.3.1\n",
      "Introducing labels\n",
      "Labels are a simple, yet incredibly powerful, Kubernetes feature for organizing not\n",
      "only pods, but all other Kubernetes resources. A label is an arbitrary key-value pair you\n",
      "attach to a resource, which is then utilized when selecting resources using label selectors\n",
      "(resources are filtered based on whether they include the label specified in the selec-\n",
      "tor). A resource can have more than one label, as long as the keys of those labels are\n",
      "unique within that resource. You usually attach labels to resources when you create\n",
      "them, but you can also add additional labels or even modify the values of existing\n",
      "labels later without having to recreate the resource. \n",
      "UI pod\n",
      "UI pod\n",
      "UI pod\n",
      "Account\n",
      "Service\n",
      "pod\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Shopping\n",
      "Cart\n",
      "pod\n",
      "Shopping\n",
      "Cart\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "UI pod\n",
      "UI pod\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "Account\n",
      "Service\n",
      "pod\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "Figure 3.6\n",
      "Uncategorized pods in a microservices architecture\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 101, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "69\n",
      "Organizing pods with labels\n",
      " Let’s turn back to the microservices example from figure 3.6. By adding labels to\n",
      "those pods, you get a much-better-organized system that everyone can easily make\n",
      "sense of. Each pod is labeled with two labels:\n",
      "\n",
      "app, which specifies which app, component, or microservice the pod belongs to. \n",
      "\n",
      "rel, which shows whether the application running in the pod is a stable, beta,\n",
      "or a canary release.\n",
      "DEFINITION\n",
      "A canary release is when you deploy a new version of an applica-\n",
      "tion next to the stable version, and only let a small fraction of users hit the\n",
      "new version to see how it behaves before rolling it out to all users. This pre-\n",
      "vents bad releases from being exposed to too many users.\n",
      "By adding these two labels, you’ve essentially organized your pods into two dimen-\n",
      "sions (horizontally by app and vertically by release), as shown in figure 3.7.\n",
      "Every developer or ops person with access to your cluster can now easily see the sys-\n",
      "tem’s structure and where each pod fits in by looking at the pod’s labels.\n",
      "3.3.2\n",
      "Specifying labels when creating a pod\n",
      "Now, you’ll see labels in action by creating a new pod with two labels. Create a new file\n",
      "called kubia-manual-with-labels.yaml with the contents of the following listing.\n",
      "apiVersion: v1                                         \n",
      "kind: Pod                                              \n",
      "metadata:                                              \n",
      "  name: kubia-manual-v2\n",
      "Listing 3.3\n",
      "A pod with labels: kubia-manual-with-labels.yaml\n",
      "UI pod\n",
      "app: ui\n",
      "rel: stable\n",
      "rel=stable\n",
      "app=ui\n",
      "Account\n",
      "Service\n",
      "pod\n",
      "app: as\n",
      "rel: stable\n",
      "app=as\n",
      "app: pc\n",
      "rel: stable\n",
      "app=pc\n",
      "app: sc\n",
      "rel: stable\n",
      "app=sc\n",
      "app: os\n",
      "rel: stable\n",
      "app=os\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Shopping\n",
      "Cart\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "UI pod\n",
      "app: ui\n",
      "rel: beta\n",
      "rel=beta\n",
      "app: pc\n",
      "rel: beta\n",
      "app: os\n",
      "rel: beta\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "rel=canary\n",
      "Account\n",
      "Service\n",
      "pod\n",
      "app: as\n",
      "rel: canary\n",
      "app: pc\n",
      "rel: canary\n",
      "app: os\n",
      "rel: canary\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "Figure 3.7\n",
      "Organizing pods in a microservices architecture with pod labels\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 102, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "70\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "  labels:    \n",
      "    creation_method: manual          \n",
      "    env: prod                        \n",
      "spec: \n",
      "  containers: \n",
      "  - image: luksa/kubia\n",
      "    name: kubia\n",
      "    ports: \n",
      "    - containerPort: 8080\n",
      "      protocol: TCP\n",
      "You’ve included the labels creation_method=manual and env=data.labels section.\n",
      "You’ll create this pod now:\n",
      "$ kubectl create -f kubia-manual-with-labels.yaml\n",
      "pod \"kubia-manual-v2\" created\n",
      "The kubectl get pods command doesn’t list any labels by default, but you can see\n",
      "them by using the --show-labels switch:\n",
      "$ kubectl get po --show-labels\n",
      "NAME            READY  STATUS   RESTARTS  AGE LABELS\n",
      "kubia-manual    1/1    Running  0         16m <none>\n",
      "kubia-manual-v2 1/1    Running  0         2m  creat_method=manual,env=prod\n",
      "kubia-zxzij     1/1    Running  0         1d  run=kubia\n",
      "Instead of listing all labels, if you’re only interested in certain labels, you can specify\n",
      "them with the -L switch and have each displayed in its own column. List pods again\n",
      "and show the columns for the two labels you’ve attached to your kubia-manual-v2 pod:\n",
      "$ kubectl get po -L creation_method,env\n",
      "NAME            READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV\n",
      "kubia-manual    1/1     Running   0          16m   <none>            <none>\n",
      "kubia-manual-v2 1/1     Running   0          2m    manual            prod\n",
      "kubia-zxzij     1/1     Running   0          1d    <none>            <none>\n",
      "3.3.3\n",
      "Modifying labels of existing pods\n",
      "Labels can also be added to and modified on existing pods. Because the kubia-man-\n",
      "ual pod was also created manually, let’s add the creation_method=manual label to it: \n",
      "$ kubectl label po kubia-manual creation_method=manual\n",
      "pod \"kubia-manual\" labeled\n",
      "Now, let’s also change the env=prod label to env=debug on the kubia-manual-v2 pod,\n",
      "to see how existing labels can be changed.\n",
      "NOTE\n",
      "You need to use the --overwrite option when changing existing labels.\n",
      "$ kubectl label po kubia-manual-v2 env=debug --overwrite\n",
      "pod \"kubia-manual-v2\" labeled\n",
      "Two labels are \n",
      "attached to the pod.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 103, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "71\n",
      "Listing subsets of pods through label selectors\n",
      "List the pods again to see the updated labels:\n",
      "$ kubectl get po -L creation_method,env\n",
      "NAME            READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV\n",
      "kubia-manual    1/1     Running   0          16m   manual            <none>\n",
      "kubia-manual-v2 1/1     Running   0          2m    manual            debug\n",
      "kubia-zxzij     1/1     Running   0          1d    <none>            <none>\n",
      "As you can see, attaching labels to resources is trivial, and so is changing them on\n",
      "existing resources. It may not be evident right now, but this is an incredibly powerful\n",
      "feature, as you’ll see in the next chapter. But first, let’s see what you can do with these\n",
      "labels, in addition to displaying them when listing pods.\n",
      "3.4\n",
      "Listing subsets of pods through label selectors\n",
      "Attaching labels to resources so you can see the labels next to each resource when list-\n",
      "ing them isn’t that interesting. But labels go hand in hand with label selectors. Label\n",
      "selectors allow you to select a subset of pods tagged with certain labels and perform an\n",
      "operation on those pods. A label selector is a criterion, which filters resources based\n",
      "on whether they include a certain label with a certain value. \n",
      " A label selector can select resources based on whether the resource\n",
      "Contains (or doesn’t contain) a label with a certain key\n",
      "Contains a label with a certain key and value\n",
      "Contains a label with a certain key, but with a value not equal to the one you\n",
      "specify\n",
      "3.4.1\n",
      "Listing pods using a label selector\n",
      "Let’s use label selectors on the pods you’ve created so far. To see all pods you created\n",
      "manually (you labeled them with creation_method=manual), do the following:\n",
      "$ kubectl get po -l creation_method=manual\n",
      "NAME              READY     STATUS    RESTARTS   AGE\n",
      "kubia-manual      1/1       Running   0          51m\n",
      "kubia-manual-v2   1/1       Running   0          37m\n",
      "To list all pods that include the env label, whatever its value is:\n",
      "$ kubectl get po -l env\n",
      "NAME              READY     STATUS    RESTARTS   AGE\n",
      "kubia-manual-v2   1/1       Running   0          37m\n",
      "And those that don’t have the env label:\n",
      "$ kubectl get po -l '!env'\n",
      "NAME           READY     STATUS    RESTARTS   AGE\n",
      "kubia-manual   1/1       Running   0          51m\n",
      "kubia-zxzij    1/1       Running   0          10d\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 104, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "72\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "NOTE\n",
      "Make sure to use single quotes around !env, so the bash shell doesn’t\n",
      "evaluate the exclamation mark.\n",
      "Similarly, you could also match pods with the following label selectors:\n",
      "\n",
      "creation_method!=manual to select pods with the creation_method label with\n",
      "any value other than manual\n",
      "\n",
      "env in (prod,devel) to select pods with the env label set to either prod or\n",
      "devel\n",
      "\n",
      "env notin (prod,devel) to select pods with the env label set to any value other\n",
      "than prod or devel\n",
      "Turning back to the pods in the microservices-oriented architecture example, you\n",
      "could select all pods that are part of the product catalog microservice by using the\n",
      "app=pc label selector (shown in the following figure).\n",
      "3.4.2\n",
      "Using multiple conditions in a label selector\n",
      "A selector can also include multiple comma-separated criteria. Resources need to\n",
      "match all of them to match the selector. If, for example, you want to select only pods\n",
      "running the beta release of the product catalog microservice, you’d use the following\n",
      "selector: app=pc,rel=beta (visualized in figure 3.9).\n",
      " Label selectors aren’t useful only for listing pods, but also for performing actions\n",
      "on a subset of all pods. For example, later in the chapter, you’ll see how to use label\n",
      "selectors to delete multiple pods at once. But label selectors aren’t used only by\n",
      "kubectl. They’re also used internally, as you’ll see next.\n",
      "UI pod\n",
      "app: ui\n",
      "rel: stable\n",
      "rel=stable\n",
      "app=ui\n",
      "Account\n",
      "Service\n",
      "pod\n",
      "app: as\n",
      "rel: stable\n",
      "app=as\n",
      "app: pc\n",
      "rel: stable\n",
      "app=pc\n",
      "app: sc\n",
      "rel: stable\n",
      "app=sc\n",
      "app: os\n",
      "rel: stable\n",
      "app=os\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Shopping\n",
      "Cart\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "UI pod\n",
      "app: ui\n",
      "rel: beta\n",
      "rel=beta\n",
      "app: pc\n",
      "rel: beta\n",
      "app: os\n",
      "rel: beta\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "rel=canary\n",
      "Account\n",
      "Service\n",
      "pod\n",
      "app: as\n",
      "rel: canary\n",
      "app: pc\n",
      "rel: canary\n",
      "app: os\n",
      "rel: canary\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "Figure 3.8\n",
      "Selecting the product catalog microservice pods using the “app=pc” label selector\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 105, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "73\n",
      "Using labels and selectors to constrain pod scheduling\n",
      "3.5\n",
      "Using labels and selectors to constrain pod scheduling\n",
      "All the pods you’ve created so far have been scheduled pretty much randomly across\n",
      "your worker nodes. As I’ve mentioned in the previous chapter, this is the proper way\n",
      "of working in a Kubernetes cluster. Because Kubernetes exposes all the nodes in the\n",
      "cluster as a single, large deployment platform, it shouldn’t matter to you what node a\n",
      "pod is scheduled to. Because each pod gets the exact amount of computational\n",
      "resources it requests (CPU, memory, and so on) and its accessibility from other pods\n",
      "isn’t at all affected by the node the pod is scheduled to, usually there shouldn’t be any\n",
      "need for you to tell Kubernetes exactly where to schedule your pods. \n",
      " Certain cases exist, however, where you’ll want to have at least a little say in where\n",
      "a pod should be scheduled. A good example is when your hardware infrastructure\n",
      "isn’t homogenous. If part of your worker nodes have spinning hard drives, whereas\n",
      "others have SSDs, you may want to schedule certain pods to one group of nodes and\n",
      "the rest to the other. Another example is when you need to schedule pods perform-\n",
      "ing intensive GPU-based computation only to nodes that provide the required GPU\n",
      "acceleration. \n",
      " You never want to say specifically what node a pod should be scheduled to, because\n",
      "that would couple the application to the infrastructure, whereas the whole idea of\n",
      "Kubernetes is hiding the actual infrastructure from the apps that run on it. But if you\n",
      "want to have a say in where a pod should be scheduled, instead of specifying an exact\n",
      "node, you should describe the node requirements and then let Kubernetes select a\n",
      "node that matches those requirements. This can be done through node labels and\n",
      "node label selectors. \n",
      "UI pod\n",
      "app: ui\n",
      "rel: stable\n",
      "rel=stable\n",
      "app=ui\n",
      "Account\n",
      "Service\n",
      "pod\n",
      "app: as\n",
      "rel: stable\n",
      "app=as\n",
      "app: pc\n",
      "rel: stable\n",
      "app=pc\n",
      "app: sc\n",
      "rel: stable\n",
      "app=sc\n",
      "app: os\n",
      "rel: stable\n",
      "app=os\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Shopping\n",
      "Cart\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "UI pod\n",
      "app: ui\n",
      "rel: beta\n",
      "rel=beta\n",
      "app: pc\n",
      "rel: beta\n",
      "app: os\n",
      "rel: beta\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "rel=canary\n",
      "Account\n",
      "Service\n",
      "pod\n",
      "app: as\n",
      "rel: canary\n",
      "app: pc\n",
      "rel: canary\n",
      "app: os\n",
      "rel: canary\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "Figure 3.9\n",
      "Selecting pods with multiple label selectors\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 106, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "74\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "3.5.1\n",
      "Using labels for categorizing worker nodes\n",
      "As you learned earlier, pods aren’t the only Kubernetes resource type that you can\n",
      "attach a label to. Labels can be attached to any Kubernetes object, including nodes.\n",
      "Usually, when the ops team adds a new node to the cluster, they’ll categorize the node\n",
      "by attaching labels specifying the type of hardware the node provides or anything else\n",
      "that may come in handy when scheduling pods. \n",
      " Let’s imagine one of the nodes in your cluster contains a GPU meant to be used\n",
      "for general-purpose GPU computing. You want to add a label to the node showing this\n",
      "feature. You’re going to add the label gpu=true to one of your nodes (pick one out of\n",
      "the list returned by kubectl get nodes):\n",
      "$ kubectl label node gke-kubia-85f6-node-0rrx gpu=true\n",
      "node \"gke-kubia-85f6-node-0rrx\" labeled\n",
      "Now you can use a label selector when listing the nodes, like you did before with pods.\n",
      "List only nodes that include the label gpu=true:\n",
      "$ kubectl get nodes -l gpu=true\n",
      "NAME                      STATUS AGE\n",
      "gke-kubia-85f6-node-0rrx  Ready  1d\n",
      "As expected, only one node has this label. You can also try listing all the nodes and tell\n",
      "kubectl to display an additional column showing the values of each node’s gpu label\n",
      "(kubectl get nodes -L gpu).\n",
      "3.5.2\n",
      "Scheduling pods to specific nodes\n",
      "Now imagine you want to deploy a new pod that needs a GPU to perform its work.\n",
      "To ask the scheduler to only choose among the nodes that provide a GPU, you’ll\n",
      "add a node selector to the pod’s YAML. Create a file called kubia-gpu.yaml with the\n",
      "following listing’s contents and then use kubectl create -f kubia-gpu.yaml to cre-\n",
      "ate the pod.\n",
      "apiVersion: v1                                         \n",
      "kind: Pod                                              \n",
      "metadata:                                              \n",
      "  name: kubia-gpu\n",
      "spec: \n",
      "  nodeSelector:               \n",
      "    gpu: \"true\"               \n",
      "  containers: \n",
      "  - image: luksa/kubia\n",
      "    name: kubia\n",
      "Listing 3.4\n",
      "Using a label selector to schedule a pod to a specific node: kubia-gpu.yaml\n",
      "nodeSelector tells Kubernetes \n",
      "to deploy this pod only to \n",
      "nodes containing the \n",
      "gpu=true label.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 107, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "75\n",
      "Annotating pods\n",
      "You’ve added a nodeSelector field under the spec section. When you create the pod,\n",
      "the scheduler will only choose among the nodes that contain the gpu=true label\n",
      "(which is only a single node in your case). \n",
      "3.5.3\n",
      "Scheduling to one specific node\n",
      "Similarly, you could also schedule a pod to an exact node, because each node also has\n",
      "a unique label with the key kubernetes.io/hostname and value set to the actual host-\n",
      "name of the node. But setting the nodeSelector to a specific node by the hostname\n",
      "label may lead to the pod being unschedulable if the node is offline. You shouldn’t\n",
      "think in terms of individual nodes. Always think about logical groups of nodes that sat-\n",
      "isfy certain criteria specified through label selectors.\n",
      " This was a quick demonstration of how labels and label selectors work and how\n",
      "they can be used to influence the operation of Kubernetes. The importance and use-\n",
      "fulness of label selectors will become even more evident when we talk about Replication-\n",
      "Controllers and Services in the next two chapters. \n",
      "NOTE\n",
      "Additional ways of influencing which node a pod is scheduled to are\n",
      "covered in chapter 16.\n",
      "3.6\n",
      "Annotating pods\n",
      "In addition to labels, pods and other objects can also contain annotations. Annotations\n",
      "are also key-value pairs, so in essence, they’re similar to labels, but they aren’t meant to\n",
      "hold identifying information. They can’t be used to group objects the way labels can.\n",
      "While objects can be selected through label selectors, there’s no such thing as an\n",
      "annotation selector. \n",
      " On the other hand, annotations can hold much larger pieces of information and\n",
      "are primarily meant to be used by tools. Certain annotations are automatically added\n",
      "to objects by Kubernetes, but others are added by users manually.\n",
      " Annotations are also commonly used when introducing new features to Kuberne-\n",
      "tes. Usually, alpha and beta versions of new features don’t introduce any new fields to\n",
      "API objects. Annotations are used instead of fields, and then once the required API\n",
      "changes have become clear and been agreed upon by the Kubernetes developers, new\n",
      "fields are introduced and the related annotations deprecated.\n",
      " A great use of annotations is adding descriptions for each pod or other API object,\n",
      "so that everyone using the cluster can quickly look up information about each individ-\n",
      "ual object. For example, an annotation used to specify the name of the person who\n",
      "created the object can make collaboration between everyone working on the cluster\n",
      "much easier.\n",
      "3.6.1\n",
      "Looking up an object’s annotations\n",
      "Let’s see an example of an annotation that Kubernetes added automatically to the\n",
      "pod you created in the previous chapter. To see the annotations, you’ll need to\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 108, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "76\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "request the full YAML of the pod or use the kubectl describe command. You’ll use the\n",
      "first option in the following listing.\n",
      "$ kubectl get po kubia-zxzij -o yaml\n",
      "apiVersion: v1\n",
      "kind: pod\n",
      "metadata:\n",
      "  annotations:\n",
      "    kubernetes.io/created-by: |\n",
      "      {\"kind\":\"SerializedReference\", \"apiVersion\":\"v1\", \n",
      "      \"reference\":{\"kind\":\"ReplicationController\", \"namespace\":\"default\", ...\n",
      "Without going into too many details, as you can see, the kubernetes.io/created-by\n",
      "annotation holds JSON data about the object that created the pod. That’s not some-\n",
      "thing you’d want to put into a label. Labels should be short, whereas annotations can\n",
      "contain relatively large blobs of data (up to 256 KB in total).\n",
      "NOTE\n",
      "The kubernetes.io/created-by annotations was deprecated in ver-\n",
      "sion 1.8 and will be removed in 1.9, so you will no longer see it in the YAML.\n",
      "3.6.2\n",
      "Adding and modifying annotations\n",
      "Annotations can obviously be added to pods at creation time, the same way labels can.\n",
      "They can also be added to or modified on existing pods later. The simplest way to add\n",
      "an annotation to an existing object is through the kubectl annotate command. \n",
      " You’ll try adding an annotation to your kubia-manual pod now:\n",
      "$ kubectl annotate pod kubia-manual mycompany.com/someannotation=\"foo bar\"\n",
      "pod \"kubia-manual\" annotated\n",
      "You added the annotation mycompany.com/someannotation with the value foo bar.\n",
      "It’s a good idea to use this format for annotation keys to prevent key collisions. When\n",
      "different tools or libraries add annotations to objects, they may accidentally override\n",
      "each other’s annotations if they don’t use unique prefixes like you did here.\n",
      " You can use kubectl describe to see the annotation you added:\n",
      "$ kubectl describe pod kubia-manual\n",
      "...\n",
      "Annotations:    mycompany.com/someannotation=foo bar\n",
      "...\n",
      "3.7\n",
      "Using namespaces to group resources\n",
      "Let’s turn back to labels for a moment. We’ve seen how they organize pods and other\n",
      "objects into groups. Because each object can have multiple labels, those groups of\n",
      "objects can overlap. Plus, when working with the cluster (through kubectl for example),\n",
      "if you don’t explicitly specify a label selector, you’ll always see all objects. \n",
      "Listing 3.5\n",
      "A pod’s annotations\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 109, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "77\n",
      "Using namespaces to group resources\n",
      " But what about times when you want to split objects into separate, non-overlapping\n",
      "groups? You may want to only operate inside one group at a time. For this and other\n",
      "reasons, Kubernetes also groups objects into namespaces. These aren’t the Linux\n",
      "namespaces we talked about in chapter 2, which are used to isolate processes from\n",
      "each other. Kubernetes namespaces provide a scope for objects names. Instead of hav-\n",
      "ing all your resources in one single namespace, you can split them into multiple name-\n",
      "spaces, which also allows you to use the same resource names multiple times (across\n",
      "different namespaces).\n",
      "3.7.1\n",
      "Understanding the need for namespaces\n",
      "Using multiple namespaces allows you to split complex systems with numerous com-\n",
      "ponents into smaller distinct groups. They can also be used for separating resources\n",
      "in a multi-tenant environment, splitting up resources into production, development,\n",
      "and QA environments, or in any other way you may need. Resource names only need\n",
      "to be unique within a namespace. Two different namespaces can contain resources of\n",
      "the same name. But, while most types of resources are namespaced, a few aren’t. One\n",
      "of them is the Node resource, which is global and not tied to a single namespace.\n",
      "You’ll learn about other cluster-level resources in later chapters.\n",
      " Let’s see how to use namespaces now.\n",
      "3.7.2\n",
      "Discovering other namespaces and their pods\n",
      "First, let’s list all namespaces in your cluster:\n",
      "$ kubectl get ns\n",
      "NAME          LABELS    STATUS    AGE\n",
      "default       <none>    Active    1h\n",
      "kube-public   <none>    Active    1h\n",
      "kube-system   <none>    Active    1h\n",
      "Up to this point, you’ve operated only in the default namespace. When listing resources\n",
      "with the kubectl get command, you’ve never specified the namespace explicitly, so\n",
      "kubectl always defaulted to the default namespace, showing you only the objects in\n",
      "that namespace. But as you can see from the list, the kube-public and the kube-system\n",
      "namespaces also exist. Let’s look at the pods that belong to the kube-system name-\n",
      "space, by telling kubectl to list pods in that namespace only:\n",
      "$ kubectl get po --namespace kube-system\n",
      "NAME                                 READY     STATUS    RESTARTS   AGE\n",
      "fluentd-cloud-kubia-e8fe-node-txje   1/1       Running   0          1h\n",
      "heapster-v11-fz1ge                   1/1       Running   0          1h\n",
      "kube-dns-v9-p8a4t                    0/4       Pending   0          1h\n",
      "kube-ui-v4-kdlai                     1/1       Running   0          1h\n",
      "l7-lb-controller-v0.5.2-bue96        2/2       Running   92         1h\n",
      "TIP\n",
      "You can also use -n instead of --namespace.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 110, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "78\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "You’ll learn about these pods later in the book (don’t worry if the pods shown here\n",
      "don’t match the ones on your system exactly). It’s clear from the name of the name-\n",
      "space that these are resources related to the Kubernetes system itself. By having\n",
      "them in this separate namespace, it keeps everything nicely organized. If they were\n",
      "all in the default namespace, mixed in with the resources you create yourself, you’d\n",
      "have a hard time seeing what belongs where, and you might inadvertently delete sys-\n",
      "tem resources. \n",
      " Namespaces enable you to separate resources that don’t belong together into non-\n",
      "overlapping groups. If several users or groups of users are using the same Kubernetes\n",
      "cluster, and they each manage their own distinct set of resources, they should each use\n",
      "their own namespace. This way, they don’t need to take any special care not to inad-\n",
      "vertently modify or delete the other users’ resources and don’t need to concern them-\n",
      "selves with name conflicts, because namespaces provide a scope for resource names,\n",
      "as has already been mentioned.\n",
      "  Besides isolating resources, namespaces are also used for allowing only certain users\n",
      "access to particular resources and even for limiting the amount of computational\n",
      "resources available to individual users. You’ll learn about this in chapters 12 through 14.\n",
      "3.7.3\n",
      "Creating a namespace\n",
      "A namespace is a Kubernetes resource like any other, so you can create it by posting a\n",
      "YAML file to the Kubernetes API server. Let’s see how to do this now. \n",
      "CREATING A NAMESPACE FROM A YAML FILE\n",
      "First, create a custom-namespace.yaml file with the following listing’s contents (you’ll\n",
      "find the file in the book’s code archive).\n",
      "apiVersion: v1\n",
      "kind: Namespace         \n",
      "metadata:\n",
      "  name: custom-namespace  \n",
      "Now, use kubectl to post the file to the Kubernetes API server:\n",
      "$ kubectl create -f custom-namespace.yaml\n",
      "namespace \"custom-namespace\" created\n",
      "CREATING A NAMESPACE WITH KUBECTL CREATE NAMESPACE\n",
      "Although writing a file like the previous one isn’t a big deal, it’s still a hassle. Luckily,\n",
      "you can also create namespaces with the dedicated kubectl create namespace com-\n",
      "mand, which is quicker than writing a YAML file. By having you create a YAML mani-\n",
      "fest for the namespace, I wanted to reinforce the idea that everything in Kubernetes\n",
      "Listing 3.6\n",
      "A YAML definition of a namespace: custom-namespace.yaml\n",
      "This says you’re \n",
      "defining a namespace.\n",
      "This is the name \n",
      "of the namespace.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 111, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "79\n",
      "Using namespaces to group resources\n",
      "has a corresponding API object that you can create, read, update, and delete by post-\n",
      "ing a YAML manifest to the API server.\n",
      " You could have created the namespace like this:\n",
      "$ kubectl create namespace custom-namespace\n",
      "namespace \"custom-namespace\" created\n",
      "NOTE\n",
      "Although most objects’ names must conform to the naming conven-\n",
      "tions specified in RFC 1035 (Domain names), which means they may contain\n",
      "only letters, digits, dashes, and dots, namespaces (and a few others) aren’t\n",
      "allowed to contain dots. \n",
      "3.7.4\n",
      "Managing objects in other namespaces\n",
      "To create resources in the namespace you’ve created, either add a namespace: custom-\n",
      "namespace entry to the metadata section, or specify the namespace when creating the\n",
      "resource with the kubectl create command:\n",
      "$ kubectl create -f kubia-manual.yaml -n custom-namespace\n",
      "pod \"kubia-manual\" created\n",
      "You now have two pods with the same name (kubia-manual). One is in the default\n",
      "namespace, and the other is in your custom-namespace.\n",
      " When listing, describing, modifying, or deleting objects in other namespaces, you\n",
      "need to pass the --namespace (or -n) flag to kubectl. If you don’t specify the name-\n",
      "space, kubectl performs the action in the default namespace configured in the cur-\n",
      "rent kubectl context. The current context’s namespace and the current context itself\n",
      "can be changed through kubectl config commands. To learn more about managing\n",
      "kubectl contexts, refer to appendix A. \n",
      "TIP\n",
      "To quickly switch to a different namespace, you can set up the following\n",
      "alias: alias kcd='kubectl config set-context $(kubectl config current-\n",
      "context) --namespace '. You can then switch between namespaces using kcd\n",
      "some-namespace.\n",
      "3.7.5\n",
      "Understanding the isolation provided by namespaces\n",
      "To wrap up this section about namespaces, let me explain what namespaces don’t pro-\n",
      "vide—at least not out of the box. Although namespaces allow you to isolate objects\n",
      "into distinct groups, which allows you to operate only on those belonging to the speci-\n",
      "fied namespace, they don’t provide any kind of isolation of running objects. \n",
      " For example, you may think that when different users deploy pods across different\n",
      "namespaces, those pods are isolated from each other and can’t communicate, but that’s\n",
      "not necessarily the case. Whether namespaces provide network isolation depends on\n",
      "which networking solution is deployed with Kubernetes. When the solution doesn’t\n",
      "provide inter-namespace network isolation, if a pod in namespace foo knows the IP\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 112, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "80\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "address of a pod in namespace bar, there is nothing preventing it from sending traffic,\n",
      "such as HTTP requests, to the other pod. \n",
      "3.8\n",
      "Stopping and removing pods\n",
      "You’ve created a number of pods, which should all still be running. You have four\n",
      "pods running in the default namespace and one pod in custom-namespace. You’re\n",
      "going to stop all of them now, because you don’t need them anymore.\n",
      "3.8.1\n",
      "Deleting a pod by name\n",
      "First, delete the kubia-gpu pod by name:\n",
      "$ kubectl delete po kubia-gpu\n",
      "pod \"kubia-gpu\" deleted\n",
      "By deleting a pod, you’re instructing Kubernetes to terminate all the containers that are\n",
      "part of that pod. Kubernetes sends a SIGTERM signal to the process and waits a certain\n",
      "number of seconds (30 by default) for it to shut down gracefully. If it doesn’t shut down\n",
      "in time, the process is then killed through SIGKILL. To make sure your processes are\n",
      "always shut down gracefully, they need to handle the SIGTERM signal properly. \n",
      "TIP\n",
      "You can also delete more than one pod by specifying multiple, space-sep-\n",
      "arated names (for example, kubectl delete po pod1 pod2).\n",
      "3.8.2\n",
      "Deleting pods using label selectors\n",
      "Instead of specifying each pod to delete by name, you’ll now use what you’ve learned\n",
      "about label selectors to stop both the kubia-manual and the kubia-manual-v2 pod.\n",
      "Both pods include the creation_method=manual label, so you can delete them by\n",
      "using a label selector:\n",
      "$ kubectl delete po -l creation_method=manual\n",
      "pod \"kubia-manual\" deleted\n",
      "pod \"kubia-manual-v2\" deleted \n",
      "In the earlier microservices example, where you had tens (or possibly hundreds) of\n",
      "pods, you could, for instance, delete all canary pods at once by specifying the\n",
      "rel=canary label selector (visualized in figure 3.10):\n",
      "$ kubectl delete po -l rel=canary\n",
      "3.8.3\n",
      "Deleting pods by deleting the whole namespace\n",
      "Okay, back to your real pods. What about the pod in the custom-namespace? You no\n",
      "longer need either the pods in that namespace, or the namespace itself. You can\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 113, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "81\n",
      "Stopping and removing pods\n",
      "delete the whole namespace (the pods will be deleted along with the namespace auto-\n",
      "matically), using the following command:\n",
      "$ kubectl delete ns custom-namespace\n",
      "namespace \"custom-namespace\" deleted\n",
      "3.8.4\n",
      "Deleting all pods in a namespace, while keeping the namespace\n",
      "You’ve now cleaned up almost everything. But what about the pod you created with\n",
      "the kubectl run command in chapter 2? That one is still running:\n",
      "$ kubectl get pods\n",
      "NAME            READY   STATUS    RESTARTS   AGE\n",
      "kubia-zxzij     1/1     Running   0          1d    \n",
      "This time, instead of deleting the specific pod, tell Kubernetes to delete all pods in the\n",
      "current namespace by using the --all option:\n",
      "$ kubectl delete po --all\n",
      "pod \"kubia-zxzij\" deleted\n",
      "Now, double check that no pods were left running:\n",
      "$ kubectl get pods\n",
      "NAME            READY   STATUS        RESTARTS   AGE\n",
      "kubia-09as0     1/1     Running       0          1d    \n",
      "kubia-zxzij     1/1     Terminating   0          1d    \n",
      "UI pod\n",
      "app: ui\n",
      "rel: stable\n",
      "rel=stable\n",
      "app=ui\n",
      "Account\n",
      "Service\n",
      "pod\n",
      "app: as\n",
      "rel: stable\n",
      "app=as\n",
      "app: pc\n",
      "rel: stable\n",
      "app=pc\n",
      "app: sc\n",
      "rel: stable\n",
      "app=sc\n",
      "app: os\n",
      "rel: stable\n",
      "app=os\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Shopping\n",
      "Cart\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "UI pod\n",
      "app: ui\n",
      "rel: beta\n",
      "rel=beta\n",
      "app: pc\n",
      "rel: beta\n",
      "app: os\n",
      "rel: beta\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "rel=canary\n",
      "Account\n",
      "Service\n",
      "pod\n",
      "app: as\n",
      "rel: canary\n",
      "app: pc\n",
      "rel: canary\n",
      "app: os\n",
      "rel: canary\n",
      "Product\n",
      "Catalog\n",
      "pod\n",
      "Order\n",
      "Service\n",
      "pod\n",
      "Figure 3.10\n",
      "Selecting and deleting all canary pods through the rel=canary label selector\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 114, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "82\n",
      "CHAPTER 3\n",
      "Pods: running containers in Kubernetes\n",
      "Wait, what!?! The kubia-zxzij pod is terminating, but a new pod called kubia-09as0,\n",
      "which wasn’t there before, has appeared. No matter how many times you delete all\n",
      "pods, a new pod called kubia-something will emerge. \n",
      " You may remember you created your first pod with the kubectl run command. In\n",
      "chapter 2, I mentioned that this doesn’t create a pod directly, but instead creates a\n",
      "ReplicationController, which then creates the pod. As soon as you delete a pod cre-\n",
      "ated by the ReplicationController, it immediately creates a new one. To delete the\n",
      "pod, you also need to delete the ReplicationController. \n",
      "3.8.5\n",
      "Deleting (almost) all resources in a namespace\n",
      "You can delete the ReplicationController and the pods, as well as all the Services\n",
      "you’ve created, by deleting all resources in the current namespace with a single\n",
      "command:\n",
      "$ kubectl delete all --all\n",
      "pod \"kubia-09as0\" deleted\n",
      "replicationcontroller \"kubia\" deleted\n",
      "service \"kubernetes\" deleted\n",
      "service \"kubia-http\" deleted\n",
      "The first all in the command specifies that you’re deleting resources of all types, and\n",
      "the --all option specifies that you’re deleting all resource instances instead of speci-\n",
      "fying them by name (you already used this option when you ran the previous delete\n",
      "command).\n",
      "NOTE\n",
      "Deleting everything with the all keyword doesn’t delete absolutely\n",
      "everything. Certain resources (like Secrets, which we’ll introduce in chapter 7)\n",
      "are preserved and need to be deleted explicitly.\n",
      "As it deletes resources, kubectl will print the name of every resource it deletes. In the\n",
      "list, you should see the kubia ReplicationController and the kubia-http Service you\n",
      "created in chapter 2. \n",
      "NOTE\n",
      "The kubectl delete all --all command also deletes the kubernetes\n",
      "Service, but it should be recreated automatically in a few moments.\n",
      "3.9\n",
      "Summary\n",
      "After reading this chapter, you should now have a decent knowledge of the central\n",
      "building block in Kubernetes. Every other concept you’ll learn about in the next few\n",
      "chapters is directly related to pods. \n",
      " In this chapter, you’ve learned\n",
      "How to decide whether certain containers should be grouped together in a pod\n",
      "or not.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 115, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "83\n",
      "Summary\n",
      "Pods can run multiple processes and are similar to physical hosts in the non-\n",
      "container world.\n",
      "YAML or JSON descriptors can be written and used to create pods and then\n",
      "examined to see the specification of a pod and its current state.\n",
      "Labels and label selectors should be used to organize pods and easily perform\n",
      "operations on multiple pods at once.\n",
      "You can use node labels and selectors to schedule pods only to nodes that have\n",
      "certain features.\n",
      "Annotations allow attaching larger blobs of data to pods either by people or\n",
      "tools and libraries.\n",
      "Namespaces can be used to allow different teams to use the same cluster as\n",
      "though they were using separate Kubernetes clusters.\n",
      "How to use the kubectl explain command to quickly look up the information\n",
      "on any Kubernetes resource. \n",
      "In the next chapter, you’ll learn about ReplicationControllers and other resources\n",
      "that manage pods.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 116, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "84\n",
      "Replication and other\n",
      "controllers: deploying\n",
      "managed pods\n",
      "As you’ve learned so far, pods represent the basic deployable unit in Kubernetes.\n",
      "You know how to create, supervise, and manage them manually. But in real-world\n",
      "use cases, you want your deployments to stay up and running automatically and\n",
      "remain healthy without any manual intervention. To do this, you almost never cre-\n",
      "ate pods directly. Instead, you create other types of resources, such as Replication-\n",
      "Controllers or Deployments, which then create and manage the actual pods.\n",
      " When you create unmanaged pods (such as the ones you created in the previ-\n",
      "ous chapter), a cluster node is selected to run the pod and then its containers are\n",
      "run on that node. In this chapter, you’ll learn that Kubernetes then monitors\n",
      "This chapter covers\n",
      "Keeping pods healthy\n",
      "Running multiple instances of the same pod\n",
      "Automatically rescheduling pods after a node fails\n",
      "Scaling pods horizontally\n",
      "Running system-level pods on each cluster node\n",
      "Running batch jobs\n",
      "Scheduling jobs to run periodically or once in \n",
      "the future\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 117, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "85\n",
      "Keeping pods healthy\n",
      "those containers and automatically restarts them if they fail. But if the whole node\n",
      "fails, the pods on the node are lost and will not be replaced with new ones, unless\n",
      "those pods are managed by the previously mentioned ReplicationControllers or simi-\n",
      "lar. In this chapter, you’ll learn how Kubernetes checks if a container is still alive and\n",
      "restarts it if it isn’t. You’ll also learn how to run managed pods—both those that run\n",
      "indefinitely and those that perform a single task and then stop. \n",
      "4.1\n",
      "Keeping pods healthy\n",
      "One of the main benefits of using Kubernetes is the ability to give it a list of contain-\n",
      "ers and let it keep those containers running somewhere in the cluster. You do this by\n",
      "creating a Pod resource and letting Kubernetes pick a worker node for it and run\n",
      "the pod’s containers on that node. But what if one of those containers dies? What if\n",
      "all containers of a pod die? \n",
      " As soon as a pod is scheduled to a node, the Kubelet on that node will run its con-\n",
      "tainers and, from then on, keep them running as long as the pod exists. If the con-\n",
      "tainer’s main process crashes, the Kubelet will restart the container. If your\n",
      "application has a bug that causes it to crash every once in a while, Kubernetes will\n",
      "restart it automatically, so even without doing anything special in the app itself, run-\n",
      "ning the app in Kubernetes automatically gives it the ability to heal itself. \n",
      " But sometimes apps stop working without their process crashing. For example, a\n",
      "Java app with a memory leak will start throwing OutOfMemoryErrors, but the JVM\n",
      "process will keep running. It would be great to have a way for an app to signal to\n",
      "Kubernetes that it’s no longer functioning properly and have Kubernetes restart it. \n",
      " We’ve said that a container that crashes is restarted automatically, so maybe you’re\n",
      "thinking you could catch these types of errors in the app and exit the process when\n",
      "they occur. You can certainly do that, but it still doesn’t solve all your problems. \n",
      " For example, what about those situations when your app stops responding because\n",
      "it falls into an infinite loop or a deadlock? To make sure applications are restarted in\n",
      "such cases, you must check an application’s health from the outside and not depend\n",
      "on the app doing it internally. \n",
      "4.1.1\n",
      "Introducing liveness probes\n",
      "Kubernetes can check if a container is still alive through liveness probes. You can specify\n",
      "a liveness probe for each container in the pod’s specification. Kubernetes will periodi-\n",
      "cally execute the probe and restart the container if the probe fails. \n",
      "NOTE\n",
      "Kubernetes also supports readiness probes, which we’ll learn about in the\n",
      "next chapter. Be sure not to confuse the two. They’re used for two different\n",
      "things.\n",
      "Kubernetes can probe a container using one of the three mechanisms:\n",
      "An HTTP GET probe performs an HTTP GET request on the container’s IP\n",
      "address, a port and path you specify. If the probe receives a response, and the\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 118, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "86\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "response code doesn’t represent an error (in other words, if the HTTP response\n",
      "code is 2xx or 3xx), the probe is considered successful. If the server returns an\n",
      "error response code or if it doesn’t respond at all, the probe is considered a fail-\n",
      "ure and the container will be restarted as a result.\n",
      "A TCP Socket probe tries to open a TCP connection to the specified port of the\n",
      "container. If the connection is established successfully, the probe is successful.\n",
      "Otherwise, the container is restarted.\n",
      "An Exec probe executes an arbitrary command inside the container and checks\n",
      "the command’s exit status code. If the status code is 0, the probe is successful.\n",
      "All other codes are considered failures. \n",
      "4.1.2\n",
      "Creating an HTTP-based liveness probe\n",
      "Let’s see how to add a liveness probe to your Node.js app. Because it’s a web app, it\n",
      "makes sense to add a liveness probe that will check whether its web server is serving\n",
      "requests. But because this particular Node.js app is too simple to ever fail, you’ll need\n",
      "to make the app fail artificially. \n",
      " To properly demo liveness probes, you’ll modify the app slightly and make it\n",
      "return a 500 Internal Server Error HTTP status code for each request after the fifth\n",
      "one—your app will handle the first five client requests properly and then return an\n",
      "error on every subsequent request. Thanks to the liveness probe, it should be restarted\n",
      "when that happens, allowing it to properly handle client requests again.\n",
      " You can find the code of the new app in the book’s code archive (in the folder\n",
      "Chapter04/kubia-unhealthy). I’ve pushed the container image to Docker Hub, so you\n",
      "don’t need to build it yourself. \n",
      " You’ll create a new pod that includes an HTTP GET liveness probe. The following\n",
      "listing shows the YAML for the pod.\n",
      "apiVersion: v1\n",
      "kind: pod\n",
      "metadata:\n",
      "  name: kubia-liveness\n",
      "spec:\n",
      "  containers:\n",
      "  - image: luksa/kubia-unhealthy   \n",
      "    name: kubia\n",
      "    livenessProbe:                 \n",
      "      httpGet:                     \n",
      "        path: /                     \n",
      "        port: 8080       \n",
      "Listing 4.1\n",
      "Adding a liveness probe to a pod: kubia-liveness-probe.yaml\n",
      "This is the image \n",
      "containing the \n",
      "(somewhat) \n",
      "broken app.\n",
      "A liveness probe that will \n",
      "perform an HTTP GET\n",
      "The path to \n",
      "request in the \n",
      "HTTP request\n",
      "The network port\n",
      "the probe should\n",
      "connect to\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 119, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "87\n",
      "Keeping pods healthy\n",
      "The pod descriptor defines an httpGet liveness probe, which tells Kubernetes to peri-\n",
      "odically perform HTTP GET requests on path / on port 8080 to determine if the con-\n",
      "tainer is still healthy. These requests start as soon as the container is run.\n",
      " After five such requests (or actual client requests), your app starts returning\n",
      "HTTP status code 500, which Kubernetes will treat as a probe failure, and will thus\n",
      "restart the container. \n",
      "4.1.3\n",
      "Seeing a liveness probe in action\n",
      "To see what the liveness probe does, try creating the pod now. After about a minute and\n",
      "a half, the container will be restarted. You can see that by running kubectl get:\n",
      "$ kubectl get po kubia-liveness\n",
      "NAME             READY     STATUS    RESTARTS   AGE\n",
      "kubia-liveness   1/1       Running   1          2m\n",
      "The RESTARTS column shows that the pod’s container has been restarted once (if you\n",
      "wait another minute and a half, it gets restarted again, and then the cycle continues\n",
      "indefinitely).\n",
      "You can see why the container had to be restarted by looking at what kubectl describe\n",
      "prints out, as shown in the following listing.\n",
      "$ kubectl describe po kubia-liveness\n",
      "Name:           kubia-liveness\n",
      "...\n",
      "Containers:\n",
      "  kubia:\n",
      "    Container ID:       docker://480986f8\n",
      "    Image:              luksa/kubia-unhealthy\n",
      "    Image ID:           docker://sha256:2b208508\n",
      "    Port:\n",
      "    State:              Running                            \n",
      "      Started:          Sun, 14 May 2017 11:41:40 +0200    \n",
      "Obtaining the application log of a crashed container\n",
      "In the previous chapter, you learned how to print the application’s log with kubectl\n",
      "logs. If your container is restarted, the kubectl logs command will show the log of\n",
      "the current container. \n",
      "When you want to figure out why the previous container terminated, you’ll want to\n",
      "see those logs instead of the current container’s logs. This can be done by using\n",
      "the --previous option:\n",
      "$ kubectl logs mypod --previous\n",
      "Listing 4.2\n",
      "A pod’s description after its container is restarted\n",
      "The container is \n",
      "currently running.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 120, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "88\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "    Last State:         Terminated                         \n",
      "      Reason:           Error                              \n",
      "      Exit Code:        137                                \n",
      "      Started:          Mon, 01 Jan 0001 00:00:00 +0000    \n",
      "      Finished:         Sun, 14 May 2017 11:41:38 +0200    \n",
      "    Ready:              True\n",
      "    Restart Count:      1                                 \n",
      "    Liveness:           http-get http://:8080/ delay=0s timeout=1s\n",
      "                        period=10s #success=1 #failure=3\n",
      "    ...\n",
      "Events:\n",
      "... Killing container with id docker://95246981:pod \"kubia-liveness ...\"\n",
      "    container \"kubia\" is unhealthy, it will be killed and re-created.\n",
      "You can see that the container is currently running, but it previously terminated\n",
      "because of an error. The exit code was 137, which has a special meaning—it denotes\n",
      "that the process was terminated by an external signal. The number 137 is a sum of two\n",
      "numbers: 128+x, where x is the signal number sent to the process that caused it to ter-\n",
      "minate. In the example, x equals 9, which is the number of the SIGKILL signal, mean-\n",
      "ing the process was killed forcibly.\n",
      " The events listed at the bottom show why the container was killed—Kubernetes\n",
      "detected the container was unhealthy, so it killed and re-created it. \n",
      "NOTE\n",
      "When a container is killed, a completely new container is created—it’s\n",
      "not the same container being restarted again.\n",
      "4.1.4\n",
      "Configuring additional properties of the liveness probe\n",
      "You may have noticed that kubectl describe also displays additional information\n",
      "about the liveness probe:\n",
      "Liveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 \n",
      "          ➥ #failure=3\n",
      "Beside the liveness probe options you specified explicitly, you can also see additional\n",
      "properties, such as delay, timeout, period, and so on. The delay=0s part shows that\n",
      "the probing begins immediately after the container is started. The timeout is set to\n",
      "only 1 second, so the container must return a response in 1 second or the probe is\n",
      "counted as failed. The container is probed every 10 seconds (period=10s) and the\n",
      "container is restarted after the probe fails three consecutive times (#failure=3). \n",
      " These additional parameters can be customized when defining the probe. For\n",
      "example, to set the initial delay, add the initialDelaySeconds property to the live-\n",
      "ness probe as shown in the following listing.\n",
      "   livenessProbe:          \n",
      "     httpGet:              \n",
      "       path: /             \n",
      "Listing 4.3\n",
      "A liveness probe with an initial delay: kubia-liveness-probe-initial-delay.yaml\n",
      "The previous \n",
      "container terminated \n",
      "with an error and \n",
      "exited with code 137.\n",
      "The container \n",
      "has been \n",
      "restarted once.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 121, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "89\n",
      "Keeping pods healthy\n",
      "       port: 8080          \n",
      "     initialDelaySeconds: 15   \n",
      "If you don’t set the initial delay, the prober will start probing the container as soon as\n",
      "it starts, which usually leads to the probe failing, because the app isn’t ready to start\n",
      "receiving requests. If the number of failures exceeds the failure threshold, the con-\n",
      "tainer is restarted before it’s even able to start responding to requests properly. \n",
      "TIP\n",
      "Always remember to set an initial delay to account for your app’s startup\n",
      "time.\n",
      "I’ve seen this on many occasions and users were confused why their container was\n",
      "being restarted. But if they’d used kubectl describe, they’d have seen that the con-\n",
      "tainer terminated with exit code 137 or 143, telling them that the pod was terminated\n",
      "externally. Additionally, the listing of the pod’s events would show that the container\n",
      "was killed because of a failed liveness probe. If you see this happening at pod startup,\n",
      "it’s because you failed to set initialDelaySeconds appropriately.\n",
      "NOTE\n",
      "Exit code 137 signals that the process was killed by an external signal\n",
      "(exit code is 128 + 9 (SIGKILL). Likewise, exit code 143 corresponds to 128 +\n",
      "15 (SIGTERM).\n",
      "4.1.5\n",
      "Creating effective liveness probes\n",
      "For pods running in production, you should always define a liveness probe. Without\n",
      "one, Kubernetes has no way of knowing whether your app is still alive or not. As long\n",
      "as the process is still running, Kubernetes will consider the container to be healthy. \n",
      "WHAT A LIVENESS PROBE SHOULD CHECK\n",
      "Your simplistic liveness probe simply checks if the server is responding. While this may\n",
      "seem overly simple, even a liveness probe like this does wonders, because it causes the\n",
      "container to be restarted if the web server running within the container stops\n",
      "responding to HTTP requests. Compared to having no liveness probe, this is a major\n",
      "improvement, and may be sufficient in most cases.\n",
      " But for a better liveness check, you’d configure the probe to perform requests on a\n",
      "specific URL path (/health, for example) and have the app perform an internal sta-\n",
      "tus check of all the vital components running inside the app to ensure none of them\n",
      "has died or is unresponsive. \n",
      "TIP\n",
      "Make sure the /health HTTP endpoint doesn’t require authentication;\n",
      "otherwise the probe will always fail, causing your container to be restarted\n",
      "indefinitely.\n",
      "Be sure to check only the internals of the app and nothing influenced by an external\n",
      "factor. For example, a frontend web server’s liveness probe shouldn’t return a failure\n",
      "when the server can’t connect to the backend database. If the underlying cause is in\n",
      "the database itself, restarting the web server container will not fix the problem.\n",
      "Kubernetes will wait 15 seconds \n",
      "before executing the first probe.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 122, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "90\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "Because the liveness probe will fail again, you’ll end up with the container restarting\n",
      "repeatedly until the database becomes accessible again. \n",
      "KEEPING PROBES LIGHT\n",
      "Liveness probes shouldn’t use too many computational resources and shouldn’t take\n",
      "too long to complete. By default, the probes are executed relatively often and are\n",
      "only allowed one second to complete. Having a probe that does heavy lifting can slow\n",
      "down your container considerably. Later in the book, you’ll also learn about how to\n",
      "limit CPU time available to a container. The probe’s CPU time is counted in the con-\n",
      "tainer’s CPU time quota, so having a heavyweight liveness probe will reduce the CPU\n",
      "time available to the main application processes.\n",
      "TIP\n",
      "If you’re running a Java app in your container, be sure to use an HTTP\n",
      "GET liveness probe instead of an Exec probe, where you spin up a whole new\n",
      "JVM to get the liveness information. The same goes for any JVM-based or sim-\n",
      "ilar applications, whose start-up procedure requires considerable computa-\n",
      "tional resources.\n",
      "DON’T BOTHER IMPLEMENTING RETRY LOOPS IN YOUR PROBES\n",
      "You’ve already seen that the failure threshold for the probe is configurable and usu-\n",
      "ally the probe must fail multiple times before the container is killed. But even if you\n",
      "set the failure threshold to 1, Kubernetes will retry the probe several times before con-\n",
      "sidering it a single failed attempt. Therefore, implementing your own retry loop into\n",
      "the probe is wasted effort.\n",
      "LIVENESS PROBE WRAP-UP\n",
      "You now understand that Kubernetes keeps your containers running by restarting\n",
      "them if they crash or if their liveness probes fail. This job is performed by the Kubelet\n",
      "on the node hosting the pod—the Kubernetes Control Plane components running on\n",
      "the master(s) have no part in this process. \n",
      " But if the node itself crashes, it’s the Control Plane that must create replacements for\n",
      "all the pods that went down with the node. It doesn’t do that for pods that you create\n",
      "directly. Those pods aren’t managed by anything except by the Kubelet, but because the\n",
      "Kubelet runs on the node itself, it can’t do anything if the node fails. \n",
      " To make sure your app is restarted on another node, you need to have the pod\n",
      "managed by a ReplicationController or similar mechanism, which we’ll discuss in the\n",
      "rest of this chapter. \n",
      "4.2\n",
      "Introducing ReplicationControllers\n",
      "A ReplicationController is a Kubernetes resource that ensures its pods are always\n",
      "kept running. If the pod disappears for any reason, such as in the event of a node\n",
      "disappearing from the cluster or because the pod was evicted from the node, the\n",
      "ReplicationController notices the missing pod and creates a replacement pod. \n",
      " Figure 4.1 shows what happens when a node goes down and takes two pods with it.\n",
      "Pod A was created directly and is therefore an unmanaged pod, while pod B is managed\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 123, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "91\n",
      "Introducing ReplicationControllers\n",
      "by a ReplicationController. After the node fails, the ReplicationController creates a\n",
      "new pod (pod B2) to replace the missing pod B, whereas pod A is lost completely—\n",
      "nothing will ever recreate it.\n",
      " The ReplicationController in the figure manages only a single pod, but Replication-\n",
      "Controllers, in general, are meant to create and manage multiple copies (replicas) of a\n",
      "pod. That’s where ReplicationControllers got their name from. \n",
      "4.2.1\n",
      "The operation of a ReplicationController\n",
      "A ReplicationController constantly monitors the list of running pods and makes sure\n",
      "the actual number of pods of a “type” always matches the desired number. If too few\n",
      "such pods are running, it creates new replicas from a pod template. If too many such\n",
      "pods are running, it removes the excess replicas. \n",
      " You might be wondering how there can be more than the desired number of repli-\n",
      "cas. This can happen for a few reasons: \n",
      "Someone creates a pod of the same type manually.\n",
      "Someone changes an existing pod’s “type.”\n",
      "Someone decreases the desired number of pods, and so on.\n",
      "Node 1\n",
      "Node 1 fails\n",
      "Pod A\n",
      "Pod B\n",
      "Node 2\n",
      "Various\n",
      "other pods\n",
      "Creates and\n",
      "manages\n",
      "Node 1\n",
      "Pod A\n",
      "Pod B\n",
      "Node 2\n",
      "Various\n",
      "other pods\n",
      "ReplicationController\n",
      "ReplicationController\n",
      "Pod A goes down with Node 1 and is\n",
      "not recreated, because there is no\n",
      "ReplicationController overseeing it.\n",
      "RC notices pod B is\n",
      "missing and creates\n",
      "a new pod instance.\n",
      "Pod B2\n",
      "Figure 4.1\n",
      "When a node fails, only pods backed by a ReplicationController are recreated.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 124, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "92\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "I’ve used the term pod “type” a few times. But no such thing exists. Replication-\n",
      "Controllers don’t operate on pod types, but on sets of pods that match a certain label\n",
      "selector (you learned about them in the previous chapter). \n",
      "INTRODUCING THE CONTROLLER’S RECONCILIATION LOOP\n",
      "A ReplicationController’s job is to make sure that an exact number of pods always\n",
      "matches its label selector. If it doesn’t, the ReplicationController takes the appropriate\n",
      "action to reconcile the actual with the desired number. The operation of a Replication-\n",
      "Controller is shown in figure 4.2.\n",
      "UNDERSTANDING THE THREE PARTS OF A REPLICATIONCONTROLLER\n",
      "A ReplicationController has three essential parts (also shown in figure 4.3):\n",
      "A label selector, which determines what pods are in the ReplicationController’s scope\n",
      "A replica count, which specifies the desired number of pods that should be running\n",
      "A pod template, which is used when creating new pod replicas\n",
      "Start\n",
      "Compare\n",
      "matched vs.\n",
      "desired pod\n",
      "count\n",
      "Find pods\n",
      "matching the\n",
      "label selector\n",
      "Create additional\n",
      "pod(s) from\n",
      "current template\n",
      "Delete the\n",
      "excess pod(s)\n",
      "Too many\n",
      "Just enough\n",
      "Too few\n",
      "Figure 4.2\n",
      "A ReplicationController’s reconciliation loop\n",
      "app: kubia\n",
      "Pod\n",
      "Pod template\n",
      "ReplicationController: kubia\n",
      "Pod selector:\n",
      "app=kubia\n",
      "Replicas: 3\n",
      "Figure 4.3\n",
      "The three key parts of a \n",
      "ReplicationController (pod selector, \n",
      "replica count, and pod template)\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 125, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "93\n",
      "Introducing ReplicationControllers\n",
      "A ReplicationController’s replica count, the label selector, and even the pod tem-\n",
      "plate can all be modified at any time, but only changes to the replica count affect\n",
      "existing pods. \n",
      "UNDERSTANDING THE EFFECT OF CHANGING THE CONTROLLER’S LABEL SELECTOR OR POD TEMPLATE\n",
      "Changes to the label selector and the pod template have no effect on existing pods.\n",
      "Changing the label selector makes the existing pods fall out of the scope of the\n",
      "ReplicationController, so the controller stops caring about them. ReplicationCon-\n",
      "trollers also don’t care about the actual “contents” of its pods (the container images,\n",
      "environment variables, and other things) after they create the pod. The template\n",
      "therefore only affects new pods created by this ReplicationController. You can think\n",
      "of it as a cookie cutter for cutting out new pods.\n",
      "UNDERSTANDING THE BENEFITS OF USING A REPLICATIONCONTROLLER\n",
      "Like many things in Kubernetes, a ReplicationController, although an incredibly sim-\n",
      "ple concept, provides or enables the following powerful features:\n",
      "It makes sure a pod (or multiple pod replicas) is always running by starting a\n",
      "new pod when an existing one goes missing.\n",
      "When a cluster node fails, it creates replacement replicas for all the pods that\n",
      "were running on the failed node (those that were under the Replication-\n",
      "Controller’s control).\n",
      "It enables easy horizontal scaling of pods—both manual and automatic (see\n",
      "horizontal pod auto-scaling in chapter 15).\n",
      "NOTE\n",
      "A pod instance is never relocated to another node. Instead, the\n",
      "ReplicationController creates a completely new pod instance that has no rela-\n",
      "tion to the instance it’s replacing. \n",
      "4.2.2\n",
      "Creating a ReplicationController\n",
      "Let’s look at how to create a ReplicationController and then see how it keeps your\n",
      "pods running. Like pods and other Kubernetes resources, you create a Replication-\n",
      "Controller by posting a JSON or YAML descriptor to the Kubernetes API server.\n",
      " You’re going to create a YAML file called kubia-rc.yaml for your Replication-\n",
      "Controller, as shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: ReplicationController     \n",
      "metadata:\n",
      "  name: kubia                      \n",
      "spec:\n",
      "  replicas: 3                     \n",
      "  selector:              \n",
      "    app: kubia           \n",
      "Listing 4.4\n",
      "A YAML definition of a ReplicationController: kubia-rc.yaml\n",
      "This manifest defines a \n",
      "ReplicationController (RC)\n",
      "The name of this \n",
      "ReplicationController\n",
      "The desired number \n",
      "of pod instances\n",
      "The pod selector determining \n",
      "what pods the RC is operating on\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 126, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "94\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "  template:                        \n",
      "    metadata:                      \n",
      "      labels:                      \n",
      "        app: kubia                 \n",
      "    spec:                          \n",
      "      containers:                  \n",
      "      - name: kubia                \n",
      "        image: luksa/kubia         \n",
      "        ports:                     \n",
      "        - containerPort: 8080      \n",
      "When you post the file to the API server, Kubernetes creates a new Replication-\n",
      "Controller named kubia, which makes sure three pod instances always match the\n",
      "label selector app=kubia. When there aren’t enough pods, new pods will be created\n",
      "from the provided pod template. The contents of the template are almost identical to\n",
      "the pod definition you created in the previous chapter. \n",
      " The pod labels in the template must obviously match the label selector of the\n",
      "ReplicationController; otherwise the controller would create new pods indefinitely,\n",
      "because spinning up a new pod wouldn’t bring the actual replica count any closer to\n",
      "the desired number of replicas. To prevent such scenarios, the API server verifies the\n",
      "ReplicationController definition and will not accept it if it’s misconfigured.\n",
      " Not specifying the selector at all is also an option. In that case, it will be configured\n",
      "automatically from the labels in the pod template. \n",
      "TIP\n",
      "Don’t specify a pod selector when defining a ReplicationController. Let\n",
      "Kubernetes extract it from the pod template. This will keep your YAML\n",
      "shorter and simpler.\n",
      "To create the ReplicationController, use the kubectl create command, which you\n",
      "already know:\n",
      "$ kubectl create -f kubia-rc.yaml\n",
      "replicationcontroller \"kubia\" created\n",
      "As soon as the ReplicationController is created, it goes to work. Let’s see what\n",
      "it does.\n",
      "4.2.3\n",
      "Seeing the ReplicationController in action\n",
      "Because no pods exist with the app=kubia label, the ReplicationController should\n",
      "spin up three new pods from the pod template. List the pods to see if the Replication-\n",
      "Controller has done what it’s supposed to:\n",
      "$ kubectl get pods\n",
      "NAME          READY     STATUS              RESTARTS   AGE\n",
      "kubia-53thy   0/1       ContainerCreating   0          2s\n",
      "kubia-k0xz6   0/1       ContainerCreating   0          2s\n",
      "kubia-q3vkg   0/1       ContainerCreating   0          2s\n",
      "The pod template \n",
      "for creating new \n",
      "pods\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 127, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "95\n",
      "Introducing ReplicationControllers\n",
      "Indeed, it has! You wanted three pods, and it created three pods. It’s now managing\n",
      "those three pods. Next you’ll mess with them a little to see how the Replication-\n",
      "Controller responds. \n",
      "SEEING THE REPLICATIONCONTROLLER RESPOND TO A DELETED POD\n",
      "First, you’ll delete one of the pods manually to see how the ReplicationController spins\n",
      "up a new one immediately, bringing the number of matching pods back to three:\n",
      "$ kubectl delete pod kubia-53thy\n",
      "pod \"kubia-53thy\" deleted\n",
      "Listing the pods again shows four of them, because the one you deleted is terminat-\n",
      "ing, and a new pod has already been created:\n",
      "$ kubectl get pods\n",
      "NAME          READY     STATUS              RESTARTS   AGE\n",
      "kubia-53thy   1/1       Terminating         0          3m\n",
      "kubia-oini2   0/1       ContainerCreating   0          2s\n",
      "kubia-k0xz6   1/1       Running             0          3m\n",
      "kubia-q3vkg   1/1       Running             0          3m\n",
      "The ReplicationController has done its job again. It’s a nice little helper, isn’t it?\n",
      "GETTING INFORMATION ABOUT A REPLICATIONCONTROLLER\n",
      "Now, let’s see what information the kubectl get command shows for Replication-\n",
      "Controllers:\n",
      "$ kubectl get rc\n",
      "NAME      DESIRED   CURRENT   READY     AGE\n",
      "kubia     3         3         2         3m\n",
      "NOTE\n",
      "We’re using rc as a shorthand for replicationcontroller.\n",
      "You see three columns showing the desired number of pods, the actual number of\n",
      "pods, and how many of them are ready (you’ll learn what that means in the next chap-\n",
      "ter, when we talk about readiness probes).\n",
      " You can see additional information about your ReplicationController with the\n",
      "kubectl describe command, as shown in the following listing.\n",
      "$ kubectl describe rc kubia\n",
      "Name:           kubia\n",
      "Namespace:      default\n",
      "Selector:       app=kubia\n",
      "Labels:         app=kubia\n",
      "Annotations:    <none>\n",
      "Replicas:       3 current / 3 desired               \n",
      "Pods Status:    4 Running / 0 Waiting / 0 Succeeded / 0 Failed  \n",
      "Pod Template:\n",
      "  Labels:       app=kubia\n",
      "  Containers:   ...\n",
      "Listing 4.5\n",
      "Displaying details of a ReplicationController with kubectl describe\n",
      "The actual vs. the \n",
      "desired number of \n",
      "pod instances\n",
      "Number of \n",
      "pod instances \n",
      "per pod \n",
      "status\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 128, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "96\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "  Volumes:      <none>\n",
      "Events:                                                   \n",
      "From                    Type      Reason           Message\n",
      "----                    -------  ------            -------\n",
      "replication-controller  Normal   SuccessfulCreate  Created pod: kubia-53thy\n",
      "replication-controller  Normal   SuccessfulCreate  Created pod: kubia-k0xz6\n",
      "replication-controller  Normal   SuccessfulCreate  Created pod: kubia-q3vkg\n",
      "replication-controller  Normal   SuccessfulCreate  Created pod: kubia-oini2\n",
      "The current number of replicas matches the desired number, because the controller\n",
      "has already created a new pod. It shows four running pods because a pod that’s termi-\n",
      "nating is still considered running, although it isn’t counted in the current replica count. \n",
      " The list of events at the bottom shows the actions taken by the Replication-\n",
      "Controller—it has created four pods so far.\n",
      "UNDERSTANDING EXACTLY WHAT CAUSED THE CONTROLLER TO CREATE A NEW POD\n",
      "The controller is responding to the deletion of a pod by creating a new replacement\n",
      "pod (see figure 4.4). Well, technically, it isn’t responding to the deletion itself, but the\n",
      "resulting state—the inadequate number of pods.\n",
      " While a ReplicationController is immediately notified about a pod being deleted\n",
      "(the API server allows clients to watch for changes to resources and resource lists), that’s\n",
      "not what causes it to create a replacement pod. The notification triggers the controller\n",
      "to check the actual number of pods and take appropriate action.\n",
      "The events \n",
      "related to this \n",
      "ReplicationController\n",
      "Before deletion\n",
      "After deletion\n",
      "ReplicationController: kubia\n",
      "Replicas: 3\n",
      "Selector: app=kubia\n",
      "app: kubia\n",
      "Pod:\n",
      "kubia-q3vkg\n",
      "app: kubia\n",
      "Pod:\n",
      "kubia-oini2\n",
      "[ContainerCreating]\n",
      "[Terminating]\n",
      "app: kubia\n",
      "Pod:\n",
      "kubia-k0xz6\n",
      "app: kubia\n",
      "Pod:\n",
      "kubia-53thy\n",
      "ReplicationController: kubia\n",
      "Replicas: 3\n",
      "Selector: app=kubia\n",
      "app: kubia\n",
      "Pod:\n",
      "kubia-q3vkg\n",
      "app: kubia\n",
      "Pod:\n",
      "kubia-k0xz6\n",
      "app: kubia\n",
      "Pod:\n",
      "kubia-53thy\n",
      "Delete kubia-53thy\n",
      "Figure 4.4\n",
      "If a pod disappears, the ReplicationController sees too few pods and creates a new replacement pod.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 129, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "97\n",
      "Introducing ReplicationControllers\n",
      "RESPONDING TO A NODE FAILURE\n",
      "Seeing the ReplicationController respond to the manual deletion of a pod isn’t too\n",
      "interesting, so let’s look at a better example. If you’re using Google Kubernetes Engine\n",
      "to run these examples, you have a three-node Kubernetes cluster. You’re going to dis-\n",
      "connect one of the nodes from the network to simulate a node failure.\n",
      "NOTE\n",
      "If you’re using Minikube, you can’t do this exercise, because you only\n",
      "have one node that acts both as a master and a worker node.\n",
      "If a node fails in the non-Kubernetes world, the ops team would need to migrate the\n",
      "applications running on that node to other machines manually. Kubernetes, on the\n",
      "other hand, does that automatically. Soon after the ReplicationController detects that\n",
      "its pods are down, it will spin up new pods to replace them. \n",
      " Let’s see this in action. You need to ssh into one of the nodes with the gcloud\n",
      "compute ssh command and then shut down its network interface with sudo ifconfig\n",
      "eth0 down, as shown in the following listing.\n",
      "NOTE\n",
      "Choose a node that runs at least one of your pods by listing pods with\n",
      "the -o wide option.\n",
      "$ gcloud compute ssh gke-kubia-default-pool-b46381f1-zwko\n",
      "Enter passphrase for key '/home/luksa/.ssh/google_compute_engine':\n",
      "Welcome to Kubernetes v1.6.4!\n",
      "...\n",
      "luksa@gke-kubia-default-pool-b46381f1-zwko ~ $ sudo ifconfig eth0 down\n",
      "When you shut down the network interface, the ssh session will stop responding, so\n",
      "you need to open up another terminal or hard-exit from the ssh session. In the new\n",
      "terminal you can list the nodes to see if Kubernetes has detected that the node is\n",
      "down. This takes a minute or so. Then, the node’s status is shown as NotReady:\n",
      "$ kubectl get node\n",
      "NAME                                   STATUS     AGE\n",
      "gke-kubia-default-pool-b46381f1-opc5   Ready      5h\n",
      "gke-kubia-default-pool-b46381f1-s8gj   Ready      5h\n",
      "gke-kubia-default-pool-b46381f1-zwko   NotReady   5h    \n",
      "If you list the pods now, you’ll still see the same three pods as before, because Kuber-\n",
      "netes waits a while before rescheduling pods (in case the node is unreachable because\n",
      "of a temporary network glitch or because the Kubelet is restarting). If the node stays\n",
      "unreachable for several minutes, the status of the pods that were scheduled to that\n",
      "node changes to Unknown. At that point, the ReplicationController will immediately\n",
      "spin up a new pod. You can see this by listing the pods again:\n",
      "Listing 4.6\n",
      "Simulating a node failure by shutting down its network interface\n",
      "Node isn’t ready, \n",
      "because it’s \n",
      "disconnected from \n",
      "the network\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 130, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "98\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "$ kubectl get pods\n",
      "NAME          READY   STATUS    RESTARTS   AGE\n",
      "kubia-oini2   1/1     Running   0          10m\n",
      "kubia-k0xz6   1/1     Running   0          10m\n",
      "kubia-q3vkg   1/1     Unknown   0          10m    \n",
      "kubia-dmdck   1/1     Running   0          5s    \n",
      "Looking at the age of the pods, you see that the kubia-dmdck pod is new. You again\n",
      "have three pod instances running, which means the ReplicationController has again\n",
      "done its job of bringing the actual state of the system to the desired state. \n",
      " The same thing happens if a node fails (either breaks down or becomes unreach-\n",
      "able). No immediate human intervention is necessary. The system heals itself\n",
      "automatically. \n",
      " To bring the node back, you need to reset it with the following command:\n",
      "$ gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko\n",
      "When the node boots up again, its status should return to Ready, and the pod whose\n",
      "status was Unknown will be deleted.\n",
      "4.2.4\n",
      "Moving pods in and out of the scope of a ReplicationController\n",
      "Pods created by a ReplicationController aren’t tied to the ReplicationController in\n",
      "any way. At any moment, a ReplicationController manages pods that match its label\n",
      "selector. By changing a pod’s labels, it can be removed from or added to the scope\n",
      "of a ReplicationController. It can even be moved from one ReplicationController to\n",
      "another.\n",
      "TIP\n",
      "Although a pod isn’t tied to a ReplicationController, the pod does refer-\n",
      "ence it in the metadata.ownerReferences field, which you can use to easily\n",
      "find which ReplicationController a pod belongs to.\n",
      "If you change a pod’s labels so they no longer match a ReplicationController’s label\n",
      "selector, the pod becomes like any other manually created pod. It’s no longer man-\n",
      "aged by anything. If the node running the pod fails, the pod is obviously not resched-\n",
      "uled. But keep in mind that when you changed the pod’s labels, the replication\n",
      "controller noticed one pod was missing and spun up a new pod to replace it.\n",
      " Let’s try this with your pods. Because your ReplicationController manages pods\n",
      "that have the app=kubia label, you need to either remove this label or change its value\n",
      "to move the pod out of the ReplicationController’s scope. Adding another label will\n",
      "have no effect, because the ReplicationController doesn’t care if the pod has any addi-\n",
      "tional labels. It only cares whether the pod has all the labels referenced in the label\n",
      "selector. \n",
      "This pod’s status is \n",
      "unknown, because its \n",
      "node is unreachable.\n",
      "This pod was created \n",
      "five seconds ago.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 131, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "99\n",
      "Introducing ReplicationControllers\n",
      "ADDING LABELS TO PODS MANAGED BY A REPLICATIONCONTROLLER\n",
      "Let’s confirm that a ReplicationController doesn’t care if you add additional labels to\n",
      "its managed pods:\n",
      "$ kubectl label pod kubia-dmdck type=special\n",
      "pod \"kubia-dmdck\" labeled\n",
      "$ kubectl get pods --show-labels\n",
      "NAME          READY   STATUS    RESTARTS   AGE   LABELS\n",
      "kubia-oini2   1/1     Running   0          11m   app=kubia\n",
      "kubia-k0xz6   1/1     Running   0          11m   app=kubia\n",
      "kubia-dmdck   1/1     Running   0          1m    app=kubia,type=special\n",
      "You’ve added the type=special label to one of the pods. Listing all pods again shows\n",
      "the same three pods as before, because no change occurred as far as the Replication-\n",
      "Controller is concerned.\n",
      "CHANGING THE LABELS OF A MANAGED POD\n",
      "Now, you’ll change the app=kubia label to something else. This will make the pod no\n",
      "longer match the ReplicationController’s label selector, leaving it to only match two\n",
      "pods. The ReplicationController should therefore start a new pod to bring the num-\n",
      "ber back to three:\n",
      "$ kubectl label pod kubia-dmdck app=foo --overwrite\n",
      "pod \"kubia-dmdck\" labeled\n",
      "The --overwrite argument is necessary; otherwise kubectl will only print out a warn-\n",
      "ing and won’t change the label, to prevent you from inadvertently changing an exist-\n",
      "ing label’s value when your intent is to add a new one. \n",
      " Listing all the pods again should now show four pods: \n",
      "$ kubectl get pods -L app\n",
      "NAME         READY  STATUS             RESTARTS  AGE  APP\n",
      "kubia-2qneh  0/1    ContainerCreating  0         2s   kubia   \n",
      "kubia-oini2  1/1    Running            0         20m  kubia\n",
      "kubia-k0xz6  1/1    Running            0         20m  kubia\n",
      "kubia-dmdck  1/1    Running            0         10m  foo    \n",
      "NOTE\n",
      "You’re using the -L app option to display the app label in a column.\n",
      "There, you now have four pods altogether: one that isn’t managed by your Replication-\n",
      "Controller and three that are. Among them is the newly created pod.\n",
      " Figure 4.5 illustrates what happened when you changed the pod’s labels so they no\n",
      "longer matched the ReplicationController’s pod selector. You can see your three pods\n",
      "and your ReplicationController. After you change the pod’s label from app=kubia to\n",
      "app=foo, the ReplicationController no longer cares about the pod. Because the con-\n",
      "troller’s replica count is set to 3 and only two pods match the label selector, the\n",
      "Newly created pod that replaces\n",
      "the pod you removed from the\n",
      "scope of the ReplicationController\n",
      "Pod no longer \n",
      "managed by the \n",
      "ReplicationController\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 132, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "100\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "ReplicationController spins up pod kubia-2qneh to bring the number back up to\n",
      "three. Pod kubia-dmdck is now completely independent and will keep running until\n",
      "you delete it manually (you can do that now, because you don’t need it anymore).\n",
      "REMOVING PODS FROM CONTROLLERS IN PRACTICE\n",
      "Removing a pod from the scope of the ReplicationController comes in handy when\n",
      "you want to perform actions on a specific pod. For example, you might have a bug\n",
      "that causes your pod to start behaving badly after a specific amount of time or a spe-\n",
      "cific event. If you know a pod is malfunctioning, you can take it out of the Replication-\n",
      "Controller’s scope, let the controller replace it with a new one, and then debug or\n",
      "play with the pod in any way you want. Once you’re done, you delete the pod. \n",
      "CHANGING THE REPLICATIONCONTROLLER’S LABEL SELECTOR\n",
      "As an exercise to see if you fully understand ReplicationControllers, what do you\n",
      "think would happen if instead of changing the labels of a pod, you modified the\n",
      "ReplicationController’s label selector? \n",
      " If your answer is that it would make all the pods fall out of the scope of the\n",
      "ReplicationController, which would result in it creating three new pods, you’re abso-\n",
      "lutely right. And it shows that you understand how ReplicationControllers work. \n",
      " Kubernetes does allow you to change a ReplicationController’s label selector, but\n",
      "that’s not the case for the other resources that are covered in the second half of this\n",
      "Initial state\n",
      "After re-labelling\n",
      "Re-label kubia-dmdck\n",
      "app: kubia\n",
      "Pod:\n",
      "kubia-oini2\n",
      "app: kubia\n",
      "Pod:\n",
      "kubia-2qneh\n",
      "[ContainerCreating]\n",
      "Pod:\n",
      "kubia-dmdck\n",
      "app: kubia\n",
      "Pod:\n",
      "kubia-k0xz6\n",
      "app: kubia\n",
      "type: special\n",
      "type: special\n",
      "app: foo\n",
      "app: kubia\n",
      "Pod:\n",
      "kubia-dmdck\n",
      "app: kubia\n",
      "Pod:\n",
      "kubia-k0xz6\n",
      "ReplicationController: kubia\n",
      "Replicas: 3\n",
      "Selector: app=kubia\n",
      "ReplicationController: kubia\n",
      "Replicas: 3\n",
      "Selector: app=kubia\n",
      "Pod:\n",
      "kubia-oini2\n",
      "Figure 4.5\n",
      "Removing a pod from the scope of a ReplicationController by changing its labels \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 133, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "101\n",
      "Introducing ReplicationControllers\n",
      "chapter and which are also used for managing pods. You’ll never change a controller’s\n",
      "label selector, but you’ll regularly change its pod template. Let’s take a look at that.\n",
      "4.2.5\n",
      "Changing the pod template\n",
      "A ReplicationController’s pod template can be modified at any time. Changing the pod\n",
      "template is like replacing a cookie cutter with another one. It will only affect the cookies\n",
      "you cut out afterward and will have no effect on the ones you’ve already cut (see figure\n",
      "4.6). To modify the old pods, you’d need to delete them and let the Replication-\n",
      "Controller replace them with new ones based on the new template.\n",
      "As an exercise, you can try editing the ReplicationController and adding a label to the\n",
      "pod template. You can edit the ReplicationController with the following command:\n",
      "$ kubectl edit rc kubia\n",
      "This will open the ReplicationController’s YAML definition in your default text editor.\n",
      "Find the pod template section and add an additional label to the metadata. After you\n",
      "save your changes and exit the editor, kubectl will update the ReplicationController\n",
      "and print the following message:\n",
      "replicationcontroller \"kubia\" edited\n",
      "You can now list pods and their labels again and confirm that they haven’t changed.\n",
      "But if you delete the pods and wait for their replacements to be created, you’ll see the\n",
      "new label.\n",
      " Editing a ReplicationController like this to change the container image in the pod\n",
      "template, deleting the existing pods, and letting them be replaced with new ones from\n",
      "the new template could be used for upgrading pods, but you’ll learn a better way of\n",
      "doing that in chapter 9. \n",
      "Replication\n",
      "Controller\n",
      "Replicas: 3\n",
      "Template:\n",
      "A\n",
      "B\n",
      "C\n",
      "Replication\n",
      "Controller\n",
      "Replicas: 3\n",
      "Template:\n",
      "A\n",
      "Replication\n",
      "Controller\n",
      "Replicas: 3\n",
      "Template:\n",
      "A\n",
      "Replication\n",
      "Controller\n",
      "Replicas: 3\n",
      "Template:\n",
      "D\n",
      "A\n",
      "B\n",
      "C\n",
      "A\n",
      "B\n",
      "C\n",
      "A\n",
      "B\n",
      "Change\n",
      "template\n",
      "Delete\n",
      "a pod\n",
      "RC creates\n",
      "new pod\n",
      "Figure 4.6\n",
      "Changing a ReplicationController’s pod template only affects pods created afterward and has no \n",
      "effect on existing pods.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 134, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "102\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "4.2.6\n",
      "Horizontally scaling pods\n",
      "You’ve seen how ReplicationControllers make sure a specific number of pod instances\n",
      "is always running. Because it’s incredibly simple to change the desired number of rep-\n",
      "licas, this also means scaling pods horizontally is trivial. \n",
      " Scaling the number of pods up or down is as easy as changing the value of the rep-\n",
      "licas field in the ReplicationController resource. After the change, the Replication-\n",
      "Controller will either see too many pods exist (when scaling down) and delete part of\n",
      "them, or see too few of them (when scaling up) and create additional pods. \n",
      "SCALING UP A REPLICATIONCONTROLLER\n",
      "Your ReplicationController has been keeping three instances of your pod running.\n",
      "You’re going to scale that number up to 10 now. As you may remember, you’ve\n",
      "already scaled a ReplicationController in chapter 2. You could use the same com-\n",
      "mand as before:\n",
      "$ kubectl scale rc kubia --replicas=10\n",
      "But you’ll do it differently this time. \n",
      "SCALING A REPLICATIONCONTROLLER BY EDITING ITS DEFINITION\n",
      "Instead of using the kubectl scale command, you’re going to scale it in a declarative\n",
      "way by editing the ReplicationController’s definition:\n",
      "$ kubectl edit rc kubia\n",
      "When the text editor opens, find the spec.replicas field and change its value to 10,\n",
      "as shown in the following listing.\n",
      "# Please edit the object below. Lines beginning with a '#' will be ignored,\n",
      "# and an empty file will abort the edit. If an error occurs while saving \n",
      "# this file will be reopened with the relevant failures.\n",
      "apiVersion: v1\n",
      "kind: ReplicationController\n",
      "Configuring kubectl edit to use a different text editor\n",
      "You can tell kubectl to use a text editor of your choice by setting the KUBE_EDITOR\n",
      "environment variable. For example, if you’d like to use nano for editing Kubernetes\n",
      "resources, execute the following command (or put it into your ~/.bashrc or an\n",
      "equivalent file):\n",
      "export KUBE_EDITOR=\"/usr/bin/nano\"\n",
      "If the KUBE_EDITOR environment variable isn’t set, kubectl edit falls back to using\n",
      "the default editor, usually configured through the EDITOR environment variable.\n",
      "Listing 4.7\n",
      "Editing the RC in a text editor by running kubectl edit\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 135, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "103\n",
      "Introducing ReplicationControllers\n",
      "metadata:\n",
      "  ...\n",
      "spec:\n",
      "  replicas: 3        \n",
      "  selector:\n",
      "    app: kubia\n",
      "  ...\n",
      "When you save the file and close the editor, the ReplicationController is updated and\n",
      "it immediately scales the number of pods to 10:\n",
      "$ kubectl get rc\n",
      "NAME      DESIRED   CURRENT   READY     AGE\n",
      "kubia     10        10        4         21m\n",
      "There you go. If the kubectl scale command makes it look as though you’re telling\n",
      "Kubernetes exactly what to do, it’s now much clearer that you’re making a declarative\n",
      "change to the desired state of the ReplicationController and not telling Kubernetes to\n",
      "do something.\n",
      "SCALING DOWN WITH THE KUBECTL SCALE COMMAND\n",
      "Now scale back down to 3. You can use the kubectl scale command:\n",
      "$ kubectl scale rc kubia --replicas=3\n",
      "All this command does is modify the spec.replicas field of the ReplicationController’s\n",
      "definition—like when you changed it through kubectl edit. \n",
      "UNDERSTANDING THE DECLARATIVE APPROACH TO SCALING\n",
      "Horizontally scaling pods in Kubernetes is a matter of stating your desire: “I want to\n",
      "have x number of instances running.” You’re not telling Kubernetes what or how to do\n",
      "it. You’re just specifying the desired state. \n",
      " This declarative approach makes interacting with a Kubernetes cluster easy. Imag-\n",
      "ine if you had to manually determine the current number of running instances and\n",
      "then explicitly tell Kubernetes how many additional instances to run. That’s more\n",
      "work and is much more error-prone. Changing a simple number is much easier, and\n",
      "in chapter 15, you’ll learn that even that can be done by Kubernetes itself if you\n",
      "enable horizontal pod auto-scaling. \n",
      "4.2.7\n",
      "Deleting a ReplicationController\n",
      "When you delete a ReplicationController through kubectl delete, the pods are also\n",
      "deleted. But because pods created by a ReplicationController aren’t an integral part\n",
      "of the ReplicationController, and are only managed by it, you can delete only the\n",
      "ReplicationController and leave the pods running, as shown in figure 4.7.\n",
      " This may be useful when you initially have a set of pods managed by a Replication-\n",
      "Controller, and then decide to replace the ReplicationController with a ReplicaSet,\n",
      "for example (you’ll learn about them next.). You can do this without affecting the\n",
      "Change the number 3 \n",
      "to number 10 in \n",
      "this line.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 136, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "104\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "pods and keep them running without interruption while you replace the Replication-\n",
      "Controller that manages them. \n",
      " When deleting a ReplicationController with kubectl delete, you can keep its\n",
      "pods running by passing the --cascade=false option to the command. Try that now:\n",
      "$ kubectl delete rc kubia --cascade=false\n",
      "replicationcontroller \"kubia\" deleted\n",
      "You’ve deleted the ReplicationController so the pods are on their own. They are no\n",
      "longer managed. But you can always create a new ReplicationController with the\n",
      "proper label selector and make them managed again.\n",
      "4.3\n",
      "Using ReplicaSets instead of ReplicationControllers\n",
      "Initially, ReplicationControllers were the only Kubernetes component for replicating\n",
      "pods and rescheduling them when nodes failed. Later, a similar resource called a\n",
      "ReplicaSet was introduced. It’s a new generation of ReplicationController and\n",
      "replaces it completely (ReplicationControllers will eventually be deprecated). \n",
      " You could have started this chapter by creating a ReplicaSet instead of a Replication-\n",
      "Controller, but I felt it would be a good idea to start with what was initially available in\n",
      "Kubernetes. Plus, you’ll still see ReplicationControllers used in the wild, so it’s good\n",
      "for you to know about them. That said, you should always create ReplicaSets instead\n",
      "of ReplicationControllers from now on. They’re almost identical, so you shouldn’t\n",
      "have any trouble using them instead. \n",
      "Before the RC deletion\n",
      "After the RC deletion\n",
      "Delete RC\n",
      "Pod:\n",
      "kubia-q3vkg\n",
      "Pod:\n",
      "kubia-53thy\n",
      "Pod:\n",
      "kubia-k0xz6\n",
      "Pod:\n",
      "kubia-q3vkg\n",
      "Pod:\n",
      "kubia-53thy\n",
      "Pod:\n",
      "kubia-k0xz6\n",
      "ReplicationController: kubia\n",
      "Replicas: 3\n",
      "Selector: app=kubia\n",
      "app: kubia\n",
      "app: kubia\n",
      "app: kubia\n",
      "app: kubia\n",
      "app: kubia\n",
      "app: kubia\n",
      "Figure 4.7\n",
      "Deleting a replication controller with --cascade=false leaves pods unmanaged.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 137, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "105\n",
      "Using ReplicaSets instead of ReplicationControllers\n",
      " You usually won’t create them directly, but instead have them created automati-\n",
      "cally when you create the higher-level Deployment resource, which you’ll learn about\n",
      "in chapter 9. In any case, you should understand ReplicaSets, so let’s see how they dif-\n",
      "fer from ReplicationControllers.\n",
      "4.3.1\n",
      "Comparing a ReplicaSet to a ReplicationController\n",
      "A ReplicaSet behaves exactly like a ReplicationController, but it has more expressive\n",
      "pod selectors. Whereas a ReplicationController’s label selector only allows matching\n",
      "pods that include a certain label, a ReplicaSet’s selector also allows matching pods\n",
      "that lack a certain label or pods that include a certain label key, regardless of\n",
      "its value.\n",
      " Also, for example, a single ReplicationController can’t match pods with the label\n",
      "env=production and those with the label env=devel at the same time. It can only match\n",
      "either pods with the env=production label or pods with the env=devel label. But a sin-\n",
      "gle ReplicaSet can match both sets of pods and treat them as a single group. \n",
      " Similarly, a ReplicationController can’t match pods based merely on the presence\n",
      "of a label key, regardless of its value, whereas a ReplicaSet can. For example, a Replica-\n",
      "Set can match all pods that include a label with the key env, whatever its actual value is\n",
      "(you can think of it as env=*).\n",
      "4.3.2\n",
      "Defining a ReplicaSet\n",
      "You’re going to create a ReplicaSet now to see how the orphaned pods that were cre-\n",
      "ated by your ReplicationController and then abandoned earlier can now be adopted\n",
      "by a ReplicaSet. First, you’ll rewrite your ReplicationController into a ReplicaSet by\n",
      "creating a new file called kubia-replicaset.yaml with the contents in the following\n",
      "listing.\n",
      "apiVersion: apps/v1beta2      \n",
      "kind: ReplicaSet                    \n",
      "metadata:\n",
      "  name: kubia\n",
      "spec:\n",
      "  replicas: 3\n",
      "  selector:\n",
      "    matchLabels:                 \n",
      "      app: kubia                 \n",
      "  template:                        \n",
      "    metadata:                      \n",
      "      labels:                      \n",
      "        app: kubia                 \n",
      "    spec:                          \n",
      "      containers:                  \n",
      "      - name: kubia                \n",
      "        image: luksa/kubia         \n",
      "Listing 4.8\n",
      "A YAML definition of a ReplicaSet: kubia-replicaset.yaml\n",
      "ReplicaSets aren’t part of the v1 \n",
      "API, but belong to the apps API \n",
      "group and version v1beta2.\n",
      "You’re using the simpler matchLabels \n",
      "selector here, which is much like a \n",
      "ReplicationController’s selector.\n",
      "The template is \n",
      "the same as in the \n",
      "ReplicationController.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 138, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "106\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "The first thing to note is that ReplicaSets aren’t part of the v1 API, so you need to\n",
      "ensure you specify the proper apiVersion when creating the resource. You’re creating a\n",
      "resource of type ReplicaSet which has much the same contents as the Replication-\n",
      "Controller you created earlier. \n",
      " The only difference is in the selector. Instead of listing labels the pods need to\n",
      "have directly under the selector property, you’re specifying them under selector\n",
      ".matchLabels. This is the simpler (and less expressive) way of defining label selectors\n",
      "in a ReplicaSet. Later, you’ll look at the more expressive option, as well.\n",
      "Because you still have three pods matching the app=kubia selector running from ear-\n",
      "lier, creating this ReplicaSet will not cause any new pods to be created. The ReplicaSet\n",
      "will take those existing three pods under its wing. \n",
      "4.3.3\n",
      "Creating and examining a ReplicaSet\n",
      "Create the ReplicaSet from the YAML file with the kubectl create command. After\n",
      "that, you can examine the ReplicaSet with kubectl get and kubectl describe:\n",
      "$ kubectl get rs\n",
      "NAME      DESIRED   CURRENT   READY     AGE\n",
      "kubia     3         3         3         3s\n",
      "TIP\n",
      "Use rs shorthand, which stands for replicaset.\n",
      "$ kubectl describe rs\n",
      "Name:           kubia\n",
      "Namespace:      default\n",
      "Selector:       app=kubia\n",
      "Labels:         app=kubia\n",
      "Annotations:    <none>\n",
      "Replicas:       3 current / 3 desired\n",
      "Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed\n",
      "Pod Template:\n",
      "  Labels:       app=kubia\n",
      "About the API version attribute\n",
      "This is your first opportunity to see that the apiVersion property specifies two things:\n",
      "The API group (which is apps in this case)\n",
      "The actual API version (v1beta2)\n",
      "You’ll see throughout the book that certain Kubernetes resources are in what’s called\n",
      "the core API group, which doesn’t need to be specified in the apiVersion field (you\n",
      "just specify the version—for example, you’ve been using apiVersion: v1 when\n",
      "defining Pod resources). Other resources, which were introduced in later Kubernetes\n",
      "versions, are categorized into several API groups. Look at the inside of the book’s\n",
      "covers to see all resources and their respective API groups.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 139, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "107\n",
      "Using ReplicaSets instead of ReplicationControllers\n",
      "  Containers:   ...\n",
      "  Volumes:      <none>\n",
      "Events:         <none>\n",
      "As you can see, the ReplicaSet isn’t any different from a ReplicationController. It’s\n",
      "showing it has three replicas matching the selector. If you list all the pods, you’ll see\n",
      "they’re still the same three pods you had before. The ReplicaSet didn’t create any new\n",
      "ones. \n",
      "4.3.4\n",
      "Using the ReplicaSet’s more expressive label selectors\n",
      "The main improvements of ReplicaSets over ReplicationControllers are their more\n",
      "expressive label selectors. You intentionally used the simpler matchLabels selector in\n",
      "the first ReplicaSet example to see that ReplicaSets are no different from Replication-\n",
      "Controllers. Now, you’ll rewrite the selector to use the more powerful matchExpressions\n",
      "property, as shown in the following listing.\n",
      " selector:\n",
      "   matchExpressions:                 \n",
      "     - key: app           \n",
      "       operator: In                  \n",
      "       values:                       \n",
      "         - kubia                     \n",
      "NOTE\n",
      "Only the selector is shown. You’ll find the whole ReplicaSet definition\n",
      "in the book’s code archive.\n",
      "You can add additional expressions to the selector. As in the example, each expression\n",
      "must contain a key, an operator, and possibly (depending on the operator) a list of\n",
      "values. You’ll see four valid operators:\n",
      "\n",
      "In—Label’s value must match one of the specified values.\n",
      "\n",
      "NotIn—Label’s value must not match any of the specified values.\n",
      "\n",
      "Exists—Pod must include a label with the specified key (the value isn’t import-\n",
      "ant). When using this operator, you shouldn’t specify the values field.\n",
      "\n",
      "DoesNotExist—Pod must not include a label with the specified key. The values\n",
      "property must not be specified.\n",
      "If you specify multiple expressions, all those expressions must evaluate to true for the\n",
      "selector to match a pod. If you specify both matchLabels and matchExpressions, all\n",
      "the labels must match and all the expressions must evaluate to true for the pod to\n",
      "match the selector.\n",
      "Listing 4.9\n",
      "A matchExpressions selector: kubia-replicaset-matchexpressions.yaml\n",
      "This selector requires the pod to \n",
      "contain a label with the “app” key.\n",
      "The label’s value \n",
      "must be “kubia”.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 140, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "108\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "4.3.5\n",
      "Wrapping up ReplicaSets\n",
      "This was a quick introduction to ReplicaSets as an alternative to ReplicationControllers.\n",
      "Remember, always use them instead of ReplicationControllers, but you may still find\n",
      "ReplicationControllers in other people’s deployments.\n",
      " Now, delete the ReplicaSet to clean up your cluster a little. You can delete the\n",
      "ReplicaSet the same way you’d delete a ReplicationController:\n",
      "$ kubectl delete rs kubia\n",
      "replicaset \"kubia\" deleted\n",
      "Deleting the ReplicaSet should delete all the pods. List the pods to confirm that’s\n",
      "the case. \n",
      "4.4\n",
      "Running exactly one pod on each node with \n",
      "DaemonSets\n",
      "Both ReplicationControllers and ReplicaSets are used for running a specific number\n",
      "of pods deployed anywhere in the Kubernetes cluster. But certain cases exist when you\n",
      "want a pod to run on each and every node in the cluster (and each node needs to run\n",
      "exactly one instance of the pod, as shown in figure 4.8).\n",
      " Those cases include infrastructure-related pods that perform system-level opera-\n",
      "tions. For example, you’ll want to run a log collector and a resource monitor on every\n",
      "node. Another good example is Kubernetes’ own kube-proxy process, which needs to\n",
      "run on all nodes to make services work.\n",
      "Node 1\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "ReplicaSet\n",
      "Replicas: 5\n",
      "Node 2\n",
      "Pod\n",
      "Pod\n",
      "Node 3\n",
      "Pod\n",
      "DaemonSet\n",
      "Exactly one replica\n",
      "on each node\n",
      "Node 4\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Figure 4.8\n",
      "DaemonSets run only a single pod replica on each node, whereas ReplicaSets \n",
      "scatter them around the whole cluster randomly. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 141, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "109\n",
      "Running exactly one pod on each node with DaemonSets\n",
      "Outside of Kubernetes, such processes would usually be started through system init\n",
      "scripts or the systemd daemon during node boot up. On Kubernetes nodes, you can\n",
      "still use systemd to run your system processes, but then you can’t take advantage of all\n",
      "the features Kubernetes provides. \n",
      "4.4.1\n",
      "Using a DaemonSet to run a pod on every node\n",
      "To run a pod on all cluster nodes, you create a DaemonSet object, which is much\n",
      "like a ReplicationController or a ReplicaSet, except that pods created by a Daemon-\n",
      "Set already have a target node specified and skip the Kubernetes Scheduler. They\n",
      "aren’t scattered around the cluster randomly. \n",
      " A DaemonSet makes sure it creates as many pods as there are nodes and deploys\n",
      "each one on its own node, as shown in figure 4.8.\n",
      " Whereas a ReplicaSet (or ReplicationController) makes sure that a desired num-\n",
      "ber of pod replicas exist in the cluster, a DaemonSet doesn’t have any notion of a\n",
      "desired replica count. It doesn’t need it because its job is to ensure that a pod match-\n",
      "ing its pod selector is running on each node. \n",
      " If a node goes down, the DaemonSet doesn’t cause the pod to be created else-\n",
      "where. But when a new node is added to the cluster, the DaemonSet immediately\n",
      "deploys a new pod instance to it. It also does the same if someone inadvertently\n",
      "deletes one of the pods, leaving the node without the DaemonSet’s pod. Like a Replica-\n",
      "Set, a DaemonSet creates the pod from the pod template configured in it.\n",
      "4.4.2\n",
      "Using a DaemonSet to run pods only on certain nodes\n",
      "A DaemonSet deploys pods to all nodes in the cluster, unless you specify that the pods\n",
      "should only run on a subset of all the nodes. This is done by specifying the node-\n",
      "Selector property in the pod template, which is part of the DaemonSet definition\n",
      "(similar to the pod template in a ReplicaSet or ReplicationController). \n",
      " You’ve already used node selectors to deploy a pod onto specific nodes in chapter 3.\n",
      "A node selector in a DaemonSet is similar—it defines the nodes the DaemonSet must\n",
      "deploy its pods to. \n",
      "NOTE\n",
      "Later in the book, you’ll learn that nodes can be made unschedulable,\n",
      "preventing pods from being deployed to them. A DaemonSet will deploy pods\n",
      "even to such nodes, because the unschedulable attribute is only used by the\n",
      "Scheduler, whereas pods managed by a DaemonSet bypass the Scheduler\n",
      "completely. This is usually desirable, because DaemonSets are meant to run\n",
      "system services, which usually need to run even on unschedulable nodes.\n",
      "EXPLAINING DAEMONSETS WITH AN EXAMPLE\n",
      "Let’s imagine having a daemon called ssd-monitor that needs to run on all nodes\n",
      "that contain a solid-state drive (SSD). You’ll create a DaemonSet that runs this dae-\n",
      "mon on all nodes that are marked as having an SSD. The cluster administrators have\n",
      "added the disk=ssd label to all such nodes, so you’ll create the DaemonSet with a\n",
      "node selector that only selects nodes with that label, as shown in figure 4.9.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 142, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "110\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "CREATING A DAEMONSET YAML DEFINITION\n",
      "You’ll create a DaemonSet that runs a mock ssd-monitor process, which prints\n",
      "“SSD OK” to the standard output every five seconds. I’ve already prepared the mock\n",
      "container image and pushed it to Docker Hub, so you can use it instead of building\n",
      "your own. Create the YAML for the DaemonSet, as shown in the following listing.\n",
      "apiVersion: apps/v1beta2      \n",
      "kind: DaemonSet                     \n",
      "metadata:\n",
      "  name: ssd-monitor\n",
      "spec:                            \n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: ssd-monitor\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: ssd-monitor\n",
      "    spec:\n",
      "      nodeSelector:                \n",
      "        disk: ssd                  \n",
      "      containers:\n",
      "      - name: main\n",
      "        image: luksa/ssd-monitor\n",
      "You’re defining a DaemonSet that will run a pod with a single container based on the\n",
      "luksa/ssd-monitor container image. An instance of this pod will be created for each\n",
      "node that has the disk=ssd label.\n",
      "Listing 4.10\n",
      "A YAML for a DaemonSet: ssd-monitor-daemonset.yaml\n",
      "Node 1\n",
      "Pod:\n",
      "ssd-monitor\n",
      "Node 2\n",
      "Node 3\n",
      "DaemonSet:\n",
      "sssd-monitor\n",
      "Node selector:\n",
      "disk=ssd\n",
      "Node 4\n",
      "disk: ssd\n",
      "disk: ssd\n",
      "disk: ssd\n",
      "Unschedulable\n",
      "Pod:\n",
      "ssd-monitor\n",
      "Pod:\n",
      "ssd-monitor\n",
      "Figure 4.9\n",
      "Using a DaemonSet with a node selector to deploy system pods only on certain \n",
      "nodes\n",
      "DaemonSets are in the \n",
      "apps API group, \n",
      "version v1beta2.\n",
      "The pod template includes a \n",
      "node selector, which selects \n",
      "nodes with the disk=ssd label.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 143, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "111\n",
      "Running exactly one pod on each node with DaemonSets\n",
      "CREATING THE DAEMONSET\n",
      "You’ll create the DaemonSet like you always create resources from a YAML file:\n",
      "$ kubectl create -f ssd-monitor-daemonset.yaml\n",
      "daemonset \"ssd-monitor\" created\n",
      "Let’s see the created DaemonSet:\n",
      "$ kubectl get ds\n",
      "NAME          DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE-SELECTOR  \n",
      "ssd-monitor   0        0        0      0           0          disk=ssd\n",
      "Those zeroes look strange. Didn’t the DaemonSet deploy any pods? List the pods:\n",
      "$ kubectl get po\n",
      "No resources found.\n",
      "Where are the pods? Do you know what’s going on? Yes, you forgot to label your nodes\n",
      "with the disk=ssd label. No problem—you can do that now. The DaemonSet should\n",
      "detect that the nodes’ labels have changed and deploy the pod to all nodes with a\n",
      "matching label. Let’s see if that’s true. \n",
      "ADDING THE REQUIRED LABEL TO YOUR NODE(S)\n",
      "Regardless if you’re using Minikube, GKE, or another multi-node cluster, you’ll need\n",
      "to list the nodes first, because you’ll need to know the node’s name when labeling it:\n",
      "$ kubectl get node\n",
      "NAME       STATUS    AGE       VERSION\n",
      "minikube   Ready     4d        v1.6.0\n",
      "Now, add the disk=ssd label to one of your nodes like this:\n",
      "$ kubectl label node minikube disk=ssd\n",
      "node \"minikube\" labeled\n",
      "NOTE\n",
      "Replace minikube with the name of one of your nodes if you’re not\n",
      "using Minikube.\n",
      "The DaemonSet should have created one pod now. Let’s see:\n",
      "$ kubectl get po\n",
      "NAME                READY     STATUS    RESTARTS   AGE\n",
      "ssd-monitor-hgxwq   1/1       Running   0          35s\n",
      "Okay; so far so good. If you have multiple nodes and you add the same label to further\n",
      "nodes, you’ll see the DaemonSet spin up pods for each of them. \n",
      "REMOVING THE REQUIRED LABEL FROM THE NODE\n",
      "Now, imagine you’ve made a mistake and have mislabeled one of the nodes. It has a\n",
      "spinning disk drive, not an SSD. What happens if you change the node’s label?\n",
      "$ kubectl label node minikube disk=hdd --overwrite\n",
      "node \"minikube\" labeled\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 144, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "112\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "Let’s see if the change has any effect on the pod that was running on that node:\n",
      "$ kubectl get po\n",
      "NAME                READY     STATUS        RESTARTS   AGE\n",
      "ssd-monitor-hgxwq   1/1       Terminating   0          4m\n",
      "The pod is being terminated. But you knew that was going to happen, right? This\n",
      "wraps up your exploration of DaemonSets, so you may want to delete your ssd-monitor\n",
      "DaemonSet. If you still have any other daemon pods running, you’ll see that deleting\n",
      "the DaemonSet deletes those pods as well. \n",
      "4.5\n",
      "Running pods that perform a single completable task \n",
      "Up to now, we’ve only talked about pods than need to run continuously. You’ll have\n",
      "cases where you only want to run a task that terminates after completing its work.\n",
      "ReplicationControllers, ReplicaSets, and DaemonSets run continuous tasks that are\n",
      "never considered completed. Processes in such pods are restarted when they exit. But\n",
      "in a completable task, after its process terminates, it should not be restarted again. \n",
      "4.5.1\n",
      "Introducing the Job resource\n",
      "Kubernetes includes support for this through the Job resource, which is similar to the\n",
      "other resources we’ve discussed in this chapter, but it allows you to run a pod whose\n",
      "container isn’t restarted when the process running inside finishes successfully. Once it\n",
      "does, the pod is considered complete. \n",
      " In the event of a node failure, the pods on that node that are managed by a Job will\n",
      "be rescheduled to other nodes the way ReplicaSet pods are. In the event of a failure of\n",
      "the process itself (when the process returns an error exit code), the Job can be config-\n",
      "ured to either restart the container or not.\n",
      " Figure 4.10 shows how a pod created by a Job is rescheduled to a new node if the\n",
      "node it was initially scheduled to fails. The figure also shows both a managed pod,\n",
      "which isn’t rescheduled, and a pod backed by a ReplicaSet, which is.\n",
      " For example, Jobs are useful for ad hoc tasks, where it’s crucial that the task fin-\n",
      "ishes properly. You could run the task in an unmanaged pod and wait for it to finish,\n",
      "but in the event of a node failing or the pod being evicted from the node while it is\n",
      "performing its task, you’d need to manually recreate it. Doing this manually doesn’t\n",
      "make sense—especially if the job takes hours to complete. \n",
      " An example of such a job would be if you had data stored somewhere and you\n",
      "needed to transform and export it somewhere. You’re going to emulate this by run-\n",
      "ning a container image built on top of the busybox image, which invokes the sleep\n",
      "command for two minutes. I’ve already built the image and pushed it to Docker Hub,\n",
      "but you can peek into its Dockerfile in the book’s code archive.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 145, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "113\n",
      "Running pods that perform a single completable task\n",
      "4.5.2\n",
      "Defining a Job resource\n",
      "Create the Job manifest as in the following listing.\n",
      "apiVersion: batch/v1        \n",
      "kind: Job                   \n",
      "metadata:\n",
      "  name: batch-job\n",
      "spec:                                \n",
      "  template: \n",
      "    metadata:\n",
      "      labels:                        \n",
      "        app: batch-job               \n",
      "    spec:\n",
      "      restartPolicy: OnFailure         \n",
      "      containers:\n",
      "      - name: main\n",
      "        image: luksa/batch-job\n",
      "Jobs are part of the batch API group and v1 API version. The YAML defines a\n",
      "resource of type Job that will run the luksa/batch-job image, which invokes a pro-\n",
      "cess that runs for exactly 120 seconds and then exits. \n",
      " In a pod’s specification, you can specify what Kubernetes should do when the\n",
      "processes running in the container finish. This is done through the restartPolicy\n",
      "Listing 4.11\n",
      "A YAML definition of a Job: exporter.yaml\n",
      "Node 1\n",
      "Pod A (unmanaged)\n",
      "Pod B (managed by a ReplicaSet)\n",
      "Pod C (managed by a Job)\n",
      "Node 2\n",
      "Node 1 fails\n",
      "Job C2 ﬁnishes\n",
      "Time\n",
      "Pod B2 (managed by a ReplicaSet)\n",
      "Pod C2 (managed by a Job)\n",
      "Pod A isn’t rescheduled,\n",
      "because there is nothing\n",
      "managing it.\n",
      "Figure 4.10\n",
      "Pods managed by Jobs are rescheduled until they finish successfully.\n",
      "Jobs are in the batch \n",
      "API group, version v1.\n",
      "You’re not specifying a pod \n",
      "selector (it will be created \n",
      "based on the labels in the \n",
      "pod template).\n",
      "Jobs can’t use the \n",
      "default restart policy, \n",
      "which is Always.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 146, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "114\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "pod spec property, which defaults to Always. Job pods can’t use the default policy,\n",
      "because they’re not meant to run indefinitely. Therefore, you need to explicitly set\n",
      "the restart policy to either OnFailure or Never. This setting is what prevents the con-\n",
      "tainer from being restarted when it finishes (not the fact that the pod is being man-\n",
      "aged by a Job resource).\n",
      "4.5.3\n",
      "Seeing a Job run a pod\n",
      "After you create this Job with the kubectl create command, you should see it start up\n",
      "a pod immediately:\n",
      "$ kubectl get jobs\n",
      "NAME        DESIRED   SUCCESSFUL   AGE\n",
      "batch-job   1         0            2s\n",
      "$ kubectl get po\n",
      "NAME              READY     STATUS    RESTARTS   AGE\n",
      "batch-job-28qf4   1/1       Running   0          4s\n",
      "After the two minutes have passed, the pod will no longer show up in the pod list and\n",
      "the Job will be marked as completed. By default, completed pods aren’t shown when\n",
      "you list pods, unless you use the --show-all (or -a) switch:\n",
      "$ kubectl get po -a\n",
      "NAME              READY     STATUS      RESTARTS   AGE\n",
      "batch-job-28qf4   0/1       Completed   0          2m\n",
      "The reason the pod isn’t deleted when it completes is to allow you to examine its logs;\n",
      "for example:\n",
      "$ kubectl logs batch-job-28qf4\n",
      "Fri Apr 29 09:58:22 UTC 2016 Batch job starting\n",
      "Fri Apr 29 10:00:22 UTC 2016 Finished succesfully\n",
      "The pod will be deleted when you delete it or the Job that created it. Before you do\n",
      "that, let’s look at the Job resource again:\n",
      "$ kubectl get job\n",
      "NAME        DESIRED   SUCCESSFUL   AGE\n",
      "batch-job   1         1            9m\n",
      "The Job is shown as having completed successfully. But why is that piece of informa-\n",
      "tion shown as a number instead of as yes or true? And what does the DESIRED column\n",
      "indicate? \n",
      "4.5.4\n",
      "Running multiple pod instances in a Job\n",
      "Jobs may be configured to create more than one pod instance and run them in paral-\n",
      "lel or sequentially. This is done by setting the completions and the parallelism prop-\n",
      "erties in the Job spec.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 147, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "115\n",
      "Running pods that perform a single completable task\n",
      "RUNNING JOB PODS SEQUENTIALLY\n",
      "If you need a Job to run more than once, you set completions to how many times you\n",
      "want the Job’s pod to run. The following listing shows an example.\n",
      "apiVersion: batch/v1\n",
      "kind: Job\n",
      "metadata:\n",
      "  name: multi-completion-batch-job\n",
      "spec:\n",
      "  completions: 5                  \n",
      "  template:\n",
      "    <template is the same as in listing 4.11>\n",
      "This Job will run five pods one after the other. It initially creates one pod, and when\n",
      "the pod’s container finishes, it creates the second pod, and so on, until five pods com-\n",
      "plete successfully. If one of the pods fails, the Job creates a new pod, so the Job may\n",
      "create more than five pods overall.\n",
      "RUNNING JOB PODS IN PARALLEL\n",
      "Instead of running single Job pods one after the other, you can also make the Job run\n",
      "multiple pods in parallel. You specify how many pods are allowed to run in parallel\n",
      "with the parallelism  Job spec property, as shown in the following listing.\n",
      "apiVersion: batch/v1\n",
      "kind: Job\n",
      "metadata:\n",
      "  name: multi-completion-batch-job\n",
      "spec:\n",
      "  completions: 5                    \n",
      "  parallelism: 2                    \n",
      "  template:\n",
      "    <same as in listing 4.11>\n",
      "By setting parallelism to 2, the Job creates two pods and runs them in parallel:\n",
      "$ kubectl get po\n",
      "NAME                               READY   STATUS     RESTARTS   AGE\n",
      "multi-completion-batch-job-lmmnk   1/1     Running    0          21s\n",
      "multi-completion-batch-job-qx4nq   1/1     Running    0          21s\n",
      "As soon as one of them finishes, the Job will run the next pod, until five pods finish\n",
      "successfully.\n",
      "Listing 4.12\n",
      "A Job requiring multiple completions: multi-completion-batch-job.yaml\n",
      "Listing 4.13\n",
      "Running Job pods in parallel: multi-completion-parallel-batch-job.yaml\n",
      "Setting completions to \n",
      "5 makes this Job run \n",
      "five pods sequentially.\n",
      "This job must ensure \n",
      "five pods complete \n",
      "successfully.\n",
      "Up to two pods \n",
      "can run in parallel.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 148, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "116\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      "SCALING A JOB\n",
      "You can even change a Job’s parallelism property while the Job is running. This is\n",
      "similar to scaling a ReplicaSet or ReplicationController, and can be done with the\n",
      "kubectl scale command:\n",
      "$ kubectl scale job multi-completion-batch-job --replicas 3\n",
      "job \"multi-completion-batch-job\" scaled\n",
      "Because you’ve increased parallelism from 2 to 3, another pod is immediately spun\n",
      "up, so three pods are now running.\n",
      "4.5.5\n",
      "Limiting the time allowed for a Job pod to complete\n",
      "We need to discuss one final thing about Jobs. How long should the Job wait for a pod\n",
      "to finish? What if the pod gets stuck and can’t finish at all (or it can’t finish fast\n",
      "enough)?\n",
      " A pod’s time can be limited by setting the activeDeadlineSeconds property in the\n",
      "pod spec. If the pod runs longer than that, the system will try to terminate it and will\n",
      "mark the Job as failed. \n",
      "NOTE\n",
      "You can configure how many times a Job can be retried before it is\n",
      "marked as failed by specifying the spec.backoffLimit field in the Job mani-\n",
      "fest. If you don't explicitly specify it, it defaults to 6.\n",
      "4.6\n",
      "Scheduling Jobs to run periodically or once \n",
      "in the future\n",
      "Job resources run their pods immediately when you create the Job resource. But many\n",
      "batch jobs need to be run at a specific time in the future or repeatedly in the specified\n",
      "interval. In Linux- and UNIX-like operating systems, these jobs are better known as\n",
      "cron jobs. Kubernetes supports them, too.\n",
      " A cron job in Kubernetes is configured by creating a CronJob resource. The\n",
      "schedule for running the job is specified in the well-known cron format, so if you’re\n",
      "familiar with regular cron jobs, you’ll understand Kubernetes’ CronJobs in a matter\n",
      "of seconds.\n",
      " At the configured time, Kubernetes will create a Job resource according to the Job\n",
      "template configured in the CronJob object. When the Job resource is created, one or\n",
      "more pod replicas will be created and started according to the Job’s pod template, as\n",
      "you learned in the previous section. There’s nothing more to it.\n",
      " Let’s look at how to create CronJobs. \n",
      "4.6.1\n",
      "Creating a CronJob\n",
      "Imagine you need to run the batch job from your previous example every 15 minutes.\n",
      "To do that, create a CronJob resource with the following specification.\n",
      " \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 149, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "117\n",
      "Scheduling Jobs to run periodically or once in the future\n",
      "apiVersion: batch/v1beta1               \n",
      "kind: CronJob\n",
      "metadata:\n",
      "  name: batch-job-every-fifteen-minutes\n",
      "spec:\n",
      "  schedule: \"0,15,30,45 * * * *\"           \n",
      "  jobTemplate:\n",
      "    spec:\n",
      "      template:                            \n",
      "        metadata:                          \n",
      "          labels:                          \n",
      "            app: periodic-batch-job        \n",
      "        spec:                              \n",
      "          restartPolicy: OnFailure         \n",
      "          containers:                      \n",
      "          - name: main                     \n",
      "            image: luksa/batch-job         \n",
      "As you can see, it’s not too complicated. You’ve specified a schedule and a template\n",
      "from which the Job objects will be created. \n",
      "CONFIGURING THE SCHEDULE\n",
      "If you’re unfamiliar with the cron schedule format, you’ll find great tutorials and\n",
      "explanations online, but as a quick introduction, from left to right, the schedule con-\n",
      "tains the following five entries:\n",
      "Minute\n",
      "Hour\n",
      "Day of month\n",
      "Month\n",
      "Day of week.\n",
      "In the example, you want to run the job every 15 minutes, so the schedule needs to be\n",
      "\"0,15,30,45 * * * *\", which means at the 0, 15, 30 and 45 minutes mark of every hour\n",
      "(first asterisk), of every day of the month (second asterisk), of every month (third\n",
      "asterisk) and on every day of the week (fourth asterisk). \n",
      " If, instead, you wanted it to run every 30 minutes, but only on the first day of the\n",
      "month, you’d set the schedule to \"0,30 * 1 * *\", and if you want it to run at 3AM every\n",
      "Sunday, you’d set it to \"0 3 * * 0\" (the last zero stands for Sunday).\n",
      "CONFIGURING THE JOB TEMPLATE\n",
      "A CronJob creates Job resources from the jobTemplate property configured in the\n",
      "CronJob spec, so refer to section 4.5 for more information on how to configure it.\n",
      "4.6.2\n",
      "Understanding how scheduled jobs are run\n",
      "Job resources will be created from the CronJob resource at approximately the sched-\n",
      "uled time. The Job then creates the pods. \n",
      "Listing 4.14\n",
      "YAML for a CronJob resource: cronjob.yaml\n",
      "API group is batch, \n",
      "version is v1beta1\n",
      "This job should run at the \n",
      "0, 15, 30 and 45 minutes of \n",
      "every hour, every day.\n",
      "The template for the \n",
      "Job resources that \n",
      "will be created by \n",
      "this CronJob\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 150, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "118\n",
      "CHAPTER 4\n",
      "Replication and other controllers: deploying managed pods\n",
      " It may happen that the Job or pod is created and run relatively late. You may have\n",
      "a hard requirement for the job to not be started too far over the scheduled time. In\n",
      "that case, you can specify a deadline by specifying the startingDeadlineSeconds field\n",
      "in the CronJob specification as shown in the following listing.\n",
      "apiVersion: batch/v1beta1\n",
      "kind: CronJob\n",
      "spec:\n",
      "  schedule: \"0,15,30,45 * * * *\"\n",
      "  startingDeadlineSeconds: 15    \n",
      "  ...\n",
      "In the example in listing 4.15, one of the times the job is supposed to run is 10:30:00.\n",
      "If it doesn’t start by 10:30:15 for whatever reason, the job will not run and will be\n",
      "shown as Failed. \n",
      " In normal circumstances, a CronJob always creates only a single Job for each exe-\n",
      "cution configured in the schedule, but it may happen that two Jobs are created at the\n",
      "same time, or none at all. To combat the first problem, your jobs should be idempo-\n",
      "tent (running them multiple times instead of once shouldn’t lead to unwanted\n",
      "results). For the second problem, make sure that the next job run performs any work\n",
      "that should have been done by the previous (missed) run.\n",
      "4.7\n",
      "Summary\n",
      "You’ve now learned how to keep pods running and have them rescheduled in the\n",
      "event of node failures. You should now know that\n",
      "You can specify a liveness probe to have Kubernetes restart your container as\n",
      "soon as it’s no longer healthy (where the app defines what’s considered\n",
      "healthy).\n",
      "Pods shouldn’t be created directly, because they will not be re-created if they’re\n",
      "deleted by mistake, if the node they’re running on fails, or if they’re evicted\n",
      "from the node.\n",
      "ReplicationControllers always keep the desired number of pod replicas\n",
      "running.\n",
      "Scaling pods horizontally is as easy as changing the desired replica count on a\n",
      "ReplicationController.\n",
      "Pods aren’t owned by the ReplicationControllers and can be moved between\n",
      "them if necessary.\n",
      "A ReplicationController creates new pods from a pod template. Changing the\n",
      "template has no effect on existing pods.\n",
      "Listing 4.15\n",
      "Specifying a startingDeadlineSeconds for a CronJob\n",
      "At the latest, the pod must \n",
      "start running at 15 seconds \n",
      "past the scheduled time.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 151, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "119\n",
      "Summary\n",
      "ReplicationControllers should be replaced with ReplicaSets and Deployments,\n",
      "which provide the same functionality, but with additional powerful features.\n",
      "ReplicationControllers and ReplicaSets schedule pods to random cluster nodes,\n",
      "whereas DaemonSets make sure every node runs a single instance of a pod\n",
      "defined in the DaemonSet.\n",
      "Pods that perform a batch task should be created through a Kubernetes Job\n",
      "resource, not directly or through a ReplicationController or similar object.\n",
      "Jobs that need to run sometime in the future can be created through CronJob\n",
      "resources. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 152, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "120\n",
      "Services: enabling\n",
      "clients to discover\n",
      "and talk to pods\n",
      "You’ve learned about pods and how to deploy them through ReplicaSets and similar\n",
      "resources to ensure they keep running. Although certain pods can do their work\n",
      "independently of an external stimulus, many applications these days are meant to\n",
      "respond to external requests. For example, in the case of microservices, pods will\n",
      "usually respond to HTTP requests coming either from other pods inside the cluster\n",
      "or from clients outside the cluster. \n",
      " Pods need a way of finding other pods if they want to consume the services they\n",
      "provide. Unlike in the non-Kubernetes world, where a sysadmin would configure\n",
      "This chapter covers\n",
      "Creating Service resources to expose a group of \n",
      "pods at a single address\n",
      "Discovering services in the cluster\n",
      "Exposing services to external clients\n",
      "Connecting to external services from inside the \n",
      "cluster\n",
      "Controlling whether a pod is ready to be part of \n",
      "the service or not\n",
      "Troubleshooting services\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 153, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "121\n",
      "Introducing services\n",
      "each client app by specifying the exact IP address or hostname of the server providing\n",
      "the service in the client’s configuration files, doing the same in Kubernetes wouldn’t\n",
      "work, because\n",
      "Pods are ephemeral—They may come and go at any time, whether it’s because a\n",
      "pod is removed from a node to make room for other pods, because someone\n",
      "scaled down the number of pods, or because a cluster node has failed.\n",
      "Kubernetes assigns an IP address to a pod after the pod has been scheduled to a node\n",
      "and before it’s started—Clients thus can’t know the IP address of the server pod\n",
      "up front.\n",
      "Horizontal scaling means multiple pods may provide the same service—Each of those\n",
      "pods has its own IP address. Clients shouldn’t care how many pods are backing\n",
      "the service and what their IPs are. They shouldn’t have to keep a list of all the\n",
      "individual IPs of pods. Instead, all those pods should be accessible through a\n",
      "single IP address.\n",
      "To solve these problems, Kubernetes also provides another resource type—Services—\n",
      "that we’ll discuss in this chapter.\n",
      "5.1\n",
      "Introducing services\n",
      "A Kubernetes Service is a resource you create to make a single, constant point of\n",
      "entry to a group of pods providing the same service. Each service has an IP address\n",
      "and port that never change while the service exists. Clients can open connections to\n",
      "that IP and port, and those connections are then routed to one of the pods backing\n",
      "that service. This way, clients of a service don’t need to know the location of individ-\n",
      "ual pods providing the service, allowing those pods to be moved around the cluster\n",
      "at any time. \n",
      "EXPLAINING SERVICES WITH AN EXAMPLE\n",
      "Let’s revisit the example where you have a frontend web server and a backend data-\n",
      "base server. There may be multiple pods that all act as the frontend, but there may\n",
      "only be a single backend database pod. You need to solve two problems to make the\n",
      "system function:\n",
      "External clients need to connect to the frontend pods without caring if there’s\n",
      "only a single web server or hundreds.\n",
      "The frontend pods need to connect to the backend database. Because the data-\n",
      "base runs inside a pod, it may be moved around the cluster over time, causing\n",
      "its IP address to change. You don’t want to reconfigure the frontend pods every\n",
      "time the backend database is moved.\n",
      "By creating a service for the frontend pods and configuring it to be accessible from\n",
      "outside the cluster, you expose a single, constant IP address through which external\n",
      "clients can connect to the pods. Similarly, by also creating a service for the backend\n",
      "pod, you create a stable address for the backend pod. The service address doesn’t\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 154, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "122\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "change even if the pod’s IP address changes. Additionally, by creating the service, you\n",
      "also enable the frontend pods to easily find the backend service by its name through\n",
      "either environment variables or DNS. All the components of your system (the two ser-\n",
      "vices, the two sets of pods backing those services, and the interdependencies between\n",
      "them) are shown in figure 5.1.\n",
      "You now understand the basic idea behind services. Now, let’s dig deeper by first see-\n",
      "ing how they can be created.\n",
      "5.1.1\n",
      "Creating services\n",
      "As you’ve seen, a service can be backed by more than one pod. Connections to the ser-\n",
      "vice are load-balanced across all the backing pods. But how exactly do you define\n",
      "which pods are part of the service and which aren’t? \n",
      " You probably remember label selectors and how they’re used in Replication-\n",
      "Controllers and other pod controllers to specify which pods belong to the same set.\n",
      "The same mechanism is used by services in the same way, as you can see in figure 5.2.\n",
      " In the previous chapter, you created a ReplicationController which then ran three\n",
      "instances of the pod containing the Node.js app. Create the ReplicationController\n",
      "again and verify three pod instances are up and running. After that, you’ll create a\n",
      "Service for those three pods. \n",
      "Frontend pod 1\n",
      "IP: 2.1.1.1\n",
      "External client\n",
      "Frontend pod 2\n",
      "IP: 2.1.1.2\n",
      "Frontend pod 3\n",
      "IP: 2.1.1.3\n",
      "Backend pod\n",
      "IP: 2.1.1.4\n",
      "Frontend service\n",
      "IP: 1.1.1.1\n",
      "Backend service\n",
      "IP: 1.1.1.2\n",
      "Frontend components\n",
      "Backend components\n",
      "Figure 5.1\n",
      "Both internal and external clients usually connect to pods through services.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 155, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "123\n",
      "Introducing services\n",
      "CREATING A SERVICE THROUGH KUBECTL EXPOSE\n",
      "The easiest way to create a service is through kubectl expose, which you’ve already\n",
      "used in chapter 2 to expose the ReplicationController you created earlier. The\n",
      "expose command created a Service resource with the same pod selector as the one\n",
      "used by the ReplicationController, thereby exposing all its pods through a single IP\n",
      "address and port. \n",
      " Now, instead of using the expose command, you’ll create a service manually by\n",
      "posting a YAML to the Kubernetes API server. \n",
      "CREATING A SERVICE THROUGH A YAML DESCRIPTOR\n",
      "Create a file called kubia-svc.yaml with the following listing’s contents.\n",
      "apiVersion: v1\n",
      "kind: Service             \n",
      "metadata:\n",
      "  name: kubia              \n",
      "spec:\n",
      "  ports:\n",
      "  - port: 80              \n",
      "    targetPort: 8080       \n",
      "  selector:                 \n",
      "    app: kubia              \n",
      "You’re defining a service called kubia, which will accept connections on port 80 and\n",
      "route each connection to port 8080 of one of the pods matching the app=kubia\n",
      "label selector. \n",
      " Go ahead and create the service by posting the file using kubectl create.\n",
      "Listing 5.1\n",
      "A definition of a service: kubia-svc.yaml\n",
      "app: kubia\n",
      "Pod: kubia-q3vkg\n",
      "Pod: kubia-k0xz6\n",
      "Pod: kubia-53thy\n",
      "Client\n",
      "Service: kubia\n",
      "Selector: app=kubia\n",
      "app: kubia\n",
      "app: kubia\n",
      "Figure 5.2\n",
      "Label selectors \n",
      "determine which pods belong \n",
      "to the Service.\n",
      "The port this service \n",
      "will be available on\n",
      "The container port the \n",
      "service will forward to\n",
      "All pods with the app=kubia \n",
      "label will be part of this service.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 156, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "124\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "EXAMINING YOUR NEW SERVICE\n",
      "After posting the YAML, you can list all Service resources in your namespace and see\n",
      "that an internal cluster IP has been assigned to your service:\n",
      "$ kubectl get svc\n",
      "NAME         CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE\n",
      "kubernetes   10.111.240.1     <none>        443/TCP   30d\n",
      "kubia        10.111.249.153   <none>        80/TCP    6m     \n",
      "The list shows that the IP address assigned to the service is 10.111.249.153. Because\n",
      "this is the cluster IP, it’s only accessible from inside the cluster. The primary purpose\n",
      "of services is exposing groups of pods to other pods in the cluster, but you’ll usually\n",
      "also want to expose services externally. You’ll see how to do that later. For now, let’s\n",
      "use your service from inside the cluster and see what it does.\n",
      "TESTING YOUR SERVICE FROM WITHIN THE CLUSTER\n",
      "You can send requests to your service from within the cluster in a few ways:\n",
      "The obvious way is to create a pod that will send the request to the service’s\n",
      "cluster IP and log the response. You can then examine the pod’s log to see\n",
      "what the service’s response was.\n",
      "You can ssh into one of the Kubernetes nodes and use the curl command.\n",
      "You can execute the curl command inside one of your existing pods through\n",
      "the kubectl exec command.\n",
      "Let’s go for the last option, so you also learn how to run commands in existing pods. \n",
      "REMOTELY EXECUTING COMMANDS IN RUNNING CONTAINERS\n",
      "The kubectl exec command allows you to remotely run arbitrary commands inside\n",
      "an existing container of a pod. This comes in handy when you want to examine the\n",
      "contents, state, and/or environment of a container. List the pods with the kubectl\n",
      "get pods command and choose one as your target for the exec command (in the fol-\n",
      "lowing example, I’ve chosen the kubia-7nog1 pod as the target). You’ll also need to\n",
      "obtain the cluster IP of your service (using kubectl get svc, for example). When run-\n",
      "ning the following commands yourself, be sure to replace the pod name and the ser-\n",
      "vice IP with your own: \n",
      "$ kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153\n",
      "You’ve hit kubia-gzwli\n",
      "If you’ve used ssh to execute commands on a remote system before, you’ll recognize\n",
      "that kubectl exec isn’t much different.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Here’s your \n",
      "service.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 157, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "125\n",
      "Introducing services\n",
      "Let’s go over what transpired when you ran the command. Figure 5.3 shows the\n",
      "sequence of events. You instructed Kubernetes to execute the curl command inside the\n",
      "container of one of your pods. Curl sent an HTTP request to the service IP, which is\n",
      "backed by three pods. The Kubernetes service proxy intercepted the connection,\n",
      "selected a random pod among the three pods, and forwarded the request to it. Node.js\n",
      "running inside that pod then handled the request and returned an HTTP response con-\n",
      "taining the pod’s name. Curl then printed the response to the standard output, which\n",
      "was intercepted and printed to its standard output on your local machine by kubectl.\n",
      "Why the double dash?\n",
      "The double dash (--) in the command signals the end of command options for\n",
      "kubectl. Everything after the double dash is the command that should be executed\n",
      "inside the pod. Using the double dash isn’t necessary if the command has no\n",
      "arguments that start with a dash. But in your case, if you don’t use the double dash\n",
      "there, the -s option would be interpreted as an option for kubectl exec and would\n",
      "result in the following strange and highly misleading error:\n",
      "$ kubectl exec kubia-7nog1 curl -s http://10.111.249.153\n",
      "The connection to the server 10.111.249.153 was refused – did you \n",
      "specify the right host or port?\n",
      "This has nothing to do with your service refusing the connection. It’s because\n",
      "kubectl is not able to connect to an API server at 10.111.249.153 (the -s option\n",
      "is used to tell kubectl to connect to a different API server than the default).\n",
      "3. Curl sends HTTP\n",
      "GET request\n",
      "4. Service redirects HTTP\n",
      "connection to a randomly\n",
      "selected pod\n",
      "2. Curl is executed\n",
      "inside the container\n",
      "running node.js\n",
      "6. The output of the\n",
      "command is sent\n",
      "curl\n",
      "back to kubectl and\n",
      "printed by it\n",
      "5. HTTP response is\n",
      "sent back to curl\n",
      "Pod: kubia-7nog1\n",
      "Container\n",
      "node.js\n",
      "curl http://\n",
      "10.111.249.153\n",
      "Pod: kubia-gzwli\n",
      "Container\n",
      "node.js\n",
      "Pod: kubia-5fje3\n",
      "Container\n",
      "node.js\n",
      "1. kubectl exec\n",
      "Service: kubia\n",
      "10.111.249.153:80\n",
      "Figure 5.3\n",
      "Using kubectl exec to test out a connection to the service by running curl in one of the pods\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 158, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "126\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "In the previous example, you executed the curl command as a separate process, but\n",
      "inside the pod’s main container. This isn’t much different from the actual main pro-\n",
      "cess in the container talking to the service.\n",
      "CONFIGURING SESSION AFFINITY ON THE SERVICE\n",
      "If you execute the same command a few more times, you should hit a different pod\n",
      "with every invocation, because the service proxy normally forwards each connection\n",
      "to a randomly selected backing pod, even if the connections are coming from the\n",
      "same client. \n",
      " If, on the other hand, you want all requests made by a certain client to be redi-\n",
      "rected to the same pod every time, you can set the service’s sessionAffinity property\n",
      "to ClientIP (instead of None, which is the default), as shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Service             \n",
      "spec:\n",
      "  sessionAffinity: ClientIP\n",
      "  ...\n",
      "This makes the service proxy redirect all requests originating from the same client IP\n",
      "to the same pod. As an exercise, you can create an additional service with session affin-\n",
      "ity set to ClientIP and try sending requests to it.\n",
      " Kubernetes supports only two types of service session affinity: None and ClientIP.\n",
      "You may be surprised it doesn’t have a cookie-based session affinity option, but you\n",
      "need to understand that Kubernetes services don’t operate at the HTTP level. Services\n",
      "deal with TCP and UDP packets and don’t care about the payload they carry. Because\n",
      "cookies are a construct of the HTTP protocol, services don’t know about them, which\n",
      "explains why session affinity cannot be based on cookies. \n",
      "EXPOSING MULTIPLE PORTS IN THE SAME SERVICE\n",
      "Your service exposes only a single port, but services can also support multiple ports. For\n",
      "example, if your pods listened on two ports—let’s say 8080 for HTTP and 8443 for\n",
      "HTTPS—you could use a single service to forward both port 80 and 443 to the pod’s\n",
      "ports 8080 and 8443. You don’t need to create two different services in such cases. Using\n",
      "a single, multi-port service exposes all the service’s ports through a single cluster IP.\n",
      "NOTE\n",
      "When creating a service with multiple ports, you must specify a name\n",
      "for each port.\n",
      "The spec for a multi-port service is shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Service             \n",
      "metadata:\n",
      "  name: kubia              \n",
      "Listing 5.2\n",
      "A example of a service with ClientIP session affinity configured\n",
      "Listing 5.3\n",
      "Specifying multiple ports in a service definition\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 159, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "127\n",
      "Introducing services\n",
      "spec:\n",
      "  ports:\n",
      "  - name: http              \n",
      "    port: 80                \n",
      "    targetPort: 8080        \n",
      "  - name: https             \n",
      "    port: 443               \n",
      "    targetPort: 8443        \n",
      "  selector:                 \n",
      "    app: kubia              \n",
      "NOTE\n",
      "The label selector applies to the service as a whole—it can’t be config-\n",
      "ured for each port individually. If you want different ports to map to different\n",
      "subsets of pods, you need to create two services.\n",
      "Because your kubia pods don’t listen on multiple ports, creating a multi-port service\n",
      "and a multi-port pod is left as an exercise to you.\n",
      "USING NAMED PORTS\n",
      "In all these examples, you’ve referred to the target port by its number, but you can also\n",
      "give a name to each pod’s port and refer to it by name in the service spec. This makes\n",
      "the service spec slightly clearer, especially if the port numbers aren’t well-known.\n",
      " For example, suppose your pod defines names for its ports as shown in the follow-\n",
      "ing listing.\n",
      "kind: Pod\n",
      "spec:\n",
      "  containers:\n",
      "  - name: kubia\n",
      "    ports:\n",
      "    - name: http               \n",
      "      containerPort: 8080      \n",
      "    - name: https              \n",
      "      containerPort: 8443      \n",
      "You can then refer to those ports by name in the service spec, as shown in the follow-\n",
      "ing listing.\n",
      "apiVersion: v1\n",
      "kind: Service             \n",
      "spec:\n",
      "  ports:\n",
      "  - name: http              \n",
      "    port: 80                \n",
      "    targetPort: http        \n",
      "  - name: https             \n",
      "    port: 443               \n",
      "    targetPort: https       \n",
      "Listing 5.4\n",
      "Specifying port names in a pod definition\n",
      "Listing 5.5\n",
      "Referring to named ports in a service\n",
      "Port 80 is mapped to \n",
      "the pods’ port 8080.\n",
      "Port 443 is mapped to \n",
      "pods’ port 8443.\n",
      "The label selector always \n",
      "applies to the whole service.\n",
      "Container’s port \n",
      "8080 is called http\n",
      "Port 8443 is called https.\n",
      "Port 80 is mapped to the \n",
      "container’s port called http.\n",
      "Port 443 is mapped to the container’s \n",
      "port, whose name is https.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 160, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "128\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "But why should you even bother with naming ports? The biggest benefit of doing so is\n",
      "that it enables you to change port numbers later without having to change the service\n",
      "spec. Your pod currently uses port 8080 for http, but what if you later decide you’d\n",
      "like to move that to port 80? \n",
      " If you’re using named ports, all you need to do is change the port number in the\n",
      "pod spec (while keeping the port’s name unchanged). As you spin up pods with the\n",
      "new ports, client connections will be forwarded to the appropriate port numbers,\n",
      "depending on the pod receiving the connection (port 8080 on old pods and port 80\n",
      "on the new ones).\n",
      "5.1.2\n",
      "Discovering services\n",
      "By creating a service, you now have a single and stable IP address and port that you\n",
      "can hit to access your pods. This address will remain unchanged throughout the\n",
      "whole lifetime of the service. Pods behind this service may come and go, their IPs may\n",
      "change, their number can go up or down, but they’ll always be accessible through the\n",
      "service’s single and constant IP address. \n",
      " But how do the client pods know the IP and port of a service? Do you need to cre-\n",
      "ate the service first, then manually look up its IP address and pass the IP to the config-\n",
      "uration options of the client pod? Not really. Kubernetes also provides ways for client\n",
      "pods to discover a service’s IP and port.\n",
      "DISCOVERING SERVICES THROUGH ENVIRONMENT VARIABLES\n",
      "When a pod is started, Kubernetes initializes a set of environment variables pointing\n",
      "to each service that exists at that moment. If you create the service before creating the\n",
      "client pods, processes in those pods can get the IP address and port of the service by\n",
      "inspecting their environment variables. \n",
      " Let’s see what those environment variables look like by examining the environment\n",
      "of one of your running pods. You’ve already learned that you can use the kubectl exec\n",
      "command to run a command in the pod, but because you created the service only\n",
      "after your pods had been created, the environment variables for the service couldn’t\n",
      "have been set yet. You’ll need to address that first.\n",
      " Before you can see environment variables for your service, you first need to delete\n",
      "all the pods and let the ReplicationController create new ones. You may remember\n",
      "you can delete all pods without specifying their names like this:\n",
      "$ kubectl delete po --all\n",
      "pod \"kubia-7nog1\" deleted\n",
      "pod \"kubia-bf50t\" deleted\n",
      "pod \"kubia-gzwli\" deleted\n",
      "Now you can list the new pods (I’m sure you know how to do that) and pick one as\n",
      "your target for the kubectl exec command. Once you’ve selected your target pod,\n",
      "you can list environment variables by running the env command inside the container,\n",
      "as shown in the following listing.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 161, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "129\n",
      "Introducing services\n",
      "$ kubectl exec kubia-3inly env\n",
      "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "HOSTNAME=kubia-3inly\n",
      "KUBERNETES_SERVICE_HOST=10.111.240.1\n",
      "KUBERNETES_SERVICE_PORT=443\n",
      "...\n",
      "KUBIA_SERVICE_HOST=10.111.249.153             \n",
      "KUBIA_SERVICE_PORT=80                            \n",
      "...\n",
      "Two services are defined in your cluster: the kubernetes and the kubia service (you\n",
      "saw this earlier with the kubectl get svc command); consequently, two sets of service-\n",
      "related environment variables are in the list. Among the variables that pertain to the\n",
      "kubia service you created at the beginning of the chapter, you’ll see the KUBIA_SERVICE\n",
      "_HOST and the KUBIA_SERVICE_PORT environment variables, which hold the IP address\n",
      "and port of the kubia service, respectively. \n",
      " Turning back to the frontend-backend example we started this chapter with, when\n",
      "you have a frontend pod that requires the use of a backend database server pod, you\n",
      "can expose the backend pod through a service called backend-database and then\n",
      "have the frontend pod look up its IP address and port through the environment vari-\n",
      "ables BACKEND_DATABASE_SERVICE_HOST and BACKEND_DATABASE_SERVICE_PORT.\n",
      "NOTE\n",
      "Dashes in the service name are converted to underscores and all let-\n",
      "ters are uppercased when the service name is used as the prefix in the envi-\n",
      "ronment variable’s name. \n",
      "Environment variables are one way of looking up the IP and port of a service, but isn’t\n",
      "this usually the domain of DNS? Why doesn’t Kubernetes include a DNS server and\n",
      "allow you to look up service IPs through DNS instead? As it turns out, it does!\n",
      "DISCOVERING SERVICES THROUGH DNS\n",
      "Remember in chapter 3 when you listed pods in the kube-system namespace? One of\n",
      "the pods was called kube-dns. The kube-system namespace also includes a corre-\n",
      "sponding service with the same name.\n",
      " As the name suggests, the pod runs a DNS server, which all other pods running in\n",
      "the cluster are automatically configured to use (Kubernetes does that by modifying\n",
      "each container’s /etc/resolv.conf file). Any DNS query performed by a process run-\n",
      "ning in a pod will be handled by Kubernetes’ own DNS server, which knows all the ser-\n",
      "vices running in your system. \n",
      "NOTE\n",
      "Whether a pod uses the internal DNS server or not is configurable\n",
      "through the dnsPolicy property in each pod’s spec.\n",
      "Each service gets a DNS entry in the internal DNS server, and client pods that know\n",
      "the name of the service can access it through its fully qualified domain name (FQDN)\n",
      "instead of resorting to environment variables. \n",
      "Listing 5.6\n",
      "Service-related environment variables in a container\n",
      "Here’s the cluster \n",
      "IP of the service.\n",
      "And here’s the port the \n",
      "service is available on.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 162, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "130\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "CONNECTING TO THE SERVICE THROUGH ITS FQDN\n",
      "To revisit the frontend-backend example, a frontend pod can connect to the backend-\n",
      "database service by opening a connection to the following FQDN:\n",
      "backend-database.default.svc.cluster.local\n",
      "backend-database corresponds to the service name, default stands for the name-\n",
      "space the service is defined in, and svc.cluster.local is a configurable cluster\n",
      "domain suffix used in all cluster local service names. \n",
      "NOTE\n",
      "The client must still know the service’s port number. If the service is\n",
      "using a standard port (for example, 80 for HTTP or 5432 for Postgres), that\n",
      "shouldn’t be a problem. If not, the client can get the port number from the\n",
      "environment variable.\n",
      "Connecting to a service can be even simpler than that. You can omit the svc.cluster\n",
      ".local suffix and even the namespace, when the frontend pod is in the same name-\n",
      "space as the database pod. You can thus refer to the service simply as backend-\n",
      "database. That’s incredibly simple, right?\n",
      " Let’s try this. You’ll try to access the kubia service through its FQDN instead of its\n",
      "IP. Again, you’ll need to do that inside an existing pod. You already know how to use\n",
      "kubectl exec to run a single command in a pod’s container, but this time, instead of\n",
      "running the curl command directly, you’ll run the bash shell instead, so you can then\n",
      "run multiple commands in the container. This is similar to what you did in chapter 2\n",
      "when you entered the container you ran with Docker by using the docker exec -it\n",
      "bash command. \n",
      "RUNNING A SHELL IN A POD’S CONTAINER\n",
      "You can use the kubectl exec command to run bash (or any other shell) inside a\n",
      "pod’s container. This way you’re free to explore the container as long as you want,\n",
      "without having to perform a kubectl exec for every command you want to run.\n",
      "NOTE\n",
      "The shell’s binary executable must be available in the container image\n",
      "for this to work.\n",
      "To use the shell properly, you need to pass the -it option to kubectl exec:\n",
      "$ kubectl exec -it kubia-3inly bash\n",
      "root@kubia-3inly:/# \n",
      "You’re now inside the container. You can use the curl command to access the kubia\n",
      "service in any of the following ways:\n",
      "root@kubia-3inly:/# curl http://kubia.default.svc.cluster.local\n",
      "You’ve hit kubia-5asi2\n",
      "root@kubia-3inly:/# curl http://kubia.default\n",
      "You’ve hit kubia-3inly\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 163, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "131\n",
      "Connecting to services living outside the cluster\n",
      "root@kubia-3inly:/# curl http://kubia\n",
      "You’ve hit kubia-8awf3\n",
      "You can hit your service by using the service’s name as the hostname in the requested\n",
      "URL. You can omit the namespace and the svc.cluster.local suffix because of how\n",
      "the DNS resolver inside each pod’s container is configured. Look at the /etc/resolv.conf\n",
      "file in the container and you’ll understand:\n",
      "root@kubia-3inly:/# cat /etc/resolv.conf\n",
      "search default.svc.cluster.local svc.cluster.local cluster.local ...\n",
      "UNDERSTANDING WHY YOU CAN’T PING A SERVICE IP\n",
      "One last thing before we move on. You know how to create services now, so you’ll soon\n",
      "create your own. But what if, for whatever reason, you can’t access your service?\n",
      " You’ll probably try to figure out what’s wrong by entering an existing pod and try-\n",
      "ing to access the service like you did in the last example. Then, if you still can’t access\n",
      "the service with a simple curl command, maybe you’ll try to ping the service IP to see\n",
      "if it’s up. Let’s try that now:\n",
      "root@kubia-3inly:/# ping kubia\n",
      "PING kubia.default.svc.cluster.local (10.111.249.153): 56 data bytes\n",
      "^C--- kubia.default.svc.cluster.local ping statistics ---\n",
      "54 packets transmitted, 0 packets received, 100% packet loss\n",
      "Hmm. curl-ing the service works, but pinging it doesn’t. That’s because the service’s\n",
      "cluster IP is a virtual IP, and only has meaning when combined with the service port.\n",
      "We’ll explain what that means and how services work in chapter 11. I wanted to men-\n",
      "tion that here because it’s the first thing users do when they try to debug a broken\n",
      "service and it catches most of them off guard.\n",
      "5.2\n",
      "Connecting to services living outside the cluster\n",
      "Up to now, we’ve talked about services backed by one or more pods running inside\n",
      "the cluster. But cases exist when you’d like to expose external services through the\n",
      "Kubernetes services feature. Instead of having the service redirect connections to\n",
      "pods in the cluster, you want it to redirect to external IP(s) and port(s). \n",
      " This allows you to take advantage of both service load balancing and service discov-\n",
      "ery. Client pods running in the cluster can connect to the external service like they\n",
      "connect to internal services.\n",
      "5.2.1\n",
      "Introducing service endpoints\n",
      "Before going into how to do this, let me first shed more light on services. Services\n",
      "don’t link to pods directly. Instead, a resource sits in between—the Endpoints\n",
      "resource. You may have already noticed endpoints if you used the kubectl describe\n",
      "command on your service, as shown in the following listing.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 164, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "132\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "$ kubectl describe svc kubia\n",
      "Name:                kubia\n",
      "Namespace:           default\n",
      "Labels:              <none>\n",
      "Selector:            app=kubia         \n",
      "Type:                ClusterIP\n",
      "IP:                  10.111.249.153\n",
      "Port:                <unset> 80/TCP\n",
      "Endpoints:           10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   \n",
      "Session Affinity:    None\n",
      "No events.\n",
      "An Endpoints resource (yes, plural) is a list of IP addresses and ports exposing a ser-\n",
      "vice. The Endpoints resource is like any other Kubernetes resource, so you can display\n",
      "its basic info with kubectl get:\n",
      "$ kubectl get endpoints kubia\n",
      "NAME    ENDPOINTS                                         AGE\n",
      "kubia   10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   1h\n",
      "Although the pod selector is defined in the service spec, it’s not used directly when\n",
      "redirecting incoming connections. Instead, the selector is used to build a list of IPs\n",
      "and ports, which is then stored in the Endpoints resource. When a client connects to a\n",
      "service, the service proxy selects one of those IP and port pairs and redirects the\n",
      "incoming connection to the server listening at that location.\n",
      "5.2.2\n",
      "Manually configuring service endpoints\n",
      "You may have probably realized this already, but having the service’s endpoints decou-\n",
      "pled from the service allows them to be configured and updated manually. \n",
      " If you create a service without a pod selector, Kubernetes won’t even create the\n",
      "Endpoints resource (after all, without a selector, it can’t know which pods to include\n",
      "in the service). It’s up to you to create the Endpoints resource to specify the list of\n",
      "endpoints for the service.\n",
      " To create a service with manually managed endpoints, you need to create both a\n",
      "Service and an Endpoints resource. \n",
      "CREATING A SERVICE WITHOUT A SELECTOR\n",
      "You’ll first create the YAML for the service itself, as shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: external-service     \n",
      "spec:                       \n",
      "  ports:\n",
      "  - port: 80                  \n",
      "Listing 5.7\n",
      "Full details of a service displayed with kubectl describe\n",
      "Listing 5.8\n",
      "A service without a pod selector: external-service.yaml\n",
      "The service’s pod \n",
      "selector is used to \n",
      "create the list of \n",
      "endpoints.\n",
      "The list of pod\n",
      "IPs and ports\n",
      "that represent\n",
      "the endpoints of\n",
      "this service\n",
      "The name of the service must \n",
      "match the name of the Endpoints \n",
      "object (see next listing).\n",
      "This service has no \n",
      "selector defined.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 165, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "133\n",
      "Connecting to services living outside the cluster\n",
      "You’re defining a service called external-service that will accept incoming connec-\n",
      "tions on port 80. You didn’t define a pod selector for the service.\n",
      "CREATING AN ENDPOINTS RESOURCE FOR A SERVICE WITHOUT A SELECTOR\n",
      "Endpoints are a separate resource and not an attribute of a service. Because you cre-\n",
      "ated the service without a selector, the corresponding Endpoints resource hasn’t been\n",
      "created automatically, so it’s up to you to create it. The following listing shows its\n",
      "YAML manifest.\n",
      "apiVersion: v1\n",
      "kind: Endpoints\n",
      "metadata:\n",
      "  name: external-service      \n",
      "subsets:\n",
      "  - addresses:\n",
      "    - ip: 11.11.11.11         \n",
      "    - ip: 22.22.22.22         \n",
      "    ports:\n",
      "    - port: 80      \n",
      "The Endpoints object needs to have the same name as the service and contain the list\n",
      "of target IP addresses and ports for the service. After both the Service and the End-\n",
      "points resource are posted to the server, the service is ready to be used like any regular\n",
      "service with a pod selector. Containers created after the service is created will include\n",
      "the environment variables for the service, and all connections to its IP:port pair will be\n",
      "load balanced between the service’s endpoints. \n",
      " Figure 5.4 shows three pods connecting to the service with external endpoints.\n",
      "If you later decide to migrate the external service to pods running inside Kubernetes,\n",
      "you can add a selector to the service, thereby making its Endpoints managed automat-\n",
      "ically. The same is also true in reverse—by removing the selector from a Service,\n",
      "Listing 5.9\n",
      "A manually created Endpoints resource: external-service-endpoints.yaml\n",
      "The name of the Endpoints object \n",
      "must match the name of the \n",
      "service (see previous listing).\n",
      "The IPs of the endpoints that the \n",
      "service will forward connections to\n",
      "The target port of the endpoints\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "External server 1\n",
      "IP: 11.11.11.11:80\n",
      "External server 2\n",
      "IP: 22.22.22.22:80\n",
      "Service\n",
      "10.111.249.214:80\n",
      "Kubernetes cluster\n",
      "Internet\n",
      "Figure 5.4\n",
      "Pods consuming a service with two external endpoints.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 166, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "134\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "Kubernetes stops updating its Endpoints. This means a service IP address can remain\n",
      "constant while the actual implementation of the service is changed. \n",
      "5.2.3\n",
      "Creating an alias for an external service\n",
      "Instead of exposing an external service by manually configuring the service’s End-\n",
      "points, a simpler method allows you to refer to an external service by its fully qualified\n",
      "domain name (FQDN).\n",
      "CREATING AN EXTERNALNAME SERVICE\n",
      "To create a service that serves as an alias for an external service, you create a Service\n",
      "resource with the type field set to ExternalName. For example, let’s imagine there’s a\n",
      "public API available at api.somecompany.com. You can define a service that points to\n",
      "it as shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: external-service\n",
      "spec:\n",
      "  type: ExternalName                       \n",
      "  externalName: someapi.somecompany.com     \n",
      "  ports:\n",
      "  - port: 80\n",
      "After the service is created, pods can connect to the external service through the\n",
      "external-service.default.svc.cluster.local domain name (or even external-\n",
      "service) instead of using the service’s actual FQDN. This hides the actual service\n",
      "name and its location from pods consuming the service, allowing you to modify the\n",
      "service definition and point it to a different service any time later, by only changing\n",
      "the externalName attribute or by changing the type back to ClusterIP and creating\n",
      "an Endpoints object for the service—either manually or by specifying a label selector\n",
      "on the service and having it created automatically.\n",
      " ExternalName services are implemented solely at the DNS level—a simple CNAME\n",
      "DNS record is created for the service. Therefore, clients connecting to the service will\n",
      "connect to the external service directly, bypassing the service proxy completely. For\n",
      "this reason, these types of services don’t even get a cluster IP. \n",
      "NOTE\n",
      "A CNAME record points to a fully qualified domain name instead of a\n",
      "numeric IP address.\n",
      "5.3\n",
      "Exposing services to external clients\n",
      "Up to now, we’ve only talked about how services can be consumed by pods from inside\n",
      "the cluster. But you’ll also want to expose certain services, such as frontend webserv-\n",
      "ers, to the outside, so external clients can access them, as depicted in figure 5.5.\n",
      "Listing 5.10\n",
      "An ExternalName-type service: external-service-externalname.yaml\n",
      "Service type is set \n",
      "to ExternalName\n",
      "The fully qualified domain \n",
      "name of the actual service\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 167, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "135\n",
      "Exposing services to external clients\n",
      "You have a few ways to make a service accessible externally:\n",
      "Setting the service type to NodePort—For a NodePort service, each cluster node\n",
      "opens a port on the node itself (hence the name) and redirects traffic received\n",
      "on that port to the underlying service. The service isn’t accessible only at the\n",
      "internal cluster IP and port, but also through a dedicated port on all nodes. \n",
      "Setting the service type to LoadBalancer, an extension of the NodePort type—This\n",
      "makes the service accessible through a dedicated load balancer, provisioned\n",
      "from the cloud infrastructure Kubernetes is running on. The load balancer redi-\n",
      "rects traffic to the node port across all the nodes. Clients connect to the service\n",
      "through the load balancer’s IP.\n",
      "Creating an Ingress resource, a radically different mechanism for exposing multiple ser-\n",
      "vices through a single IP address—It operates at the HTTP level (network layer 7)\n",
      "and can thus offer more features than layer 4 services can. We’ll explain Ingress\n",
      "resources in section 5.4. \n",
      "5.3.1\n",
      "Using a NodePort service\n",
      "The first method of exposing a set of pods to external clients is by creating a service\n",
      "and setting its type to NodePort. By creating a NodePort service, you make Kubernetes\n",
      "reserve a port on all its nodes (the same port number is used across all of them) and\n",
      "forward incoming connections to the pods that are part of the service. \n",
      " This is similar to a regular service (their actual type is ClusterIP), but a NodePort\n",
      "service can be accessed not only through the service’s internal cluster IP, but also\n",
      "through any node’s IP and the reserved node port. \n",
      " This will make more sense when you try interacting with a NodePort service.\n",
      "CREATING A NODEPORT SERVICE\n",
      "You’ll now create a NodePort service to see how you can use it. The following listing\n",
      "shows the YAML for the service.\n",
      " \n",
      "Kubernetes cluster\n",
      "External client\n",
      "Service\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Figure 5.5\n",
      "Exposing a service to external clients\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 168, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "136\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: kubia-nodeport\n",
      "spec:\n",
      "  type: NodePort            \n",
      "  ports:\n",
      "  - port: 80                 \n",
      "    targetPort: 8080        \n",
      "    nodePort: 30123        \n",
      "  selector:\n",
      "    app: kubia\n",
      "You set the type to NodePort and specify the node port this service should be bound to\n",
      "across all cluster nodes. Specifying the port isn’t mandatory; Kubernetes will choose a\n",
      "random port if you omit it. \n",
      "NOTE\n",
      "When you create the service in GKE, kubectl prints out a warning\n",
      "about having to configure firewall rules. We’ll see how to do that soon. \n",
      "EXAMINING YOUR NODEPORT SERVICE\n",
      "Let’s see the basic information of your service to learn more about it:\n",
      "$ kubectl get svc kubia-nodeport\n",
      "NAME             CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\n",
      "kubia-nodeport   10.111.254.223   <nodes>       80:30123/TCP   2m\n",
      "Look at the EXTERNAL-IP column. It shows <nodes>, indicating the service is accessible\n",
      "through the IP address of any cluster node. The PORT(S) column shows both the\n",
      "internal port of the cluster IP (80) and the node port (30123). The service is accessi-\n",
      "ble at the following addresses:\n",
      "\n",
      "10.11.254.223:80\n",
      "\n",
      "<1st node’s IP>:30123\n",
      "\n",
      "<2nd node’s IP>:30123, and so on.\n",
      "Figure 5.6 shows your service exposed on port 30123 of both of your cluster nodes\n",
      "(this applies if you’re running this on GKE; Minikube only has a single node, but the\n",
      "principle is the same). An incoming connection to one of those ports will be redi-\n",
      "rected to a randomly selected pod, which may or may not be the one running on the\n",
      "node the connection is being made to. \n",
      " \n",
      " \n",
      " \n",
      "Listing 5.11\n",
      "A NodePort service definition: kubia-svc-nodeport.yaml\n",
      "Set the service \n",
      "type to NodePort.\n",
      "This is the port of the \n",
      "service’s internal cluster IP.\n",
      "This is the target port \n",
      "of the backing pods.\n",
      "The service will be accessible \n",
      "through port 30123 of each of \n",
      "your cluster nodes.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 169, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "137\n",
      "Exposing services to external clients\n",
      "A connection received on port 30123 of the first node might be forwarded either to\n",
      "the pod running on the first node or to one of the pods running on the second node.\n",
      "CHANGING FIREWALL RULES TO LET EXTERNAL CLIENTS ACCESS OUR NODEPORT SERVICE\n",
      "As I’ve mentioned previously, before you can access your service through the node\n",
      "port, you need to configure the Google Cloud Platform’s firewalls to allow external\n",
      "connections to your nodes on that port. You’ll do this now:\n",
      "$ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123\n",
      "Created [https://www.googleapis.com/compute/v1/projects/kubia-\n",
      "1295/global/firewalls/kubia-svc-rule].\n",
      "NAME            NETWORK  SRC_RANGES  RULES      SRC_TAGS  TARGET_TAGS\n",
      "kubia-svc-rule  default  0.0.0.0/0   tcp:30123\n",
      "You can access your service through port 30123 of one of the node’s IPs. But you need\n",
      "to figure out the IP of a node first. Refer to the sidebar on how to do that.\n",
      " \n",
      " \n",
      " \n",
      "Kubernetes cluster\n",
      "External client\n",
      "Pod\n",
      "Node 2\n",
      "IP: 130.211.99.206\n",
      "Node 1\n",
      "IP: 130.211.97.55\n",
      "Port 30123\n",
      "Port 8080\n",
      "Pod\n",
      "Port 8080\n",
      "Pod\n",
      "Port 30123\n",
      "Port 8080\n",
      "Service\n",
      "Figure 5.6\n",
      "An external client connecting to a NodePort service either through Node 1 or 2\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 170, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "138\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "Once you know the IPs of your nodes, you can try accessing your service through them:\n",
      "$ curl http://130.211.97.55:30123\n",
      "You've hit kubia-ym8or\n",
      "$ curl http://130.211.99.206:30123\n",
      "You've hit kubia-xueq1\n",
      "TIP\n",
      "When using Minikube, you can easily access your NodePort services\n",
      "through your browser by running minikube service <service-name> [-n\n",
      "<namespace>].\n",
      "As you can see, your pods are now accessible to the whole internet through port 30123\n",
      "on any of your nodes. It doesn’t matter what node a client sends the request to. But if\n",
      "you only point your clients to the first node, when that node fails, your clients can’t\n",
      "access the service anymore. That’s why it makes sense to put a load balancer in front\n",
      "of the nodes to make sure you’re spreading requests across all healthy nodes and\n",
      "never sending them to a node that’s offline at that moment. \n",
      " If your Kubernetes cluster supports it (which is mostly true when Kubernetes is\n",
      "deployed on cloud infrastructure), the load balancer can be provisioned automati-\n",
      "cally by creating a LoadBalancer instead of a NodePort service. We’ll look at this next.\n",
      "5.3.2\n",
      "Exposing a service through an external load balancer\n",
      "Kubernetes clusters running on cloud providers usually support the automatic provi-\n",
      "sion of a load balancer from the cloud infrastructure. All you need to do is set the\n",
      "Using JSONPath to get the IPs of all your nodes \n",
      "You can find the IP in the JSON or YAML descriptors of the nodes. But instead of\n",
      "sifting through the relatively large JSON, you can tell kubectl to print out only the\n",
      "node IP instead of the whole service definition: \n",
      "$ kubectl get nodes -o jsonpath='{.items[*].status.\n",
      "➥ addresses[?(@.type==\"ExternalIP\")].address}'\n",
      "130.211.97.55 130.211.99.206\n",
      "You’re telling kubectl to only output the information you want by specifying a\n",
      "JSONPath. You’re probably familiar with XPath and how it’s used with XML. JSONPath\n",
      "is basically XPath for JSON. The JSONPath in the previous example instructs kubectl\n",
      "to do the following:\n",
      "Go through all the elements in the items attribute.\n",
      "For each element, enter the status attribute.\n",
      "Filter elements of the addresses attribute, taking only those that have the\n",
      "type attribute set to ExternalIP.\n",
      "Finally, print the address attribute of the filtered elements.\n",
      "To learn more about how to use JSONPath with kubectl, refer to the documentation\n",
      "at http:/\n",
      "/kubernetes.io/docs/user-guide/jsonpath. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 171, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "139\n",
      "Exposing services to external clients\n",
      "service’s type to LoadBalancer instead of NodePort. The load balancer will have its\n",
      "own unique, publicly accessible IP address and will redirect all connections to your\n",
      "service. You can thus access your service through the load balancer’s IP address. \n",
      " If Kubernetes is running in an environment that doesn’t support LoadBalancer\n",
      "services, the load balancer will not be provisioned, but the service will still behave like\n",
      "a NodePort service. That’s because a LoadBalancer service is an extension of a Node-\n",
      "Port service. You’ll run this example on Google Kubernetes Engine, which supports\n",
      "LoadBalancer services. Minikube doesn’t, at least not as of this writing. \n",
      "CREATING A LOADBALANCER SERVICE\n",
      "To create a service with a load balancer in front, create the service from the following\n",
      "YAML manifest, as shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: kubia-loadbalancer\n",
      "spec:\n",
      "  type: LoadBalancer          \n",
      "  ports:\n",
      "  - port: 80\n",
      "    targetPort: 8080\n",
      "  selector:\n",
      "    app: kubia\n",
      "The service type is set to LoadBalancer instead of NodePort. You’re not specifying a spe-\n",
      "cific node port, although you could (you’re letting Kubernetes choose one instead). \n",
      "CONNECTING TO THE SERVICE THROUGH THE LOAD BALANCER\n",
      "After you create the service, it takes time for the cloud infrastructure to create the\n",
      "load balancer and write its IP address into the Service object. Once it does that, the IP\n",
      "address will be listed as the external IP address of your service:\n",
      "$ kubectl get svc kubia-loadbalancer\n",
      "NAME                 CLUSTER-IP       EXTERNAL-IP      PORT(S)         AGE\n",
      "kubia-loadbalancer   10.111.241.153   130.211.53.173   80:32143/TCP    1m\n",
      "In this case, the load balancer is available at IP 130.211.53.173, so you can now access\n",
      "the service at that IP address:\n",
      "$ curl http://130.211.53.173\n",
      "You've hit kubia-xueq1\n",
      "Success! As you may have noticed, this time you didn’t need to mess with firewalls the\n",
      "way you had to before with the NodePort service.\n",
      "Listing 5.12\n",
      "A LoadBalancer-type service: kubia-svc-loadbalancer.yaml\n",
      "This type of service obtains \n",
      "a load balancer from the \n",
      "infrastructure hosting the \n",
      "Kubernetes cluster.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 172, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "140\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "See figure 5.7 to see how HTTP requests are delivered to the pod. External clients\n",
      "(curl in your case) connect to port 80 of the load balancer and get routed to the\n",
      "Session affinity and web browsers\n",
      "Because your service is now exposed externally, you may try accessing it with your\n",
      "web browser. You’ll see something that may strike you as odd—the browser will hit\n",
      "the exact same pod every time. Did the service’s session affinity change in the\n",
      "meantime? With kubectl explain, you can double-check that the service’s session\n",
      "affinity is still set to None, so why don’t different browser requests hit different\n",
      "pods, as is the case when using curl?\n",
      "Let me explain what’s happening. The browser is using keep-alive connections and\n",
      "sends all its requests through a single connection, whereas curl opens a new\n",
      "connection every time. Services work at the connection level, so when a connection to a\n",
      "service is first opened, a random pod is selected and then all network packets belonging\n",
      "to that connection are all sent to that single pod. Even if session affinity is set to None,\n",
      "users will always hit the same pod (until the connection is closed).\n",
      "Kubernetes cluster\n",
      "External client\n",
      "Load balancer\n",
      "IP: 130.211.53.173:80\n",
      "Pod\n",
      "Node 2\n",
      "IP: 130.211.99.206\n",
      "Node 1\n",
      "IP: 130.211.97.55\n",
      "Port 32143\n",
      "Port 8080\n",
      "Pod\n",
      "Port 8080\n",
      "Pod\n",
      "Port 32143\n",
      "Port 8080\n",
      "Service\n",
      "Figure 5.7\n",
      "An external client connecting to a LoadBalancer service\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 173, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "141\n",
      "Exposing services to external clients\n",
      "implicitly assigned node port on one of the nodes. From there, the connection is for-\n",
      "warded to one of the pod instances.\n",
      " As already mentioned, a LoadBalancer-type service is a NodePort service with an\n",
      "additional infrastructure-provided load balancer. If you use kubectl describe to dis-\n",
      "play additional info about the service, you’ll see that a node port has been selected for\n",
      "the service. If you were to open the firewall for this port, the way you did in the previ-\n",
      "ous section about NodePort services, you could access the service through the node\n",
      "IPs as well.\n",
      "TIP\n",
      "If you’re using Minikube, even though the load balancer will never be\n",
      "provisioned, you can still access the service through the node port (at the\n",
      "Minikube VM’s IP address).\n",
      "5.3.3\n",
      "Understanding the peculiarities of external connections\n",
      "You must be aware of several things related to externally originating connections to\n",
      "services. \n",
      "UNDERSTANDING AND PREVENTING UNNECESSARY NETWORK HOPS\n",
      "When an external client connects to a service through the node port (this also\n",
      "includes cases when it goes through the load balancer first), the randomly chosen\n",
      "pod may or may not be running on the same node that received the connection. An\n",
      "additional network hop is required to reach the pod, but this may not always be\n",
      "desirable. \n",
      " You can prevent this additional hop by configuring the service to redirect external\n",
      "traffic only to pods running on the node that received the connection. This is done by\n",
      "setting the externalTrafficPolicy field in the service’s spec section:\n",
      "spec:\n",
      "  externalTrafficPolicy: Local\n",
      "  ...\n",
      "If a service definition includes this setting and an external connection is opened\n",
      "through the service’s node port, the service proxy will choose a locally running pod. If\n",
      "no local pods exist, the connection will hang (it won’t be forwarded to a random\n",
      "global pod, the way connections are when not using the annotation). You therefore\n",
      "need to ensure the load balancer forwards connections only to nodes that have at\n",
      "least one such pod.\n",
      " Using this annotation also has other drawbacks. Normally, connections are spread\n",
      "evenly across all the pods, but when using this annotation, that’s no longer the case.\n",
      " Imagine having two nodes and three pods. Let’s say node A runs one pod and\n",
      "node B runs the other two. If the load balancer spreads connections evenly across the\n",
      "two nodes, the pod on node A will receive 50% of all connections, but the two pods on\n",
      "node B will only receive 25% each, as shown in figure 5.8.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 174, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "142\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "BEING AWARE OF THE NON-PRESERVATION OF THE CLIENT’S IP\n",
      "Usually, when clients inside the cluster connect to a service, the pods backing the ser-\n",
      "vice can obtain the client’s IP address. But when the connection is received through a\n",
      "node port, the packets’ source IP is changed, because Source Network Address Trans-\n",
      "lation (SNAT) is performed on the packets. \n",
      " The backing pod can’t see the actual client’s IP, which may be a problem for some\n",
      "applications that need to know the client’s IP. In the case of a web server, for example,\n",
      "this means the access log won’t show the browser’s IP.\n",
      " The Local external traffic policy described in the previous section affects the pres-\n",
      "ervation of the client’s IP, because there’s no additional hop between the node receiv-\n",
      "ing the connection and the node hosting the target pod (SNAT isn’t performed).\n",
      "5.4\n",
      "Exposing services externally through an Ingress \n",
      "resource\n",
      "You’ve now seen two ways of exposing a service to clients outside the cluster, but\n",
      "another method exists—creating an Ingress resource.\n",
      "DEFINITION\n",
      "Ingress (noun)—The act of going in or entering; the right to\n",
      "enter; a means or place of entering; entryway. \n",
      "Let me first explain why you need another way to access Kubernetes services from the\n",
      "outside. \n",
      "UNDERSTANDING WHY INGRESSES ARE NEEDED\n",
      "One important reason is that each LoadBalancer service requires its own load bal-\n",
      "ancer with its own public IP address, whereas an Ingress only requires one, even when\n",
      "providing access to dozens of services. When a client sends an HTTP request to the\n",
      "Ingress, the host and path in the request determine which service the request is for-\n",
      "warded to, as shown in figure 5.9.\n",
      " \n",
      "50%\n",
      "50%\n",
      "50%\n",
      "25%\n",
      "25%\n",
      "Node A\n",
      "Pod\n",
      "Node B\n",
      "Pod\n",
      "Pod\n",
      "Load balancer\n",
      "Figure 5.8\n",
      "A Service using \n",
      "the Local external traffic \n",
      "policy may lead to uneven \n",
      "load distribution across pods.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 175, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "143\n",
      "Exposing services externally through an Ingress resource\n",
      "Ingresses operate at the application layer of the network stack (HTTP) and can pro-\n",
      "vide features such as cookie-based session affinity and the like, which services can’t.\n",
      "UNDERSTANDING THAT AN INGRESS CONTROLLER IS REQUIRED\n",
      "Before we go into the features an Ingress object provides, let me emphasize that to\n",
      "make Ingress resources work, an Ingress controller needs to be running in the cluster.\n",
      "Different Kubernetes environments use different implementations of the controller,\n",
      "but several don’t provide a default controller at all. \n",
      " For example, Google Kubernetes Engine uses Google Cloud Platform’s own HTTP\n",
      "load-balancing features to provide the Ingress functionality. Initially, Minikube didn’t\n",
      "provide a controller out of the box, but it now includes an add-on that can be enabled\n",
      "to let you try out the Ingress functionality. Follow the instructions in the following\n",
      "sidebar to ensure it’s enabled.\n",
      "Enabling the Ingress add-on in Minikube\n",
      "If you’re using Minikube to run the examples in this book, you’ll need to ensure the\n",
      "Ingress add-on is enabled. You can check whether it is by listing all the add-ons:\n",
      "$ minikube addons list\n",
      "- default-storageclass: enabled\n",
      "- kube-dns: enabled\n",
      "- heapster: disabled\n",
      "- ingress: disabled               \n",
      "- registry-creds: disabled\n",
      "- addon-manager: enabled\n",
      "- dashboard: enabled\n",
      "You’ll learn about what these add-ons are throughout the book, but it should be\n",
      "pretty clear what the dashboard and the kube-dns add-ons do. Enable the Ingress\n",
      "add-on so you can see Ingresses in action:\n",
      "$ minikube addons enable ingress\n",
      "ingress was successfully enabled\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Ingress\n",
      "Client\n",
      "Service\n",
      "kubia.example.com/kubia\n",
      "foo.example.com\n",
      "kubia.example.com/foo\n",
      "Service\n",
      "bar.example.com\n",
      "Service\n",
      "Service\n",
      "Figure 5.9\n",
      "Multiple services can be exposed through a single Ingress.\n",
      "The Ingress add-on \n",
      "isn’t enabled.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 176, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "144\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "TIP\n",
      "The --all-namespaces option mentioned in the sidebar is handy when\n",
      "you don’t know what namespace your pod (or other type of resource) is in, or\n",
      "if you want to list resources across all namespaces.\n",
      "5.4.1\n",
      "Creating an Ingress resource\n",
      "You’ve confirmed there’s an Ingress controller running in your cluster, so you can\n",
      "now create an Ingress resource. The following listing shows what the YAML manifest\n",
      "for the Ingress looks like.\n",
      "apiVersion: extensions/v1beta1\n",
      "kind: Ingress\n",
      "metadata:\n",
      "  name: kubia\n",
      "spec:\n",
      "  rules:\n",
      "  - host: kubia.example.com             \n",
      "    http:\n",
      "      paths:\n",
      "      - path: /                           \n",
      "        backend:\n",
      "          serviceName: kubia-nodeport     \n",
      "          servicePort: 80                 \n",
      "This defines an Ingress with a single rule, which makes sure all HTTP requests received\n",
      "by the Ingress controller, in which the host kubia.example.com is requested, will be\n",
      "sent to the kubia-nodeport service on port 80. \n",
      "(continued)\n",
      "This should have spun up an Ingress controller as another pod. Most likely, the\n",
      "controller pod will be in the kube-system namespace, but not necessarily, so list all\n",
      "the running pods across all namespaces by using the --all-namespaces option:\n",
      "$ kubectl get po --all-namespaces\n",
      "NAMESPACE    NAME                            READY  STATUS    RESTARTS AGE\n",
      "default      kubia-rsv5m                     1/1    Running   0        13h\n",
      "default      kubia-fe4ad                     1/1    Running   0        13h\n",
      "default      kubia-ke823                     1/1    Running   0        13h\n",
      "kube-system  default-http-backend-5wb0h      1/1    Running   0        18m\n",
      "kube-system  kube-addon-manager-minikube     1/1    Running   3        6d\n",
      "kube-system  kube-dns-v20-101vq              3/3    Running   9        6d\n",
      "kube-system  kubernetes-dashboard-jxd9l      1/1    Running   3        6d\n",
      "kube-system  nginx-ingress-controller-gdts0  1/1    Running   0        18m\n",
      "At the bottom of the output, you see the Ingress controller pod. The name suggests\n",
      "that Nginx (an open-source HTTP server and reverse proxy) is used to provide the\n",
      "Ingress functionality.\n",
      "Listing 5.13\n",
      "An Ingress resource definition: kubia-ingress.yaml\n",
      "This Ingress maps the \n",
      "kubia.example.com domain \n",
      "name to your service.\n",
      "All requests will be sent to \n",
      "port 80 of the kubia-\n",
      "nodeport service.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 177, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "145\n",
      "Exposing services externally through an Ingress resource\n",
      "NOTE\n",
      "Ingress controllers on cloud providers (in GKE, for example) require\n",
      "the Ingress to point to a NodePort service. But that’s not a requirement of\n",
      "Kubernetes itself.\n",
      "5.4.2\n",
      "Accessing the service through the Ingress\n",
      "To access your service through http:/\n",
      "/kubia.example.com, you’ll need to make sure\n",
      "the domain name resolves to the IP of the Ingress controller. \n",
      "OBTAINING THE IP ADDRESS OF THE INGRESS\n",
      "To look up the IP, you need to list Ingresses:\n",
      "$ kubectl get ingresses\n",
      "NAME      HOSTS               ADDRESS          PORTS     AGE\n",
      "kubia     kubia.example.com   192.168.99.100   80        29m\n",
      "NOTE\n",
      "When running on cloud providers, the address may take time to appear,\n",
      "because the Ingress controller provisions a load balancer behind the scenes.\n",
      "The IP is shown in the ADDRESS column. \n",
      "ENSURING THE HOST CONFIGURED IN THE INGRESS POINTS TO THE INGRESS’ IP ADDRESS\n",
      "Once you know the IP, you can then either configure your DNS servers to resolve\n",
      "kubia.example.com to that IP or you can add the following line to /etc/hosts (or\n",
      "C:\\windows\\system32\\drivers\\etc\\hosts on Windows):\n",
      "192.168.99.100    kubia.example.com\n",
      "ACCESSING PODS THROUGH THE INGRESS\n",
      "Everything is now set up, so you can access the service at http:/\n",
      "/kubia.example.com\n",
      "(using a browser or curl):\n",
      "$ curl http://kubia.example.com\n",
      "You've hit kubia-ke823\n",
      "You’ve successfully accessed the service through an Ingress. Let’s take a better look at\n",
      "how that unfolded.\n",
      "UNDERSTANDING HOW INGRESSES WORK\n",
      "Figure 5.10 shows how the client connected to one of the pods through the Ingress\n",
      "controller. The client first performed a DNS lookup of kubia.example.com, and the\n",
      "DNS server (or the local operating system) returned the IP of the Ingress controller.\n",
      "The client then sent an HTTP request to the Ingress controller and specified\n",
      "kubia.example.com in the Host header. From that header, the controller determined\n",
      "which service the client is trying to access, looked up the pod IPs through the End-\n",
      "points object associated with the service, and forwarded the client’s request to one of\n",
      "the pods.\n",
      " As you can see, the Ingress controller didn’t forward the request to the service. It\n",
      "only used it to select a pod. Most, if not all, controllers work like this. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 178, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "146\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "5.4.3\n",
      "Exposing multiple services through the same Ingress\n",
      "If you look at the Ingress spec closely, you’ll see that both rules and paths are arrays,\n",
      "so they can contain multiple items. An Ingress can map multiple hosts and paths to\n",
      "multiple services, as you’ll see next. Let’s focus on paths first. \n",
      "MAPPING DIFFERENT SERVICES TO DIFFERENT PATHS OF THE SAME HOST\n",
      "You can map multiple paths on the same host to different services, as shown in the\n",
      "following listing.\n",
      "...\n",
      "  - host: kubia.example.com\n",
      "    http:\n",
      "      paths:\n",
      "      - path: /kubia                \n",
      "        backend:                    \n",
      "          serviceName: kubia        \n",
      "          servicePort: 80           \n",
      "      - path: /foo                \n",
      "        backend:                  \n",
      "          serviceName: bar        \n",
      "          servicePort: 80         \n",
      "In this case, requests will be sent to two different services, depending on the path in\n",
      "the requested URL. Clients can therefore reach two different services through a single\n",
      "IP address (that of the Ingress controller).\n",
      "Listing 5.14\n",
      "Ingress exposing multiple services on same host, but different paths\n",
      "Node A\n",
      "Pod\n",
      "Node B\n",
      "Pod\n",
      "Pod\n",
      "Ingress\n",
      "controller\n",
      "Endpoints\n",
      "Service\n",
      "Ingress\n",
      "Client\n",
      "2. Client sends HTTP GET\n",
      "request with header\n",
      "Host: kubia.example.com\n",
      "3. Controller sends\n",
      "request to one of\n",
      "the pods.\n",
      "1. Client looks up\n",
      "kubia.example.com\n",
      "DNS\n",
      "Figure 5.10\n",
      "Accessing pods through an Ingress\n",
      "Requests to kubia.example.com/kubia \n",
      "will be routed to the kubia service.\n",
      "Requests to kubia.example.com/bar \n",
      "will be routed to the bar service.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 179, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "147\n",
      "Exposing services externally through an Ingress resource\n",
      "MAPPING DIFFERENT SERVICES TO DIFFERENT HOSTS\n",
      "Similarly, you can use an Ingress to map to different services based on the host in the\n",
      "HTTP request instead of (only) the path, as shown in the next listing.\n",
      "spec:\n",
      "  rules:\n",
      "  - host: foo.example.com          \n",
      "    http:\n",
      "      paths:\n",
      "      - path: / \n",
      "        backend:\n",
      "          serviceName: foo         \n",
      "          servicePort: 80\n",
      "  - host: bar.example.com          \n",
      "    http:\n",
      "      paths:\n",
      "      - path: /\n",
      "        backend:\n",
      "          serviceName: bar         \n",
      "          servicePort: 80\n",
      "Requests received by the controller will be forwarded to either service foo or bar,\n",
      "depending on the Host header in the request (the way virtual hosts are handled in\n",
      "web servers). DNS needs to point both the foo.example.com and the bar.exam-\n",
      "ple.com domain names to the Ingress controller’s IP address. \n",
      "5.4.4\n",
      "Configuring Ingress to handle TLS traffic\n",
      "You’ve seen how an Ingress forwards HTTP traffic. But what about HTTPS? Let’s take\n",
      "a quick look at how to configure Ingress to support TLS. \n",
      "CREATING A TLS CERTIFICATE FOR THE INGRESS\n",
      "When a client opens a TLS connection to an Ingress controller, the controller termi-\n",
      "nates the TLS connection. The communication between the client and the controller\n",
      "is encrypted, whereas the communication between the controller and the backend\n",
      "pod isn’t. The application running in the pod doesn’t need to support TLS. For exam-\n",
      "ple, if the pod runs a web server, it can accept only HTTP traffic and let the Ingress\n",
      "controller take care of everything related to TLS. To enable the controller to do that,\n",
      "you need to attach a certificate and a private key to the Ingress. The two need to be\n",
      "stored in a Kubernetes resource called a Secret, which is then referenced in the\n",
      "Ingress manifest. We’ll explain Secrets in detail in chapter 7. For now, you’ll create the\n",
      "Secret without paying too much attention to it.\n",
      " First, you need to create the private key and certificate:\n",
      "$ openssl genrsa -out tls.key 2048\n",
      "$ openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj \n",
      "➥ /CN=kubia.example.com\n",
      "Listing 5.15\n",
      "Ingress exposing multiple services on different hosts\n",
      "Requests for \n",
      "foo.example.com will be \n",
      "routed to service foo.\n",
      "Requests for \n",
      "bar.example.com will be \n",
      "routed to service bar.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 180, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "148\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "Then you create the Secret from the two files like this:\n",
      "$ kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key\n",
      "secret \"tls-secret\" created\n",
      "The private key and the certificate are now stored in the Secret called tls-secret.\n",
      "Now, you can update your Ingress object so it will also accept HTTPS requests for\n",
      "kubia.example.com. The Ingress manifest should now look like the following listing.\n",
      "apiVersion: extensions/v1beta1\n",
      "kind: Ingress\n",
      "metadata:\n",
      "  name: kubia\n",
      "spec:\n",
      "  tls:                           \n",
      "  - hosts:                        \n",
      "    - kubia.example.com           \n",
      "    secretName: tls-secret       \n",
      "  rules:\n",
      "  - host: kubia.example.com\n",
      "    http:\n",
      "      paths:\n",
      "      - path: /\n",
      "        backend:\n",
      "          serviceName: kubia-nodeport\n",
      "          servicePort: 80\n",
      "TIP\n",
      "Instead of deleting the Ingress and re-creating it from the new file, you\n",
      "can invoke kubectl apply -f kubia-ingress-tls.yaml, which updates the\n",
      "Ingress resource with what’s specified in the file.\n",
      "Signing certificates through the CertificateSigningRequest resource\n",
      "Instead of signing the certificate ourselves, you can get the certificate signed by\n",
      "creating a CertificateSigningRequest (CSR) resource. Users or their applications\n",
      "can create a regular certificate request, put it into a CSR, and then either a human\n",
      "operator or an automated process can approve the request like this:\n",
      "$ kubectl certificate approve <name of the CSR> \n",
      "The signed certificate can then be retrieved from the CSR’s status.certificate\n",
      "field. \n",
      "Note that a certificate signer component must be running in the cluster; otherwise\n",
      "creating CertificateSigningRequest and approving or denying them won’t have\n",
      "any effect.\n",
      "Listing 5.16\n",
      "Ingress handling TLS traffic: kubia-ingress-tls.yaml\n",
      "The whole TLS configuration \n",
      "is under this attribute.\n",
      "TLS connections will be accepted for \n",
      "the kubia.example.com hostname.\n",
      "The private key and the certificate \n",
      "should be obtained from the tls-\n",
      "secret you created previously.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 181, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "149\n",
      "Signaling when a pod is ready to accept connections\n",
      "You can now use HTTPS to access your service through the Ingress:\n",
      "$ curl -k -v https://kubia.example.com/kubia\n",
      "* About to connect() to kubia.example.com port 443 (#0)\n",
      "...\n",
      "* Server certificate:\n",
      "*   subject: CN=kubia.example.com\n",
      "...\n",
      "> GET /kubia HTTP/1.1\n",
      "> ...\n",
      "You've hit kubia-xueq1\n",
      "The command’s output shows the response from the app, as well as the server certifi-\n",
      "cate you configured the Ingress with.\n",
      "NOTE\n",
      "Support for Ingress features varies between the different Ingress con-\n",
      "troller implementations, so check the implementation-specific documenta-\n",
      "tion to see what’s supported. \n",
      "Ingresses are a relatively new Kubernetes feature, so you can expect to see many\n",
      "improvements and new features in the future. Although they currently support only\n",
      "L7 (HTTP/HTTPS) load balancing, support for L4 load balancing is also planned.\n",
      "5.5\n",
      "Signaling when a pod is ready to accept connections\n",
      "There’s one more thing we need to cover regarding both Services and Ingresses.\n",
      "You’ve already learned that pods are included as endpoints of a service if their labels\n",
      "match the service’s pod selector. As soon as a new pod with proper labels is created, it\n",
      "becomes part of the service and requests start to be redirected to the pod. But what if\n",
      "the pod isn’t ready to start serving requests immediately? \n",
      " The pod may need time to load either configuration or data, or it may need to per-\n",
      "form a warm-up procedure to prevent the first user request from taking too long and\n",
      "affecting the user experience. In such cases you don’t want the pod to start receiving\n",
      "requests immediately, especially when the already-running instances can process\n",
      "requests properly and quickly. It makes sense to not forward requests to a pod that’s in\n",
      "the process of starting up until it’s fully ready.\n",
      "5.5.1\n",
      "Introducing readiness probes\n",
      "In the previous chapter you learned about liveness probes and how they help keep\n",
      "your apps healthy by ensuring unhealthy containers are restarted automatically.\n",
      "Similar to liveness probes, Kubernetes allows you to also define a readiness probe\n",
      "for your pod.\n",
      " The readiness probe is invoked periodically and determines whether the specific\n",
      "pod should receive client requests or not. When a container’s readiness probe returns\n",
      "success, it’s signaling that the container is ready to accept requests. \n",
      " This notion of being ready is obviously something that’s specific to each container.\n",
      "Kubernetes can merely check if the app running in the container responds to a simple\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 182, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "150\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "GET / request or it can hit a specific URL path, which causes the app to perform a\n",
      "whole list of checks to determine if it’s ready. Such a detailed readiness probe, which\n",
      "takes the app’s specifics into account, is the app developer’s responsibility. \n",
      "TYPES OF READINESS PROBES\n",
      "Like liveness probes, three types of readiness probes exist:\n",
      "An Exec probe, where a process is executed. The container’s status is deter-\n",
      "mined by the process’ exit status code.\n",
      "An HTTP GET probe, which sends an HTTP GET request to the container and\n",
      "the HTTP status code of the response determines whether the container is\n",
      "ready or not.\n",
      "A TCP Socket probe, which opens a TCP connection to a specified port of the\n",
      "container. If the connection is established, the container is considered ready.\n",
      "UNDERSTANDING THE OPERATION OF READINESS PROBES\n",
      "When a container is started, Kubernetes can be configured to wait for a configurable\n",
      "amount of time to pass before performing the first readiness check. After that, it\n",
      "invokes the probe periodically and acts based on the result of the readiness probe. If a\n",
      "pod reports that it’s not ready, it’s removed from the service. If the pod then becomes\n",
      "ready again, it’s re-added. \n",
      " Unlike liveness probes, if a container fails the readiness check, it won’t be killed or\n",
      "restarted. This is an important distinction between liveness and readiness probes.\n",
      "Liveness probes keep pods healthy by killing off unhealthy containers and replacing\n",
      "them with new, healthy ones, whereas readiness probes make sure that only pods that\n",
      "are ready to serve requests receive them. This is mostly necessary during container\n",
      "start up, but it’s also useful after the container has been running for a while. \n",
      " As you can see in figure 5.11, if a pod’s readiness probe fails, the pod is removed\n",
      "from the Endpoints object. Clients connecting to the service will not be redirected to\n",
      "the pod. The effect is the same as when the pod doesn’t match the service’s label\n",
      "selector at all.\n",
      "Endpoints\n",
      "Service\n",
      "Selector: app=kubia\n",
      "app: kubia\n",
      "Pod: kubia-q3vkg\n",
      "app: kubia\n",
      "Pod: kubia-k0xz6\n",
      "app: kubia\n",
      "Pod: kubia-53thy\n",
      "Not ready\n",
      "This pod is no longer\n",
      "an endpoint, because its\n",
      "readiness probe has failed.\n",
      "Figure 5.11\n",
      "A pod whose readiness probe fails is removed as an endpoint of a service.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 183, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "151\n",
      "Signaling when a pod is ready to accept connections\n",
      "UNDERSTANDING WHY READINESS PROBES ARE IMPORTANT\n",
      "Imagine that a group of pods (for example, pods running application servers)\n",
      "depends on a service provided by another pod (a backend database, for example). If\n",
      "at any point one of the frontend pods experiences connectivity problems and can’t\n",
      "reach the database anymore, it may be wise for its readiness probe to signal to Kuber-\n",
      "netes that the pod isn’t ready to serve any requests at that time. If other pod instances\n",
      "aren’t experiencing the same type of connectivity issues, they can serve requests nor-\n",
      "mally. A readiness probe makes sure clients only talk to those healthy pods and never\n",
      "notice there’s anything wrong with the system.\n",
      "5.5.2\n",
      "Adding a readiness probe to a pod\n",
      "Next you’ll add a readiness probe to your existing pods by modifying the Replication-\n",
      "Controller’s pod template. \n",
      "ADDING A READINESS PROBE TO THE POD TEMPLATE\n",
      "You’ll use the kubectl edit command to add the probe to the pod template in your\n",
      "existing ReplicationController:\n",
      "$ kubectl edit rc kubia\n",
      "When the ReplicationController’s YAML opens in the text editor, find the container\n",
      "specification in the pod template and add the following readiness probe definition to\n",
      "the first container under spec.template.spec.containers. The YAML should look\n",
      "like the following listing.\n",
      "apiVersion: v1\n",
      "kind: ReplicationController\n",
      "...\n",
      "spec:\n",
      "  ...\n",
      "  template:\n",
      "    ...\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: kubia\n",
      "        image: luksa/kubia\n",
      "        readinessProbe:       \n",
      "          exec:               \n",
      "            command:          \n",
      "            - ls              \n",
      "            - /var/ready      \n",
      "        ...\n",
      "The readiness probe will periodically perform the command ls /var/ready inside the\n",
      "container. The ls command returns exit code zero if the file exists, or a non-zero exit\n",
      "code otherwise. If the file exists, the readiness probe will succeed; otherwise, it will fail. \n",
      "Listing 5.17\n",
      "RC creating a pod with a readiness probe: kubia-rc-readinessprobe.yaml\n",
      "A readinessProbe may \n",
      "be defined for each \n",
      "container in the pod.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 184, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "152\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      " The reason you’re defining such a strange readiness probe is so you can toggle its\n",
      "result by creating or removing the file in question. The file doesn’t exist yet, so all the\n",
      "pods should now report not being ready, right? Well, not exactly. As you may remem-\n",
      "ber from the previous chapter, changing a ReplicationController’s pod template has\n",
      "no effect on existing pods. \n",
      " In other words, all your existing pods still have no readiness probe defined. You\n",
      "can see this by listing the pods with kubectl get pods and looking at the READY col-\n",
      "umn. You need to delete the pods and have them re-created by the Replication-\n",
      "Controller. The new pods will fail the readiness check and won’t be included as\n",
      "endpoints of the service until you create the /var/ready file in each of them. \n",
      "OBSERVING AND MODIFYING THE PODS’ READINESS STATUS\n",
      "List the pods again and inspect whether they’re ready or not:\n",
      "$ kubectl get po\n",
      "NAME          READY     STATUS    RESTARTS   AGE\n",
      "kubia-2r1qb   0/1       Running   0          1m\n",
      "kubia-3rax1   0/1       Running   0          1m\n",
      "kubia-3yw4s   0/1       Running   0          1m\n",
      "The READY column shows that none of the containers are ready. Now make the readi-\n",
      "ness probe of one of them start returning success by creating the /var/ready file,\n",
      "whose existence makes your mock readiness probe succeed:\n",
      "$ kubectl exec kubia-2r1qb -- touch /var/ready\n",
      "You’ve used the kubectl exec command to execute the touch command inside the\n",
      "container of the kubia-2r1qb pod. The touch command creates the file if it doesn’t\n",
      "yet exist. The pod’s readiness probe command should now exit with status code 0,\n",
      "which means the probe is successful, and the pod should now be shown as ready. Let’s\n",
      "see if it is:\n",
      "$ kubectl get po kubia-2r1qb\n",
      "NAME          READY     STATUS    RESTARTS   AGE\n",
      "kubia-2r1qb   0/1       Running   0          2m\n",
      "The pod still isn’t ready. Is there something wrong or is this the expected result? Take\n",
      "a more detailed look at the pod with kubectl describe. The output should contain\n",
      "the following line:\n",
      "Readiness: exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1\n",
      "➥ #failure=3\n",
      "The readiness probe is checked periodically—every 10 seconds by default. The pod\n",
      "isn’t ready because the readiness probe hasn’t been invoked yet. But in 10 seconds at\n",
      "the latest, the pod should become ready and its IP should be listed as the only end-\n",
      "point of the service (run kubectl get endpoints kubia-loadbalancer to confirm). \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 185, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "153\n",
      "Signaling when a pod is ready to accept connections\n",
      "HITTING THE SERVICE WITH THE SINGLE READY POD\n",
      "You can now hit the service URL a few times to see that each and every request is redi-\n",
      "rected to this one pod:\n",
      "$ curl http://130.211.53.173\n",
      "You’ve hit kubia-2r1qb\n",
      "$ curl http://130.211.53.173\n",
      "You’ve hit kubia-2r1qb\n",
      "...\n",
      "$ curl http://130.211.53.173\n",
      "You’ve hit kubia-2r1qb\n",
      "Even though there are three pods running, only a single pod is reporting as being\n",
      "ready and is therefore the only pod receiving requests. If you now delete the file, the\n",
      "pod will be removed from the service again. \n",
      "5.5.3\n",
      "Understanding what real-world readiness probes should do\n",
      "This mock readiness probe is useful only for demonstrating what readiness probes do.\n",
      "In the real world, the readiness probe should return success or failure depending on\n",
      "whether the app can (and wants to) receive client requests or not. \n",
      " Manually removing pods from services should be performed by either deleting the\n",
      "pod or changing the pod’s labels instead of manually flipping a switch in the probe. \n",
      "TIP\n",
      "If you want to add or remove a pod from a service manually, add\n",
      "enabled=true as a label to your pod and to the label selector of your service.\n",
      "Remove the label when you want to remove the pod from the service.\n",
      "ALWAYS DEFINE A READINESS PROBE\n",
      "Before we conclude this section, there are two final notes about readiness probes that\n",
      "I need to emphasize. First, if you don’t add a readiness probe to your pods, they’ll\n",
      "become service endpoints almost immediately. If your application takes too long to\n",
      "start listening for incoming connections, client requests hitting the service will be for-\n",
      "warded to the pod while it’s still starting up and not ready to accept incoming connec-\n",
      "tions. Clients will therefore see “Connection refused” types of errors. \n",
      "TIP\n",
      "You should always define a readiness probe, even if it’s as simple as send-\n",
      "ing an HTTP request to the base URL. \n",
      "DON’T INCLUDE POD SHUTDOWN LOGIC INTO YOUR READINESS PROBES\n",
      "The other thing I need to mention applies to the other end of the pod’s life (pod\n",
      "shutdown) and is also related to clients experiencing connection errors. \n",
      " When a pod is being shut down, the app running in it usually stops accepting con-\n",
      "nections as soon as it receives the termination signal. Because of this, you might think\n",
      "you need to make your readiness probe start failing as soon as the shutdown proce-\n",
      "dure is initiated, ensuring the pod is removed from all services it’s part of. But that’s\n",
      "not necessary, because Kubernetes removes the pod from all services as soon as you\n",
      "delete the pod.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 186, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "154\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "5.6\n",
      "Using a headless service for discovering individual pods\n",
      "You’ve seen how services can be used to provide a stable IP address allowing clients to\n",
      "connect to pods (or other endpoints) backing each service. Each connection to the\n",
      "service is forwarded to one randomly selected backing pod. But what if the client\n",
      "needs to connect to all of those pods? What if the backing pods themselves need to\n",
      "each connect to all the other backing pods? Connecting through the service clearly\n",
      "isn’t the way to do this. What is?\n",
      " For a client to connect to all pods, it needs to figure out the the IP of each individ-\n",
      "ual pod. One option is to have the client call the Kubernetes API server and get the\n",
      "list of pods and their IP addresses through an API call, but because you should always\n",
      "strive to keep your apps Kubernetes-agnostic, using the API server isn’t ideal. \n",
      " Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually,\n",
      "when you perform a DNS lookup for a service, the DNS server returns a single IP—the\n",
      "service’s cluster IP. But if you tell Kubernetes you don’t need a cluster IP for your service\n",
      "(you do this by setting the clusterIP field to None in the service specification), the DNS\n",
      "server will return the pod IPs instead of the single service IP.\n",
      " Instead of returning a single DNS A record, the DNS server will return multiple A\n",
      "records for the service, each pointing to the IP of an individual pod backing the ser-\n",
      "vice at that moment. Clients can therefore do a simple DNS A record lookup and get\n",
      "the IPs of all the pods that are part of the service. The client can then use that infor-\n",
      "mation to connect to one, many, or all of them.\n",
      "5.6.1\n",
      "Creating a headless service\n",
      "Setting the clusterIP field in a service spec to None makes the service headless, as\n",
      "Kubernetes won’t assign it a cluster IP through which clients could connect to the\n",
      "pods backing it. \n",
      " You’ll create a headless service called kubia-headless now. The following listing\n",
      "shows its definition.\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: kubia-headless\n",
      "spec:\n",
      "  clusterIP: None       \n",
      "  ports:\n",
      "  - port: 80\n",
      "    targetPort: 8080\n",
      "  selector:\n",
      "    app: kubia\n",
      "After you create the service with kubectl create, you can inspect it with kubectl get\n",
      "and kubectl describe. You’ll see it has no cluster IP and its endpoints include (part of)\n",
      "Listing 5.18\n",
      "A headless service: kubia-svc-headless.yaml\n",
      "This makes the \n",
      "service headless.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 187, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "155\n",
      "Using a headless service for discovering individual pods\n",
      "the pods matching its pod selector. I say “part of” because your pods contain a readi-\n",
      "ness probe, so only pods that are ready will be listed as endpoints of the service.\n",
      "Before continuing, please make sure at least two pods report being ready, by creating\n",
      "the /var/ready file, as in the previous example:\n",
      "$ kubectl exec <pod name> -- touch /var/ready\n",
      "5.6.2\n",
      "Discovering pods through DNS\n",
      "With your pods ready, you can now try performing a DNS lookup to see if you get the\n",
      "actual pod IPs or not. You’ll need to perform the lookup from inside one of the pods.\n",
      "Unfortunately, your kubia container image doesn’t include the nslookup (or the dig)\n",
      "binary, so you can’t use it to perform the DNS lookup.\n",
      " All you’re trying to do is perform a DNS lookup from inside a pod running in the\n",
      "cluster. Why not run a new pod based on an image that contains the binaries you\n",
      "need? To perform DNS-related actions, you can use the tutum/dnsutils container\n",
      "image, which is available on Docker Hub and contains both the nslookup and the dig\n",
      "binaries. To run the pod, you can go through the whole process of creating a YAML\n",
      "manifest for it and passing it to kubectl create, but that’s too much work, right?\n",
      "Luckily, there’s a faster way.\n",
      "RUNNING A POD WITHOUT WRITING A YAML MANIFEST\n",
      "In chapter 1, you already created pods without writing a YAML manifest by using the\n",
      "kubectl run command. But this time you want to create only a pod—you don’t need\n",
      "to create a ReplicationController to manage the pod. You can do that like this:\n",
      "$ kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1\n",
      "➥ --command -- sleep infinity\n",
      "pod \"dnsutils\" created\n",
      "The trick is in the --generator=run-pod/v1 option, which tells kubectl to create the\n",
      "pod directly, without any kind of ReplicationController or similar behind it. \n",
      "UNDERSTANDING DNS A RECORDS RETURNED FOR A HEADLESS SERVICE\n",
      "Let’s use the newly created pod to perform a DNS lookup:\n",
      "$ kubectl exec dnsutils nslookup kubia-headless\n",
      "...\n",
      "Name:    kubia-headless.default.svc.cluster.local\n",
      "Address: 10.108.1.4 \n",
      "Name:    kubia-headless.default.svc.cluster.local\n",
      "Address: 10.108.2.5 \n",
      "The DNS server returns two different IPs for the kubia-headless.default.svc\n",
      ".cluster.local FQDN. Those are the IPs of the two pods that are reporting being\n",
      "ready. You can confirm this by listing pods with kubectl get pods -o wide, which\n",
      "shows the pods’ IPs. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 188, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "156\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      " This is different from what DNS returns for regular (non-headless) services, such\n",
      "as for your kubia service, where the returned IP is the service’s cluster IP:\n",
      "$ kubectl exec dnsutils nslookup kubia\n",
      "...\n",
      "Name:    kubia.default.svc.cluster.local\n",
      "Address: 10.111.249.153\n",
      "Although headless services may seem different from regular services, they aren’t that\n",
      "different from the clients’ perspective. Even with a headless service, clients can con-\n",
      "nect to its pods by connecting to the service’s DNS name, as they can with regular ser-\n",
      "vices. But with headless services, because DNS returns the pods’ IPs, clients connect\n",
      "directly to the pods, instead of through the service proxy. \n",
      "NOTE\n",
      "A headless services still provides load balancing across pods, but through\n",
      "the DNS round-robin mechanism instead of through the service proxy.\n",
      "5.6.3\n",
      "Discovering all pods—even those that aren’t ready\n",
      "You’ve seen that only pods that are ready become endpoints of services. But some-\n",
      "times you want to use the service discovery mechanism to find all pods matching the\n",
      "service’s label selector, even those that aren’t ready. \n",
      " Luckily, you don’t have to resort to querying the Kubernetes API server. You can\n",
      "use the DNS lookup mechanism to find even those unready pods. To tell Kubernetes\n",
      "you want all pods added to a service, regardless of the pod’s readiness status, you must\n",
      "add the following annotation to the service:\n",
      "kind: Service\n",
      "metadata:\n",
      "  annotations:\n",
      "    service.alpha.kubernetes.io/tolerate-unready-endpoints: \"true\"\n",
      "WARNING\n",
      "As the annotation name suggests, as I’m writing this, this is an alpha\n",
      "feature. The Kubernetes Service API already supports a new service spec field\n",
      "called publishNotReadyAddresses, which will replace the tolerate-unready-\n",
      "endpoints annotation. In Kubernetes version 1.9.0, the field is not honored yet\n",
      "(the annotation is what determines whether unready endpoints are included in\n",
      "the DNS or not). Check the documentation to see whether that’s changed.\n",
      "5.7\n",
      "Troubleshooting services\n",
      "Services are a crucial Kubernetes concept and the source of frustration for many\n",
      "developers. I’ve seen many developers lose heaps of time figuring out why they can’t\n",
      "connect to their pods through the service IP or FQDN. For this reason, a short look at\n",
      "how to troubleshoot services is in order.\n",
      " When you’re unable to access your pods through the service, you should start by\n",
      "going through the following list:\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 189, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "157\n",
      "Summary\n",
      "First, make sure you’re connecting to the service’s cluster IP from within the\n",
      "cluster, not from the outside.\n",
      "Don’t bother pinging the service IP to figure out if the service is accessible\n",
      "(remember, the service’s cluster IP is a virtual IP and pinging it will never work).\n",
      "If you’ve defined a readiness probe, make sure it’s succeeding; otherwise the\n",
      "pod won’t be part of the service.\n",
      "To confirm that a pod is part of the service, examine the corresponding End-\n",
      "points object with kubectl get endpoints.\n",
      "If you’re trying to access the service through its FQDN or a part of it (for exam-\n",
      "ple, myservice.mynamespace.svc.cluster.local or myservice.mynamespace) and\n",
      "it doesn’t work, see if you can access it using its cluster IP instead of the FQDN.\n",
      "Check whether you’re connecting to the port exposed by the service and not\n",
      "the target port.\n",
      "Try connecting to the pod IP directly to confirm your pod is accepting connec-\n",
      "tions on the correct port.\n",
      "If you can’t even access your app through the pod’s IP, make sure your app isn’t\n",
      "only binding to localhost.\n",
      "This should help you resolve most of your service-related problems. You’ll learn much\n",
      "more about how services work in chapter 11. By understanding exactly how they’re\n",
      "implemented, it should be much easier for you to troubleshoot them.\n",
      "5.8\n",
      "Summary\n",
      "In this chapter, you’ve learned how to create Kubernetes Service resources to expose\n",
      "the services available in your application, regardless of how many pod instances are\n",
      "providing each service. You’ve learned how Kubernetes\n",
      "Exposes multiple pods that match a certain label selector under a single, stable\n",
      "IP address and port\n",
      "Makes services accessible from inside the cluster by default, but allows you to\n",
      "make the service accessible from outside the cluster by setting its type to either\n",
      "NodePort or LoadBalancer\n",
      "Enables pods to discover services together with their IP addresses and ports by\n",
      "looking up environment variables\n",
      "Allows discovery of and communication with services residing outside the\n",
      "cluster by creating a Service resource without specifying a selector, by creating\n",
      "an associated Endpoints resource instead\n",
      "Provides a DNS CNAME alias for external services with the ExternalName ser-\n",
      "vice type\n",
      "Exposes multiple HTTP services through a single Ingress (consuming a sin-\n",
      "gle IP)\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 190, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "158\n",
      "CHAPTER 5\n",
      "Services: enabling clients to discover and talk to pods\n",
      "Uses a pod container’s readiness probe to determine whether a pod should or\n",
      "shouldn’t be included as a service endpoint\n",
      "Enables discovery of pod IPs through DNS when you create a headless service\n",
      "Along with getting a better understanding of services, you’ve also learned how to\n",
      "Troubleshoot them\n",
      "Modify firewall rules in Google Kubernetes/Compute Engine\n",
      "Execute commands in pod containers through kubectl exec \n",
      "Run a bash shell in an existing pod’s container\n",
      "Modify Kubernetes resources through the kubectl apply command\n",
      "Run an unmanaged ad hoc pod with kubectl run --generator=run-pod/v1\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 191, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "159\n",
      "Volumes: attaching\n",
      "disk storage to containers\n",
      "In the previous three chapters, we introduced pods and other Kubernetes resources\n",
      "that interact with them, namely ReplicationControllers, ReplicaSets, DaemonSets,\n",
      "Jobs, and Services. Now, we’re going back inside the pod to learn how its containers\n",
      "can access external disk storage and/or share storage between them.\n",
      " We’ve said that pods are similar to logical hosts where processes running inside\n",
      "them share resources such as CPU, RAM, network interfaces, and others. One\n",
      "would expect the processes to also share disks, but that’s not the case. You’ll remem-\n",
      "ber that each container in a pod has its own isolated filesystem, because the file-\n",
      "system comes from the container’s image.\n",
      "This chapter covers\n",
      "Creating multi-container pods\n",
      "Creating a volume to share disk storage between \n",
      "containers\n",
      "Using a Git repository inside a pod\n",
      "Attaching persistent storage such as a GCE \n",
      "Persistent Disk to pods\n",
      "Using pre-provisioned persistent storage\n",
      "Dynamic provisioning of persistent storage\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 192, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "160\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      " Every new container starts off with the exact set of files that was added to the image\n",
      "at build time. Combine this with the fact that containers in a pod get restarted (either\n",
      "because the process died or because the liveness probe signaled to Kubernetes that\n",
      "the container wasn’t healthy anymore) and you’ll realize that the new container will\n",
      "not see anything that was written to the filesystem by the previous container, even\n",
      "though the newly started container runs in the same pod.\n",
      " In certain scenarios you want the new container to continue where the last one fin-\n",
      "ished, such as when restarting a process on a physical machine. You may not need (or\n",
      "want) the whole filesystem to be persisted, but you do want to preserve the directories\n",
      "that hold actual data.\n",
      " Kubernetes provides this by defining storage volumes. They aren’t top-level resources\n",
      "like pods, but are instead defined as a part of a pod and share the same lifecycle as the\n",
      "pod. This means a volume is created when the pod is started and is destroyed when\n",
      "the pod is deleted. Because of this, a volume’s contents will persist across container\n",
      "restarts. After a container is restarted, the new container can see all the files that were\n",
      "written to the volume by the previous container. Also, if a pod contains multiple con-\n",
      "tainers, the volume can be used by all of them at once. \n",
      "6.1\n",
      "Introducing volumes\n",
      "Kubernetes volumes are a component of a pod and are thus defined in the pod’s spec-\n",
      "ification—much like containers. They aren’t a standalone Kubernetes object and can-\n",
      "not be created or deleted on their own. A volume is available to all containers in the\n",
      "pod, but it must be mounted in each container that needs to access it. In each con-\n",
      "tainer, you can mount the volume in any location of its filesystem.\n",
      "6.1.1\n",
      "Explaining volumes in an example\n",
      "Imagine you have a pod with three containers (shown in figure 6.1). One container\n",
      "runs a web server that serves HTML pages from the /var/htdocs directory and stores\n",
      "the access log to /var/logs. The second container runs an agent that creates HTML\n",
      "files and stores them in /var/html. The third container processes the logs it finds in\n",
      "the /var/logs directory (rotates them, compresses them, analyzes them, or whatever).\n",
      " Each container has a nicely defined single responsibility, but on its own each con-\n",
      "tainer wouldn’t be of much use. Creating a pod with these three containers without\n",
      "them sharing disk storage doesn’t make any sense, because the content generator\n",
      "would write the generated HTML files inside its own container and the web server\n",
      "couldn’t access those files, as it runs in a separate isolated container. Instead, it would\n",
      "serve an empty directory or whatever you put in the /var/htdocs directory in its con-\n",
      "tainer image. Similarly, the log rotator would never have anything to do, because its\n",
      "/var/logs directory would always remain empty with nothing writing logs there. A pod\n",
      "with these three containers and no volumes basically does nothing.\n",
      " But if you add two volumes to the pod and mount them at appropriate paths inside\n",
      "the three containers, as shown in figure 6.2, you’ve created a system that’s much more\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 193, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "161\n",
      "Introducing volumes\n",
      "Pod\n",
      "Container: WebServer\n",
      "Filesystem\n",
      "Webserver\n",
      "process\n",
      "Writes\n",
      "Reads\n",
      "/\n",
      "var/\n",
      "htdocs/\n",
      "logs/\n",
      "Container: ContentAgent\n",
      "Filesystem\n",
      "ContentAgent\n",
      "process\n",
      "Writes\n",
      "/\n",
      "var/\n",
      "html/\n",
      "Container: LogRotator\n",
      "Filesystem\n",
      "LogRotator\n",
      "process\n",
      "Reads\n",
      "/\n",
      "var/\n",
      "logs/\n",
      "Figure 6.1\n",
      "Three containers of the \n",
      "same pod without shared storage\n",
      "Pod\n",
      "Container: WebServer\n",
      "Filesystem\n",
      "/\n",
      "var/\n",
      "htdocs/\n",
      "logs/\n",
      "Container: ContentAgent\n",
      "Filesystem\n",
      "/\n",
      "var/\n",
      "html/\n",
      "Container: LogRotator\n",
      "Filesystem\n",
      "/\n",
      "var/\n",
      "logs/\n",
      "Volume:\n",
      "publicHtml\n",
      "Volume:\n",
      "logVol\n",
      "Figure 6.2\n",
      "Three containers sharing two \n",
      "volumes mounted at various mount paths\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 194, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "162\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "than the sum of its parts. Linux allows you to mount a filesystem at arbitrary locations\n",
      "in the file tree. When you do that, the contents of the mounted filesystem are accessi-\n",
      "ble in the directory it’s mounted into. By mounting the same volume into two contain-\n",
      "ers, they can operate on the same files. In your case, you’re mounting two volumes in\n",
      "three containers. By doing this, your three containers can work together and do some-\n",
      "thing useful. Let me explain how.\n",
      " First, the pod has a volume called publicHtml. This volume is mounted in the Web-\n",
      "Server container at /var/htdocs, because that’s the directory the web server serves\n",
      "files from. The same volume is also mounted in the ContentAgent container, but at\n",
      "/var/html, because that’s where the agent writes the files to. By mounting this single vol-\n",
      "ume like that, the web server will now serve the content generated by the content agent.\n",
      " Similarly, the pod also has a volume called logVol for storing logs. This volume is\n",
      "mounted at /var/logs in both the WebServer and the LogRotator containers. Note\n",
      "that it isn’t mounted in the ContentAgent container. The container cannot access its\n",
      "files, even though the container and the volume are part of the same pod. It’s not\n",
      "enough to define a volume in the pod; you need to define a VolumeMount inside the\n",
      "container’s spec also, if you want the container to be able to access it.\n",
      " The two volumes in this example can both initially be empty, so you can use a type\n",
      "of volume called emptyDir. Kubernetes also supports other types of volumes that are\n",
      "either populated during initialization of the volume from an external source, or an\n",
      "existing directory is mounted inside the volume. This process of populating or mount-\n",
      "ing a volume is performed before the pod’s containers are started. \n",
      " A volume is bound to the lifecycle of a pod and will stay in existence only while the\n",
      "pod exists, but depending on the volume type, the volume’s files may remain intact\n",
      "even after the pod and volume disappear, and can later be mounted into a new vol-\n",
      "ume. Let’s see what types of volumes exist.\n",
      "6.1.2\n",
      "Introducing available volume types\n",
      "A wide variety of volume types is available. Several are generic, while others are spe-\n",
      "cific to the actual storage technologies used underneath. Don’t worry if you’ve never\n",
      "heard of those technologies—I hadn’t heard of at least half of them. You’ll probably\n",
      "only use volume types for the technologies you already know and use. Here’s a list of\n",
      "several of the available volume types:\n",
      "\n",
      "emptyDir—A simple empty directory used for storing transient data.\n",
      "\n",
      "hostPath—Used for mounting directories from the worker node’s filesystem\n",
      "into the pod.\n",
      "\n",
      "gitRepo—A volume initialized by checking out the contents of a Git repository.\n",
      "\n",
      "nfs—An NFS share mounted into the pod.\n",
      "\n",
      "gcePersistentDisk (Google Compute Engine Persistent Disk), awsElastic-\n",
      "BlockStore (Amazon Web Services Elastic Block Store Volume), azureDisk\n",
      "(Microsoft Azure Disk Volume)—Used for mounting cloud provider-specific\n",
      "storage.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 195, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "163\n",
      "Using volumes to share data between containers\n",
      "\n",
      "cinder, cephfs, iscsi, flocker, glusterfs, quobyte, rbd, flexVolume, vsphere-\n",
      "Volume, photonPersistentDisk, scaleIO—Used for mounting other types of\n",
      "network storage.\n",
      "\n",
      "configMap, secret, downwardAPI—Special types of volumes used to expose cer-\n",
      "tain Kubernetes resources and cluster information to the pod.\n",
      "\n",
      "persistentVolumeClaim—A way to use a pre- or dynamically provisioned per-\n",
      "sistent storage. (We’ll talk about them in the last section of this chapter.)\n",
      "These volume types serve various purposes. You’ll learn about some of them in the\n",
      "following sections. Special types of volumes (secret, downwardAPI, configMap) are\n",
      "covered in the next two chapters, because they aren’t used for storing data, but for\n",
      "exposing Kubernetes metadata to apps running in the pod. \n",
      " A single pod can use multiple volumes of different types at the same time, and, as\n",
      "we’ve mentioned before, each of the pod’s containers can either have the volume\n",
      "mounted or not.\n",
      "6.2\n",
      "Using volumes to share data between containers\n",
      "Although a volume can prove useful even when used by a single container, let’s first\n",
      "focus on how it’s used for sharing data between multiple containers in a pod.\n",
      "6.2.1\n",
      "Using an emptyDir volume\n",
      "The simplest volume type is the emptyDir volume, so let’s look at it in the first exam-\n",
      "ple of how to define a volume in a pod. As the name suggests, the volume starts out as\n",
      "an empty directory. The app running inside the pod can then write any files it needs\n",
      "to it. Because the volume’s lifetime is tied to that of the pod, the volume’s contents are\n",
      "lost when the pod is deleted.\n",
      " An emptyDir volume is especially useful for sharing files between containers\n",
      "running in the same pod. But it can also be used by a single container for when a con-\n",
      "tainer needs to write data to disk temporarily, such as when performing a sort\n",
      "operation on a large dataset, which can’t fit into the available memory. The data could\n",
      "also be written to the container’s filesystem itself (remember the top read-write layer\n",
      "in a container?), but subtle differences exist between the two options. A container’s\n",
      "filesystem may not even be writable (we’ll talk about this toward the end of the book),\n",
      "so writing to a mounted volume might be the only option. \n",
      "USING AN EMPTYDIR VOLUME IN A POD\n",
      "Let’s revisit the previous example where a web server, a content agent, and a log rota-\n",
      "tor share two volumes, but let’s simplify a bit. You’ll build a pod with only the web\n",
      "server container and the content agent and a single volume for the HTML. \n",
      " You’ll use Nginx as the web server and the UNIX fortune command to generate\n",
      "the HTML content. The fortune command prints out a random quote every time you\n",
      "run it. You’ll create a script that invokes the fortune command every 10 seconds and\n",
      "stores its output in index.html. You’ll find an existing Nginx image available on\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 196, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "164\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "Docker Hub, but you’ll need to either create the fortune image yourself or use the\n",
      "one I’ve already built and pushed to Docker Hub under luksa/fortune. If you want a\n",
      "refresher on how to build Docker images, refer to the sidebar.\n",
      "CREATING THE POD\n",
      "Now that you have the two images required to run your pod, it’s time to create the pod\n",
      "manifest. Create a file called fortune-pod.yaml with the contents shown in the follow-\n",
      "ing listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: fortune\n",
      "spec:\n",
      "  containers:\n",
      "Building the fortune container image\n",
      "Here’s how to build the image. Create a new directory called fortune and then inside\n",
      "it, create a fortuneloop.sh shell script with the following contents:\n",
      "#!/bin/bash\n",
      "trap \"exit\" SIGINT\n",
      "mkdir /var/htdocs\n",
      "while :\n",
      "do\n",
      "  echo $(date) Writing fortune to /var/htdocs/index.html\n",
      "  /usr/games/fortune > /var/htdocs/index.html\n",
      "  sleep 10\n",
      "done\n",
      "Then, in the same directory, create a file called Dockerfile containing the following:\n",
      "FROM ubuntu:latest\n",
      "RUN apt-get update ; apt-get -y install fortune\n",
      "ADD fortuneloop.sh /bin/fortuneloop.sh\n",
      "ENTRYPOINT /bin/fortuneloop.sh\n",
      "The image is based on the ubuntu:latest image, which doesn’t include the fortune\n",
      "binary by default. That’s why in the second line of the Dockerfile you install it with\n",
      "apt-get. After that, you add the fortuneloop.sh script to the image’s /bin folder.\n",
      "In the last line of the Dockerfile, you specify that the fortuneloop.sh script should\n",
      "be executed when the image is run.\n",
      "After preparing both files, build and upload the image to Docker Hub with the following\n",
      "two commands (replace luksa with your own Docker Hub user ID):\n",
      "$ docker build -t luksa/fortune .\n",
      "$ docker push luksa/fortune\n",
      "Listing 6.1\n",
      "A pod with two containers sharing the same volume: fortune-pod.yaml\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 197, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "165\n",
      "Using volumes to share data between containers\n",
      "  - image: luksa/fortune                   \n",
      "    name: html-generator                   \n",
      "    volumeMounts:                          \n",
      "    - name: html                           \n",
      "      mountPath: /var/htdocs               \n",
      "  - image: nginx:alpine                   \n",
      "    name: web-server                      \n",
      "    volumeMounts:                         \n",
      "    - name: html                          \n",
      "      mountPath: /usr/share/nginx/html    \n",
      "      readOnly: true                      \n",
      "    ports:\n",
      "    - containerPort: 80\n",
      "      protocol: TCP\n",
      "  volumes:                 \n",
      "  - name: html             \n",
      "    emptyDir: {}           \n",
      "The pod contains two containers and a single volume that’s mounted in both of\n",
      "them, yet at different paths. When the html-generator container starts, it starts writ-\n",
      "ing the output of the fortune command to the /var/htdocs/index.html file every 10\n",
      "seconds. Because the volume is mounted at /var/htdocs, the index.html file is writ-\n",
      "ten to the volume instead of the container’s top layer. As soon as the web-server con-\n",
      "tainer starts, it starts serving whatever HTML files are in the /usr/share/nginx/html\n",
      "directory (this is the default directory Nginx serves files from). Because you mounted\n",
      "the volume in that exact location, Nginx will serve the index.html file written there\n",
      "by the container running the fortune loop. The end effect is that a client sending an\n",
      "HTTP request to the pod on port 80 will receive the current fortune message as\n",
      "the response. \n",
      "SEEING THE POD IN ACTION\n",
      "To see the fortune message, you need to enable access to the pod. You’ll do that by\n",
      "forwarding a port from your local machine to the pod:\n",
      "$ kubectl port-forward fortune 8080:80\n",
      "Forwarding from 127.0.0.1:8080 -> 80\n",
      "Forwarding from [::1]:8080 -> 80\n",
      "NOTE\n",
      "As an exercise, you can also expose the pod through a service instead\n",
      "of using port forwarding.\n",
      "Now you can access the Nginx server through port 8080 of your local machine. Use\n",
      "curl to do that:\n",
      "$ curl http://localhost:8080\n",
      "Beware of a tall blond man with one black shoe.\n",
      "If you wait a few seconds and send another request, you should receive a different\n",
      "message. By combining two containers, you created a simple app to see how a volume\n",
      "can glue together two containers and enhance what each of them does.\n",
      "The first container is called html-generator \n",
      "and runs the luksa/fortune image.\n",
      "The volume called html is mounted \n",
      "at /var/htdocs in the container.\n",
      "The second container is called web-server \n",
      "and runs the nginx:alpine image.\n",
      "The same volume as above is \n",
      "mounted at /usr/share/nginx/html \n",
      "as read-only.\n",
      "A single emptyDir volume \n",
      "called html that’s mounted \n",
      "in the two containers above\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "129\n",
      "height\n",
      "54\n",
      "PIX BUFFER SIZE\n",
      "20898\n",
      "Original IMG_BUFFER_SIZE\n",
      "20898\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011900>\n",
      "page_image_dict\n",
      "{'page': 198, 'img_cnt': 1, 'img_npy_lst': []}\n",
      "166\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "SPECIFYING THE MEDIUM TO USE FOR THE EMPTYDIR\n",
      "The emptyDir you used as the volume was created on the actual disk of the worker\n",
      "node hosting your pod, so its performance depends on the type of the node’s disks.\n",
      "But you can tell Kubernetes to create the emptyDir on a tmpfs filesystem (in memory\n",
      "instead of on disk). To do this, set the emptyDir’s medium to Memory like this:\n",
      "volumes:\n",
      "  - name: html\n",
      "    emptyDir:\n",
      "      medium: Memory    \n",
      "An emptyDir volume is the simplest type of volume, but other types build upon it.\n",
      "After the empty directory is created, they populate it with data. One such volume type\n",
      "is the gitRepo volume type, which we’ll introduce next.\n",
      "6.2.2\n",
      "Using a Git repository as the starting point for a volume \n",
      "A gitRepo volume is basically an emptyDir volume that gets populated by cloning a\n",
      "Git repository and checking out a specific revision when the pod is starting up (but\n",
      "before its containers are created). Figure 6.3 shows how this unfolds.\n",
      "NOTE\n",
      "After the gitRepo volume is created, it isn’t kept in sync with the repo\n",
      "it’s referencing. The files in the volume will not be updated when you push\n",
      "additional commits to the Git repository. However, if your pod is managed by\n",
      "a ReplicationController, deleting the pod will result in a new pod being cre-\n",
      "ated and this new pod’s volume will then contain the latest commits. \n",
      "For example, you can use a Git repository to store static HTML files of your website\n",
      "and create a pod containing a web server container and a gitRepo volume. Every time\n",
      "the pod is created, it pulls the latest version of your website and starts serving it. The\n",
      "This emptyDir’s \n",
      "files should be \n",
      "stored in memory.\n",
      "Pod\n",
      "Container\n",
      "User\n",
      "gitRepo\n",
      "volume\n",
      "1. User (or a replication\n",
      "controller) creates pod\n",
      "with gitRepo volume\n",
      "2. Kubernetes creates\n",
      "an empty directory and\n",
      "clones the speciﬁed Git\n",
      "repository into it\n",
      "3. The pod’s container is started\n",
      "(with the volume mounted at\n",
      "the mount path)\n",
      "Repository\n",
      "Figure 6.3\n",
      "A gitRepo volume is an emptyDir volume initially populated with the contents of a \n",
      "Git repository.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 199, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "167\n",
      "Using volumes to share data between containers\n",
      "only drawback to this is that you need to delete the pod every time you push changes\n",
      "to the gitRepo and want to start serving the new version of the website. \n",
      " Let’s do this right now. It’s not that different from what you did before. \n",
      "RUNNING A WEB SERVER POD SERVING FILES FROM A CLONED GIT REPOSITORY\n",
      "Before you create your pod, you’ll need an actual Git repository with HTML files in it.\n",
      "I’ve created a repo on GitHub at https:/\n",
      "/github.com/luksa/kubia-website-example.git.\n",
      "You’ll need to fork it (create your own copy of the repo on GitHub) so you can push\n",
      "changes to it later. \n",
      " Once you’ve created your fork, you can move on to creating the pod. This time,\n",
      "you’ll only need a single Nginx container and a single gitRepo volume in the pod (be\n",
      "sure to point the gitRepo volume to your own fork of my repository), as shown in the\n",
      "following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: gitrepo-volume-pod\n",
      "spec:\n",
      "  containers:\n",
      "  - image: nginx:alpine\n",
      "    name: web-server\n",
      "    volumeMounts:\n",
      "    - name: html\n",
      "      mountPath: /usr/share/nginx/html\n",
      "      readOnly: true\n",
      "    ports:\n",
      "    - containerPort: 80\n",
      "      protocol: TCP\n",
      "  volumes:\n",
      "  - name: html\n",
      "    gitRepo:                     \n",
      "      repository: https://github.com/luksa/kubia-website-example.git   \n",
      "      revision: master                     \n",
      "      directory: .      \n",
      "When you create the pod, the volume is first initialized as an empty directory and then\n",
      "the specified Git repository is cloned into it. If you hadn’t set the directory to . (dot),\n",
      "the repository would have been cloned into the kubia-website-example subdirectory,\n",
      "which isn’t what you want. You want the repo to be cloned into the root directory of\n",
      "your volume. Along with the repository, you also specified you want Kubernetes to\n",
      "check out whatever revision the master branch is pointing to at the time the volume\n",
      "is created. \n",
      " With the pod running, you can try hitting it through port forwarding, a service, or by\n",
      "executing the curl command from within the pod (or any other pod inside the cluster). \n",
      "Listing 6.2\n",
      "A pod using a gitRepo volume: gitrepo-volume-pod.yaml\n",
      "You’re creating a \n",
      "gitRepo volume.\n",
      "The volume will clone\n",
      "this Git repository.\n",
      "The master branch \n",
      "will be checked out.\n",
      "You want the repo to \n",
      "be cloned into the root \n",
      "dir of the volume.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 200, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "168\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "CONFIRMING THE FILES AREN’T KEPT IN SYNC WITH THE GIT REPO\n",
      "Now you’ll make changes to the index.html file in your GitHub repository. If you\n",
      "don’t use Git locally, you can edit the file on GitHub directly—click on the file in your\n",
      "GitHub repository to open it and then click on the pencil icon to start editing it.\n",
      "Change the text and then commit the changes by clicking the button at the bottom.\n",
      " The master branch of the Git repository now includes the changes you made to the\n",
      "HTML file. These changes will not be visible on your Nginx web server yet, because\n",
      "the gitRepo volume isn’t kept in sync with the Git repository. You can confirm this by\n",
      "hitting the pod again. \n",
      " To see the new version of the website, you need to delete the pod and create\n",
      "it again. Instead of having to delete the pod every time you make changes, you could\n",
      "run an additional process, which keeps your volume in sync with the Git repository.\n",
      "I won’t explain in detail how to do this. Instead, try doing this yourself as an exer-\n",
      "cise, but here are a few pointers.\n",
      "INTRODUCING SIDECAR CONTAINERS\n",
      "The Git sync process shouldn’t run in the same container as the Nginx web server, but\n",
      "in a second container: a sidecar container. A sidecar container is a container that aug-\n",
      "ments the operation of the main container of the pod. You add a sidecar to a pod so\n",
      "you can use an existing container image instead of cramming additional logic into the\n",
      "main app’s code, which would make it overly complex and less reusable. \n",
      " To find an existing container image, which keeps a local directory synchronized\n",
      "with a Git repository, go to Docker Hub and search for “git sync.” You’ll find many\n",
      "images that do that. Then use the image in a new container in the pod from the previ-\n",
      "ous example, mount the pod’s existing gitRepo volume in the new container, and\n",
      "configure the Git sync container to keep the files in sync with your Git repo. If you set\n",
      "everything up correctly, you should see that the files the web server is serving are kept\n",
      "in sync with your GitHub repo. \n",
      "NOTE\n",
      "An example in chapter 18 includes using a Git sync container like the\n",
      "one explained here, so you can wait until you reach chapter 18 and follow the\n",
      "step-by-step instructions then instead of doing this exercise on your own now. \n",
      "USING A GITREPO VOLUME WITH PRIVATE GIT REPOSITORIES\n",
      "There’s one other reason for having to resort to Git sync sidecar containers. We\n",
      "haven’t talked about whether you can use a gitRepo volume with a private Git repo. It\n",
      "turns out you can’t. The current consensus among Kubernetes developers is to keep\n",
      "the gitRepo volume simple and not add any support for cloning private repositories\n",
      "through the SSH protocol, because that would require adding additional config\n",
      "options to the gitRepo volume. \n",
      " If you want to clone a private Git repo into your container, you should use a git-\n",
      "sync sidecar or a similar method instead of a gitRepo volume.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 201, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "169\n",
      "Accessing files on the worker node’s filesystem\n",
      "WRAPPING UP THE GITREPO VOLUME\n",
      "A gitRepo volume, like the emptyDir volume, is basically a dedicated directory cre-\n",
      "ated specifically for, and used exclusively by, the pod that contains the volume. When\n",
      "the pod is deleted, the volume and its contents are deleted. Other types of volumes,\n",
      "however, don’t create a new directory, but instead mount an existing external direc-\n",
      "tory into the pod’s container’s filesystem. The contents of that volume can survive\n",
      "multiple pod instantiations. We’ll learn about those types of volumes next.\n",
      "6.3\n",
      "Accessing files on the worker node’s filesystem\n",
      "Most  pods should be oblivious of their host node, so they shouldn’t access any files on\n",
      "the node’s filesystem. But certain system-level pods (remember, these will usually be\n",
      "managed by a DaemonSet) do need to either read the node’s files or use the node’s\n",
      "filesystem to access the node’s devices through the filesystem. Kubernetes makes this\n",
      "possible through a hostPath volume. \n",
      "6.3.1\n",
      "Introducing the hostPath volume\n",
      "A hostPath volume points to a specific file or directory on the node’s filesystem (see\n",
      "figure 6.4). Pods running on the same node and using the same path in their host-\n",
      "Path volume see the same files.\n",
      "hostPath volumes are the first type of persistent storage we’re introducing, because\n",
      "both the gitRepo and emptyDir volumes’ contents get deleted when a pod is torn\n",
      "down, whereas a hostPath volume’s contents don’t. If a pod is deleted and the next\n",
      "pod uses a hostPath volume pointing to the same path on the host, the new pod will\n",
      "see whatever was left behind by the previous pod, but only if it’s scheduled to the same\n",
      "node as the first pod.\n",
      "Node 1\n",
      "Pod\n",
      "hostPath\n",
      "volume\n",
      "Pod\n",
      "hostPath\n",
      "volume\n",
      "Node 2\n",
      "Pod\n",
      "hostPath\n",
      "volume\n",
      "/some/path/on/host\n",
      "/some/path/on/host\n",
      "Figure 6.4\n",
      "A hostPath volume mounts a file or directory on the worker node into \n",
      "the container’s filesystem.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 202, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "170\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      " If you’re thinking of using a hostPath volume as the place to store a database’s\n",
      "data directory, think again. Because the volume’s contents are stored on a specific\n",
      "node’s filesystem, when the database pod gets rescheduled to another node, it will no\n",
      "longer see the data. This explains why it’s not a good idea to use a hostPath volume\n",
      "for regular pods, because it makes the pod sensitive to what node it’s scheduled to.\n",
      "6.3.2\n",
      "Examining system pods that use hostPath volumes\n",
      "Let’s see how a hostPath volume can be used properly. Instead of creating a new pod,\n",
      "let’s see if any existing system-wide pods are already using this type of volume. As you\n",
      "may remember from one of the previous chapters, several such pods are running in\n",
      "the kube-system namespace. Let’s list them again:\n",
      "$ kubectl get pod s --namespace kube-system\n",
      "NAME                          READY     STATUS    RESTARTS   AGE\n",
      "fluentd-kubia-4ebc2f1e-9a3e   1/1       Running   1          4d\n",
      "fluentd-kubia-4ebc2f1e-e2vz   1/1       Running   1          31d\n",
      "...\n",
      "Pick the first one and see what kinds of volumes it uses (shown in the following listing).\n",
      "$ kubectl describe po fluentd-kubia-4ebc2f1e-9a3e --namespace kube-system\n",
      "Name:           fluentd-cloud-logging-gke-kubia-default-pool-4ebc2f1e-9a3e\n",
      "Namespace:      kube-system\n",
      "...\n",
      "Volumes:\n",
      "  varlog:\n",
      "    Type:       HostPath (bare host directory volume)\n",
      "    Path:       /var/log\n",
      "  varlibdockercontainers:\n",
      "    Type:       HostPath (bare host directory volume)\n",
      "    Path:       /var/lib/docker/containers\n",
      "TIP\n",
      "If you’re using Minikube, try the kube-addon-manager-minikube pod.\n",
      "Aha! The pod uses two hostPath volumes to gain access to the node’s /var/log and\n",
      "the /var/lib/docker/containers directories. You’d think you were lucky to find a pod\n",
      "using a hostPath volume on the first try, but not really (at least not on GKE). Check\n",
      "the other pods, and you’ll see most use this type of volume either to access the node’s\n",
      "log files, kubeconfig (the Kubernetes config file), or the CA certificates.\n",
      " If you inspect the other pods, you’ll see none of them uses the hostPath volume\n",
      "for storing their own data. They all use it to get access to the node’s data. But as we’ll\n",
      "see later in the chapter, hostPath volumes are often used for trying out persistent stor-\n",
      "age in single-node clusters, such as the one created by Minikube. Read on to learn\n",
      "about the types of volumes you should use for storing persistent data properly even in\n",
      "a multi-node cluster.\n",
      "Listing 6.3\n",
      " A pod using hostPath volumes to access the node’s logs\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 203, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "171\n",
      "Using persistent storage\n",
      "TIP\n",
      "Remember to use hostPath volumes only if you need to read or write sys-\n",
      "tem files on the node. Never use them to persist data across pods. \n",
      "6.4\n",
      "Using persistent storage\n",
      "When an application running in a pod needs to persist data to disk and have that\n",
      "same data available even when the pod is rescheduled to another node, you can’t use\n",
      "any of the volume types we’ve mentioned so far. Because this data needs to be accessi-\n",
      "ble from any cluster node, it must be stored on some type of network-attached stor-\n",
      "age (NAS).\n",
      " To learn about volumes that allow persisting data, you’ll create a pod that will run\n",
      "the MongoDB document-oriented NoSQL database. Running a database pod without\n",
      "a volume or with a non-persistent volume doesn’t make sense, except for testing\n",
      "purposes, so you’ll add an appropriate type of volume to the pod and mount it in the\n",
      "MongoDB container. \n",
      "6.4.1\n",
      "Using a GCE Persistent Disk in a pod volume\n",
      "If you’ve been running these examples on Google Kubernetes Engine, which runs\n",
      "your cluster nodes on Google Compute Engine (GCE), you’ll use a GCE Persistent\n",
      "Disk as your underlying storage mechanism. \n",
      " In the early versions, Kubernetes didn’t provision the underlying storage automati-\n",
      "cally—you had to do that manually. Automatic provisioning is now possible, and you’ll\n",
      "learn about it later in the chapter, but first, you’ll start by provisioning the storage\n",
      "manually. It will give you a chance to learn exactly what’s going on underneath. \n",
      "CREATING A GCE PERSISTENT DISK\n",
      "You’ll start by creating the GCE persistent disk first. You need to create it in the same\n",
      "zone as your Kubernetes cluster. If you don’t remember what zone you created the\n",
      "cluster in, you can see it by listing your Kubernetes clusters with the gcloud command\n",
      "like this:\n",
      "$ gcloud container clusters list\n",
      "NAME   ZONE            MASTER_VERSION  MASTER_IP       ...\n",
      "kubia  europe-west1-b  1.2.5           104.155.84.137  ...\n",
      "This shows you’ve created your cluster in zone europe-west1-b, so you need to create\n",
      "the GCE persistent disk in the same zone as well. You create the disk like this:\n",
      "$ gcloud compute disks create --size=1GiB --zone=europe-west1-b mongodb\n",
      "WARNING: You have selected a disk size of under [200GB]. This may result in \n",
      "poor I/O performance. For more information, see: \n",
      "https://developers.google.com/compute/docs/disks#pdperformance.\n",
      "Created [https://www.googleapis.com/compute/v1/projects/rapid-pivot-\n",
      "136513/zones/europe-west1-b/disks/mongodb].\n",
      "NAME     ZONE            SIZE_GB  TYPE         STATUS\n",
      "mongodb  europe-west1-b  1        pd-standard  READY\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 204, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "172\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "This command creates a 1 GiB large GCE persistent disk called mongodb. You can\n",
      "ignore the warning about the disk size, because you don’t care about the disk’s perfor-\n",
      "mance for the tests you’re about to run.\n",
      "CREATING A POD USING A GCEPERSISTENTDISK VOLUME\n",
      "Now that you have your physical storage properly set up, you can use it in a volume\n",
      "inside your MongoDB pod. You’re going to prepare the YAML for the pod, which is\n",
      "shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: mongodb \n",
      "spec:\n",
      "  volumes:\n",
      "  - name: mongodb-data          \n",
      "    gcePersistentDisk:           \n",
      "      pdName: mongodb            \n",
      "      fsType: ext4             \n",
      "  containers:\n",
      "  - image: mongo\n",
      "    name: mongodb\n",
      "    volumeMounts:                \n",
      "    - name: mongodb-data         \n",
      "      mountPath: /data/db      \n",
      "    ports:\n",
      "    - containerPort: 27017\n",
      "      protocol: TCP\n",
      "NOTE\n",
      "If you’re using Minikube, you can’t use a GCE Persistent Disk, but you\n",
      "can deploy mongodb-pod-hostpath.yaml, which uses a hostPath volume\n",
      "instead of a GCE PD.\n",
      "The pod contains a single container and a single volume backed by the GCE Per-\n",
      "sistent Disk you’ve created (as shown in figure 6.5). You’re mounting the volume\n",
      "inside the container at /data/db, because that’s where MongoDB stores its data.\n",
      "Listing 6.4\n",
      "A pod using a gcePersistentDisk volume: mongodb-pod-gcepd.yaml\n",
      "The name\n",
      "of the\n",
      "volume\n",
      "(also\n",
      "referenced\n",
      "when\n",
      "mounting\n",
      "the volume)\n",
      "The type of the volume \n",
      "is a GCE Persistent Disk.\n",
      "The name of the persistent \n",
      "disk must match the actual \n",
      "PD you created earlier.\n",
      "The filesystem type is EXT4 \n",
      "(a type of Linux filesystem).\n",
      "The path where MongoDB \n",
      "stores its data\n",
      "Pod: mongodb\n",
      "Container: mongodb\n",
      "volumeMounts:\n",
      "name: mongodb-data\n",
      "mountPath: /data/db\n",
      "gcePersistentDisk:\n",
      "pdName: mongodb\n",
      "GCE\n",
      "Persistent Disk:\n",
      "mongodb\n",
      "Volume:\n",
      "mongodb\n",
      "Figure 6.5\n",
      "A pod with a single container running MongoDB, which mounts a volume referencing an \n",
      "external GCE Persistent Disk\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 205, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "173\n",
      "Using persistent storage\n",
      "WRITING DATA TO THE PERSISTENT STORAGE BY ADDING DOCUMENTS TO YOUR MONGODB DATABASE\n",
      "Now that you’ve created the pod and the container has been started, you can run the\n",
      "MongoDB shell inside the container and use it to write some data to the data store.\n",
      " You’ll run the shell as shown in the following listing.\n",
      "$ kubectl exec -it mongodb mongo\n",
      "MongoDB shell version: 3.2.8\n",
      "connecting to: mongodb://127.0.0.1:27017\n",
      "Welcome to the MongoDB shell.\n",
      "For interactive help, type \"help\".\n",
      "For more comprehensive documentation, see\n",
      "    http://docs.mongodb.org/\n",
      "Questions? Try the support group\n",
      "    http://groups.google.com/group/mongodb-user\n",
      "...\n",
      "> \n",
      "MongoDB allows storing JSON documents, so you’ll store one to see if it’s stored per-\n",
      "sistently and can be retrieved after the pod is re-created. Insert a new JSON document\n",
      "with the following commands: \n",
      "> use mystore\n",
      "switched to db mystore\n",
      "> db.foo.insert({name:'foo'})\n",
      "WriteResult({ \"nInserted\" : 1 })\n",
      "You’ve inserted a simple JSON document with a single property (name: ’foo’). Now,\n",
      "use the find() command to see the document you inserted:\n",
      "> db.foo.find()\n",
      "{ \"_id\" : ObjectId(\"57a61eb9de0cfd512374cc75\"), \"name\" : \"foo\" }\n",
      "There it is. The document should be stored in your GCE persistent disk now. \n",
      "RE-CREATING THE POD AND VERIFYING THAT IT CAN READ THE DATA PERSISTED BY THE PREVIOUS POD\n",
      "You can now exit the mongodb shell (type exit and press Enter), and then delete the\n",
      "pod and recreate it:\n",
      "$ kubectl delete pod mongodb\n",
      "pod \"mongodb\" deleted\n",
      "$ kubectl create -f mongodb-pod-gcepd.yaml\n",
      "pod \"mongodb\" created\n",
      "The new pod uses the exact same GCE persistent disk as the previous pod, so the\n",
      "MongoDB container running inside it should see the exact same data, even if the pod\n",
      "is scheduled to a different node.\n",
      "TIP\n",
      "You can see what node a pod is scheduled to by running kubectl get po\n",
      "-o wide.\n",
      "Listing 6.5\n",
      "Entering the MongoDB shell inside the mongodb pod\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 206, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "174\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "Once the container is up, you can again run the MongoDB shell and check to see if the\n",
      "document you stored earlier can still be retrieved, as shown in the following listing.\n",
      "$ kubectl exec -it mongodb mongo\n",
      "MongoDB shell version: 3.2.8\n",
      "connecting to: mongodb://127.0.0.1:27017\n",
      "Welcome to the MongoDB shell.\n",
      "...\n",
      "> use mystore\n",
      "switched to db mystore\n",
      "> db.foo.find()\n",
      "{ \"_id\" : ObjectId(\"57a61eb9de0cfd512374cc75\"), \"name\" : \"foo\" }\n",
      "As expected, the data is still there, even though you deleted the pod and re-created it.\n",
      "This confirms you can use a GCE persistent disk to persist data across multiple pod\n",
      "instances. \n",
      " You’re done playing with the MongoDB pod, so go ahead and delete it again, but\n",
      "hold off on deleting the underlying GCE persistent disk. You’ll use it again later in\n",
      "the chapter.\n",
      "6.4.2\n",
      "Using other types of volumes with underlying persistent storage\n",
      "The reason you created the GCE Persistent Disk volume is because your Kubernetes\n",
      "cluster runs on Google Kubernetes Engine. When you run your cluster elsewhere, you\n",
      "should use other types of volumes, depending on the underlying infrastructure.\n",
      " If your Kubernetes cluster is running on Amazon’s AWS EC2, for example, you can\n",
      "use an awsElasticBlockStore volume to provide persistent storage for your pods. If\n",
      "your cluster runs on Microsoft Azure, you can use the azureFile or the azureDisk\n",
      "volume. We won’t go into detail on how to do that here, but it’s virtually the same as in\n",
      "the previous example. First, you need to create the actual underlying storage, and\n",
      "then set the appropriate properties in the volume definition.\n",
      "USING AN AWS ELASTIC BLOCK STORE VOLUME\n",
      "For example, to use an AWS elastic block store instead of the GCE Persistent Disk,\n",
      "you’d only need to change the volume definition as shown in the following listing (see\n",
      "those lines printed in bold).\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: mongodb \n",
      "spec:\n",
      "  volumes:                       \n",
      "  - name: mongodb-data           \n",
      "    awsElasticBlockStore:          \n",
      "Listing 6.6\n",
      "Retrieving MongoDB’s persisted data in a new pod\n",
      "Listing 6.7\n",
      "A pod using an awsElasticBlockStore volume: mongodb-pod-aws.yaml\n",
      "Using awsElasticBlockStore \n",
      "instead of gcePersistentDisk\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 207, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "175\n",
      "Using persistent storage\n",
      "      volumeId: my-volume          \n",
      "      fsType: ext4       \n",
      "  containers:\n",
      "  - ...\n",
      "USING AN NFS VOLUME\n",
      "If your cluster is running on your own set of servers, you have a vast array of other sup-\n",
      "ported options for mounting external storage inside your volume. For example, to\n",
      "mount a simple NFS share, you only need to specify the NFS server and the path\n",
      "exported by the server, as shown in the following listing.\n",
      "  volumes:                       \n",
      "  - name: mongodb-data           \n",
      "    nfs:                     \n",
      "      server: 1.2.3.4         \n",
      "      path: /some/path     \n",
      "USING OTHER STORAGE TECHNOLOGIES\n",
      "Other supported options include iscsi for mounting an ISCSI disk resource, glusterfs\n",
      "for a GlusterFS mount, rbd for a RADOS Block Device, flexVolume, cinder, cephfs,\n",
      "flocker, fc (Fibre Channel), and others. You don’t need to know all of them if you’re\n",
      "not using them. They’re mentioned here to show you that Kubernetes supports a\n",
      "broad range of storage technologies and you can use whichever you prefer and are\n",
      "used to.\n",
      " To see details on what properties you need to set for each of these volume types,\n",
      "you can either turn to the Kubernetes API definitions in the Kubernetes API refer-\n",
      "ence or look up the information through kubectl explain, as shown in chapter 3. If\n",
      "you’re already familiar with a particular storage technology, using the explain com-\n",
      "mand should allow you to easily figure out how to mount a volume of the proper type\n",
      "and use it in your pods.\n",
      " But does a developer need to know all this stuff? Should a developer, when creat-\n",
      "ing a pod, have to deal with infrastructure-related storage details, or should that be\n",
      "left to the cluster administrator? \n",
      " Having a pod’s volumes refer to the actual underlying infrastructure isn’t what\n",
      "Kubernetes is about, is it? For example, for a developer to have to specify the host-\n",
      "name of the NFS server feels wrong. And that’s not even the worst thing about it. \n",
      " Including this type of infrastructure-related information into a pod definition\n",
      "means the pod definition is pretty much tied to a specific Kubernetes cluster. You\n",
      "can’t use the same pod definition in another one. That’s why using volumes like this\n",
      "isn’t the best way to attach persistent storage to your pods. You’ll learn how to improve\n",
      "on this in the next section.\n",
      "Listing 6.8\n",
      "A pod using an nfs volume: mongodb-pod-nfs.yaml\n",
      "Specify the ID of the EBS \n",
      "volume you created.\n",
      "The filesystem type \n",
      "is EXT4 as before.\n",
      "This volume is backed \n",
      "by an NFS share.\n",
      "The IP of the \n",
      "NFS server\n",
      "The path exported \n",
      "by the server\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 208, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "176\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "6.5\n",
      "Decoupling pods from the underlying storage technology\n",
      "All the persistent volume types we’ve explored so far have required the developer of the\n",
      "pod to have knowledge of the actual network storage infrastructure available in the clus-\n",
      "ter. For example, to create a NFS-backed volume, the developer has to know the actual\n",
      "server the NFS export is located on. This is against the basic idea of Kubernetes, which\n",
      "aims to hide the actual infrastructure from both the application and its developer, leav-\n",
      "ing them free from worrying about the specifics of the infrastructure and making apps\n",
      "portable across a wide array of cloud providers and on-premises datacenters.\n",
      " Ideally, a developer deploying their apps on Kubernetes should never have to\n",
      "know what kind of storage technology is used underneath, the same way they don’t\n",
      "have to know what type of physical servers are being used to run their pods. Infrastruc-\n",
      "ture-related dealings should be the sole domain of the cluster administrator.\n",
      " When a developer needs a certain amount of persistent storage for their applica-\n",
      "tion, they can request it from Kubernetes, the same way they can request CPU, mem-\n",
      "ory, and other resources when creating a pod. The system administrator can configure\n",
      "the cluster so it can give the apps what they request.\n",
      "6.5.1\n",
      "Introducing PersistentVolumes and PersistentVolumeClaims\n",
      "To enable apps to request storage in a Kubernetes cluster without having to deal with\n",
      "infrastructure specifics, two new resources were introduced. They are Persistent-\n",
      "Volumes and PersistentVolumeClaims. The names may be a bit misleading, because as\n",
      "you’ve seen in the previous few sections, even regular Kubernetes volumes can be\n",
      "used to store persistent data. \n",
      " Using a PersistentVolume inside a pod is a little more complex than using a regular\n",
      "pod volume, so let’s illustrate how pods, PersistentVolumeClaims, PersistentVolumes,\n",
      "and the actual underlying storage relate to each other in figure 6.6.\n",
      "Pod\n",
      "Admin\n",
      "Volume\n",
      "1. Cluster admin sets up some type of\n",
      "network storage (NFS export or similar)\n",
      "2. Admin then creates a PersistentVolume (PV)\n",
      "by posting a PV descriptor to the Kubernetes API\n",
      "NFS\n",
      "export\n",
      "Persistent\n",
      "Volume\n",
      "User\n",
      "Persistent\n",
      "VolumeClaim\n",
      "3. User creates a\n",
      "PersistentVolumeClaim (PVC)\n",
      "4. Kubernetes ﬁnds a PV of\n",
      "adequate size and access\n",
      "mode and binds the PVC\n",
      "to the PV\n",
      "5. User creates a\n",
      "pod with a volume\n",
      "referencing the PVC\n",
      "Figure 6.6\n",
      "PersistentVolumes are provisioned by cluster admins and consumed by pods \n",
      "through PersistentVolumeClaims.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 209, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "177\n",
      "Decoupling pods from the underlying storage technology\n",
      "Instead of the developer adding a technology-specific volume to their pod, it’s the\n",
      "cluster administrator who sets up the underlying storage and then registers it in\n",
      "Kubernetes by creating a PersistentVolume resource through the Kubernetes API\n",
      "server. When creating the PersistentVolume, the admin specifies its size and the access\n",
      "modes it supports. \n",
      " When a cluster user needs to use persistent storage in one of their pods, they first\n",
      "create a PersistentVolumeClaim manifest, specifying the minimum size and the access\n",
      "mode they require. The user then submits the PersistentVolumeClaim manifest to the\n",
      "Kubernetes API server, and Kubernetes finds the appropriate PersistentVolume and\n",
      "binds the volume to the claim. \n",
      " The PersistentVolumeClaim can then be used as one of the volumes inside a pod.\n",
      "Other users cannot use the same PersistentVolume until it has been released by delet-\n",
      "ing the bound PersistentVolumeClaim.\n",
      "6.5.2\n",
      "Creating a PersistentVolume\n",
      "Let’s revisit the MongoDB example, but unlike before, you won’t reference the GCE\n",
      "Persistent Disk in the pod directly. Instead, you’ll first assume the role of a cluster\n",
      "administrator and create a PersistentVolume backed by the GCE Persistent Disk. Then\n",
      "you’ll assume the role of the application developer and first claim the PersistentVol-\n",
      "ume and then use it inside your pod.\n",
      " In section 6.4.1 you set up the physical storage by provisioning the GCE Persistent\n",
      "Disk, so you don’t need to do that again. All you need to do is create the Persistent-\n",
      "Volume resource in Kubernetes by preparing the manifest shown in the following list-\n",
      "ing and posting it to the API server.\n",
      "apiVersion: v1\n",
      "kind: PersistentVolume\n",
      "metadata:\n",
      "  name: mongodb-pv\n",
      "spec:\n",
      "  capacity:                  \n",
      "    storage: 1Gi             \n",
      "  accessModes:                              \n",
      "  - ReadWriteOnce                           \n",
      "  - ReadOnlyMany                            \n",
      "  persistentVolumeReclaimPolicy: Retain    \n",
      "  gcePersistentDisk:                      \n",
      "    pdName: mongodb                       \n",
      "    fsType: ext4                          \n",
      "Listing 6.9\n",
      "A gcePersistentDisk PersistentVolume: mongodb-pv-gcepd.yaml\n",
      "Defining the \n",
      "PersistentVolume’s size\n",
      "It can either be mounted by a single \n",
      "client for reading and writing or by \n",
      "multiple clients for reading only.\n",
      "After the claim is released, \n",
      "the PersistentVolume \n",
      "should be retained (not \n",
      "erased or deleted).\n",
      "The PersistentVolume is \n",
      "backed by the GCE Persistent \n",
      "Disk you created earlier.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 210, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "178\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "NOTE\n",
      "If you’re using Minikube, create the PV using the mongodb-pv-host-\n",
      "path.yaml file.\n",
      "When creating a PersistentVolume, the administrator needs to tell Kubernetes what its\n",
      "capacity is and whether it can be read from and/or written to by a single node or by\n",
      "multiple nodes at the same time. They also need to tell Kubernetes what to do with the\n",
      "PersistentVolume when it’s released (when the PersistentVolumeClaim it’s bound to is\n",
      "deleted). And last, but certainly not least, they need to specify the type, location, and\n",
      "other properties of the actual storage this PersistentVolume is backed by. If you look\n",
      "closely, this last part is exactly the same as earlier, when you referenced the GCE Per-\n",
      "sistent Disk in the pod volume directly (shown again in the following listing).\n",
      "spec:\n",
      "  volumes:                       \n",
      "  - name: mongodb-data           \n",
      "    gcePersistentDisk:           \n",
      "      pdName: mongodb            \n",
      "      fsType: ext4               \n",
      "  ...\n",
      "After you create the PersistentVolume with the kubectl create command, it should\n",
      "be ready to be claimed. See if it is by listing all PersistentVolumes:\n",
      "$ kubectl get pv\n",
      "NAME         CAPACITY   RECLAIMPOLICY   ACCESSMODES   STATUS      CLAIM\n",
      "mongodb-pv   1Gi        Retain          RWO,ROX       Available   \n",
      "NOTE\n",
      "Several columns are omitted. Also, pv is used as a shorthand for\n",
      "persistentvolume.\n",
      "As expected, the PersistentVolume is shown as Available, because you haven’t yet cre-\n",
      "ated the PersistentVolumeClaim. \n",
      "NOTE\n",
      "PersistentVolumes don’t belong to any namespace (see figure 6.7).\n",
      "They’re cluster-level resources like nodes.\n",
      "Listing 6.10\n",
      "Referencing a GCE PD in a pod’s volume\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 211, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "179\n",
      "Decoupling pods from the underlying storage technology\n",
      "6.5.3\n",
      "Claiming a PersistentVolume by creating a \n",
      "PersistentVolumeClaim\n",
      "Now let’s lay down our admin hats and put our developer hats back on. Say you need\n",
      "to deploy a pod that requires persistent storage. You’ll use the PersistentVolume you\n",
      "created earlier. But you can’t use it directly in the pod. You need to claim it first.\n",
      " Claiming a PersistentVolume is a completely separate process from creating a pod,\n",
      "because you want the same PersistentVolumeClaim to stay available even if the pod is\n",
      "rescheduled (remember, rescheduling means the previous pod is deleted and a new\n",
      "one is created). \n",
      "CREATING A PERSISTENTVOLUMECLAIM\n",
      "You’ll create the claim now. You need to prepare a PersistentVolumeClaim manifest\n",
      "like the one shown in the following listing and post it to the Kubernetes API through\n",
      "kubectl create.\n",
      "apiVersion: v1\n",
      "kind: PersistentVolumeClaim\n",
      "metadata:\n",
      "  name: mongodb-pvc          \n",
      "Listing 6.11\n",
      "A PersistentVolumeClaim: mongodb-pvc.yaml\n",
      "Pod(s)\n",
      "Pod(s)\n",
      "Persistent\n",
      "Volume\n",
      "Persistent\n",
      "Volume\n",
      "Persistent\n",
      "Volume\n",
      "Persistent\n",
      "Volume\n",
      "...\n",
      "User A\n",
      "Persistent\n",
      "Volume\n",
      "Claim(s)\n",
      "Persistent\n",
      "Volume\n",
      "Claim(s)\n",
      "Namespace A\n",
      "User B\n",
      "Namespace B\n",
      "Node\n",
      "Node\n",
      "Node\n",
      "Node\n",
      "Node\n",
      "Node\n",
      "Persistent\n",
      "Volume\n",
      "Figure 6.7\n",
      "PersistentVolumes, like cluster Nodes, don’t belong to any namespace, unlike pods and \n",
      "PersistentVolumeClaims.\n",
      "The name of your claim—you’ll \n",
      "need this later when using the \n",
      "claim as the pod’s volume.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 212, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "180\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "spec:\n",
      "  resources:\n",
      "    requests:                \n",
      "      storage: 1Gi           \n",
      "  accessModes:              \n",
      "  - ReadWriteOnce           \n",
      "  storageClassName: \"\"     \n",
      "As soon as you create the claim, Kubernetes finds the appropriate PersistentVolume\n",
      "and binds it to the claim. The PersistentVolume’s capacity must be large enough to\n",
      "accommodate what the claim requests. Additionally, the volume’s access modes must\n",
      "include the access modes requested by the claim. In your case, the claim requests 1 GiB\n",
      "of storage and a ReadWriteOnce access mode. The PersistentVolume you created ear-\n",
      "lier matches those two requirements so it is bound to your claim. You can see this by\n",
      "inspecting the claim.\n",
      "LISTING PERSISTENTVOLUMECLAIMS\n",
      "List all PersistentVolumeClaims to see the state of your PVC:\n",
      "$ kubectl get pvc\n",
      "NAME          STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE\n",
      "mongodb-pvc   Bound     mongodb-pv   1Gi        RWO,ROX       3s\n",
      "NOTE\n",
      "We’re using pvc as a shorthand for persistentvolumeclaim.\n",
      "The claim is shown as Bound to PersistentVolume mongodb-pv. Note the abbreviations\n",
      "used for the access modes:\n",
      "\n",
      "RWO—ReadWriteOnce—Only a single node can mount the volume for reading\n",
      "and writing.\n",
      "\n",
      "ROX—ReadOnlyMany—Multiple nodes can mount the volume for reading.\n",
      "\n",
      "RWX—ReadWriteMany—Multiple nodes can mount the volume for both reading\n",
      "and writing.\n",
      "NOTE\n",
      "RWO, ROX, and RWX pertain to the number of worker nodes that can use\n",
      "the volume at the same time, not to the number of pods!\n",
      "LISTING PERSISTENTVOLUMES\n",
      "You can also see that the PersistentVolume is now Bound and no longer Available by\n",
      "inspecting it with kubectl get:\n",
      "$ kubectl get pv\n",
      "NAME         CAPACITY   ACCESSMODES   STATUS   CLAIM                 AGE\n",
      "mongodb-pv   1Gi        RWO,ROX       Bound    default/mongodb-pvc   1m\n",
      "The PersistentVolume shows it’s bound to claim default/mongodb-pvc. The default\n",
      "part is the namespace the claim resides in (you created the claim in the default\n",
      "Requesting 1 GiB of storage\n",
      "You want the storage to support a single \n",
      "client (performing both reads and writes).\n",
      "You’ll learn about this in the section \n",
      "about dynamic provisioning.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 213, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "181\n",
      "Decoupling pods from the underlying storage technology\n",
      "namespace). We’ve already said that PersistentVolume resources are cluster-scoped\n",
      "and thus cannot be created in a specific namespace, but PersistentVolumeClaims can\n",
      "only be created in a specific namespace. They can then only be used by pods in the\n",
      "same namespace.\n",
      "6.5.4\n",
      "Using a PersistentVolumeClaim in a pod\n",
      "The PersistentVolume is now yours to use. Nobody else can claim the same volume\n",
      "until you release it. To use it inside a pod, you need to reference the Persistent-\n",
      "VolumeClaim by name inside the pod’s volume (yes, the PersistentVolumeClaim, not\n",
      "the PersistentVolume directly!), as shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: mongodb \n",
      "spec:\n",
      "  containers:\n",
      "  - image: mongo\n",
      "    name: mongodb\n",
      "    volumeMounts:\n",
      "    - name: mongodb-data\n",
      "      mountPath: /data/db\n",
      "    ports:\n",
      "    - containerPort: 27017\n",
      "      protocol: TCP\n",
      "  volumes:\n",
      "  - name: mongodb-data\n",
      "    persistentVolumeClaim:       \n",
      "      claimName: mongodb-pvc     \n",
      "Go ahead and create the pod. Now, check to see if the pod is indeed using the same\n",
      "PersistentVolume and its underlying GCE PD. You should see the data you stored ear-\n",
      "lier by running the MongoDB shell again, as shown in the following listing.\n",
      "$ kubectl exec -it mongodb mongo\n",
      "MongoDB shell version: 3.2.8\n",
      "connecting to: mongodb://127.0.0.1:27017\n",
      "Welcome to the MongoDB shell.\n",
      "...\n",
      "> use mystore\n",
      "switched to db mystore\n",
      "> db.foo.find()\n",
      "{ \"_id\" : ObjectId(\"57a61eb9de0cfd512374cc75\"), \"name\" : \"foo\" }\n",
      "And there it is. You‘re able to retrieve the document you stored into MongoDB\n",
      "previously.\n",
      "Listing 6.12\n",
      "A pod using a PersistentVolumeClaim volume: mongodb-pod-pvc.yaml\n",
      "Listing 6.13\n",
      "Retrieving MongoDB’s persisted data in the pod using the PVC and PV\n",
      "Referencing the PersistentVolumeClaim \n",
      "by name in the pod volume\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 214, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "182\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "6.5.5\n",
      "Understanding the benefits of using PersistentVolumes and claims\n",
      "Examine figure 6.8, which shows both ways a pod can use a GCE Persistent Disk—\n",
      "directly or through a PersistentVolume and claim.\n",
      "Consider how using this indirect method of obtaining storage from the infrastructure\n",
      "is much simpler for the application developer (or cluster user). Yes, it does require\n",
      "the additional steps of creating the PersistentVolume and the PersistentVolumeClaim,\n",
      "but the developer doesn’t have to know anything about the actual storage technology\n",
      "used underneath. \n",
      " Additionally, the same pod and claim manifests can now be used on many different\n",
      "Kubernetes clusters, because they don’t refer to anything infrastructure-specific. The\n",
      "claim states, “I need x amount of storage and I need to be able to read and write to it\n",
      "by a single client at once,” and then the pod references the claim by name in one of\n",
      "its volumes.\n",
      "Pod: mongodb\n",
      "Container: mongodb\n",
      "volumeMounts:\n",
      "name: mongodb-data\n",
      "mountPath: /data/db\n",
      "gcePersistentDisk:\n",
      "pdName: mongodb\n",
      "GCE\n",
      "Persistent Disk:\n",
      "mongodb\n",
      "Volume:\n",
      "mongodb\n",
      "Pod: mongodb\n",
      "Container: mongodb\n",
      "volumeMounts:\n",
      "name: mongodb-data\n",
      "mountPath: /data/db\n",
      "persistentVolumeClaim:\n",
      "claimName: mongodb-pvc\n",
      "gcePersistentDisk:\n",
      "pdName: mongodb\n",
      "GCE\n",
      "Persistent Disk:\n",
      "mongodb\n",
      "PersistentVolume:\n",
      "mongodb-pv\n",
      "(1 Gi, RWO, RWX)\n",
      "Volume:\n",
      "mongodb\n",
      "Claim lists\n",
      "1Gi and\n",
      "ReadWriteOnce\n",
      "access\n",
      "PersistentVolumeClaim:\n",
      "mongodb-pvc\n",
      "Figure 6.8\n",
      "Using the GCE Persistent Disk directly or through a PVC and PV\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 215, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "183\n",
      "Decoupling pods from the underlying storage technology\n",
      "6.5.6\n",
      "Recycling PersistentVolumes\n",
      "Before you wrap up this section on PersistentVolumes, let’s do one last quick experi-\n",
      "ment. Delete the pod and the PersistentVolumeClaim:\n",
      "$ kubectl delete pod mongodb\n",
      "pod \"mongodb\" deleted\n",
      "$ kubectl delete pvc mongodb-pvc\n",
      "persistentvolumeclaim \"mongodb-pvc\" deleted\n",
      "What if you create the PersistentVolumeClaim again? Will it be bound to the Persistent-\n",
      "Volume or not? After you create the claim, what does kubectl get pvc show?\n",
      "$ kubectl get pvc\n",
      "NAME           STATUS    VOLUME       CAPACITY   ACCESSMODES   AGE\n",
      "mongodb-pvc    Pending                                         13s\n",
      "The claim’s status is shown as Pending. Interesting. When you created the claim ear-\n",
      "lier, it was immediately bound to the PersistentVolume, so why wasn’t it bound now?\n",
      "Maybe listing the PersistentVolumes can shed more light on this:\n",
      "$ kubectl get pv\n",
      "NAME        CAPACITY  ACCESSMODES  STATUS    CLAIM               REASON AGE\n",
      "mongodb-pv  1Gi       RWO,ROX      Released  default/mongodb-pvc        5m\n",
      "The STATUS column shows the PersistentVolume as Released, not Available like\n",
      "before. Because you’ve already used the volume, it may contain data and shouldn’t be\n",
      "bound to a completely new claim without giving the cluster admin a chance to clean it\n",
      "up. Without this, a new pod using the same PersistentVolume could read the data\n",
      "stored there by the previous pod, even if the claim and pod were created in a different\n",
      "namespace (and thus likely belong to a different cluster tenant).\n",
      "RECLAIMING PERSISTENTVOLUMES MANUALLY\n",
      "You told Kubernetes you wanted your PersistentVolume to behave like this when you\n",
      "created it—by setting its persistentVolumeReclaimPolicy to Retain. You wanted\n",
      "Kubernetes to retain the volume and its contents after it’s released from its claim. As\n",
      "far as I’m aware, the only way to manually recycle the PersistentVolume to make it\n",
      "available again is to delete and recreate the PersistentVolume resource. As you do\n",
      "that, it’s your decision what to do with the files on the underlying storage: you can\n",
      "either delete them or leave them alone so they can be reused by the next  pod.\n",
      "RECLAIMING PERSISTENTVOLUMES AUTOMATICALLY\n",
      "Two other possible reclaim policies exist: Recycle and Delete. The first one deletes\n",
      "the volume’s contents and makes the volume available to be claimed again. This way,\n",
      "the PersistentVolume can be reused multiple times by different PersistentVolume-\n",
      "Claims and different pods, as you can see in figure 6.9.\n",
      " The Delete policy, on the other hand, deletes the underlying storage. Note that\n",
      "the Recycle option is currently not available for GCE Persistent Disks. This type of\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 216, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "184\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "A PersistentVolume only supports the Retain or Delete policies. Other Persistent-\n",
      "Volume types may or may not support each of these options, so before creating your\n",
      "own PersistentVolume, be sure to check what reclaim policies are supported for the\n",
      "specific underlying storage you’ll use in the volume.\n",
      "TIP\n",
      "You can change the PersistentVolume reclaim policy on an existing\n",
      "PersistentVolume. For example, if it’s initially set to Delete, you can easily\n",
      "change it to Retain to prevent losing valuable data.\n",
      "6.6\n",
      "Dynamic provisioning of PersistentVolumes\n",
      "You’ve seen how using PersistentVolumes and PersistentVolumeClaims makes it easy\n",
      "to obtain persistent storage without the developer having to deal with the actual stor-\n",
      "age technology used underneath. But this still requires a cluster administrator to pro-\n",
      "vision the actual storage up front. Luckily, Kubernetes can also perform this job\n",
      "automatically through dynamic provisioning of PersistentVolumes.\n",
      " The cluster admin, instead of creating PersistentVolumes, can deploy a Persistent-\n",
      "Volume provisioner and define one or more StorageClass objects to let users choose\n",
      "what type of PersistentVolume they want. The users can refer to the StorageClass in\n",
      "their PersistentVolumeClaims and the provisioner will take that into account when\n",
      "provisioning the persistent storage. \n",
      "NOTE\n",
      "Similar to PersistentVolumes, StorageClass resources aren’t namespaced.\n",
      "Kubernetes includes provisioners for the most popular cloud providers, so the admin-\n",
      "istrator doesn’t always need to deploy a provisioner. But if Kubernetes is deployed\n",
      "on-premises, a custom provisioner needs to be deployed.\n",
      "PersistentVolume\n",
      "PersistentVolumeClaim 1\n",
      "Pod 1\n",
      "Pod 2\n",
      "PersistentVolumeClaim 2\n",
      "Pod 3\n",
      "PVC is deleted;\n",
      "PV is automatically\n",
      "recycled and ready\n",
      "to be claimed and\n",
      "re-used again\n",
      "User creates\n",
      "PersistentVolumeClaim\n",
      "Pod 2\n",
      "unmounts\n",
      "PVC\n",
      "Pod 2\n",
      "mounts\n",
      "PVC\n",
      "Pod 1\n",
      "mounts\n",
      "PVC\n",
      "Pod 1\n",
      "unmounts\n",
      "PVC\n",
      "Admin deletes\n",
      "PersistentVolume\n",
      "Admin creates\n",
      "PersistentVolume\n",
      "Time\n",
      "Figure 6.9\n",
      "The lifespan of a PersistentVolume, PersistentVolumeClaims, and pods using them\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 217, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "185\n",
      "Dynamic provisioning of PersistentVolumes\n",
      " Instead of the administrator pre-provisioning a bunch of PersistentVolumes, they\n",
      "need to define one or two (or more) StorageClasses and let the system create a new\n",
      "PersistentVolume each time one is requested through a PersistentVolumeClaim. The\n",
      "great thing about this is that it’s impossible to run out of PersistentVolumes (obviously,\n",
      "you can run out of storage space). \n",
      "6.6.1\n",
      "Defining the available storage types through StorageClass \n",
      "resources\n",
      "Before a user can create a PersistentVolumeClaim, which will result in a new Persistent-\n",
      "Volume being provisioned, an admin needs to create one or more StorageClass\n",
      "resources. Let’s look at an example of one in the following listing.\n",
      "apiVersion: storage.k8s.io/v1\n",
      "kind: StorageClass\n",
      "metadata:\n",
      "  name: fast\n",
      "provisioner: kubernetes.io/gce-pd       \n",
      "parameters:\n",
      "  type: pd-ssd                     \n",
      "  zone: europe-west1-b             \n",
      "NOTE\n",
      "If using Minikube, deploy the file storageclass-fast-hostpath.yaml.\n",
      "The StorageClass resource specifies which provisioner should be used for provision-\n",
      "ing the PersistentVolume when a PersistentVolumeClaim requests this StorageClass.\n",
      "The parameters defined in the StorageClass definition are passed to the provisioner\n",
      "and are specific to each provisioner plugin. \n",
      " The StorageClass uses the Google Compute Engine (GCE) Persistent Disk (PD)\n",
      "provisioner, which means it can be used when Kubernetes is running in GCE. For\n",
      "other cloud providers, other provisioners need to be used.\n",
      "6.6.2\n",
      "Requesting the storage class in a PersistentVolumeClaim\n",
      "After the StorageClass resource is created, users can refer to the storage class by name\n",
      "in their PersistentVolumeClaims. \n",
      "CREATING A PVC DEFINITION REQUESTING A SPECIFIC STORAGE CLASS\n",
      "You can modify your mongodb-pvc to use dynamic provisioning. The following listing\n",
      "shows the updated YAML definition of the PVC.\n",
      "apiVersion: v1\n",
      "kind: PersistentVolumeClaim\n",
      "metadata:\n",
      "  name: mongodb-pvc \n",
      "Listing 6.14\n",
      "A StorageClass definition: storageclass-fast-gcepd.yaml\n",
      "Listing 6.15\n",
      "A PVC with dynamic provisioning: mongodb-pvc-dp.yaml\n",
      "The volume plugin to \n",
      "use for provisioning \n",
      "the PersistentVolume\n",
      "The parameters passed \n",
      "to the provisioner\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 218, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "186\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "spec:\n",
      "  storageClassName: fast     \n",
      "  resources:\n",
      "    requests:\n",
      "      storage: 100Mi\n",
      "  accessModes:\n",
      "    - ReadWriteOnce\n",
      "Apart from specifying the size and access modes, your PersistentVolumeClaim now\n",
      "also specifies the class of storage you want to use. When you create the claim, the\n",
      "PersistentVolume is created by the provisioner referenced in the fast StorageClass\n",
      "resource. The provisioner is used even if an existing manually provisioned Persistent-\n",
      "Volume matches the PersistentVolumeClaim. \n",
      "NOTE\n",
      "If you reference a non-existing storage class in a PVC, the provisioning\n",
      "of the PV will fail (you’ll see a ProvisioningFailed event when you use\n",
      "kubectl describe on the PVC).\n",
      "EXAMINING THE CREATED PVC AND THE DYNAMICALLY PROVISIONED PV\n",
      "Next you’ll create the PVC and then use kubectl get to see it:\n",
      "$ kubectl get pvc mongodb-pvc\n",
      "NAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS\n",
      "mongodb-pvc   Bound    pvc-1e6bc048   1Gi        RWO           fast \n",
      "The VOLUME column shows the PersistentVolume that’s bound to this claim (the actual\n",
      "name is longer than what’s shown above). You can try listing PersistentVolumes now to\n",
      "see that a new PV has indeed been created automatically:\n",
      "$ kubectl get pv\n",
      "NAME           CAPACITY  ACCESSMODES  RECLAIMPOLICY  STATUS    STORAGECLASS   \n",
      "mongodb-pv     1Gi       RWO,ROX      Retain         Released \n",
      "pvc-1e6bc048   1Gi       RWO          Delete         Bound     fast\n",
      "NOTE\n",
      "Only pertinent columns are shown.\n",
      "You can see the dynamically provisioned PersistentVolume. Its capacity and access\n",
      "modes are what you requested in the PVC. Its reclaim policy is Delete, which means\n",
      "the PersistentVolume will be deleted when the PVC is deleted. Beside the PV, the pro-\n",
      "visioner also provisioned the actual storage. Your fast StorageClass is configured to\n",
      "use the kubernetes.io/gce-pd provisioner, which provisions GCE Persistent Disks.\n",
      "You can see the disk with the following command:\n",
      "$ gcloud compute disks list\n",
      "NAME                          ZONE            SIZE_GB  TYPE         STATUS\n",
      "gke-kubia-dyn-pvc-1e6bc048    europe-west1-d  1        pd-ssd       READY\n",
      "gke-kubia-default-pool-71df   europe-west1-d  100      pd-standard  READY\n",
      "gke-kubia-default-pool-79cd   europe-west1-d  100      pd-standard  READY\n",
      "gke-kubia-default-pool-blc4   europe-west1-d  100      pd-standard  READY\n",
      "mongodb                       europe-west1-d  1        pd-standard  READY\n",
      "This PVC requests the \n",
      "custom storage class.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 219, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "187\n",
      "Dynamic provisioning of PersistentVolumes\n",
      "As you can see, the first persistent disk’s name suggests it was provisioned dynamically\n",
      "and its type shows it’s an SSD, as specified in the storage class you created earlier. \n",
      "UNDERSTANDING HOW TO USE STORAGE CLASSES\n",
      "The cluster admin can create multiple storage classes with different performance or\n",
      "other characteristics. The developer then decides which one is most appropriate for\n",
      "each claim they create. \n",
      " The nice thing about StorageClasses is the fact that claims refer to them by\n",
      "name. The PVC definitions are therefore portable across different clusters, as long\n",
      "as the StorageClass names are the same across all of them. To see this portability\n",
      "yourself, you can try running the same example on Minikube, if you’ve been using\n",
      "GKE up to this point. As a cluster admin, you’ll have to create a different storage\n",
      "class (but with the same name). The storage class defined in the storageclass-fast-\n",
      "hostpath.yaml file is tailor-made for use in Minikube. Then, once you deploy the stor-\n",
      "age class, you as a cluster user can deploy the exact same PVC manifest and the exact\n",
      "same pod manifest as before. This shows how the pods and PVCs are portable across\n",
      "different clusters.\n",
      "6.6.3\n",
      "Dynamic provisioning without specifying a storage class\n",
      "As we’ve progressed through this chapter, attaching persistent storage to pods has\n",
      "become ever simpler. The sections in this chapter reflect how provisioning of storage\n",
      "has evolved from early Kubernetes versions to now. In this final section, we’ll look at\n",
      "the latest and simplest way of attaching a PersistentVolume to a pod. \n",
      "LISTING STORAGE CLASSES\n",
      "When you created your custom storage class called fast, you didn’t check if any exist-\n",
      "ing storage classes were already defined in your cluster. Why don’t you do that now?\n",
      "Here are the storage classes available in GKE:\n",
      "$ kubectl get sc\n",
      "NAME                 TYPE\n",
      "fast                 kubernetes.io/gce-pd\n",
      "standard (default)   kubernetes.io/gce-pd\n",
      "NOTE\n",
      "We’re using sc as shorthand for storageclass.\n",
      "Beside the fast storage class, which you created yourself, a standard storage class\n",
      "exists and is marked as default. You’ll learn what that means in a moment. Let’s list the\n",
      "storage classes available in Minikube, so we can compare:\n",
      "$ kubectl get sc\n",
      "NAME                 TYPE\n",
      "fast                 k8s.io/minikube-hostpath\n",
      "standard (default)   k8s.io/minikube-hostpath\n",
      "Again, the fast storage class was created by you and a default standard storage class\n",
      "exists here as well. Comparing the TYPE columns in the two listings, you see GKE is\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 220, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "188\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "using the kubernetes.io/gce-pd provisioner, whereas Minikube is using k8s.io/\n",
      "minikube-hostpath. \n",
      "EXAMINING THE DEFAULT STORAGE CLASS\n",
      "You’re going to use kubectl get to see more info about the standard storage class in a\n",
      "GKE cluster, as shown in the following listing.\n",
      "$ kubectl get sc standard -o yaml\n",
      "apiVersion: storage.k8s.io/v1\n",
      "kind: StorageClass\n",
      "metadata:\n",
      "  annotations:\n",
      "    storageclass.beta.kubernetes.io/is-default-class: \"true\"   \n",
      "  creationTimestamp: 2017-05-16T15:24:11Z\n",
      "  labels:\n",
      "    addonmanager.kubernetes.io/mode: EnsureExists\n",
      "    kubernetes.io/cluster-service: \"true\"\n",
      "  name: standard\n",
      "  resourceVersion: \"180\"\n",
      "  selfLink: /apis/storage.k8s.io/v1/storageclassesstandard\n",
      "  uid: b6498511-3a4b-11e7-ba2c-42010a840014\n",
      "parameters:                                    \n",
      "  type: pd-standard                            \n",
      "provisioner: kubernetes.io/gce-pd      \n",
      "If you look closely toward the top of the listing, the storage class definition includes an\n",
      "annotation, which makes this the default storage class. The default storage class is\n",
      "what’s used to dynamically provision a PersistentVolume if the PersistentVolumeClaim\n",
      "doesn’t explicitly say which storage class to use. \n",
      "CREATING A PERSISTENTVOLUMECLAIM WITHOUT SPECIFYING A STORAGE CLASS\n",
      "You can create a PVC without specifying the storageClassName attribute and (on\n",
      "Google Kubernetes Engine) a GCE Persistent Disk of type pd-standard will be provi-\n",
      "sioned for you. Try this by creating a claim from the YAML in the following listing.\n",
      "apiVersion: v1\n",
      "kind: PersistentVolumeClaim\n",
      "metadata:\n",
      "  name: mongodb-pvc2\n",
      "spec:                        \n",
      "  resources:                 \n",
      "    requests:                \n",
      "      storage: 100Mi         \n",
      "  accessModes:               \n",
      "    - ReadWriteOnce          \n",
      "Listing 6.16\n",
      "The definition of the standard storage class on GKE\n",
      "Listing 6.17\n",
      "PVC with no storage class defined: mongodb-pvc-dp-nostorageclass.yaml\n",
      "This annotation \n",
      "marks the storage \n",
      "class as default.\n",
      "The type parameter is used by the provisioner \n",
      "to know what type of GCE PD to create.\n",
      "The GCE Persistent Disk provisioner \n",
      "is used to provision PVs of this class.\n",
      "You’re not specifying \n",
      "the storageClassName \n",
      "attribute (unlike earlier \n",
      "examples).\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 221, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "189\n",
      "Dynamic provisioning of PersistentVolumes\n",
      "This PVC definition includes only the storage size request and the desired access\n",
      "modes, but no storage class. When you create the PVC, whatever storage class is\n",
      "marked as default will be used. You can confirm that’s the case:\n",
      "$ kubectl get pvc mongodb-pvc2\n",
      "NAME          STATUS   VOLUME         CAPACITY   ACCESSMODES   STORAGECLASS\n",
      "mongodb-pvc2  Bound    pvc-95a5ec12   1Gi        RWO           standard\n",
      "$ kubectl get pv pvc-95a5ec12\n",
      "NAME           CAPACITY  ACCESSMODES  RECLAIMPOLICY  STATUS    STORAGECLASS   \n",
      "pvc-95a5ec12   1Gi       RWO          Delete         Bound     standard\n",
      "$ gcloud compute disks list\n",
      "NAME                          ZONE            SIZE_GB  TYPE         STATUS\n",
      "gke-kubia-dyn-pvc-95a5ec12    europe-west1-d  1        pd-standard  READY\n",
      "...\n",
      "FORCING A PERSISTENTVOLUMECLAIM TO BE BOUND TO ONE OF THE PRE-PROVISIONED \n",
      "PERSISTENTVOLUMES\n",
      "This finally brings us to why you set storageClassName to an empty string in listing 6.11\n",
      "(when you wanted the PVC to bind to the PV you’d provisioned manually). Let me\n",
      "repeat the relevant lines of that PVC definition here:\n",
      "kind: PersistentVolumeClaim\n",
      "spec:\n",
      "  storageClassName: \"\"       \n",
      "If you hadn’t set the storageClassName attribute to an empty string, the dynamic vol-\n",
      "ume provisioner would have provisioned a new PersistentVolume, despite there being\n",
      "an appropriate pre-provisioned PersistentVolume. At that point, I wanted to demon-\n",
      "strate how a claim gets bound to a manually pre-provisioned PersistentVolume. I didn’t\n",
      "want the dynamic provisioner to interfere. \n",
      "TIP\n",
      "Explicitly set storageClassName to \"\" if you want the PVC to use a pre-\n",
      "provisioned PersistentVolume.\n",
      "UNDERSTANDING THE COMPLETE PICTURE OF DYNAMIC PERSISTENTVOLUME PROVISIONING\n",
      "This brings us to the end of this chapter. To summarize, the best way to attach per-\n",
      "sistent storage to a pod is to only create the PVC (with an explicitly specified storage-\n",
      "ClassName if necessary) and the pod (which refers to the PVC by name). Everything\n",
      "else is taken care of by the dynamic PersistentVolume provisioner.\n",
      " To get a complete picture of the steps involved in getting a dynamically provi-\n",
      "sioned PersistentVolume, examine figure 6.10.\n",
      " \n",
      " \n",
      " \n",
      "Specifying an empty string as the storage class \n",
      "name ensures the PVC binds to a pre-provisioned \n",
      "PV instead of dynamically provisioning a new one.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 222, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "190\n",
      "CHAPTER 6\n",
      "Volumes: attaching disk storage to containers\n",
      "6.7\n",
      "Summary\n",
      "This chapter has shown you how volumes are used to provide either temporary or per-\n",
      "sistent storage to a pod’s containers. You’ve learned how to\n",
      "Create a multi-container pod and have the pod’s containers operate on the\n",
      "same files by adding a volume to the pod and mounting it in each container\n",
      "Use the emptyDir volume to store temporary, non-persistent data\n",
      "Use the gitRepo volume to easily populate a directory with the contents of a Git\n",
      "repository at pod startup\n",
      "Use the hostPath volume to access files from the host node\n",
      "Mount external storage in a volume to persist pod data across pod restarts\n",
      "Decouple the pod from the storage infrastructure by using PersistentVolumes\n",
      "and PersistentVolumeClaims\n",
      "Have PersistentVolumes of the desired (or the default) storage class dynami-\n",
      "cally provisioned for each PersistentVolumeClaim\n",
      "Prevent the dynamic provisioner from interfering when you want the Persistent-\n",
      "VolumeClaim to be bound to a pre-provisioned PersistentVolume\n",
      "In the next chapter, you’ll see what mechanisms Kubernetes provides to deliver con-\n",
      "figuration data, secret information, and metadata about the pod and container to the\n",
      "processes running inside a pod. This is done with the special types of volumes we’ve\n",
      "mentioned in this chapter, but not yet explored.\n",
      "Pod\n",
      "Admin\n",
      "Volume\n",
      "1. Cluster admin sets up a PersistentVolume\n",
      "provisioner (if one’s not already deployed)\n",
      "2. Admin creates one or\n",
      "more StorageClasses\n",
      "and marks one as the\n",
      "default (it may already\n",
      "exist)\n",
      "Actual\n",
      "storage\n",
      "Persistent\n",
      "Volume\n",
      "User\n",
      "Persistent\n",
      "Volume\n",
      "provisioner\n",
      "Persistent\n",
      "VolumeClaim\n",
      "Storage\n",
      "Class\n",
      "3. User creates a PVC referencing one of the\n",
      "StorageClasses (or none to use the default)\n",
      "6. User creates a pod with\n",
      "a volume referencing the\n",
      "PVC by name\n",
      "4. Kubernetes looks up the\n",
      "StorageClass and the provisioner\n",
      "referenced in it and asks the provisioner\n",
      "to provision a new PV based on the\n",
      "PVC’s requested access mode and\n",
      "storage size and the parameters\n",
      "in the StorageClass\n",
      "5. Provisioner provisions the\n",
      "actual storage, creates\n",
      "a PersistentVolume, and\n",
      "binds it to the PVC\n",
      "Figure 6.10\n",
      "The complete picture of dynamic provisioning of PersistentVolumes\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 223, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "191\n",
      "ConfigMaps and Secrets:\n",
      "configuring applications\n",
      "Up to now you haven’t had to pass any kind of configuration data to the apps you’ve\n",
      "run in the exercises in this book. Because almost all apps require configuration (set-\n",
      "tings that differ between deployed instances, credentials for accessing external sys-\n",
      "tems, and so on), which shouldn’t be baked into the built app itself, let’s see how to\n",
      "pass configuration options to your app when running it in Kubernetes.\n",
      "7.1\n",
      "Configuring containerized applications\n",
      "Before we go over how to pass configuration data to apps running in Kubernetes,\n",
      "let’s look at how containerized applications are usually configured.\n",
      " If you skip the fact that you can bake the configuration into the application\n",
      "itself, when starting development of a new app, you usually start off by having the\n",
      "This chapter covers\n",
      "Changing the main process of a container\n",
      "Passing command-line options to the app\n",
      "Setting environment variables exposed to the app\n",
      "Configuring apps through ConfigMaps\n",
      "Passing sensitive information through Secrets\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 224, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "192\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "app configured through command-line arguments. Then, as the list of configuration\n",
      "options grows, you can move the configuration into a config file. \n",
      " Another way of passing configuration options to an application that’s widely popu-\n",
      "lar in containerized applications is through environment variables. Instead of having\n",
      "the app read a config file or command-line arguments, the app looks up the value of a\n",
      "certain environment variable. The official MySQL container image, for example, uses\n",
      "an environment variable called MYSQL_ROOT_PASSWORD for setting the password for the\n",
      "root super-user account. \n",
      " But why are environment variables so popular in containers? Using configuration\n",
      "files inside Docker containers is a bit tricky, because you’d have to bake the config file\n",
      "into the container image itself or mount a volume containing the file into the con-\n",
      "tainer. Obviously, baking files into the image is similar to hardcoding configuration\n",
      "into the source code of the application, because it requires you to rebuild the image\n",
      "every time you want to change the config. Plus, everyone with access to the image can\n",
      "see the config, including any information that should be kept secret, such as creden-\n",
      "tials or encryption keys. Using a volume is better, but still requires you to make sure\n",
      "the file is written to the volume before the container is started. \n",
      " If you’ve read the previous chapter, you might think of using a gitRepo volume as\n",
      "a configuration source. That’s not a bad idea, because it allows you to keep the config\n",
      "nicely versioned and enables you to easily rollback a config change if necessary. But a\n",
      "simpler way allows you to put the configuration data into a top-level Kubernetes\n",
      "resource and store it and all the other resource definitions in the same Git repository\n",
      "or in any other file-based storage. The Kubernetes resource for storing configuration\n",
      "data is called a ConfigMap. We’ll learn how to use it in this chapter.\n",
      " Regardless if you’re using a ConfigMap to store configuration data or not, you can\n",
      "configure your apps by\n",
      "Passing command-line arguments to containers\n",
      "Setting custom environment variables for each container\n",
      "Mounting configuration files into containers through a special type of volume\n",
      "We’ll go over all these options in the next few sections, but before we start, let’s look\n",
      "at config options from a security perspective. Though most configuration options\n",
      "don’t contain any sensitive information, several can. These include credentials, pri-\n",
      "vate encryption keys, and similar data that needs to be kept secure. This type of infor-\n",
      "mation needs to be handled with special care, which is why Kubernetes offers\n",
      "another type of first-class object called a Secret. We’ll learn about it in the last part of\n",
      "this chapter.\n",
      "7.2\n",
      "Passing command-line arguments to containers\n",
      "In all the examples so far, you’ve created containers that ran the default command\n",
      "defined in the container image, but Kubernetes allows overriding the command as\n",
      "part of the pod’s container definition when you want to run a different executable\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 225, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "193\n",
      "Passing command-line arguments to containers\n",
      "instead of the one specified in the image, or want to run it with a different set of com-\n",
      "mand-line arguments. We’ll look at how to do that now.\n",
      "7.2.1\n",
      "Defining the command and arguments in Docker\n",
      "The first thing I need to explain is that the whole command that gets executed in the\n",
      "container is composed of two parts: the command and the arguments. \n",
      "UNDERSTANDING ENTRYPOINT AND CMD\n",
      "In a Dockerfile, two instructions define the two parts:\n",
      "\n",
      "ENTRYPOINT defines the executable invoked when the container is started.\n",
      "\n",
      "CMD specifies the arguments that get passed to the ENTRYPOINT.\n",
      "Although you can use the CMD instruction to specify the command you want to execute\n",
      "when the image is run, the correct way is to do it through the ENTRYPOINT instruction\n",
      "and to only specify the CMD if you want to define the default arguments. The image can\n",
      "then be run without specifying any arguments\n",
      "$ docker run <image>\n",
      "or with additional arguments, which override whatever’s set under CMD in the Dockerfile:\n",
      "$ docker run <image> <arguments>\n",
      "UNDERSTANDING THE DIFFERENCE BETWEEN THE SHELL AND EXEC FORMS\n",
      "But there’s more. Both instructions support two different forms:\n",
      "\n",
      "shell form—For example, ENTRYPOINT node app.js.\n",
      "\n",
      "exec form—For example, ENTRYPOINT [\"node\", \"app.js\"].\n",
      "The difference is whether the specified command is invoked inside a shell or not. \n",
      " In the kubia image you created in chapter 2, you used the exec form of the ENTRY-\n",
      "POINT instruction: \n",
      "ENTRYPOINT [\"node\", \"app.js\"]\n",
      "This runs the node process directly (not inside a shell), as you can see by listing the\n",
      "processes running inside the container:\n",
      "$ docker exec 4675d ps x\n",
      "  PID TTY      STAT   TIME COMMAND\n",
      "    1 ?        Ssl    0:00 node app.js\n",
      "   12 ?        Rs     0:00 ps x\n",
      "If you’d used the shell form (ENTRYPOINT node app.js), these would have been the\n",
      "container’s processes:\n",
      "$ docker exec -it e4bad ps x\n",
      "  PID TTY      STAT   TIME COMMAND\n",
      "    1 ?        Ss     0:00 /bin/sh -c node app.js\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 226, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "194\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "    7 ?        Sl     0:00 node app.js\n",
      "   13 ?        Rs+    0:00 ps x\n",
      "As you can see, in that case, the main process (PID 1) would be the shell process\n",
      "instead of the node process. The node process (PID 7) would be started from that\n",
      "shell. The shell process is unnecessary, which is why you should always use the exec\n",
      "form of the ENTRYPOINT instruction.\n",
      "MAKING THE INTERVAL CONFIGURABLE IN YOUR FORTUNE IMAGE\n",
      "Let’s modify your fortune script and image so the delay interval in the loop is configu-\n",
      "rable. You’ll add an INTERVAL variable and initialize it with the value of the first com-\n",
      "mand-line argument, as shown in the following listing.\n",
      "#!/bin/bash\n",
      "trap \"exit\" SIGINT\n",
      "INTERVAL=$1\n",
      "echo Configured to generate new fortune every $INTERVAL seconds\n",
      "mkdir -p /var/htdocs\n",
      "while :\n",
      "do\n",
      "  echo $(date) Writing fortune to /var/htdocs/index.html\n",
      "  /usr/games/fortune > /var/htdocs/index.html\n",
      "  sleep $INTERVAL\n",
      "done\n",
      "You’ve added or modified the lines in bold font. Now, you’ll modify the Dockerfile so\n",
      "it uses the exec version of the ENTRYPOINT instruction and sets the default interval to\n",
      "10 seconds using the CMD instruction, as shown in the following listing.\n",
      "FROM ubuntu:latest\n",
      "RUN apt-get update ; apt-get -y install fortune\n",
      "ADD fortuneloop.sh /bin/fortuneloop.sh\n",
      "ENTRYPOINT [\"/bin/fortuneloop.sh\"]        \n",
      "CMD [\"10\"]                                \n",
      "You can now build and push the image to Docker Hub. This time, you’ll tag the image\n",
      "as args instead of latest:\n",
      "$ docker build -t docker.io/luksa/fortune:args .\n",
      "$ docker push docker.io/luksa/fortune:args\n",
      "You can test the image by running it locally with Docker:\n",
      "$ docker run -it docker.io/luksa/fortune:args\n",
      "Configured to generate new fortune every 10 seconds\n",
      "Fri May 19 10:39:44 UTC 2017 Writing fortune to /var/htdocs/index.html\n",
      "Listing 7.1\n",
      "Fortune script with interval configurable through argument: fortune-args/\n",
      "fortuneloop.sh\n",
      "Listing 7.2\n",
      "Dockerfile for the updated fortune image: fortune-args/Dockerfile\n",
      "The exec form of the \n",
      "ENTRYPOINT instruction\n",
      "The default argument \n",
      "for the executable\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 227, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "195\n",
      "Passing command-line arguments to containers\n",
      "NOTE\n",
      "You can stop the script with Control+C.\n",
      "And you can override the default sleep interval by passing it as an argument:\n",
      "$ docker run -it docker.io/luksa/fortune:args 15\n",
      "Configured to generate new fortune every 15 seconds\n",
      "Now that you’re sure your image honors the argument passed to it, let’s see how to use\n",
      "it in a pod.\n",
      "7.2.2\n",
      "Overriding the command and arguments in Kubernetes\n",
      "In Kubernetes, when specifying a container, you can choose to override both ENTRY-\n",
      "POINT and CMD. To do that, you set the properties command and args in the container\n",
      "specification, as shown in the following listing.\n",
      "kind: Pod\n",
      "spec:\n",
      "  containers:\n",
      "  - image: some/image\n",
      "    command: [\"/bin/command\"]\n",
      "    args: [\"arg1\", \"arg2\", \"arg3\"]\n",
      "In most cases, you’ll only set custom arguments and rarely override the command\n",
      "(except in general-purpose images such as busybox, which doesn’t define an ENTRY-\n",
      "POINT at all). \n",
      "NOTE\n",
      "The command and args fields can’t be updated after the pod is created.\n",
      "The two Dockerfile instructions and the equivalent pod spec fields are shown in table 7.1.\n",
      "RUNNING THE FORTUNE POD WITH A CUSTOM INTERVAL\n",
      "To run the fortune pod with a custom delay interval, you’ll copy your fortune-\n",
      "pod.yaml into fortune-pod-args.yaml and modify it as shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: fortune2s        \n",
      "Listing 7.3\n",
      "A pod definition specifying a custom command and arguments\n",
      "Table 7.1\n",
      "Specifying the executable and its arguments in Docker vs Kubernetes\n",
      "Docker\n",
      "Kubernetes\n",
      "Description\n",
      "ENTRYPOINT\n",
      "command\n",
      "The executable that’s executed inside the container\n",
      "CMD\n",
      "args\n",
      "The arguments passed to the executable\n",
      "Listing 7.4\n",
      "Passing an argument in the pod definition: fortune-pod-args.yaml\n",
      "You changed the \n",
      "pod’s name.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 228, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "196\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "spec:\n",
      "  containers:\n",
      "  - image: luksa/fortune:args      \n",
      "    args: [\"2\"]                  \n",
      "    name: html-generator\n",
      "    volumeMounts:\n",
      "    - name: html\n",
      "      mountPath: /var/htdocs\n",
      "...\n",
      "You added the args array to the container definition. Try creating this pod now. The\n",
      "values of the array will be passed to the container as command-line arguments when it\n",
      "is run. \n",
      " The array notation used in this listing is great if you have one argument or a few. If\n",
      "you have several, you can also use the following notation:\n",
      "    args:\n",
      "    - foo\n",
      "    - bar\n",
      "    - \"15\"\n",
      "TIP\n",
      "You don’t need to enclose string values in quotations marks (but you\n",
      "must enclose numbers). \n",
      "Specifying arguments is one way of passing config\n",
      "options to your containers through command-\n",
      "line arguments. Next, you’ll see how to do it\n",
      "through environment variables.\n",
      "7.3\n",
      "Setting environment variables for \n",
      "a container\n",
      "As I’ve already mentioned, containerized appli-\n",
      "cations often use environment variables as a\n",
      "source of configuration options. Kubernetes\n",
      "allows you to specify a custom list of environ-\n",
      "ment variables for each container of a pod, as\n",
      "shown in figure 7.1. Although it would be use-\n",
      "ful to also define environment variables at the\n",
      "pod level and have them be inherited by its\n",
      "containers, no such option currently exists.\n",
      "NOTE\n",
      "Like the container’s command and\n",
      "arguments, the list of environment variables\n",
      "also cannot be updated after the pod is created.\n",
      "Using fortune:args \n",
      "instead of fortune:latest\n",
      "This argument makes the \n",
      "script generate a new fortune \n",
      "every two seconds.\n",
      "Pod\n",
      "Container A\n",
      "Environment variables\n",
      "FOO=BAR\n",
      "ABC=123\n",
      "Container B\n",
      "Environment variables\n",
      "FOO=FOOBAR\n",
      "BAR=567\n",
      "Figure 7.1\n",
      "Environment variables can \n",
      "be set per container.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 229, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "197\n",
      "Setting environment variables for a container\n",
      "MAKING THE INTERVAL IN YOUR FORTUNE IMAGE CONFIGURABLE THROUGH AN ENVIRONMENT VARIABLE\n",
      "Let’s see how to modify your fortuneloop.sh script once again to allow it to be config-\n",
      "ured from an environment variable, as shown in the following listing.\n",
      "#!/bin/bash\n",
      "trap \"exit\" SIGINT\n",
      "echo Configured to generate new fortune every $INTERVAL seconds\n",
      "mkdir -p /var/htdocs\n",
      "while :\n",
      "do\n",
      "  echo $(date) Writing fortune to /var/htdocs/index.html\n",
      "  /usr/games/fortune > /var/htdocs/index.html\n",
      "  sleep $INTERVAL\n",
      "done\n",
      "All you had to do was remove the row where the INTERVAL variable is initialized. Because\n",
      "your “app” is a simple bash script, you didn’t need to do anything else. If the app was\n",
      "written in Java you’d use System.getenv(\"INTERVAL\"), whereas in Node.JS you’d use\n",
      "process.env.INTERVAL, and in Python you’d use os.environ['INTERVAL'].\n",
      "7.3.1\n",
      "Specifying environment variables in a container definition\n",
      "After building the new image (I’ve tagged it as luksa/fortune:env this time) and\n",
      "pushing it to Docker Hub, you can run it by creating a new pod, in which you pass the\n",
      "environment variable to the script by including it in your container definition, as\n",
      "shown in the following listing.\n",
      "kind: Pod\n",
      "spec:\n",
      " containers:\n",
      " - image: luksa/fortune:env\n",
      "   env:                        \n",
      "   - name: INTERVAL            \n",
      "     value: \"30\"               \n",
      "   name: html-generator\n",
      "...\n",
      "As mentioned previously, you set the environment variable inside the container defini-\n",
      "tion, not at the pod level. \n",
      "NOTE\n",
      "Don’t forget that in each container, Kubernetes also automatically\n",
      "exposes environment variables for each service in the same namespace. These\n",
      "environment variables are basically auto-injected configuration.\n",
      "Listing 7.5\n",
      "Fortune script with interval configurable through env var: fortune-env/\n",
      "fortuneloop.sh\n",
      "Listing 7.6\n",
      "Defining an environment variable in a pod: fortune-pod-env.yaml\n",
      "Adding a single variable to \n",
      "the environment variable list\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 230, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "198\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "7.3.2\n",
      "Referring to other environment variables in a variable’s value\n",
      "In the previous example, you set a fixed value for the environment variable, but you\n",
      "can also reference previously defined environment variables or any other existing vari-\n",
      "ables by using the $(VAR) syntax. If you define two environment variables, the second\n",
      "one can include the value of the first one as shown in the following listing.\n",
      "env:\n",
      "- name: FIRST_VAR\n",
      "  value: \"foo\"\n",
      "- name: SECOND_VAR\n",
      "  value: \"$(FIRST_VAR)bar\"\n",
      "In this case, the SECOND_VAR’s value will be \"foobar\". Similarly, both the command and\n",
      "args attributes you learned about in section 7.2 can also refer to environment vari-\n",
      "ables like this. You’ll use this method in section 7.4.5.\n",
      "7.3.3\n",
      "Understanding the drawback of hardcoding environment \n",
      "variables\n",
      "Having values effectively hardcoded in the pod definition means you need to have\n",
      "separate pod definitions for your production and your development pods. To reuse\n",
      "the same pod definition in multiple environments, it makes sense to decouple the\n",
      "configuration from the pod descriptor. Luckily, you can do that using a ConfigMap\n",
      "resource and using it as a source for environment variable values using the valueFrom\n",
      "instead of the value field. You’ll learn about this next. \n",
      "7.4\n",
      "Decoupling configuration with a ConfigMap\n",
      "The whole point of an app’s configuration is to keep the config options that vary\n",
      "between environments, or change frequently, separate from the application’s source\n",
      "code. If you think of a pod descriptor as source code for your app (and in microservices\n",
      "architectures that’s what it really is, because it defines how to compose the individual\n",
      "components into a functioning system), it’s clear you should move the configuration\n",
      "out of the pod description.\n",
      "7.4.1\n",
      "Introducing ConfigMaps\n",
      "Kubernetes allows separating configuration options into a separate object called a\n",
      "ConfigMap, which is a map containing key/value pairs with the values ranging from\n",
      "short literals to full config files. \n",
      " An application doesn’t need to read the ConfigMap directly or even know that it\n",
      "exists. The contents of the map are instead passed to containers as either environ-\n",
      "ment variables or as files in a volume (see figure 7.2). And because environment\n",
      "Listing 7.7\n",
      "Referring to an environment variable inside another one\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 231, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "199\n",
      "Decoupling configuration with a ConfigMap\n",
      "variables can be referenced in command-line arguments using the $(ENV_VAR) syn-\n",
      "tax, you can also pass ConfigMap entries to processes as command-line arguments.\n",
      "Sure, the application can also read the contents of a ConfigMap directly through the\n",
      "Kubernetes REST API endpoint if needed, but unless you have a real need for this,\n",
      "you should keep your app Kubernetes-agnostic as much as possible.\n",
      " Regardless of how an app consumes a ConfigMap, having the config in a separate\n",
      "standalone object like this allows you to keep multiple manifests for ConfigMaps with\n",
      "the same name, each for a different environment (development, testing, QA, produc-\n",
      "tion, and so on). Because pods reference the ConfigMap by name, you can use a dif-\n",
      "ferent config in each environment while using the same pod specification across all of\n",
      "them (see figure 7.3).\n",
      "Pod\n",
      "Environment variables\n",
      "ConﬁgMap\n",
      "key1=value1\n",
      "key2=value2\n",
      "...\n",
      "conﬁgMap\n",
      "volume\n",
      "Figure 7.2\n",
      "Pods use ConfigMaps \n",
      "through environment variables and \n",
      "configMap volumes.\n",
      "ConﬁgMap:\n",
      "app-conﬁg\n",
      "Namespace: development\n",
      "(contains\n",
      "development\n",
      "values)\n",
      "Pod(s)\n",
      "ConﬁgMaps created\n",
      "from different manifests\n",
      "Pods created from the\n",
      "same pod manifests\n",
      "Namespace: production\n",
      "ConﬁgMap:\n",
      "app-conﬁg\n",
      "(contains\n",
      "production\n",
      "values)\n",
      "Pod(s)\n",
      "Figure 7.3\n",
      "Two different ConfigMaps with the same name used in different \n",
      "environments\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 232, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "200\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "7.4.2\n",
      "Creating a ConfigMap\n",
      "Let’s see how to use a ConfigMap in one of your pods. To start with the simplest exam-\n",
      "ple, you’ll first create a map with a single key and use it to fill the INTERVAL environment\n",
      "variable from your previous example. You’ll create the ConfigMap with the special\n",
      "kubectl create configmap command instead of posting a YAML with the generic\n",
      "kubectl create -f command. \n",
      "USING THE KUBECTL CREATE CONFIGMAP COMMAND\n",
      "You can define the map’s entries by passing literals to the kubectl command or you\n",
      "can create the ConfigMap from files stored on your disk. Use a simple literal first:\n",
      "$ kubectl create configmap fortune-config --from-literal=sleep-interval=25\n",
      "configmap \"fortune-config\" created\n",
      "NOTE\n",
      "ConfigMap keys must be a valid DNS subdomain (they may only con-\n",
      "tain alphanumeric characters, dashes, underscores, and dots). They may\n",
      "optionally include a leading dot.\n",
      "This creates a ConfigMap called fortune-config with the single-entry sleep-interval\n",
      "=25 (figure 7.4).\n",
      "ConfigMaps usually contain more than one entry. To create a ConfigMap with multi-\n",
      "ple literal entries, you add multiple --from-literal arguments:\n",
      "$ kubectl create configmap myconfigmap\n",
      "➥  --from-literal=foo=bar --from-literal=bar=baz --from-literal=one=two\n",
      "Let’s inspect the YAML descriptor of the ConfigMap you created by using the kubectl\n",
      "get command, as shown in the following listing.\n",
      "$ kubectl get configmap fortune-config -o yaml\n",
      "apiVersion: v1\n",
      "data:\n",
      "  sleep-interval: \"25\"                      \n",
      "kind: ConfigMap                              \n",
      "metadata:\n",
      "  creationTimestamp: 2016-08-11T20:31:08Z\n",
      "  name: fortune-config                      \n",
      "  namespace: default\n",
      "  resourceVersion: \"910025\"\n",
      "  selfLink: /api/v1/namespaces/default/configmaps/fortune-config\n",
      "  uid: 88c4167e-6002-11e6-a50d-42010af00237\n",
      "Listing 7.8\n",
      "A ConfigMap definition\n",
      "sleep-interval\n",
      "25\n",
      "ConﬁgMap: fortune-conﬁg\n",
      "Figure 7.4\n",
      "The fortune-config \n",
      "ConfigMap containing a single entry\n",
      "The single entry \n",
      "in this map\n",
      "This descriptor \n",
      "describes a ConfigMap.\n",
      "The name of this map \n",
      "(you’re referencing it \n",
      "by this name)\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 233, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "201\n",
      "Decoupling configuration with a ConfigMap\n",
      "Nothing extraordinary. You could easily have written this YAML yourself (you wouldn’t\n",
      "need to specify anything but the name in the metadata section, of course) and posted\n",
      "it to the Kubernetes API with the well-known\n",
      "$ kubectl create -f fortune-config.yaml\n",
      "CREATING A CONFIGMAP ENTRY FROM THE CONTENTS OF A FILE\n",
      "ConfigMaps can also store coarse-grained config data, such as complete config files.\n",
      "To do this, the kubectl create configmap command also supports reading files from\n",
      "disk and storing them as individual entries in the ConfigMap:\n",
      "$ kubectl create configmap my-config --from-file=config-file.conf\n",
      "When you run the previous command, kubectl looks for the file config-file.conf in\n",
      "the directory you run kubectl in. It will then store the contents of the file under the\n",
      "key config-file.conf in the ConfigMap (the filename is used as the map key), but\n",
      "you can also specify a key manually like this:\n",
      "$ kubectl create configmap my-config --from-file=customkey=config-file.conf\n",
      "This command will store the file’s contents under the key customkey. As with literals,\n",
      "you can add multiple files by using the --from-file argument multiple times. \n",
      "CREATING A CONFIGMAP FROM FILES IN A DIRECTORY\n",
      "Instead of importing each file individually, you can even import all files from a file\n",
      "directory:\n",
      "$ kubectl create configmap my-config --from-file=/path/to/dir\n",
      "In this case, kubectl will create an individual map entry for each file in the specified\n",
      "directory, but only for files whose name is a valid ConfigMap key. \n",
      "COMBINING DIFFERENT OPTIONS\n",
      "When creating ConfigMaps, you can use a combination of all the options mentioned\n",
      "here (note that these files aren’t included in the book’s code archive—you can create\n",
      "them yourself if you’d like to try out the command):\n",
      "$ kubectl create configmap my-config  \n",
      "➥  --from-file=foo.json                  \n",
      "➥  --from-file=bar=foobar.conf              \n",
      "➥  --from-file=config-opts/               \n",
      "➥  --from-literal=some=thing    \n",
      "Here, you’ve created the ConfigMap from multiple sources: a whole directory, a file,\n",
      "another file (but stored under a custom key instead of using the filename as the key),\n",
      "and a literal value. Figure 7.5 shows all these sources and the resulting ConfigMap.\n",
      "A single file\n",
      "A file stored under \n",
      "a custom key\n",
      "A whole directory\n",
      "A literal value\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 234, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "202\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "7.4.3\n",
      "Passing a ConfigMap entry to a container as an environment \n",
      "variable\n",
      "How do you now get the values from this map into a pod’s container? You have three\n",
      "options. Let’s start with the simplest—setting an environment variable. You’ll use the\n",
      "valueFrom field I mentioned in section 7.3.3. The pod descriptor should look like\n",
      "the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "Listing 7.9\n",
      "Pod with env var from a config map: fortune-pod-env-configmap.yaml\n",
      "ConﬁgMap: my-conﬁg\n",
      "Key\n",
      "foo.json\n",
      "foo.json\n",
      "Value\n",
      "bar\n",
      "abc\n",
      "debug\n",
      "true\n",
      "repeat\n",
      "100\n",
      "some\n",
      "thing\n",
      "{\n",
      "foo: bar\n",
      "baz: 5\n",
      "}\n",
      "conﬁg-opts directory\n",
      "Literal\n",
      "some=thing\n",
      "{\n",
      "foo: bar\n",
      "baz: 5\n",
      "}\n",
      "--from-ﬁle=foo.json\n",
      "--from-ﬁle=conﬁg-opts/\n",
      "--from-literal=some=thing\n",
      "foobar.conf\n",
      "abc\n",
      "debug\n",
      "true\n",
      "repeat\n",
      "100\n",
      "--from-ﬁle=bar=foobar.conf\n",
      "Figure 7.5\n",
      "Creating a ConfigMap from individual files, a directory, and a literal value\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 235, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "203\n",
      "Decoupling configuration with a ConfigMap\n",
      "metadata:\n",
      "  name: fortune-env-from-configmap\n",
      "spec:\n",
      "  containers:\n",
      "  - image: luksa/fortune:env\n",
      "    env:                             \n",
      "    - name: INTERVAL                 \n",
      "      valueFrom:                       \n",
      "        configMapKeyRef:               \n",
      "          name: fortune-config      \n",
      "          key: sleep-interval    \n",
      "...\n",
      "You defined an environment variable called INTERVAL and set its value to whatever is\n",
      "stored in the fortune-config ConfigMap under the key sleep-interval. When the\n",
      "process running in the html-generator container reads the INTERVAL environment\n",
      "variable, it will see the value 25 (shown in figure 7.6).\n",
      "REFERENCING NON-EXISTING CONFIGMAPS IN A POD\n",
      "You might wonder what happens if the referenced ConfigMap doesn’t exist when you\n",
      "create the pod. Kubernetes schedules the pod normally and tries to run its containers.\n",
      "The container referencing the non-existing ConfigMap will fail to start, but the other\n",
      "container will start normally. If you then create the missing ConfigMap, the failed con-\n",
      "tainer is started without requiring you to recreate the pod.\n",
      "NOTE\n",
      "You can also mark a reference to a ConfigMap as optional (by setting\n",
      "configMapKeyRef.optional: true). In that case, the container starts even if\n",
      "the ConfigMap doesn’t exist.\n",
      "This example shows you how to decouple the configuration from the pod specifica-\n",
      "tion. This allows you to keep all the configuration options closely together (even for\n",
      "multiple pods) instead of having them splattered around the pod definition (or dupli-\n",
      "cated across multiple pod manifests). \n",
      "You’re setting the environment \n",
      "variable called INTERVAL.\n",
      "Instead of setting a fixed value, you're \n",
      "initializing it from a ConfigMap key.\n",
      "The name of the ConfigMap \n",
      "you're referencing\n",
      "You're setting the variable to whatever is\n",
      "stored under this key in the ConfigMap.\n",
      "ConﬁgMap: fortune-conﬁg\n",
      "sleep-interval\n",
      "25\n",
      "Pod\n",
      "Container: web-server\n",
      "Container: html-generator\n",
      "Environment variables\n",
      "INTERVAL=25\n",
      "fortuneloop.sh\n",
      "process\n",
      "Figure 7.6\n",
      "Passing a ConfigMap entry as \n",
      "an environment variable to a container\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 236, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "204\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "7.4.4\n",
      "Passing all entries of a ConfigMap as environment variables \n",
      "at once\n",
      "When your ConfigMap contains more than just a few entries, it becomes tedious and\n",
      "error-prone to create environment variables from each entry individually. Luckily,\n",
      "Kubernetes version 1.6 provides a way to expose all entries of a ConfigMap as environ-\n",
      "ment variables. \n",
      " Imagine having a ConfigMap with three keys called FOO, BAR, and FOO-BAR. You can\n",
      "expose them all as environment variables by using the envFrom attribute, instead of\n",
      "env the way you did in previous examples. The following listing shows an example.\n",
      "spec:\n",
      "  containers:\n",
      "  - image: some-image\n",
      "    envFrom:                \n",
      "    - prefix: CONFIG_             \n",
      "      configMapRef:              \n",
      "        name: my-config-map      \n",
      "...\n",
      "As you can see, you can also specify a prefix for the environment variables (CONFIG_ in\n",
      "this case). This results in the following two environment variables being present inside\n",
      "the container: CONFIG_FOO and CONFIG_BAR. \n",
      "NOTE\n",
      "The prefix is optional, so if you omit it the environment variables will\n",
      "have the same name as the keys. \n",
      "Did you notice I said two variables, but earlier, I said the ConfigMap has three entries\n",
      "(FOO, BAR, and FOO-BAR)? Why is there no environment variable for the FOO-BAR\n",
      "ConfigMap entry?\n",
      " The reason is that CONFIG_FOO-BAR isn’t a valid environment variable name\n",
      "because it contains a dash. Kubernetes doesn’t convert the keys in any way (it doesn’t\n",
      "convert dashes to underscores, for example). If a ConfigMap key isn’t in the proper\n",
      "format, it skips the entry (but it does record an event informing you it skipped it).\n",
      "7.4.5\n",
      "Passing a ConfigMap entry as a command-line argument\n",
      "Now, let’s also look at how to pass values from a ConfigMap as arguments to the main\n",
      "process running in the container. You can’t reference ConfigMap entries directly in\n",
      "the pod.spec.containers.args field, but you can first initialize an environment vari-\n",
      "able from the ConfigMap entry and then refer to the variable inside the arguments as\n",
      "shown in figure 7.7.\n",
      " Listing 7.11 shows an example of how to do this in the YAML.\n",
      " \n",
      "Listing 7.10\n",
      "Pod with env vars from all entries of a ConfigMap\n",
      "Using envFrom instead of env\n",
      "All environment variables will \n",
      "be prefixed with CONFIG_.\n",
      "Referencing the ConfigMap \n",
      "called my-config-map\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 237, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "205\n",
      "Decoupling configuration with a ConfigMap\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: fortune-args-from-configmap\n",
      "spec:\n",
      "  containers:\n",
      "  - image: luksa/fortune:args         \n",
      "    env:                               \n",
      "    - name: INTERVAL                   \n",
      "      valueFrom:                       \n",
      "        configMapKeyRef:               \n",
      "          name: fortune-config         \n",
      "          key: sleep-interval          \n",
      "    args: [\"$(INTERVAL)\"]      \n",
      "...\n",
      "You defined the environment variable exactly as you did before, but then you used the\n",
      "$(ENV_VARIABLE_NAME) syntax to have Kubernetes inject the value of the variable into\n",
      "the argument. \n",
      "7.4.6\n",
      "Using a configMap volume to expose ConfigMap entries as files\n",
      "Passing configuration options as environment variables or command-line arguments\n",
      "is usually used for short variable values. A ConfigMap, as you’ve seen, can also con-\n",
      "tain whole config files. When you want to expose those to the container, you can use\n",
      "one of the special volume types I mentioned in the previous chapter, namely a\n",
      "configMap volume.\n",
      " A configMap volume will expose each entry of the ConfigMap as a file. The pro-\n",
      "cess running in the container can obtain the entry’s value by reading the contents of\n",
      "the file.\n",
      "Listing 7.11\n",
      "Using ConfigMap entries as arguments: fortune-pod-args-configmap.yaml\n",
      "ConﬁgMap: fortune-conﬁg\n",
      "sleep-interval\n",
      "25\n",
      "Pod\n",
      "Container: web-server\n",
      "Container: html-generator\n",
      "Environment variables\n",
      "INTERVAL=25\n",
      "fortuneloop.sh $(INTERVAL)\n",
      "Figure 7.7\n",
      "Passing a ConfigMap entry as a command-line argument\n",
      "Using the image that takes the \n",
      "interval from the first argument, \n",
      "not from an environment variable\n",
      "Defining the \n",
      "environment variable \n",
      "exactly as before\n",
      "Referencing the environment \n",
      "variable in the argument\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 238, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "206\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      " Although this method is mostly meant for passing large config files to the con-\n",
      "tainer, nothing prevents you from passing short single values this way. \n",
      "CREATING THE CONFIGMAP\n",
      "Instead of modifying your fortuneloop.sh script once again, you’ll now try a different\n",
      "example. You’ll use a config file to configure the Nginx web server running inside the\n",
      "fortune pod’s web-server container. Let’s say you want your Nginx server to compress\n",
      "responses it sends to the client. To enable compression, the config file for Nginx\n",
      "needs to look like the following listing.\n",
      "server {\n",
      "  listen              80;\n",
      "  server_name         www.kubia-example.com;\n",
      "  gzip on;                                       \n",
      "  gzip_types text/plain application/xml;         \n",
      "  location / {\n",
      "    root   /usr/share/nginx/html;\n",
      "    index  index.html index.htm;\n",
      "  }\n",
      "}\n",
      "Now delete your existing fortune-config ConfigMap with kubectl delete config-\n",
      "map fortune-config, so that you can replace it with a new one, which will include the\n",
      "Nginx config file. You’ll create the ConfigMap from files stored on your local disk. \n",
      " Create a new directory called configmap-files and store the Nginx config from the\n",
      "previous listing into configmap-files/my-nginx-config.conf. To make the ConfigMap\n",
      "also contain the sleep-interval entry, add a plain text file called sleep-interval to the\n",
      "same directory and store the number 25 in it (see figure 7.8).\n",
      "Now create a ConfigMap from all the files in the directory like this:\n",
      "$ kubectl create configmap fortune-config --from-file=configmap-files\n",
      "configmap \"fortune-config\" created\n",
      "Listing 7.12\n",
      "An Nginx config with enabled gzip compression: my-nginx-config.conf\n",
      "This enables gzip compression \n",
      "for plain text and XML files.\n",
      "conﬁgmap-ﬁles/\n",
      "my-nginx-conﬁg.conf\n",
      "server {\n",
      "listen 80;\n",
      "server_name www.kubia...\n",
      "...\n",
      "}\n",
      "sleep-interval\n",
      "25\n",
      "Figure 7.8\n",
      "The contents of the \n",
      "configmap-files directory and its files\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 239, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "207\n",
      "Decoupling configuration with a ConfigMap\n",
      "The following listing shows what the YAML of this ConfigMap looks like.\n",
      "$ kubectl get configmap fortune-config -o yaml\n",
      "apiVersion: v1\n",
      "data:\n",
      "  my-nginx-config.conf: |                            \n",
      "    server {                                         \n",
      "      listen              80;                        \n",
      "      server_name         www.kubia-example.com;     \n",
      "      gzip on;                                       \n",
      "      gzip_types text/plain application/xml;         \n",
      "      location / {                                   \n",
      "        root   /usr/share/nginx/html;                \n",
      "        index  index.html index.htm;                 \n",
      "      }                                              \n",
      "    }                                                \n",
      "  sleep-interval: |         \n",
      "    25                      \n",
      "kind: ConfigMap\n",
      "...\n",
      "NOTE\n",
      "The pipeline character after the colon in the first line of both entries\n",
      "signals that a literal multi-line value follows.\n",
      "The ConfigMap contains two entries, with keys corresponding to the actual names\n",
      "of the files they were created from. You’ll now use the ConfigMap in both of your\n",
      "pod’s containers.\n",
      "USING THE CONFIGMAP'S ENTRIES IN A VOLUME\n",
      "Creating a volume populated with the contents of a ConfigMap is as easy as creating\n",
      "a volume that references the ConfigMap by name and mounting the volume in a\n",
      "container. You already learned how to create volumes and mount them, so the only\n",
      "thing left to learn is how to initialize the volume with files created from a Config-\n",
      "Map’s entries.\n",
      " Nginx reads its config file from /etc/nginx/nginx.conf. The Nginx image\n",
      "already contains this file with default configuration options, which you don’t want\n",
      "to override, so you don’t want to replace this file as a whole. Luckily, the default\n",
      "config file automatically includes all .conf files in the /etc/nginx/conf.d/ subdirec-\n",
      "tory as well, so you should add your config file in there. Figure 7.9 shows what you\n",
      "want to achieve.\n",
      " The pod descriptor is shown in listing 7.14 (the irrelevant parts are omitted, but\n",
      "you’ll find the complete file in the code archive).\n",
      " \n",
      " \n",
      "Listing 7.13\n",
      "YAML definition of a config map created from a file\n",
      "The entry holding the \n",
      "Nginx config file’s \n",
      "contents\n",
      "The sleep-interval entry\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 240, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "208\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: fortune-configmap-volume\n",
      "spec:\n",
      "  containers:\n",
      "  - image: nginx:alpine\n",
      "    name: web-server\n",
      "    volumeMounts:\n",
      "    ...\n",
      "    - name: config\n",
      "      mountPath: /etc/nginx/conf.d      \n",
      "      readOnly: true\n",
      "    ...\n",
      "  volumes:\n",
      "  ...\n",
      "  - name: config              \n",
      "    configMap:                 \n",
      "      name: fortune-config     \n",
      "  ...\n",
      "This pod definition includes a volume, which references your fortune-config\n",
      "ConfigMap. You mount the volume into the /etc/nginx/conf.d directory to make\n",
      "Nginx use it. \n",
      "VERIFYING NGINX IS USING THE MOUNTED CONFIG FILE\n",
      "The web server should now be configured to compress the responses it sends. You can\n",
      "verify this by enabling port-forwarding from localhost:8080 to the pod’s port 80 and\n",
      "checking the server’s response with curl, as shown in the following listing.\n",
      " \n",
      "Listing 7.14\n",
      "A pod with ConfigMap entries mounted as files: fortune-pod-configmap-\n",
      "volume.yaml\n",
      "Pod\n",
      "Container: html-generator\n",
      "Container: web-server\n",
      "Filesystem\n",
      "/\n",
      "etc/\n",
      "nginx/\n",
      "conf.d/\n",
      "ConﬁgMap: fortune-conﬁg\n",
      "my-nginx-conﬁg.conf\n",
      "server {\n",
      "…\n",
      "}\n",
      "Volume:\n",
      "conﬁg\n",
      "Figure 7.9\n",
      "Passing ConfigMap entries to a pod as files in a volume\n",
      "You’re mounting the \n",
      "configMap volume at \n",
      "this location.\n",
      "The volume refers to your \n",
      "fortune-config ConfigMap.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 241, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "209\n",
      "Decoupling configuration with a ConfigMap\n",
      "$ kubectl port-forward fortune-configmap-volume 8080:80 &\n",
      "Forwarding from 127.0.0.1:8080 -> 80\n",
      "Forwarding from [::1]:8080 -> 80\n",
      "$ curl -H \"Accept-Encoding: gzip\" -I localhost:8080\n",
      "HTTP/1.1 200 OK\n",
      "Server: nginx/1.11.1\n",
      "Date: Thu, 18 Aug 2016 11:52:57 GMT\n",
      "Content-Type: text/html\n",
      "Last-Modified: Thu, 18 Aug 2016 11:52:55 GMT\n",
      "Connection: keep-alive\n",
      "ETag: W/\"57b5a197-37\"\n",
      "Content-Encoding: gzip           \n",
      "EXAMINING THE MOUNTED CONFIGMAP VOLUME’S CONTENTS\n",
      "The response shows you achieved what you wanted, but let’s look at what’s in the\n",
      "/etc/nginx/conf.d directory now:\n",
      "$ kubectl exec fortune-configmap-volume -c web-server ls /etc/nginx/conf.d\n",
      "my-nginx-config.conf\n",
      "sleep-interval\n",
      "Both entries from the ConfigMap have been added as files to the directory. The\n",
      "sleep-interval entry is also included, although it has no business being there,\n",
      "because it’s only meant to be used by the fortuneloop container. You could create\n",
      "two different ConfigMaps and use one to configure the fortuneloop container and\n",
      "the other one to configure the web-server container. But somehow it feels wrong to\n",
      "use multiple ConfigMaps to configure containers of the same pod. After all, having\n",
      "containers in the same pod implies that the containers are closely related and should\n",
      "probably also be configured as a unit. \n",
      "EXPOSING CERTAIN CONFIGMAP ENTRIES IN THE VOLUME\n",
      "Luckily, you can populate a configMap volume with only part of the ConfigMap’s\n",
      "entries—in your case, only the my-nginx-config.conf entry. This won’t affect the\n",
      "fortuneloop container, because you’re passing the sleep-interval entry to it through\n",
      "an environment variable and not through the volume. \n",
      " To define which entries should be exposed as files in a configMap volume, use the\n",
      "volume’s items attribute as shown in the following listing.\n",
      "  volumes:\n",
      "  - name: config              \n",
      "    configMap:                                  \n",
      "      name: fortune-config                      \n",
      "      items:                       \n",
      "      - key: my-nginx-config.conf        \n",
      "        path: gzip.conf                  \n",
      "Listing 7.15\n",
      "Seeing if nginx responses have compression enabled\n",
      "Listing 7.16\n",
      "A pod with a specific ConfigMap entry mounted into a file directory: \n",
      "fortune-pod-configmap-volume-with-items.yaml\n",
      "This shows the response \n",
      "is compressed.\n",
      "Selecting which entries to include \n",
      "in the volume by listing them\n",
      "You want the entry \n",
      "under this key included.\n",
      "The entry’s value should \n",
      "be stored in this file.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 242, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "210\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "When specifying individual entries, you need to set the filename for each individual\n",
      "entry, along with the entry’s key. If you run the pod from the previous listing, the\n",
      "/etc/nginx/conf.d directory is kept nice and clean, because it only contains the\n",
      "gzip.conf file and nothing else. \n",
      "UNDERSTANDING THAT MOUNTING A DIRECTORY HIDES EXISTING FILES IN THAT DIRECTORY\n",
      "There’s one important thing to discuss at this point. In both this and in your previous\n",
      "example, you mounted the volume as a directory, which means you’ve hidden any files\n",
      "that are stored in the /etc/nginx/conf.d directory in the container image itself. \n",
      " This is generally what happens in Linux when you mount a filesystem into a non-\n",
      "empty directory. The directory then only contains the files from the mounted filesys-\n",
      "tem, whereas the original files in that directory are inaccessible for as long as the\n",
      "filesystem is mounted. \n",
      " In your case, this has no terrible side effects, but imagine mounting a volume to\n",
      "the /etc directory, which usually contains many important files. This would most likely\n",
      "break the whole container, because all of the original files that should be in the /etc\n",
      "directory would no longer be there. If you need to add a file to a directory like /etc,\n",
      "you can’t use this method at all.\n",
      "MOUNTING INDIVIDUAL CONFIGMAP ENTRIES AS FILES WITHOUT HIDING OTHER FILES IN THE DIRECTORY\n",
      "Naturally, you’re now wondering how to add individual files from a ConfigMap into\n",
      "an existing directory without hiding existing files stored in it. An additional subPath\n",
      "property on the volumeMount allows you to mount either a single file or a single direc-\n",
      "tory from the volume instead of mounting the whole volume. Perhaps this is easier to\n",
      "explain visually (see figure 7.10).\n",
      " Say you have a configMap volume containing a myconfig.conf file, which you want\n",
      "to add to the /etc directory as someconfig.conf. You can use the subPath property to\n",
      "mount it there without affecting any other files in that directory. The relevant part of\n",
      "the pod definition is shown in the following listing.\n",
      "Pod\n",
      "Container\n",
      "Filesystem\n",
      "/\n",
      "etc/\n",
      "someconﬁg.conf\n",
      "existingﬁle1\n",
      "existingﬁle2\n",
      "ConﬁgMap: app-conﬁg\n",
      "myconﬁg.conf\n",
      "Contents\n",
      "of the ﬁle\n",
      "another-ﬁle\n",
      "Contents\n",
      "of the ﬁle\n",
      "conﬁgMap\n",
      "volume\n",
      "myconﬁg.conf\n",
      "another-ﬁle\n",
      "existingﬁle1\n",
      "and existingﬁle2\n",
      "aren’t hidden.\n",
      "Only myconﬁg.conf is mounted\n",
      "into the container (yet under a\n",
      "different ﬁlename).\n",
      "another-ﬁle isn’t\n",
      "mounted into the\n",
      "container.\n",
      "Figure 7.10\n",
      "Mounting a single file from a volume\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 243, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "211\n",
      "Decoupling configuration with a ConfigMap\n",
      "spec:\n",
      "  containers:\n",
      "  - image: some/image\n",
      "    volumeMounts:\n",
      "    - name: myvolume\n",
      "      mountPath: /etc/someconfig.conf     \n",
      "      subPath: myconfig.conf            \n",
      "The subPath property can be used when mounting any kind of volume. Instead of\n",
      "mounting the whole volume, you can mount part of it. But this method of mounting\n",
      "individual files has a relatively big deficiency related to updating files. You’ll learn\n",
      "more about this in the following section, but first, let’s finish talking about the initial\n",
      "state of a configMap volume by saying a few words about file permissions.\n",
      "SETTING THE FILE PERMISSIONS FOR FILES IN A CONFIGMAP VOLUME\n",
      "By default, the permissions on all files in a configMap volume are set to 644 (-rw-r—r--).\n",
      "You can change this by setting the defaultMode property in the volume spec, as shown\n",
      "in the following listing.\n",
      "  volumes:\n",
      "  - name: config\n",
      "    configMap:\n",
      "      name: fortune-config\n",
      "      defaultMode: \"6600\"       \n",
      "Although ConfigMaps should be used for non-sensitive configuration data, you may\n",
      "want to make the file readable and writable only to the user and group the file is\n",
      "owned by, as the example in the previous listing shows. \n",
      "7.4.7\n",
      "Updating an app’s config without having to restart the app\n",
      "We’ve said that one of the drawbacks of using environment variables or command-line\n",
      "arguments as a configuration source is the inability to update them while the pro-\n",
      "cess is running. Using a ConfigMap and exposing it through a volume brings the\n",
      "ability to update the configuration without having to recreate the pod or even restart\n",
      "the container. \n",
      " When you update a ConfigMap, the files in all the volumes referencing it are\n",
      "updated. It’s then up to the process to detect that they’ve been changed and reload\n",
      "them. But Kubernetes will most likely eventually also support sending a signal to the\n",
      "container after updating the files.\n",
      "WARNING\n",
      "Be aware that as I’m writing this, it takes a surprisingly long time\n",
      "for the files to be updated after you update the ConfigMap (it can take up to\n",
      "one whole minute).\n",
      "Listing 7.17\n",
      "A pod with a specific config map entry mounted into a specific file\n",
      "Listing 7.18\n",
      "Setting file permissions: fortune-pod-configmap-volume-defaultMode.yaml \n",
      "You’re mounting into \n",
      "a file, not a directory.\n",
      "Instead of mounting the whole \n",
      "volume, you’re only mounting \n",
      "the myconfig.conf entry.\n",
      "This sets the permissions \n",
      "for all files to -rw-rw------.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 244, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "212\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "EDITING A CONFIGMAP\n",
      "Let’s see how you can change a ConfigMap and have the process running in the pod\n",
      "reload the files exposed in the configMap volume. You’ll modify the Nginx config file\n",
      "from your previous example and make Nginx use the new config without restarting\n",
      "the pod. Try switching gzip compression off by editing the fortune-config Config-\n",
      "Map with kubectl edit:\n",
      "$ kubectl edit configmap fortune-config\n",
      "Once your editor opens, change the gzip on line to gzip off, save the file, and then\n",
      "close the editor. The ConfigMap is then updated, and soon afterward, the actual file\n",
      "in the volume is updated as well. You can confirm this by printing the contents of the\n",
      "file with kubectl exec:\n",
      "$ kubectl exec fortune-configmap-volume -c web-server\n",
      "➥  cat /etc/nginx/conf.d/my-nginx-config.conf\n",
      "If you don’t see the update yet, wait a while and try again. It takes a while for the\n",
      "files to get updated. Eventually, you’ll see the change in the config file, but you’ll\n",
      "find this has no effect on Nginx, because it doesn’t watch the files and reload them\n",
      "automatically. \n",
      "SIGNALING NGINX TO RELOAD THE CONFIG\n",
      "Nginx will continue to compress its responses until you tell it to reload its config files,\n",
      "which you can do with the following command:\n",
      "$ kubectl exec fortune-configmap-volume -c web-server -- nginx -s reload\n",
      "Now, if you try hitting the server again with curl, you should see the response is no\n",
      "longer compressed (it no longer contains the Content-Encoding: gzip header).\n",
      "You’ve effectively changed the app’s config without having to restart the container or\n",
      "recreate the pod. \n",
      "UNDERSTANDING HOW THE FILES ARE UPDATED ATOMICALLY\n",
      "You may wonder what happens if an app can detect config file changes on its own and\n",
      "reloads them before Kubernetes has finished updating all the files in the configMap\n",
      "volume. Luckily, this can’t happen, because all the files are updated atomically, which\n",
      "means all updates occur at once. Kubernetes achieves this by using symbolic links. If\n",
      "you list all the files in the mounted configMap volume, you’ll see something like the\n",
      "following listing.\n",
      "$ kubectl exec -it fortune-configmap-volume -c web-server -- ls -lA \n",
      "➥  /etc/nginx/conf.d\n",
      "total 4\n",
      "drwxr-xr-x  ... 12:15 ..4984_09_04_12_15_06.865837643\n",
      "Listing 7.19\n",
      "Files in a mounted configMap volume\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 245, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "213\n",
      "Using Secrets to pass sensitive data to containers\n",
      "lrwxrwxrwx  ... 12:15 ..data -> ..4984_09_04_12_15_06.865837643\n",
      "lrwxrwxrwx  ... 12:15 my-nginx-config.conf -> ..data/my-nginx-config.conf\n",
      "lrwxrwxrwx  ... 12:15 sleep-interval -> ..data/sleep-interval\n",
      "As you can see, the files in the mounted configMap volume are symbolic links point-\n",
      "ing to files in the ..data dir. The ..data dir is also a symbolic link pointing to a direc-\n",
      "tory called ..4984_09_04_something. When the ConfigMap is updated, Kubernetes\n",
      "creates a new directory like this, writes all the files to it, and then re-links the ..data\n",
      "symbolic link to the new directory, effectively changing all files at once.\n",
      "UNDERSTANDING THAT FILES MOUNTED INTO EXISTING DIRECTORIES DON’T GET UPDATED\n",
      "One big caveat relates to updating ConfigMap-backed volumes. If you’ve mounted a\n",
      "single file in the container instead of the whole volume, the file will not be updated!\n",
      "At least, this is true at the time of writing this chapter. \n",
      " For now, if you need to add an individual file and have it updated when you update\n",
      "its source ConfigMap, one workaround is to mount the whole volume into a different\n",
      "directory and then create a symbolic link pointing to the file in question. The sym-\n",
      "link can either be created in the container image itself, or you could create the\n",
      "symlink when the container starts.\n",
      "UNDERSTANDING THE CONSEQUENCES OF UPDATING A CONFIGMAP\n",
      "One of the most important features of containers is their immutability, which allows\n",
      "us to be certain that no differences exist between multiple running containers created\n",
      "from the same image, so is it wrong to bypass this immutability by modifying a Config-\n",
      "Map used by running containers? \n",
      " The main problem occurs when the app doesn’t support reloading its configura-\n",
      "tion. This results in different running instances being configured differently—those\n",
      "pods that are created after the ConfigMap is changed will use the new config, whereas\n",
      "the old pods will still use the old one. And this isn’t limited to new pods. If a pod’s con-\n",
      "tainer is restarted (for whatever reason), the new process will also see the new config.\n",
      "Therefore, if the app doesn’t reload its config automatically, modifying an existing\n",
      "ConfigMap (while pods are using it) may not be a good idea. \n",
      " If the app does support reloading, modifying the ConfigMap usually isn’t such a\n",
      "big deal, but you do need to be aware that because files in the ConfigMap volumes\n",
      "aren’t updated synchronously across all running instances, the files in individual pods\n",
      "may be out of sync for up to a whole minute.\n",
      "7.5\n",
      "Using Secrets to pass sensitive data to containers\n",
      "All the information you’ve passed to your containers so far is regular, non-sensitive\n",
      "configuration data that doesn’t need to be kept secure. But as we mentioned at the\n",
      "start of the chapter, the config usually also includes sensitive information, such as cre-\n",
      "dentials and private encryption keys, which need to be kept secure.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 246, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "214\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "7.5.1\n",
      "Introducing Secrets\n",
      "To store and distribute such information, Kubernetes provides a separate object called\n",
      "a Secret. Secrets are much like ConfigMaps—they’re also maps that hold key-value\n",
      "pairs. They can be used the same way as a ConfigMap. You can\n",
      "Pass Secret entries to the container as environment variables\n",
      "Expose Secret entries as files in a volume\n",
      "Kubernetes helps keep your Secrets safe by making sure each Secret is only distributed\n",
      "to the nodes that run the pods that need access to the Secret. Also, on the nodes\n",
      "themselves, Secrets are always stored in memory and never written to physical storage,\n",
      "which would require wiping the disks after deleting the Secrets from them. \n",
      " On the master node itself (more specifically in etcd), Secrets used to be stored in\n",
      "unencrypted form, which meant the master node needs to be secured to keep the sensi-\n",
      "tive data stored in Secrets secure. This didn’t only include keeping the etcd storage\n",
      "secure, but also preventing unauthorized users from using the API server, because any-\n",
      "one who can create pods can mount the Secret into the pod and gain access to the sen-\n",
      "sitive data through it. From Kubernetes version 1.7, etcd stores Secrets in encrypted\n",
      "form, making the system much more secure. Because of this, it’s imperative you prop-\n",
      "erly choose when to use a Secret or a ConfigMap. Choosing between them is simple:\n",
      "Use a ConfigMap to store non-sensitive, plain configuration data.\n",
      "Use a Secret to store any data that is sensitive in nature and needs to be kept\n",
      "under key. If a config file includes both sensitive and not-sensitive data, you\n",
      "should store the file in a Secret.\n",
      "You already used Secrets in chapter 5, when you created a Secret to hold the TLS certifi-\n",
      "cate needed for the Ingress resource. Now you’ll explore Secrets in more detail.\n",
      "7.5.2\n",
      "Introducing the default token Secret\n",
      "You’ll start learning about Secrets by examining a Secret that’s mounted into every\n",
      "container you run. You may have noticed it when using kubectl describe on a pod.\n",
      "The command’s output has always contained something like this:\n",
      "Volumes:\n",
      "  default-token-cfee9:\n",
      "    Type:       Secret (a volume populated by a Secret)\n",
      "    SecretName: default-token-cfee9\n",
      "Every pod has a secret volume attached to it automatically. The volume in the previ-\n",
      "ous kubectl describe output refers to a Secret called default-token-cfee9. Because\n",
      "Secrets are resources, you can list them with kubectl get secrets and find the\n",
      "default-token Secret in that list. Let’s see:\n",
      "$ kubectl get secrets\n",
      "NAME                  TYPE                                  DATA      AGE\n",
      "default-token-cfee9   kubernetes.io/service-account-token   3         39d\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 247, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "215\n",
      "Using Secrets to pass sensitive data to containers\n",
      "You can also use kubectl describe to learn a bit more about it, as shown in the follow-\n",
      "ing listing.\n",
      "$ kubectl describe secrets\n",
      "Name:        default-token-cfee9\n",
      "Namespace:   default\n",
      "Labels:      <none>\n",
      "Annotations: kubernetes.io/service-account.name=default\n",
      "             kubernetes.io/service-account.uid=cc04bb39-b53f-42010af00237\n",
      "Type:        kubernetes.io/service-account-token\n",
      "Data\n",
      "====\n",
      "ca.crt:      1139 bytes                                   \n",
      "namespace:   7 bytes                                      \n",
      "token:       eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...      \n",
      "You can see that the Secret contains three entries—ca.crt, namespace, and token—\n",
      "which represent everything you need to securely talk to the Kubernetes API server\n",
      "from within your pods, should you need to do that. Although ideally you want your\n",
      "application to be completely Kubernetes-agnostic, when there’s no alternative other\n",
      "than to talk to Kubernetes directly, you’ll use the files provided through this secret\n",
      "volume. \n",
      " The kubectl describe pod command shows where the secret volume is mounted:\n",
      "Mounts:\n",
      "  /var/run/secrets/kubernetes.io/serviceaccount from default-token-cfee9\n",
      "NOTE\n",
      "By default, the default-token Secret is mounted into every container,\n",
      "but you can disable that in each pod by setting the automountService-\n",
      "AccountToken field in the pod spec to false or by setting it to false on the\n",
      "service account the pod is using. (You’ll learn about service accounts later in\n",
      "the book.)\n",
      "To help you visualize where and how the default token Secret is mounted, see fig-\n",
      "ure 7.11.\n",
      " We’ve said Secrets are like ConfigMaps, so because this Secret contains three\n",
      "entries, you can expect to see three files in the directory the secret volume is mounted\n",
      "into. You can check this easily with kubectl exec:\n",
      "$ kubectl exec mypod ls /var/run/secrets/kubernetes.io/serviceaccount/\n",
      "ca.crt\n",
      "namespace\n",
      "token\n",
      "You’ll see how your app can use these files to access the API server in the next chapter.\n",
      "Listing 7.20\n",
      "Describing a Secret\n",
      "This secret \n",
      "contains three \n",
      "entries.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 248, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "216\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "7.5.3\n",
      "Creating a Secret\n",
      "Now, you’ll create your own little Secret. You’ll improve your fortune-serving Nginx\n",
      "container by configuring it to also serve HTTPS traffic. For this, you need to create a\n",
      "certificate and a private key. The private key needs to be kept secure, so you’ll put it\n",
      "and the certificate into a Secret.\n",
      " First, generate the certificate and private key files (do this on your local machine).\n",
      "You can also use the files in the book’s code archive (the cert and key files are in the\n",
      "fortune-https directory):\n",
      "$ openssl genrsa -out https.key 2048\n",
      "$ openssl req -new -x509 -key https.key -out https.cert -days 3650 -subj \n",
      "/CN=www.kubia-example.com\n",
      "Now, to help better demonstrate a few things about Secrets, create an additional\n",
      "dummy file called foo and make it contain the string bar. You’ll understand why you\n",
      "need to do this in a moment or two:\n",
      "$ echo bar > foo\n",
      "Now you can use kubectl create secret to create a Secret from the three files:\n",
      "$ kubectl create secret generic fortune-https --from-file=https.key\n",
      "➥  --from-file=https.cert --from-file=foo\n",
      "secret \"fortune-https\" created\n",
      "This isn’t very different from creating ConfigMaps. In this case, you’re creating a\n",
      "generic Secret called fortune-https and including two entries in it (https.key with\n",
      "the contents of the https.key file and likewise for the https.cert key/file). As you\n",
      "learned earlier, you could also include the whole directory with --from-file=fortune-\n",
      "https instead of specifying each file individually.\n",
      "Pod\n",
      "Container\n",
      "Filesystem\n",
      "/\n",
      "var/\n",
      "run/\n",
      "secrets/\n",
      "kubernetes.io/\n",
      "serviceaccount/\n",
      "Default token Secret\n",
      "Default token\n",
      "secret\n",
      "volume\n",
      "ca.crt\n",
      "...\n",
      "...\n",
      "...\n",
      "namespace\n",
      "token\n",
      "Figure 7.11\n",
      "The default-token Secret is created automatically and a corresponding \n",
      "volume is mounted in each pod automatically.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 249, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "217\n",
      "Using Secrets to pass sensitive data to containers\n",
      "NOTE\n",
      "You’re creating a generic Secret, but you could also have created a tls\n",
      "Secret with the kubectl create secret tls command, as you did in chapter 5.\n",
      "This would create the Secret with different entry names, though.\n",
      "7.5.4\n",
      "Comparing ConfigMaps and Secrets\n",
      "Secrets and ConfigMaps have a pretty big difference. This is what drove Kubernetes\n",
      "developers to create ConfigMaps after Kubernetes had already supported Secrets for a\n",
      "while. The following listing shows the YAML of the Secret you created.\n",
      "$ kubectl get secret fortune-https -o yaml\n",
      "apiVersion: v1\n",
      "data:\n",
      "  foo: YmFyCg==\n",
      "  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...\n",
      "  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...\n",
      "kind: Secret\n",
      "...\n",
      "Now compare this to the YAML of the ConfigMap you created earlier, which is shown\n",
      "in the following listing.\n",
      "$ kubectl get configmap fortune-config -o yaml\n",
      "apiVersion: v1\n",
      "data:\n",
      "  my-nginx-config.conf: |\n",
      "    server {\n",
      "      ...\n",
      "    }\n",
      "  sleep-interval: |\n",
      "    25\n",
      "kind: ConfigMap\n",
      "...\n",
      "Notice the difference? The contents of a Secret’s entries are shown as Base64-encoded\n",
      "strings, whereas those of a ConfigMap are shown in clear text. This initially made\n",
      "working with Secrets in YAML and JSON manifests a bit more painful, because you\n",
      "had to encode and decode them when setting and reading their entries. \n",
      "USING SECRETS FOR BINARY DATA\n",
      "The reason for using Base64 encoding is simple. A Secret’s entries can contain binary\n",
      "values, not only plain-text. Base64 encoding allows you to include the binary data in\n",
      "YAML or JSON, which are both plain-text formats. \n",
      "TIP\n",
      "You can use Secrets even for non-sensitive binary data, but be aware that\n",
      "the maximum size of a Secret is limited to 1MB.\n",
      "Listing 7.21\n",
      "A Secret’s YAML definition\n",
      "Listing 7.22\n",
      "A ConfigMap’s YAML definition\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 250, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "218\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "INTRODUCING THE STRINGDATA FIELD\n",
      "Because not all sensitive data is in binary form, Kubernetes also allows setting a Secret’s\n",
      "values through the stringData field. The following listing shows how it’s used.\n",
      "kind: Secret\n",
      "apiVersion: v1\n",
      "stringData:           \n",
      "  foo: plain text      \n",
      "data:\n",
      "  https.cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCekNDQ...\n",
      "  https.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFcE...\n",
      "The stringData field is write-only (note: write-only, not read-only). It can only be\n",
      "used to set values. When you retrieve the Secret’s YAML with kubectl get -o yaml, the\n",
      "stringData field will not be shown. Instead, all entries you specified in the string-\n",
      "Data field (such as the foo entry in the previous example) will be shown under data\n",
      "and will be Base64-encoded like all the other entries. \n",
      "READING A SECRET’S ENTRY IN A POD\n",
      "When you expose the Secret to a container through a secret volume, the value of the\n",
      "Secret entry is decoded and written to the file in its actual form (regardless if it’s plain\n",
      "text or binary). The same is also true when exposing the Secret entry through an envi-\n",
      "ronment variable. In both cases, the app doesn’t need to decode it, but can read the\n",
      "file’s contents or look up the environment variable value and use it directly.\n",
      "7.5.5\n",
      "Using the Secret in a pod\n",
      "With your fortune-https Secret containing both the cert and key files, all you need to\n",
      "do now is configure Nginx to use them. \n",
      "MODIFYING THE FORTUNE-CONFIG CONFIGMAP TO ENABLE HTTPS\n",
      "For this, you need to modify the config file again by editing the ConfigMap:\n",
      "$ kubectl edit configmap fortune-config\n",
      "After the text editor opens, modify the part that defines the contents of the my-nginx-\n",
      "config.conf entry so it looks like the following listing.\n",
      "...\n",
      "data:\n",
      "  my-nginx-config.conf: |\n",
      "    server {\n",
      "      listen              80;\n",
      "      listen              443 ssl;\n",
      "      server_name         www.kubia-example.com;\n",
      "Listing 7.23\n",
      "Adding plain text entries to a Secret using the stringData field\n",
      "Listing 7.24\n",
      "Modifying the fortune-config ConfigMap’s data\n",
      "The stringData can be used \n",
      "for non-binary Secret data.\n",
      "See, “plain text” is not Base64-encoded.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 251, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "219\n",
      "Using Secrets to pass sensitive data to containers\n",
      "      ssl_certificate     certs/https.cert;           \n",
      "      ssl_certificate_key certs/https.key;            \n",
      "      ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;\n",
      "      ssl_ciphers         HIGH:!aNULL:!MD5;\n",
      "      location / {\n",
      "        root   /usr/share/nginx/html;\n",
      "        index  index.html index.htm;\n",
      "      }\n",
      "    }\n",
      "  sleep-interval: |\n",
      "...\n",
      "This configures the server to read the certificate and key files from /etc/nginx/certs,\n",
      "so you’ll need to mount the secret volume there. \n",
      "MOUNTING THE FORTUNE-HTTPS SECRET IN A POD\n",
      "Next, you’ll create a new fortune-https pod and mount the secret volume holding\n",
      "the certificate and key into the proper location in the web-server container, as shown\n",
      "in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: fortune-https\n",
      "spec:\n",
      "  containers:\n",
      "  - image: luksa/fortune:env\n",
      "    name: html-generator\n",
      "    env:\n",
      "    - name: INTERVAL\n",
      "      valueFrom: \n",
      "        configMapKeyRef:\n",
      "          name: fortune-config\n",
      "          key: sleep-interval\n",
      "    volumeMounts:\n",
      "    - name: html\n",
      "      mountPath: /var/htdocs\n",
      "  - image: nginx:alpine\n",
      "    name: web-server\n",
      "    volumeMounts:\n",
      "    - name: html\n",
      "      mountPath: /usr/share/nginx/html\n",
      "      readOnly: true\n",
      "    - name: config\n",
      "      mountPath: /etc/nginx/conf.d\n",
      "      readOnly: true\n",
      "    - name: certs                         \n",
      "      mountPath: /etc/nginx/certs/        \n",
      "      readOnly: true                      \n",
      "    ports:\n",
      "    - containerPort: 80\n",
      "Listing 7.25\n",
      "YAML definition of the fortune-https pod: fortune-pod-https.yaml\n",
      "The paths are \n",
      "relative to /etc/nginx.\n",
      "You configured Nginx to read the cert and \n",
      "key file from /etc/nginx/certs, so you need \n",
      "to mount the Secret volume there.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 252, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "220\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "    - containerPort: 443\n",
      "  volumes:\n",
      "  - name: html\n",
      "    emptyDir: {}\n",
      "  - name: config\n",
      "    configMap:\n",
      "      name: fortune-config\n",
      "      items:\n",
      "      - key: my-nginx-config.conf\n",
      "        path: https.conf\n",
      "  - name: certs                            \n",
      "    secret:                                \n",
      "      secretName: fortune-https            \n",
      "Much is going on in this pod descriptor, so let me help you visualize it. Figure 7.12\n",
      "shows the components defined in the YAML. The default-token Secret, volume, and\n",
      "volume mount, which aren’t part of the YAML, but are added to your pod automati-\n",
      "cally, aren’t shown in the figure.\n",
      "NOTE\n",
      "Like configMap volumes, secret volumes also support specifying file\n",
      "permissions for the files exposed in the volume through the defaultMode\n",
      "property.\n",
      "You define the secret \n",
      "volume here, referring to \n",
      "the fortune-https Secret.\n",
      "Container: web-server\n",
      "Container: html-generator\n",
      "Secret: fortune-https\n",
      "Default token Secret and volume not shown\n",
      "secret\n",
      "volume:\n",
      "certs\n",
      "emptyDir\n",
      "volume:\n",
      "html\n",
      "conﬁgMap\n",
      "volume:\n",
      "conﬁg\n",
      "https.cert\n",
      "...\n",
      "...\n",
      "...\n",
      "https.key\n",
      "foo\n",
      "/etc/nginx/conf.d/\n",
      "/etc/nginx/certs/\n",
      "/usr/share/nginx/html/\n",
      "/var/htdocs\n",
      "ConﬁgMap: fortune-conﬁg\n",
      "my-nginx-conﬁg.conf\n",
      "server {\n",
      "…\n",
      "}\n",
      "Pod\n",
      "Environment variables:\n",
      "INTERVAL=25\n",
      "sleep-interval\n",
      "25\n",
      "Figure 7.12\n",
      "Combining a ConfigMap and a Secret to run your fortune-https pod\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 253, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "221\n",
      "Using Secrets to pass sensitive data to containers\n",
      "TESTING WHETHER NGINX IS USING THE CERT AND KEY FROM THE SECRET\n",
      "Once the pod is running, you can see if it’s serving HTTPS traffic by opening a port-\n",
      "forward tunnel to the pod’s port 443 and using it to send a request to the server\n",
      "with curl: \n",
      "$ kubectl port-forward fortune-https 8443:443 &\n",
      "Forwarding from 127.0.0.1:8443 -> 443\n",
      "Forwarding from [::1]:8443 -> 443\n",
      "$ curl https://localhost:8443 -k\n",
      "If you configured the server properly, you should get a response. You can check the\n",
      "server’s certificate to see if it matches the one you generated earlier. This can also be\n",
      "done with curl by turning on verbose logging using the -v option, as shown in the fol-\n",
      "lowing listing.\n",
      "$ curl https://localhost:8443 -k -v\n",
      "* About to connect() to localhost port 8443 (#0)\n",
      "*   Trying ::1...\n",
      "* Connected to localhost (::1) port 8443 (#0)\n",
      "* Initializing NSS with certpath: sql:/etc/pki/nssdb\n",
      "* skipping SSL peer certificate verification\n",
      "* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\n",
      "* Server certificate:\n",
      "*   subject: CN=www.kubia-example.com          \n",
      "*   start date: aug 16 18:43:13 2016 GMT       \n",
      "*   expire date: aug 14 18:43:13 2026 GMT      \n",
      "*   common name: www.kubia-example.com         \n",
      "*   issuer: CN=www.kubia-example.com           \n",
      "UNDERSTANDING SECRET VOLUMES ARE STORED IN MEMORY\n",
      "You successfully delivered your certificate and private key to your container by mount-\n",
      "ing a secret volume in its directory tree at /etc/nginx/certs. The secret volume uses\n",
      "an in-memory filesystem (tmpfs) for the Secret files. You can see this if you list mounts\n",
      "in the container:\n",
      "$ kubectl exec fortune-https -c web-server -- mount | grep certs\n",
      "tmpfs on /etc/nginx/certs type tmpfs (ro,relatime) \n",
      "Because tmpfs is used, the sensitive data stored in the Secret is never written to disk,\n",
      "where it could be compromised. \n",
      "EXPOSING A SECRET’S ENTRIES THROUGH ENVIRONMENT VARIABLES\n",
      "Instead of using a volume, you could also have exposed individual entries from the\n",
      "secret as environment variables, the way you did with the sleep-interval entry from\n",
      "the ConfigMap. For example, if you wanted to expose the foo key from your Secret as\n",
      "environment variable FOO_SECRET, you’d add the snippet from the following listing to\n",
      "the container definition.\n",
      "Listing 7.26\n",
      "Displaying the server certificate sent by Nginx\n",
      "The certificate \n",
      "matches the one you \n",
      "created and stored \n",
      "in the Secret.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 254, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "222\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "    env:\n",
      "    - name: FOO_SECRET\n",
      "      valueFrom:                  \n",
      "        secretKeyRef:             \n",
      "          name: fortune-https    \n",
      "          key: foo           \n",
      "This is almost exactly like when you set the INTERVAL environment variable, except\n",
      "that this time you’re referring to a Secret by using secretKeyRef instead of config-\n",
      "MapKeyRef, which is used to refer to a ConfigMap.\n",
      " Even though Kubernetes enables you to expose Secrets through environment vari-\n",
      "ables, it may not be the best idea to use this feature. Applications usually dump envi-\n",
      "ronment variables in error reports or even write them to the application log at startup,\n",
      "which may unintentionally expose them. Additionally, child processes inherit all the\n",
      "environment variables of the parent process, so if your app runs a third-party binary,\n",
      "you have no way of knowing what happens with your secret data. \n",
      "TIP\n",
      "Think twice before using environment variables to pass your Secrets to\n",
      "your container, because they may get exposed inadvertently. To be safe, always\n",
      "use secret volumes for exposing Secrets.\n",
      "7.5.6\n",
      "Understanding image pull Secrets\n",
      "You’ve learned how to pass Secrets to your applications and use the data they contain.\n",
      "But sometimes Kubernetes itself requires you to pass credentials to it—for example,\n",
      "when you’d like to use images from a private container image registry. This is also\n",
      "done through Secrets.\n",
      " Up to now all your container images have been stored on public image registries,\n",
      "which don’t require any special credentials to pull images from them. But most orga-\n",
      "nizations don’t want their images to be available to everyone and thus use a private\n",
      "image registry. When deploying a pod, whose container images reside in a private reg-\n",
      "istry, Kubernetes needs to know the credentials required to pull the image. Let’s see\n",
      "how to do that.\n",
      "USING A PRIVATE IMAGE REPOSITORY ON DOCKER HUB\n",
      "Docker Hub, in addition to public image repositories, also allows you to create private\n",
      "repositories. You can mark a repository as private by logging in at http:/\n",
      "/hub.docker\n",
      ".com with your web browser, finding the repository and checking a checkbox. \n",
      " To run a pod, which uses an image from the private repository, you need to do\n",
      "two things:\n",
      "Create a Secret holding the credentials for the Docker registry.\n",
      "Reference that Secret in the imagePullSecrets field of the pod manifest.\n",
      "Listing 7.27\n",
      "Exposing a Secret’s entry as an environment variable\n",
      "The variable should be set \n",
      "from the entry of a Secret.\n",
      "The name of the Secret \n",
      "holding the key\n",
      "The key of the Secret \n",
      "to expose\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 255, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "223\n",
      "Using Secrets to pass sensitive data to containers\n",
      "CREATING A SECRET FOR AUTHENTICATING WITH A DOCKER REGISTRY\n",
      "Creating a Secret holding the credentials for authenticating with a Docker registry\n",
      "isn’t that different from creating the generic Secret you created in section 7.5.3. You\n",
      "use the same kubectl create secret command, but with a different type and\n",
      "options:\n",
      "$ kubectl create secret docker-registry mydockerhubsecret \\\n",
      "  --docker-username=myusername --docker-password=mypassword \\ \n",
      "  --docker-email=my.email@provider.com\n",
      "Rather than create a generic secret, you’re creating a docker-registry Secret called\n",
      "mydockerhubsecret. You’re specifying your Docker Hub username, password, and\n",
      "email. If you inspect the contents of the newly created Secret with kubectl describe,\n",
      "you’ll see that it includes a single entry called .dockercfg. This is equivalent to the\n",
      ".dockercfg file in your home directory, which is created by Docker when you run the\n",
      "docker login command.\n",
      "USING THE DOCKER-REGISTRY SECRET IN A POD DEFINITION\n",
      "To have Kubernetes use the Secret when pulling images from your private Docker\n",
      "Hub repository, all you need to do is specify the Secret’s name in the pod spec, as\n",
      "shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: private-pod\n",
      "spec:\n",
      "  imagePullSecrets:                 \n",
      "  - name: mydockerhubsecret         \n",
      "  containers:\n",
      "  - image: username/private:tag\n",
      "    name: main\n",
      "In the pod definition in the previous listing, you’re specifying the mydockerhubsecret\n",
      "Secret as one of the imagePullSecrets. I suggest you try this out yourself, because it’s\n",
      "likely you’ll deal with private container images soon.\n",
      "NOT HAVING TO SPECIFY IMAGE PULL SECRETS ON EVERY POD\n",
      "Given that people usually run many different pods in their systems, it makes you won-\n",
      "der if you need to add the same image pull Secrets to every pod. Luckily, that’s not the\n",
      "case. In chapter 12 you’ll learn how image pull Secrets can be added to all your pods\n",
      "automatically if you add the Secrets to a ServiceAccount.\n",
      "Listing 7.28\n",
      "A pod definition using an image pull Secret: pod-with-private-image.yaml\n",
      "This enables pulling images \n",
      "from a private image registry.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 256, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "224\n",
      "CHAPTER 7\n",
      "ConfigMaps and Secrets: configuring applications\n",
      "7.6\n",
      "Summary\n",
      "This wraps up this chapter on how to pass configuration data to containers. You’ve\n",
      "learned how to\n",
      "Override the default command defined in a container image in the pod definition\n",
      "Pass command-line arguments to the main container process\n",
      "Set environment variables for a container\n",
      "Decouple configuration from a pod specification and put it into a ConfigMap\n",
      "Store sensitive data in a Secret and deliver it securely to containers\n",
      "Create a docker-registry Secret and use it to pull images from a private image\n",
      "registry\n",
      "In the next chapter, you’ll learn how to pass pod and container metadata to applica-\n",
      "tions running inside them. You’ll also see how the default token Secret, which we\n",
      "learned about in this chapter, is used to talk to the API server from within a pod. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 257, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "225\n",
      "Accessing pod metadata\n",
      "and other resources\n",
      "from applications\n",
      "Applications often need information about the environment they’re running in,\n",
      "including details about themselves and that of other components in the cluster.\n",
      "You’ve already seen how Kubernetes enables service discovery through environ-\n",
      "ment variables or DNS, but what about other information? In this chapter, you’ll\n",
      "see how certain pod and container metadata can be passed to the container and\n",
      "how easy it is for an app running inside a container to talk to the Kubernetes API\n",
      "server to get information about the resources deployed in the cluster and even how\n",
      "to create or modify those resources.\n",
      "This chapter covers\n",
      "Using the Downward API to pass information into \n",
      "containers\n",
      "Exploring the Kubernetes REST API\n",
      "Leaving authentication and server verification to \n",
      "kubectl proxy\n",
      "Accessing the API server from within a container\n",
      "Understanding the ambassador container pattern\n",
      "Using Kubernetes client libraries\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 258, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "226\n",
      "CHAPTER 8\n",
      "Accessing pod metadata and other resources from applications\n",
      "8.1\n",
      "Passing metadata through the Downward API\n",
      "In the previous chapter you saw how you can pass configuration data to your appli-\n",
      "cations through environment variables or through configMap and secret volumes.\n",
      "This works well for data that you set yourself and that is known before the pod is\n",
      "scheduled to a node and run there. But what about data that isn’t known up until\n",
      "that point—such as the pod’s IP, the host node’s name, or even the pod’s own name\n",
      "(when the name is generated; for example, when the pod is created by a ReplicaSet\n",
      "or similar controller)? And what about data that’s already specified elsewhere, such\n",
      "as a pod’s labels and annotations? You don’t want to repeat the same information in\n",
      "multiple places.\n",
      " Both these problems are solved by the Kubernetes Downward API. It allows you to\n",
      "pass metadata about the pod and its environment through environment variables or\n",
      "files (in a downwardAPI volume). Don’t be confused by the name. The Downward API\n",
      "isn’t like a REST endpoint that your app needs to hit so it can get the data. It’s a way of\n",
      "having environment variables or files populated with values from the pod’s specifica-\n",
      "tion or status, as shown in figure 8.1.\n",
      "8.1.1\n",
      "Understanding the available metadata\n",
      "The Downward API enables you to expose the pod’s own metadata to the processes\n",
      "running inside that pod. Currently, it allows you to pass the following information to\n",
      "your containers:\n",
      "The pod’s name\n",
      "The pod’s IP address\n",
      "Container: main\n",
      "Environment\n",
      "variables\n",
      "API server\n",
      "Used to initialize environment\n",
      "variables and ﬁles in the\n",
      "downwardAPI volume\n",
      "Pod manifest\n",
      "- Metadata\n",
      "- Status\n",
      "Pod\n",
      "downwardAPI\n",
      "volume\n",
      "App process\n",
      "Figure 8.1\n",
      "The Downward API exposes pod metadata through environment variables or files.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 259, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "227\n",
      "Passing metadata through the Downward API\n",
      "The namespace the pod belongs to\n",
      "The name of the node the pod is running on\n",
      "The name of the service account the pod is running under\n",
      "The CPU and memory requests for each container\n",
      "The CPU and memory limits for each container\n",
      "The pod’s labels\n",
      "The pod’s annotations\n",
      "Most of the items in the list shouldn’t require further explanation, except perhaps the\n",
      "service account and CPU/memory requests and limits, which we haven’t introduced\n",
      "yet. We’ll cover service accounts in detail in chapter 12. For now, all you need to know\n",
      "is that a service account is the account that the pod authenticates as when talking to\n",
      "the API server. CPU and memory requests and limits are explained in chapter 14.\n",
      "They’re the amount of CPU and memory guaranteed to a container and the maxi-\n",
      "mum amount it can get.\n",
      " Most items in the list can be passed to containers either through environment vari-\n",
      "ables or through a downwardAPI volume, but labels and annotations can only be\n",
      "exposed through the volume. Part of the data can be acquired by other means (for\n",
      "example, from the operating system directly), but the Downward API provides a sim-\n",
      "pler alternative.\n",
      " Let’s look at an example to pass metadata to your containerized process.\n",
      "8.1.2\n",
      "Exposing metadata through environment variables\n",
      "First, let’s look at how you can pass the pod’s and container’s metadata to the con-\n",
      "tainer through environment variables. You’ll create a simple single-container pod\n",
      "from the following listing’s manifest.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: downward\n",
      "spec:\n",
      "  containers:\n",
      "  - name: main\n",
      "    image: busybox\n",
      "    command: [\"sleep\", \"9999999\"]\n",
      "    resources:\n",
      "      requests:\n",
      "        cpu: 15m\n",
      "        memory: 100Ki\n",
      "      limits:\n",
      "        cpu: 100m\n",
      "        memory: 4Mi\n",
      "    env:\n",
      "    - name: POD_NAME\n",
      "Listing 8.1\n",
      "Downward API used in environment variables: downward-api-env.yaml\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 260, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "228\n",
      "CHAPTER 8\n",
      "Accessing pod metadata and other resources from applications\n",
      "      valueFrom:                            \n",
      "        fieldRef:                           \n",
      "          fieldPath: metadata.name          \n",
      "    - name: POD_NAMESPACE\n",
      "      valueFrom:\n",
      "        fieldRef:\n",
      "          fieldPath: metadata.namespace\n",
      "    - name: POD_IP\n",
      "      valueFrom:\n",
      "        fieldRef:\n",
      "          fieldPath: status.podIP\n",
      "    - name: NODE_NAME\n",
      "      valueFrom:\n",
      "        fieldRef:\n",
      "          fieldPath: spec.nodeName\n",
      "    - name: SERVICE_ACCOUNT\n",
      "      valueFrom:\n",
      "        fieldRef:\n",
      "          fieldPath: spec.serviceAccountName\n",
      "    - name: CONTAINER_CPU_REQUEST_MILLICORES\n",
      "      valueFrom:                                   \n",
      "        resourceFieldRef:                          \n",
      "          resource: requests.cpu                   \n",
      "          divisor: 1m                            \n",
      "    - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES\n",
      "      valueFrom:\n",
      "        resourceFieldRef:\n",
      "          resource: limits.memory\n",
      "          divisor: 1Ki\n",
      "When your process runs, it can look up all the environment variables you defined in\n",
      "the pod spec. Figure 8.2 shows the environment variables and the sources of their val-\n",
      "ues. The pod’s name, IP, and namespace will be exposed through the POD_NAME,\n",
      "POD_IP, and POD_NAMESPACE environment variables, respectively. The name of the\n",
      "node the container is running on will be exposed through the NODE_NAME variable.\n",
      "The name of the service account is made available through the SERVICE_ACCOUNT\n",
      "environment variable. You’re also creating two environment variables that will hold\n",
      "the amount of CPU requested for this container and the maximum amount of mem-\n",
      "ory the container is allowed to consume.\n",
      " For environment variables exposing resource limits or requests, you specify a divi-\n",
      "sor. The actual value of the limit or the request will be divided by the divisor and the\n",
      "result exposed through the environment variable. In the previous example, you’re set-\n",
      "ting the divisor for CPU requests to 1m (one milli-core, or one one-thousandth of a\n",
      "CPU core). Because you’ve set the CPU request to 15m, the environment variable\n",
      "CONTAINER_CPU_REQUEST_MILLICORES will be set to 15. Likewise, you set the memory\n",
      "limit to 4Mi (4 mebibytes) and the divisor to 1Ki (1 Kibibyte), so the CONTAINER_MEMORY\n",
      "_LIMIT_KIBIBYTES environment variable will be set to 4096. \n",
      "Instead of specifying an absolute value, \n",
      "you’re referencing the metadata.name \n",
      "field from the pod manifest.\n",
      "A container’s CPU and memory \n",
      "requests and limits are referenced \n",
      "by using resourceFieldRef instead \n",
      "of fieldRef.\n",
      "For resource fields, you \n",
      "define a divisor to get the \n",
      "value in the unit you need.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 261, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "229\n",
      "Passing metadata through the Downward API\n",
      "The divisor for CPU limits and requests can be either 1, which means one whole core,\n",
      "or 1m, which is one millicore. The divisor for memory limits/requests can be 1 (byte),\n",
      "1k (kilobyte) or 1Ki (kibibyte), 1M (megabyte) or 1Mi (mebibyte), and so on.\n",
      " After creating the pod, you can use kubectl exec to see all these environment vari-\n",
      "ables in your container, as shown in the following listing.\n",
      "$ kubectl exec downward env\n",
      "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "HOSTNAME=downward\n",
      "CONTAINER_MEMORY_LIMIT_KIBIBYTES=4096\n",
      "POD_NAME=downward\n",
      "POD_NAMESPACE=default\n",
      "POD_IP=10.0.0.10\n",
      "NODE_NAME=gke-kubia-default-pool-32a2cac8-sgl7\n",
      "SERVICE_ACCOUNT=default\n",
      "CONTAINER_CPU_REQUEST_MILLICORES=15\n",
      "KUBERNETES_SERVICE_HOST=10.3.240.1\n",
      "KUBERNETES_SERVICE_PORT=443\n",
      "...\n",
      "Listing 8.2\n",
      "Environment variables in the downward pod\n",
      "Pod manifest\n",
      "metadata:\n",
      "name: downward\n",
      "namespace: default\n",
      "spec:\n",
      "nodeName: minikube\n",
      "serviceAccountName: default\n",
      "containers:\n",
      "- name: main\n",
      "image: busybox\n",
      "command: [\"sleep\", \"9999999\"]\n",
      "resources:\n",
      "requests:\n",
      "cpu: 15m\n",
      "memory: 100Ki\n",
      "limits:\n",
      "cpu: 100m\n",
      "memory: 4Mi\n",
      "...\n",
      "status:\n",
      "podIP: 172.17.0.4\n",
      "...\n",
      "Pod: downward\n",
      "Container: main\n",
      "Environment variables\n",
      "POD_NAME=downward\n",
      "POD_NAMESPACE=default\n",
      "POD_IP=172.17.0.4\n",
      "NODE_NAME=minikube\n",
      "SERVICE_ACCOUNT=default\n",
      "CONTAINER_CPU_REQUEST_MILLICORES=15\n",
      "CONTAINER_MEMORY_LIMIT_KIBIBYTES=4096\n",
      "divisor: 1m\n",
      "divisor: 1Ki\n",
      "Figure 8.2\n",
      "Pod metadata and attributes can be exposed to the pod through environment variables.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 262, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "230\n",
      "CHAPTER 8\n",
      "Accessing pod metadata and other resources from applications\n",
      "All processes running inside the container can read those variables and use them how-\n",
      "ever they need. \n",
      "8.1.3\n",
      "Passing metadata through files in a downwardAPI volume\n",
      "If you prefer to expose the metadata through files instead of environment variables,\n",
      "you can define a downwardAPI volume and mount it into your container. You must use\n",
      "a downwardAPI volume for exposing the pod’s labels or its annotations, because nei-\n",
      "ther can be exposed through environment variables. We’ll discuss why later.\n",
      " As with environment variables, you need to specify each metadata field explicitly if\n",
      "you want to have it exposed to the process. Let’s see how to modify the previous exam-\n",
      "ple to use a volume instead of environment variables, as shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: downward\n",
      "  labels:                  \n",
      "    foo: bar               \n",
      "  annotations:             \n",
      "    key1: value1           \n",
      "    key2: |                \n",
      "      multi                \n",
      "      line                 \n",
      "      value                \n",
      "spec:\n",
      "  containers:\n",
      "  - name: main\n",
      "    image: busybox\n",
      "    command: [\"sleep\", \"9999999\"]\n",
      "    resources:\n",
      "      requests:\n",
      "        cpu: 15m\n",
      "        memory: 100Ki\n",
      "      limits:\n",
      "        cpu: 100m\n",
      "        memory: 4Mi\n",
      "    volumeMounts:                        \n",
      "    - name: downward                     \n",
      "      mountPath: /etc/downward           \n",
      "  volumes:\n",
      "  - name: downward                 \n",
      "    downwardAPI:                   \n",
      "      items:\n",
      "      - path: \"podName\"                     \n",
      "        fieldRef:                           \n",
      "          fieldPath: metadata.name          \n",
      "      - path: \"podNamespace\"\n",
      "        fieldRef:\n",
      "          fieldPath: metadata.namespace\n",
      "Listing 8.3\n",
      "Pod with a downwardAPI volume: downward-api-volume.yaml\n",
      "These labels and \n",
      "annotations will be \n",
      "exposed through the \n",
      "downwardAPI volume.\n",
      "You’re mounting the \n",
      "downward volume \n",
      "under /etc/downward.\n",
      "You’re defining a downwardAPI \n",
      "volume with the name downward.\n",
      "The pod’s name (from the metadata.name \n",
      "field in the manifest) will be written to \n",
      "the podName file.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 263, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "231\n",
      "Passing metadata through the Downward API\n",
      "      - path: \"labels\"                       \n",
      "        fieldRef:                            \n",
      "          fieldPath: metadata.labels         \n",
      "      - path: \"annotations\"                       \n",
      "        fieldRef:                                 \n",
      "          fieldPath: metadata.annotations         \n",
      "      - path: \"containerCpuRequestMilliCores\"\n",
      "        resourceFieldRef:\n",
      "          containerName: main\n",
      "          resource: requests.cpu\n",
      "          divisor: 1m\n",
      "      - path: \"containerMemoryLimitBytes\"\n",
      "        resourceFieldRef:\n",
      "          containerName: main\n",
      "          resource: limits.memory\n",
      "          divisor: 1\n",
      "Instead of passing the metadata through environment variables, you’re defining a vol-\n",
      "ume called downward and mounting it in your container under /etc/downward. The\n",
      "files this volume will contain are configured under the downwardAPI.items attribute\n",
      "in the volume specification.\n",
      " Each item specifies the path (the filename) where the metadata should be written\n",
      "to and references either a pod-level field or a container resource field whose value you\n",
      "want stored in the file (see figure 8.3).\n",
      "The pod’s labels will be written \n",
      "to the /etc/downward/labels file.\n",
      "The pod’s annotations will be \n",
      "written to the /etc/downward/\n",
      "annotations file.\n",
      "downwardAPI volume\n",
      "Pod manifest\n",
      "metadata:\n",
      "name: downward\n",
      "namespace: default\n",
      "labels:\n",
      "foo: bar\n",
      "annotations:\n",
      "key1: value1\n",
      "...\n",
      "spec:\n",
      "containers:\n",
      "- name: main\n",
      "image: busybox\n",
      "command: [\"sleep\", \"9999999\"]\n",
      "resources:\n",
      "requests:\n",
      "cpu: 15m\n",
      "memory: 100Ki\n",
      "limits:\n",
      "cpu: 100m\n",
      "memory: 4Mi\n",
      "...\n",
      "/podName\n",
      "/podNamespace\n",
      "/labels\n",
      "/annotations\n",
      "/containerCpuRequestMilliCores\n",
      "/containerMemoryLimitBytes\n",
      "divisor: 1\n",
      "divisor: 1m\n",
      "Container: main\n",
      "Pod: downward\n",
      "Filesystem\n",
      "/\n",
      "etc/\n",
      "downward/\n",
      "Figure 8.3\n",
      "Using a downwardAPI volume to pass metadata to the container\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 264, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "232\n",
      "CHAPTER 8\n",
      "Accessing pod metadata and other resources from applications\n",
      "Delete the previous pod and create a new one from the manifest in the previous list-\n",
      "ing. Then look at the contents of the mounted downwardAPI volume directory. You\n",
      "mounted the volume under /etc/downward/, so list the files in there, as shown in the\n",
      "following listing.\n",
      "$ kubectl exec downward ls -lL /etc/downward\n",
      "-rw-r--r--   1 root   root   134 May 25 10:23 annotations\n",
      "-rw-r--r--   1 root   root     2 May 25 10:23 containerCpuRequestMilliCores\n",
      "-rw-r--r--   1 root   root     7 May 25 10:23 containerMemoryLimitBytes\n",
      "-rw-r--r--   1 root   root     9 May 25 10:23 labels\n",
      "-rw-r--r--   1 root   root     8 May 25 10:23 podName\n",
      "-rw-r--r--   1 root   root     7 May 25 10:23 podNamespace\n",
      "NOTE\n",
      "As with the configMap and secret volumes, you can change the file\n",
      "permissions through the downwardAPI volume’s defaultMode property in the\n",
      "pod spec.\n",
      "Each file corresponds to an item in the volume’s definition. The contents of files,\n",
      "which correspond to the same metadata fields as in the previous example, are the\n",
      "same as the values of environment variables you used before, so we won’t show them\n",
      "here. But because you couldn’t expose labels and annotations through environment\n",
      "variables before, examine the following listing for the contents of the two files you\n",
      "exposed them in.\n",
      "$ kubectl exec downward cat /etc/downward/labels\n",
      "foo=\"bar\"\n",
      "$ kubectl exec downward cat /etc/downward/annotations\n",
      "key1=\"value1\"\n",
      "key2=\"multi\\nline\\nvalue\\n\"\n",
      "kubernetes.io/config.seen=\"2016-11-28T14:27:45.664924282Z\"\n",
      "kubernetes.io/config.source=\"api\"\n",
      "As you can see, each label/annotation is written in the key=value format on a sepa-\n",
      "rate line. Multi-line values are written to a single line with newline characters denoted\n",
      "with \\n.\n",
      "UPDATING LABELS AND ANNOTATIONS\n",
      "You may remember that labels and annotations can be modified while a pod is run-\n",
      "ning. As you might expect, when they change, Kubernetes updates the files holding\n",
      "them, allowing the pod to always see up-to-date data. This also explains why labels and\n",
      "annotations can’t be exposed through environment variables. Because environment\n",
      "variable values can’t be updated afterward, if the labels or annotations of a pod were\n",
      "exposed through environment variables, there’s no way to expose the new values after\n",
      "they’re modified.\n",
      "Listing 8.4\n",
      "Files in the downwardAPI volume\n",
      "Listing 8.5\n",
      "Displaying labels and annotations in the downwardAPI volume\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 265, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "233\n",
      "Talking to the Kubernetes API server\n",
      "REFERRING TO CONTAINER-LEVEL METADATA IN THE VOLUME SPECIFICATION\n",
      "Before we wrap up this section, we need to point out one thing. When exposing con-\n",
      "tainer-level metadata, such as a container’s resource limit or requests (done using\n",
      "resourceFieldRef), you need to specify the name of the container whose resource\n",
      "field you’re referencing, as shown in the following listing.\n",
      "spec:\n",
      "  volumes:\n",
      "  - name: downward                       \n",
      "    downwardAPI:                         \n",
      "      items:\n",
      "      - path: \"containerCpuRequestMilliCores\"\n",
      "        resourceFieldRef:\n",
      "          containerName: main       \n",
      "          resource: requests.cpu\n",
      "          divisor: 1m\n",
      "The reason for this becomes obvious if you consider that volumes are defined at the\n",
      "pod level, not at the container level. When referring to a container’s resource field\n",
      "inside a volume specification, you need to explicitly specify the name of the container\n",
      "you’re referring to. This is true even for single-container pods. \n",
      " Using volumes to expose a container’s resource requests and/or limits is slightly\n",
      "more complicated than using environment variables, but the benefit is that it allows\n",
      "you to pass one container’s resource fields to a different container if needed (but\n",
      "both containers need to be in the same pod). With environment variables, a container\n",
      "can only be passed its own resource limits and requests. \n",
      "UNDERSTANDING WHEN TO USE THE DOWNWARD API\n",
      "As you’ve seen, using the Downward API isn’t complicated. It allows you to keep the\n",
      "application Kubernetes-agnostic. This is especially useful when you’re dealing with an\n",
      "existing application that expects certain data in environment variables. The Down-\n",
      "ward API allows you to expose the data to the application without having to rewrite\n",
      "the application or wrap it in a shell script, which collects the data and then exposes it\n",
      "through environment variables.\n",
      " But the metadata available through the Downward API is fairly limited. If you need\n",
      "more, you’ll need to obtain it from the Kubernetes API server directly. You’ll learn\n",
      "how to do that next.\n",
      "8.2\n",
      "Talking to the Kubernetes API server\n",
      "We’ve seen how the Downward API provides a simple way to pass certain pod and con-\n",
      "tainer metadata to the process running inside them. It only exposes the pod’s own\n",
      "metadata and a subset of all of the pod’s data. But sometimes your app will need to\n",
      "know more about other pods and even other resources defined in your cluster. The\n",
      "Downward API doesn’t help in those cases.\n",
      "Listing 8.6\n",
      "Referring to container-level metadata in a downwardAPI volume\n",
      "Container name \n",
      "must be specified\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 266, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "234\n",
      "CHAPTER 8\n",
      "Accessing pod metadata and other resources from applications\n",
      " As you’ve seen throughout the book, information about services and pods can be\n",
      "obtained by looking at the service-related environment variables or through DNS. But\n",
      "when the app needs data about other resources or when it requires access to the most\n",
      "up-to-date information as possible, it needs to talk to the API server directly (as shown\n",
      "in figure 8.4).\n",
      "Before you see how apps within pods can talk to the Kubernetes API server, let’s first\n",
      "explore the server’s REST endpoints from your local machine, so you can see what\n",
      "talking to the API server looks like.\n",
      "8.2.1\n",
      "Exploring the Kubernetes REST API\n",
      "You’ve learned about different Kubernetes resource types. But if you’re planning on\n",
      "developing apps that talk to the Kubernetes API, you’ll want to know the API first. \n",
      " To do that, you can try hitting the API server directly. You can get its URL by run-\n",
      "ning kubectl cluster-info:\n",
      "$ kubectl cluster-info\n",
      "Kubernetes master is running at https://192.168.99.100:8443\n",
      "Because the server uses HTTPS and requires authentication, it’s not simple to talk to\n",
      "it directly. You can try accessing it with curl and using curl’s --insecure (or -k)\n",
      "option to skip the server certificate check, but that doesn’t get you far:\n",
      "$ curl https://192.168.99.100:8443 -k\n",
      "Unauthorized\n",
      "Luckily, rather than dealing with authentication yourself, you can talk to the server\n",
      "through a proxy by running the kubectl proxy command. \n",
      "ACCESSING THE API SERVER THROUGH KUBECTL PROXY \n",
      "The kubectl proxy command runs a proxy server that accepts HTTP connections on\n",
      "your local machine and proxies them to the API server while taking care of authenti-\n",
      "cation, so you don’t need to pass the authentication token in every request. It also\n",
      "makes sure you’re talking to the actual API server and not a man in the middle (by\n",
      "verifying the server’s certificate on each request).\n",
      "Container\n",
      "API server\n",
      "Pod\n",
      "App process\n",
      "API objects\n",
      "Figure 8.4\n",
      "Talking to the API server \n",
      "from inside a pod to get information \n",
      "about other API objects\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 267, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "235\n",
      "Talking to the Kubernetes API server\n",
      " Running the proxy is trivial. All you need to do is run the following command:\n",
      "$ kubectl proxy\n",
      "Starting to serve on 127.0.0.1:8001\n",
      "You don’t need to pass in any other arguments, because kubectl already knows every-\n",
      "thing it needs (the API server URL, authorization token, and so on). As soon as it starts\n",
      "up, the proxy starts accepting connections on local port 8001. Let’s see if it works:\n",
      "$ curl localhost:8001\n",
      "{\n",
      "  \"paths\": [\n",
      "    \"/api\",\n",
      "    \"/api/v1\",\n",
      "    ...\n",
      "Voila! You sent the request to the proxy, it sent a request to the API server, and then\n",
      "the proxy returned whatever the server returned. Now, let’s start exploring.\n",
      "EXPLORING THE KUBERNETES API THROUGH THE KUBECTL PROXY\n",
      "You can continue to use curl, or you can open your web browser and point it to\n",
      "http:/\n",
      "/localhost:8001. Let’s examine what the API server returns when you hit its base\n",
      "URL more closely. The server responds with a list of paths, as shown in the follow-\n",
      "ing listing.\n",
      "$ curl http://localhost:8001\n",
      "{\n",
      "  \"paths\": [\n",
      "    \"/api\",\n",
      "    \"/api/v1\",                  \n",
      "    \"/apis\",\n",
      "    \"/apis/apps\",\n",
      "    \"/apis/apps/v1beta1\",\n",
      "    ...\n",
      "    \"/apis/batch\",              \n",
      "    \"/apis/batch/v1\",           \n",
      "    \"/apis/batch/v2alpha1\",     \n",
      "    ...\n",
      "These paths correspond to the API groups and versions you specify in your resource\n",
      "definitions when creating resources such as Pods, Services, and so on. \n",
      " You may recognize the batch/v1 in the /apis/batch/v1 path as the API group and\n",
      "version of the Job resources you learned about in chapter 4. Likewise, the /api/v1\n",
      "corresponds to the apiVersion: v1 you refer to in the common resources you created\n",
      "(Pods, Services, ReplicationControllers, and so on). The most common resource\n",
      "types, which were introduced in the earliest versions of Kubernetes, don’t belong to\n",
      "Listing 8.7\n",
      "Listing the API server’s REST endpoints: http:/\n",
      "/localhost:8001\n",
      "Most resource types \n",
      "can be found here.\n",
      "The batch API \n",
      "group and its \n",
      "two versions\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 268, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "236\n",
      "CHAPTER 8\n",
      "Accessing pod metadata and other resources from applications\n",
      "any specific group, because Kubernetes initially didn’t even use the concept of API\n",
      "groups; they were introduced later. \n",
      "NOTE\n",
      "These initial resource types without an API group are now considered\n",
      "to belong to the core API group.\n",
      "EXPLORING THE BATCH API GROUP’S REST ENDPOINT\n",
      "Let’s explore the Job resource API. You’ll start by looking at what’s behind the\n",
      "/apis/batch path (you’ll omit the version for now), as shown in the following listing.\n",
      "$ curl http://localhost:8001/apis/batch\n",
      "{\n",
      "  \"kind\": \"APIGroup\",\n",
      "  \"apiVersion\": \"v1\",\n",
      "  \"name\": \"batch\",\n",
      "  \"versions\": [\n",
      "    {\n",
      "      \"groupVersion\": \"batch/v1\",             \n",
      "      \"version\": \"v1\"                         \n",
      "    },\n",
      "    {\n",
      "      \"groupVersion\": \"batch/v2alpha1\",       \n",
      "      \"version\": \"v2alpha1\"                   \n",
      "    }\n",
      "  ],\n",
      "  \"preferredVersion\": {                    \n",
      "    \"groupVersion\": \"batch/v1\",            \n",
      "    \"version\": \"v1\"                        \n",
      "  },\n",
      "  \"serverAddressByClientCIDRs\": null\n",
      "}\n",
      "The response shows a description of the batch API group, including the available ver-\n",
      "sions and the preferred version clients should use. Let’s continue and see what’s\n",
      "behind the /apis/batch/v1 path. It’s shown in the following listing.\n",
      "$ curl http://localhost:8001/apis/batch/v1\n",
      "{\n",
      "  \"kind\": \"APIResourceList\",              \n",
      "  \"apiVersion\": \"v1\",\n",
      "  \"groupVersion\": \"batch/v1\",             \n",
      "  \"resources\": [                          \n",
      "    {\n",
      "      \"name\": \"jobs\",             \n",
      "      \"namespaced\": true,         \n",
      "      \"kind\": \"Job\",              \n",
      "Listing 8.8\n",
      "Listing endpoints under /apis/batch: http:/\n",
      "/localhost:8001/apis/batch\n",
      "Listing 8.9\n",
      "Resource types in batch/v1: http:/\n",
      "/localhost:8001/apis/batch/v1\n",
      "The batch API \n",
      "group contains \n",
      "two versions.\n",
      "Clients should use the \n",
      "v1 version instead of \n",
      "v2alpha1.\n",
      "This is a list of API resources \n",
      "in the batch/v1 API group.\n",
      "Here’s an array holding \n",
      "all the resource types \n",
      "in this group.\n",
      "This describes the \n",
      "Job resource, which \n",
      "is namespaced.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 269, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "237\n",
      "Talking to the Kubernetes API server\n",
      "      \"verbs\": [                 \n",
      "        \"create\",                \n",
      "        \"delete\",                \n",
      "        \"deletecollection\",      \n",
      "        \"get\",                   \n",
      "        \"list\",                  \n",
      "        \"patch\",                 \n",
      "        \"update\",                \n",
      "        \"watch\"                  \n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"jobs/status\",            \n",
      "      \"namespaced\": true,                  \n",
      "      \"kind\": \"Job\",\n",
      "      \"verbs\": [             \n",
      "        \"get\",               \n",
      "        \"patch\",             \n",
      "        \"update\"             \n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "As you can see, the API server returns a list of resource types and REST endpoints in\n",
      "the batch/v1 API group. One of those is the Job resource. In addition to the name of\n",
      "the resource and the associated kind, the API server also includes information on\n",
      "whether the resource is namespaced or not, its short name (if it has one; Jobs don’t),\n",
      "and a list of verbs you can use with the resource. \n",
      " The returned list describes the REST resources exposed in the API server. The\n",
      "\"name\": \"jobs\" line tells you that the API contains the /apis/batch/v1/jobs end-\n",
      "point. The \"verbs\" array says you can retrieve, update, and delete Job resources\n",
      "through that endpoint. For certain resources, additional API endpoints are also\n",
      "exposed (such as the jobs/status path, which allows modifying only the status of\n",
      "a Job).\n",
      "LISTING ALL JOB INSTANCES IN THE CLUSTER\n",
      "To get a list of Jobs in your cluster, perform a GET request on path /apis/batch/\n",
      "v1/jobs, as shown in the following listing.\n",
      "$ curl http://localhost:8001/apis/batch/v1/jobs\n",
      "{\n",
      "  \"kind\": \"JobList\",\n",
      "  \"apiVersion\": \"batch/v1\",\n",
      "  \"metadata\": {\n",
      "    \"selfLink\": \"/apis/batch/v1/jobs\",\n",
      "    \"resourceVersion\": \"225162\"\n",
      "  },\n",
      "Listing 8.10\n",
      "List of Jobs: http:/\n",
      "/localhost:8001/apis/batch/v1/jobs\n",
      "Here are the verbs that can be used \n",
      "with this resource (you can create \n",
      "Jobs; delete individual ones or a \n",
      "collection of them; and retrieve, \n",
      "watch, and update them).\n",
      "Resources also have a \n",
      "special REST endpoint for \n",
      "modifying their status.\n",
      "The status can be \n",
      "retrieved, patched, \n",
      "or updated.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 270, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "238\n",
      "CHAPTER 8\n",
      "Accessing pod metadata and other resources from applications\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"metadata\": {\n",
      "        \"name\": \"my-job\",\n",
      "        \"namespace\": \"default\",\n",
      "        ...\n",
      "You probably have no Job resources deployed in your cluster, so the items array will be\n",
      "empty. You can try deploying the Job in Chapter08/my-job.yaml and hitting the REST\n",
      "endpoint again to get the same output as in listing 8.10.\n",
      "RETRIEVING A SPECIFIC JOB INSTANCE BY NAME\n",
      "The previous endpoint returned a list of all Jobs across all namespaces. To get back\n",
      "only one specific Job, you need to specify its name and namespace in the URL. To\n",
      "retrieve the Job shown in the previous listing (name: my-job; namespace: default),\n",
      "you need to request the following path: /apis/batch/v1/namespaces/default/jobs/\n",
      "my-job, as shown in the following listing.\n",
      "$ curl http://localhost:8001/apis/batch/v1/namespaces/default/jobs/my-job\n",
      "{\n",
      "  \"kind\": \"Job\",\n",
      "  \"apiVersion\": \"batch/v1\",\n",
      "  \"metadata\": {\n",
      "    \"name\": \"my-job\",\n",
      "    \"namespace\": \"default\",\n",
      "    ...\n",
      "As you can see, you get back the complete JSON definition of the my-job Job resource,\n",
      "exactly like you do if you run:\n",
      "$ kubectl get job my-job -o json\n",
      "You’ve seen that you can browse the Kubernetes REST API server without using any\n",
      "special tools, but to fully explore the REST API and interact with it, a better option is\n",
      "described at the end of this chapter. For now, exploring it with curl like this is enough\n",
      "to make you understand how an application running in a pod talks to Kubernetes. \n",
      "8.2.2\n",
      "Talking to the API server from within a pod\n",
      "You’ve learned how to talk to the API server from your local machine, using the\n",
      "kubectl proxy. Now, let’s see how to talk to it from within a pod, where you (usually)\n",
      "don’t have kubectl. Therefore, to talk to the API server from inside a pod, you need\n",
      "to take care of three things:\n",
      "Find the location of the API server.\n",
      "Make sure you’re talking to the API server and not something impersonating it.\n",
      "Authenticate with the server; otherwise it won’t let you see or do anything.\n",
      "Listing 8.11\n",
      "Retrieving a resource in a specific namespace by name\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 271, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "239\n",
      "Talking to the Kubernetes API server\n",
      "You’ll see how this is done in the next three sections. \n",
      "RUNNING A POD TO TRY OUT COMMUNICATION WITH THE API SERVER\n",
      "The first thing you need is a pod from which to talk to the API server. You’ll run a pod\n",
      "that does nothing (it runs the sleep command in its only container), and then run a\n",
      "shell in the container with kubectl exec. Then you’ll try to access the API server from\n",
      "within that shell using curl.\n",
      " Therefore, you need to use a container image that contains the curl binary. If you\n",
      "search for such an image on, say, Docker Hub, you’ll find the tutum/curl image, so\n",
      "use it (you can also use any other existing image containing the curl binary or you\n",
      "can build your own). The pod definition is shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: curl\n",
      "spec:\n",
      "  containers:\n",
      "  - name: main\n",
      "    image: tutum/curl                \n",
      "    command: [\"sleep\", \"9999999\"]    \n",
      "After creating the pod, run kubectl exec to run a bash shell inside its container:\n",
      "$ kubectl exec -it curl bash\n",
      "root@curl:/#\n",
      "You’re now ready to talk to the API server.\n",
      "FINDING THE API SERVER’S ADDRESS\n",
      "First, you need to find the IP and port of the Kubernetes API server. This is easy,\n",
      "because a Service called kubernetes is automatically exposed in the default name-\n",
      "space and configured to point to the API server. You may remember seeing it every\n",
      "time you listed services with kubectl get svc:\n",
      "$ kubectl get svc\n",
      "NAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
      "kubernetes   10.0.0.1     <none>        443/TCP   46d\n",
      "And you’ll remember from chapter 5 that environment variables are configured for\n",
      "each service. You can get both the IP address and the port of the API server by looking\n",
      "up the KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT variables (inside\n",
      "the container):\n",
      "root@curl:/# env | grep KUBERNETES_SERVICE\n",
      "KUBERNETES_SERVICE_PORT=443\n",
      "KUBERNETES_SERVICE_HOST=10.0.0.1\n",
      "KUBERNETES_SERVICE_PORT_HTTPS=443\n",
      "Listing 8.12\n",
      "A pod for trying out communication with the API server: curl.yaml\n",
      "Using the tutum/curl image, \n",
      "because you need curl \n",
      "available in the container\n",
      "You’re running the sleep \n",
      "command with a long delay to \n",
      "keep your container running.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 272, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "240\n",
      "CHAPTER 8\n",
      "Accessing pod metadata and other resources from applications\n",
      "You may also remember that each service also gets a DNS entry, so you don’t even\n",
      "need to look up the environment variables, but instead simply point curl to\n",
      "https:/\n",
      "/kubernetes. To be fair, if you don’t know which port the service is available at,\n",
      "you also either need to look up the environment variables or perform a DNS SRV\n",
      "record lookup to get the service’s actual port number. \n",
      " The environment variables shown previously say that the API server is listening on\n",
      "port 443, which is the default port for HTTPS, so try hitting the server through\n",
      "HTTPS:\n",
      "root@curl:/# curl https://kubernetes\n",
      "curl: (60) SSL certificate problem: unable to get local issuer certificate\n",
      "...\n",
      "If you'd like to turn off curl's verification of the certificate, use\n",
      "  the -k (or --insecure) option.\n",
      "Although the simplest way to get around this is to use the proposed -k option (and\n",
      "this is what you’d normally use when playing with the API server manually), let’s look\n",
      "at the longer (and correct) route. Instead of blindly trusting that the server you’re\n",
      "connecting to is the authentic API server, you’ll verify its identity by having curl check\n",
      "its certificate. \n",
      "TIP\n",
      "Never skip checking the server’s certificate in an actual application.\n",
      "Doing so could make your app expose its authentication token to an attacker\n",
      "using a man-in-the-middle attack.\n",
      "VERIFYING THE SERVER’S IDENTITY\n",
      "In the previous chapter, while discussing Secrets, we looked at an automatically cre-\n",
      "ated Secret called default-token-xyz, which is mounted into each container at\n",
      "/var/run/secrets/kubernetes.io/serviceaccount/. Let’s see the contents of that Secret\n",
      "again, by listing files in that directory:\n",
      "root@curl:/# \n",
      "ls \n",
      "/var/run/secrets/kubernetes.io/serviceaccount/ \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "ca.crt    namespace    token\n",
      "The Secret has three entries (and therefore three files in the Secret volume). Right\n",
      "now, we’ll focus on the ca.crt file, which holds the certificate of the certificate author-\n",
      "ity (CA) used to sign the Kubernetes API server’s certificate. To verify you’re talking to\n",
      "the API server, you need to check if the server’s certificate is signed by the CA. curl\n",
      "allows you to specify the CA certificate with the --cacert option, so try hitting the API\n",
      "server again:\n",
      "root@curl:/# curl --cacert /var/run/secrets/kubernetes.io/serviceaccount\n",
      "             ➥ /ca.crt https://kubernetes\n",
      "Unauthorized\n",
      "NOTE\n",
      "You may see a longer error description than “Unauthorized.”\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 273, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "241\n",
      "Talking to the Kubernetes API server\n",
      "Okay, you’ve made progress. curl verified the server’s identity because its certificate\n",
      "was signed by the CA you trust. As the Unauthorized response suggests, you still need\n",
      "to take care of authentication. You’ll do that in a moment, but first let’s see how to\n",
      "make life easier by setting the CURL_CA_BUNDLE environment variable, so you don’t\n",
      "need to specify --cacert every time you run curl:\n",
      "root@curl:/# export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/\n",
      "             ➥ serviceaccount/ca.crt\n",
      "You can now hit the API server without using --cacert:\n",
      "root@curl:/# curl https://kubernetes\n",
      "Unauthorized\n",
      "This is much nicer now. Your client (curl) trusts the API server now, but the API\n",
      "server itself says you’re not authorized to access it, because it doesn’t know who\n",
      "you are.\n",
      "AUTHENTICATING WITH THE API SERVER\n",
      "You need to authenticate with the server, so it allows you to read and even update\n",
      "and/or delete the API objects deployed in the cluster. To authenticate, you need an\n",
      "authentication token. Luckily, the token is provided through the default-token Secret\n",
      "mentioned previously, and is stored in the token file in the secret volume. As the\n",
      "Secret’s name suggests, that’s the primary purpose of the Secret. \n",
      " You’re going to use the token to access the API server. First, load the token into an\n",
      "environment variable:\n",
      "root@curl:/# TOKEN=$(cat /var/run/secrets/kubernetes.io/\n",
      "             ➥ serviceaccount/token)\n",
      "The token is now stored in the TOKEN environment variable. You can use it when send-\n",
      "ing requests to the API server, as shown in the following listing.\n",
      "root@curl:/# curl -H \"Authorization: Bearer $TOKEN\" https://kubernetes\n",
      "{\n",
      "  \"paths\": [\n",
      "    \"/api\",\n",
      "    \"/api/v1\",\n",
      "    \"/apis\",\n",
      "    \"/apis/apps\",\n",
      "    \"/apis/apps/v1beta1\",\n",
      "    \"/apis/authorization.k8s.io\",    \n",
      "    ...\n",
      "    \"/ui/\",\n",
      "    \"/version\"\n",
      "  ]\n",
      "}\n",
      "Listing 8.13\n",
      "Getting a proper response from the API server\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 274, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "242\n",
      "CHAPTER 8\n",
      "Accessing pod metadata and other resources from applications\n",
      "As you can see, you passed the token inside the Authorization HTTP header in the\n",
      "request. The API server recognized the token as authentic and returned a proper\n",
      "response. You can now explore all the resources in your cluster, the way you did a few\n",
      "sections ago. \n",
      " For example, you could list all the pods in the same namespace. But first you need\n",
      "to know what namespace the curl pod is running in.\n",
      "GETTING THE NAMESPACE THE POD IS RUNNING IN\n",
      "In the first part of this chapter, you saw how to pass the namespace to the pod\n",
      "through the Downward API. But if you’re paying attention, you probably noticed\n",
      "your secret volume also contains a file called namespace. It contains the name-\n",
      "space the pod is running in, so you can read the file instead of having to explicitly\n",
      "pass the namespace to your pod through an environment variable. Load the con-\n",
      "tents of the file into the NS environment variable and then list all the pods, as shown\n",
      "in the following listing.\n",
      "root@curl:/# NS=$(cat /var/run/secrets/kubernetes.io/\n",
      "             ➥ serviceaccount/namespace)           \n",
      "root@curl:/# curl -H \"Authorization: Bearer $TOKEN\"\n",
      "             ➥ https://kubernetes/api/v1/namespaces/$NS/pods\n",
      "{\n",
      "  \"kind\": \"PodList\",\n",
      "  \"apiVersion\": \"v1\",\n",
      "  ...\n",
      "And there you go. By using the three files in the mounted secret volume directory,\n",
      "you listed all the pods running in the same namespace as your pod. In the same man-\n",
      "ner, you could also retrieve other API objects and even update them by sending PUT or\n",
      "PATCH instead of simple GET requests. \n",
      "Disabling role-based access control (RBAC)\n",
      "If you’re using a Kubernetes cluster with RBAC enabled, the service account may not\n",
      "be authorized to access (parts of) the API server. You’ll learn about service accounts\n",
      "and RBAC in chapter 12. For now, the simplest way to allow you to query the API\n",
      "server is to work around RBAC by running the following command:\n",
      "$ kubectl create clusterrolebinding permissive-binding \\\n",
      "  --clusterrole=cluster-admin \\\n",
      "  --group=system:serviceaccounts\n",
      "This gives all service accounts (we could also say all pods) cluster-admin privileges,\n",
      "allowing them to do whatever they want. Obviously, doing this is dangerous and\n",
      "should never be done on production clusters. For test purposes, it’s fine.\n",
      "Listing 8.14\n",
      "Listing pods in the pod’s own namespace\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 275, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "243\n",
      "Talking to the Kubernetes API server\n",
      "RECAPPING HOW PODS TALK TO KUBERNETES\n",
      "Let’s recap how an app running inside a pod can access the Kubernetes API properly:\n",
      "The app should verify whether the API server’s certificate is signed by the certif-\n",
      "icate authority, whose certificate is in the ca.crt file. \n",
      "The app should authenticate itself by sending the Authorization header with\n",
      "the bearer token from the token file. \n",
      "The namespace file should be used to pass the namespace to the API server when\n",
      "performing CRUD operations on API objects inside the pod’s namespace.\n",
      "DEFINITION\n",
      "CRUD stands for Create, Read, Update, and Delete. The corre-\n",
      "sponding HTTP methods are POST, GET, PATCH/PUT, and DELETE, respectively.\n",
      "All three aspects of pod to API server communication are displayed in figure 8.5.\n",
      "8.2.3\n",
      "Simplifying API server communication with ambassador \n",
      "containers\n",
      "Dealing with HTTPS, certificates, and authentication tokens sometimes seems too\n",
      "complicated to developers. I’ve seen developers disable validation of server certifi-\n",
      "cates on way too many occasions (and I’ll admit to doing it myself a few times). Luck-\n",
      "ily, you can make the communication much simpler while keeping it secure. \n",
      "API server\n",
      "GET /api/v1/namespaces/<namespace>/pods\n",
      "Authorization: Bearer <token>\n",
      "Pod\n",
      "Container\n",
      "Filesystem\n",
      "App\n",
      "/\n",
      "var/\n",
      "run/\n",
      "secrets/\n",
      "kubernetes.io/\n",
      "serviceaccount/\n",
      "Default token secret volume\n",
      "ca.crt\n",
      "token\n",
      "namespace\n",
      "Server\n",
      "certiﬁcate\n",
      "Validate\n",
      "certiﬁcate\n",
      "Figure 8.5\n",
      "Using the files from the default-token Secret to talk to the API server\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 276, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "244\n",
      "CHAPTER 8\n",
      "Accessing pod metadata and other resources from applications\n",
      " Remember the kubectl proxy command we mentioned in section 8.2.1? You ran\n",
      "the command on your local machine to make it easier to access the API server. Instead\n",
      "of sending requests to the API server directly, you sent them to the proxy and let it\n",
      "take care of authentication, encryption, and server verification. The same method can\n",
      "be used inside your pods, as well.\n",
      "INTRODUCING THE AMBASSADOR CONTAINER PATTERN\n",
      "Imagine having an application that (among other things) needs to query the API\n",
      "server. Instead of it talking to the API server directly, as you did in the previous sec-\n",
      "tion, you can run kubectl proxy in an ambassador container alongside the main con-\n",
      "tainer and communicate with the API server through it. \n",
      " Instead of talking to the API server directly, the app in the main container can con-\n",
      "nect to the ambassador through HTTP (instead of HTTPS) and let the ambassador\n",
      "proxy handle the HTTPS connection to the API server, taking care of security trans-\n",
      "parently (see figure 8.6). It does this by using the files from the default token’s secret\n",
      "volume.\n",
      "Because all containers in a pod share the same loopback network interface, your app\n",
      "can access the proxy through a port on localhost.\n",
      "RUNNING THE CURL POD WITH AN ADDITIONAL AMBASSADOR CONTAINER\n",
      "To see the ambassador container pattern in action, you’ll create a new pod like the\n",
      "curl pod you created earlier, but this time, instead of running a single container in\n",
      "the pod, you’ll run an additional ambassador container based on a general-purpose\n",
      "kubectl-proxy container image I’ve created and pushed to Docker Hub. You’ll find\n",
      "the Dockerfile for the image in the code archive (in /Chapter08/kubectl-proxy/) if\n",
      "you want to build it yourself.\n",
      " The pod’s manifest is shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: curl-with-ambassador\n",
      "spec:\n",
      "  containers:\n",
      "  - name: main\n",
      "Listing 8.15\n",
      "A pod with an ambassador container: curl-with-ambassador.yaml\n",
      "Container:\n",
      "main\n",
      "Container:\n",
      "ambassador\n",
      "HTTP\n",
      "HTTPS\n",
      "API server\n",
      "Pod\n",
      "Figure 8.6\n",
      "Using an ambassador to connect to the API server\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 277, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "245\n",
      "Talking to the Kubernetes API server\n",
      "    image: tutum/curl\n",
      "    command: [\"sleep\", \"9999999\"]\n",
      "  - name: ambassador                         \n",
      "    image: luksa/kubectl-proxy:1.6.2         \n",
      "The pod spec is almost the same as before, but with a different pod name and an addi-\n",
      "tional container. Run the pod and then enter the main container with\n",
      "$ kubectl exec -it curl-with-ambassador -c main bash\n",
      "root@curl-with-ambassador:/#\n",
      "Your pod now has two containers, and you want to run bash in the main container,\n",
      "hence the -c main option. You don’t need to specify the container explicitly if you\n",
      "want to run the command in the pod’s first container. But if you want to run a com-\n",
      "mand inside any other container, you do need to specify the container’s name using\n",
      "the -c option.\n",
      "TALKING TO THE API SERVER THROUGH THE AMBASSADOR\n",
      "Next you’ll try connecting to the API server through the ambassador container. By\n",
      "default, kubectl proxy binds to port 8001, and because both containers in the pod\n",
      "share the same network interfaces, including loopback, you can point curl to local-\n",
      "host:8001, as shown in the following listing.\n",
      "root@curl-with-ambassador:/# curl localhost:8001\n",
      "{\n",
      "  \"paths\": [\n",
      "    \"/api\",\n",
      "    ...\n",
      "  ]\n",
      "}\n",
      "Success! The output printed by curl is the same response you saw earlier, but this time\n",
      "you didn’t need to deal with authentication tokens and server certificates. \n",
      " To get a clear picture of what exactly happened, refer to figure 8.7. curl sent the\n",
      "plain HTTP request (without any authentication headers) to the proxy running inside\n",
      "the ambassador container, and then the proxy sent an HTTPS request to the API\n",
      "server, handling the client authentication by sending the token and checking the\n",
      "server’s identity by validating its certificate.\n",
      " This is a great example of how an ambassador container can be used to hide the\n",
      "complexities of connecting to an external service and simplify the app running in\n",
      "the main container. The ambassador container is reusable across many different apps,\n",
      "regardless of what language the main app is written in. The downside is that an addi-\n",
      "tional process is running and consuming additional resources.\n",
      "Listing 8.16\n",
      "Accessing the API server through the ambassador container\n",
      "The ambassador container, \n",
      "running the kubectl-proxy image\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 278, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "246\n",
      "CHAPTER 8\n",
      "Accessing pod metadata and other resources from applications\n",
      "8.2.4\n",
      "Using client libraries to talk to the API server\n",
      "If your app only needs to perform a few simple operations on the API server, you can\n",
      "often use a regular HTTP client library and perform simple HTTP requests, especially\n",
      "if you take advantage of the kubectl-proxy ambassador container the way you did in\n",
      "the previous example. But if you plan on doing more than simple API requests, it’s\n",
      "better to use one of the existing Kubernetes API client libraries.\n",
      "USING EXISTING CLIENT LIBRARIES\n",
      "Currently, two Kubernetes API client libraries exist that are supported by the API\n",
      "Machinery special interest group (SIG):\n",
      "Golang client—https:/\n",
      "/github.com/kubernetes/client-go\n",
      "Python—https:/\n",
      "/github.com/kubernetes-incubator/client-python\n",
      "NOTE\n",
      "The Kubernetes community has a number of Special Interest Groups\n",
      "(SIGs) and Working Groups that focus on specific parts of the Kubernetes\n",
      "ecosystem. You’ll find a list of them at https:/\n",
      "/github.com/kubernetes/com-\n",
      "munity/blob/master/sig-list.md.\n",
      "In addition to the two officially supported libraries, here’s a list of user-contributed cli-\n",
      "ent libraries for many other languages:\n",
      "Java client by Fabric8—https:/\n",
      "/github.com/fabric8io/kubernetes-client\n",
      "Java client by Amdatu—https:/\n",
      "/bitbucket.org/amdatulabs/amdatu-kubernetes\n",
      "Node.js client by tenxcloud—https:/\n",
      "/github.com/tenxcloud/node-kubernetes-client\n",
      "Node.js client by GoDaddy—https:/\n",
      "/github.com/godaddy/kubernetes-client\n",
      "PHP—https:/\n",
      "/github.com/devstub/kubernetes-api-php-client\n",
      "Another PHP client—https:/\n",
      "/github.com/maclof/kubernetes-client\n",
      "Container: main\n",
      "API server\n",
      "sleep\n",
      "curl\n",
      "Container: ambassador\n",
      "kubectl proxy\n",
      "Port 8001\n",
      "GET http://localhost:8001\n",
      "GET https://kubernetes:443\n",
      "Authorization: Bearer <token>\n",
      "Pod\n",
      "Figure 8.7\n",
      "Offloading encryption, authentication, and server verification to kubectl proxy in an \n",
      "ambassador container \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 279, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "247\n",
      "Talking to the Kubernetes API server\n",
      "Ruby—https:/\n",
      "/github.com/Ch00k/kubr\n",
      "Another Ruby client—https:/\n",
      "/github.com/abonas/kubeclient\n",
      "Clojure—https:/\n",
      "/github.com/yanatan16/clj-kubernetes-api\n",
      "Scala—https:/\n",
      "/github.com/doriordan/skuber\n",
      "Perl—https:/\n",
      "/metacpan.org/pod/Net::Kubernetes\n",
      "These libraries usually support HTTPS and take care of authentication, so you won’t\n",
      "need to use the ambassador container. \n",
      "AN EXAMPLE OF INTERACTING WITH KUBERNETES WITH THE FABRIC8 JAVA CLIENT\n",
      "To give you a sense of how client libraries enable you to talk to the API server, the fol-\n",
      "lowing listing shows an example of how to list services in a Java app using the Fabric8\n",
      "Kubernetes client.\n",
      "import java.util.Arrays;\n",
      "import io.fabric8.kubernetes.api.model.Pod;\n",
      "import io.fabric8.kubernetes.api.model.PodList;\n",
      "import io.fabric8.kubernetes.client.DefaultKubernetesClient;\n",
      "import io.fabric8.kubernetes.client.KubernetesClient;\n",
      "public class Test {\n",
      "  public static void main(String[] args) throws Exception {\n",
      "    KubernetesClient client = new DefaultKubernetesClient();\n",
      "    // list pods in the default namespace\n",
      "    PodList pods = client.pods().inNamespace(\"default\").list();\n",
      "    pods.getItems().stream()\n",
      "      .forEach(s -> System.out.println(\"Found pod: \" +\n",
      "               s.getMetadata().getName()));\n",
      "    // create a pod\n",
      "    System.out.println(\"Creating a pod\");\n",
      "    Pod pod = client.pods().inNamespace(\"default\")\n",
      "      .createNew()\n",
      "      .withNewMetadata()\n",
      "        .withName(\"programmatically-created-pod\")\n",
      "      .endMetadata()\n",
      "      .withNewSpec()\n",
      "        .addNewContainer()\n",
      "          .withName(\"main\")\n",
      "          .withImage(\"busybox\")\n",
      "          .withCommand(Arrays.asList(\"sleep\", \"99999\"))\n",
      "        .endContainer()\n",
      "      .endSpec()\n",
      "      .done();\n",
      "    System.out.println(\"Created pod: \" + pod);\n",
      "    // edit the pod (add a label to it)\n",
      "    client.pods().inNamespace(\"default\")\n",
      "      .withName(\"programmatically-created-pod\")\n",
      "      .edit()\n",
      "      .editMetadata()\n",
      "Listing 8.17\n",
      "Listing, creating, updating, and deleting pods with the Fabric8 Java client\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 280, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "248\n",
      "CHAPTER 8\n",
      "Accessing pod metadata and other resources from applications\n",
      "        .addToLabels(\"foo\", \"bar\")\n",
      "      .endMetadata()\n",
      "      .done();\n",
      "    System.out.println(\"Added label foo=bar to pod\");\n",
      "    System.out.println(\"Waiting 1 minute before deleting pod...\");\n",
      "    Thread.sleep(60000);\n",
      "    // delete the pod\n",
      "    client.pods().inNamespace(\"default\")\n",
      "      .withName(\"programmatically-created-pod\")\n",
      "      .delete();\n",
      "    System.out.println(\"Deleted the pod\");\n",
      "  }\n",
      "}\n",
      "The code should be self-explanatory, especially because the Fabric8 client exposes\n",
      "a nice, fluent Domain-Specific-Language (DSL) API, which is easy to read and\n",
      "understand.\n",
      "BUILDING YOUR OWN LIBRARY WITH SWAGGER AND OPENAPI\n",
      "If no client is available for your programming language of choice, you can use the\n",
      "Swagger API framework to generate the client library and documentation. The Kuber-\n",
      "netes API server exposes Swagger API definitions at /swaggerapi and OpenAPI spec at\n",
      "/swagger.json. \n",
      " To find out more about the Swagger framework, visit the website at http:/\n",
      "/swagger.io.\n",
      "EXPLORING THE API WITH SWAGGER UI\n",
      "Earlier in the chapter I said I’d point you to a better way of exploring the REST API\n",
      "instead of hitting the REST endpoints with curl. Swagger, which I mentioned in the\n",
      "previous section, is not just a tool for specifying an API, but also provides a web UI for\n",
      "exploring REST APIs if they expose the Swagger API definitions. The better way of\n",
      "exploring REST APIs is through this UI.\n",
      " Kubernetes not only exposes the Swagger API, but it also has Swagger UI inte-\n",
      "grated into the API server, though it’s not enabled by default. You can enable it by\n",
      "running the API server with the --enable-swagger-ui=true option.\n",
      "TIP\n",
      "If you’re using Minikube, you can enable Swagger UI when starting the\n",
      "cluster: minikube start --extra-config=apiserver.Features.Enable-\n",
      "SwaggerUI=true\n",
      "After you enable the UI, you can open it in your browser by pointing it to:\n",
      "http(s)://<api server>:<port>/swagger-ui\n",
      "I urge you to give Swagger UI a try. It not only allows you to browse the Kubernetes\n",
      "API, but also interact with it (you can POST JSON resource manifests, PATCH resources,\n",
      "or DELETE them, for example). \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 281, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "249\n",
      "Summary\n",
      "8.3\n",
      "Summary\n",
      "After reading this chapter, you now know how your app, running inside a pod, can get\n",
      "data about itself, other pods, and other components deployed in the cluster. You’ve\n",
      "learned\n",
      "How a pod’s name, namespace, and other metadata can be exposed to the pro-\n",
      "cess either through environment variables or files in a downwardAPI volume\n",
      "How CPU and memory requests and limits are passed to your app in any unit\n",
      "the app requires\n",
      "How a pod can use downwardAPI volumes to get up-to-date metadata, which\n",
      "may change during the lifetime of the pod (such as labels and annotations) \n",
      "How you can browse the Kubernetes REST API through kubectl proxy\n",
      "How pods can find the API server’s location through environment variables or\n",
      "DNS, similar to any other Service defined in Kubernetes\n",
      "How an application running in a pod can verify that it’s talking to the API\n",
      "server and how it can authenticate itself\n",
      "How using an ambassador container can make talking to the API server from\n",
      "within an app much simpler\n",
      "How client libraries can get you interacting with Kubernetes in minutes\n",
      "In this chapter, you learned how to talk to the API server, so the next step is learning\n",
      "more about how it works. You’ll do that in chapter 11, but before we dive into such\n",
      "details, you still need to learn about two other Kubernetes resources—Deployments\n",
      "and StatefulSets. They’re explained in the next two chapters.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 282, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "250\n",
      "Deployments: updating\n",
      "applications declaratively\n",
      "You now know how to package your app components into containers, group them\n",
      "into pods, provide them with temporary or permanent storage, pass both secret\n",
      "and non-secret config data to them, and allow pods to find and talk to each other.\n",
      "You know how to run a full-fledged system composed of independently running\n",
      "smaller components—microservices, if you will. Is there anything else? \n",
      " Eventually, you’re going to want to update your app. This chapter covers how to\n",
      "update apps running in a Kubernetes cluster and how Kubernetes helps you move\n",
      "toward a true zero-downtime update process. Although this can be achieved using\n",
      "only ReplicationControllers or ReplicaSets, Kubernetes also provides a Deployment\n",
      "This chapter covers\n",
      "Replacing pods with newer versions\n",
      "Updating managed pods\n",
      "Updating pods declaratively using Deployment \n",
      "resources\n",
      "Performing rolling updates\n",
      "Automatically blocking rollouts of bad versions\n",
      "Controlling the rate of the rollout\n",
      "Reverting pods to a previous version\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 283, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "251\n",
      "Updating applications running in pods\n",
      "resource that sits on top of ReplicaSets and enables declarative application updates. If\n",
      "you’re not completely sure what that means, keep reading—it’s not as complicated as\n",
      "it sounds.\n",
      "9.1\n",
      "Updating applications running in pods\n",
      "Let’s start off with a simple example. Imagine having a set of pod instances providing a\n",
      "service to other pods and/or external clients. After reading this book up to this point,\n",
      "you likely recognize that these pods are backed by a ReplicationController or a\n",
      "ReplicaSet. A Service also exists through which clients (apps running in other pods or\n",
      "external clients) access the pods. This is how a basic application looks in Kubernetes\n",
      "(shown in figure 9.1).\n",
      "Initially, the pods run the first version of your application—let’s suppose its image is\n",
      "tagged as v1. You then develop a newer version of the app and push it to an image\n",
      "repository as a new image, tagged as v2. You’d next like to replace all the pods with\n",
      "this new version. Because you can’t change an existing pod’s image after the pod is\n",
      "created, you need to remove the old pods and replace them with new ones running\n",
      "the new image. \n",
      " You have two ways of updating all those pods. You can do one of the following:\n",
      "Delete all existing pods first and then start the new ones.\n",
      "Start new ones and, once they’re up, delete the old ones. You can do this either\n",
      "by adding all the new pods and then deleting all the old ones at once, or\n",
      "sequentially, by adding new pods and removing old ones gradually.\n",
      "Both these strategies have their benefits and drawbacks. The first option would lead to\n",
      "a short period of time when your application is unavailable. The second option\n",
      "requires your app to handle running two versions of the app at the same time. If your\n",
      "app stores data in a data store, the new version shouldn’t modify the data schema or\n",
      "the data in such a way that breaks the previous version.\n",
      "ReplicationController\n",
      "or ReplicaSet\n",
      "Clients\n",
      "Service\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Figure 9.1\n",
      "The basic outline of an \n",
      "application running in Kubernetes\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 284, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "252\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      " How do you perform these two update methods in Kubernetes? First, let’s look at\n",
      "how to do this manually; then, once you know what’s involved in the process, you’ll\n",
      "learn how to have Kubernetes perform the update automatically.\n",
      "9.1.1\n",
      "Deleting old pods and replacing them with new ones\n",
      "You already know how to get a ReplicationController to replace all its pod instances\n",
      "with pods running a new version. You probably remember the pod template of a\n",
      "ReplicationController can be updated at any time. When the ReplicationController\n",
      "creates new instances, it uses the updated pod template to create them.\n",
      " If you have a ReplicationController managing a set of v1 pods, you can easily\n",
      "replace them by modifying the pod template so it refers to version v2 of the image and\n",
      "then deleting the old pod instances. The ReplicationController will notice that no\n",
      "pods match its label selector and it will spin up new instances. The whole process is\n",
      "shown in figure 9.2.\n",
      "This is the simplest way to update a set of pods, if you can accept the short downtime\n",
      "between the time the old pods are deleted and new ones are started.\n",
      "9.1.2\n",
      "Spinning up new pods and then deleting the old ones\n",
      "If you don’t want to see any downtime and your app supports running multiple ver-\n",
      "sions at once, you can turn the process around and first spin up all the new pods and\n",
      "Pod template\n",
      "changed\n",
      "v pods deleted\n",
      "1\n",
      "manually\n",
      "ReplicationController\n",
      "Service\n",
      "Pod: v1\n",
      "Pod: v1\n",
      "Pod\n",
      "template: v2\n",
      "ReplicationController\n",
      "Pod\n",
      "template: v2\n",
      "Pod: v1\n",
      "Service\n",
      "Pod: v2\n",
      "Pod: v2\n",
      "Pod: v2\n",
      "ReplicationController\n",
      "Service\n",
      "Pod: v1\n",
      "Pod: v1\n",
      "Pod\n",
      "template: v1\n",
      "Pod: v1\n",
      "ReplicationController\n",
      "Service\n",
      "Pod: v1\n",
      "Pod: v1\n",
      "Pod: v1\n",
      "Pod\n",
      "template: v2\n",
      "Short period of\n",
      "downtime here\n",
      "v2 pods created by\n",
      "ReplicationController\n",
      "Figure 9.2\n",
      "Updating pods by changing a ReplicationController’s pod template and deleting old Pods\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 285, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "253\n",
      "Updating applications running in pods\n",
      "only then delete the old ones. This will require more hardware resources, because\n",
      "you’ll have double the number of pods running at the same time for a short while. \n",
      " This is a slightly more complex method compared to the previous one, but you\n",
      "should be able to do it by combining what you’ve learned about ReplicationControl-\n",
      "lers and Services so far.\n",
      "SWITCHING FROM THE OLD TO THE NEW VERSION AT ONCE\n",
      "Pods are usually fronted by a Service. It’s possible to have the Service front only the\n",
      "initial version of the pods while you bring up the pods running the new version. Then,\n",
      "once all the new pods are up, you can change the Service’s label selector and have the\n",
      "Service switch over to the new pods, as shown in figure 9.3. This is called a blue-green\n",
      "deployment. After switching over, and once you’re sure the new version functions cor-\n",
      "rectly, you’re free to delete the old pods by deleting the old ReplicationController.\n",
      "NOTE\n",
      "You can change a Service’s pod selector with the kubectl set selec-\n",
      "tor command.\n",
      "PERFORMING A ROLLING UPDATE\n",
      "Instead of bringing up all the new pods and deleting the old pods at once, you can\n",
      "also perform a rolling update, which replaces pods step by step. You do this by slowly\n",
      "scaling down the previous ReplicationController and scaling up the new one. In this\n",
      "case, you’ll want the Service’s pod selector to include both the old and the new pods,\n",
      "so it directs requests toward both sets of pods. See figure 9.4.\n",
      " Doing a rolling update manually is laborious and error-prone. Depending on the\n",
      "number of replicas, you’d need to run a dozen or more commands in the proper\n",
      "order to perform the update process. Luckily, Kubernetes allows you to perform the\n",
      "rolling update with a single command. You’ll learn how in the next section.\n",
      "Service\n",
      "Service\n",
      "ReplicationController:\n",
      "v1\n",
      "Pod: v1\n",
      "Pod: v1\n",
      "Pod\n",
      "template: v1\n",
      "Pod: v1\n",
      "ReplicationController:\n",
      "v2\n",
      "Pod\n",
      "template: v2\n",
      "Pod: v2\n",
      "Pod: v2\n",
      "Pod: v2\n",
      "ReplicationController:\n",
      "v1\n",
      "Pod: v1\n",
      "Pod: v1\n",
      "Pod\n",
      "template: v1\n",
      "Pod: v1\n",
      "ReplicationController:\n",
      "v2\n",
      "Pod\n",
      "template: v2\n",
      "Pod: v2\n",
      "Pod: v2\n",
      "Pod: v2\n",
      "Figure 9.3\n",
      "Switching a Service from the old pods to the new ones\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 286, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "254\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      "9.2\n",
      "Performing an automatic rolling update with a \n",
      "ReplicationController\n",
      "Instead of performing rolling updates using ReplicationControllers manually, you can\n",
      "have kubectl perform them. Using kubectl to perform the update makes the process\n",
      "much easier, but, as you’ll see later, this is now an outdated way of updating apps. Nev-\n",
      "ertheless, we’ll walk through this option first, because it was historically the first way of\n",
      "doing an automatic rolling update, and also allows us to discuss the process without\n",
      "introducing too many additional concepts. \n",
      "9.2.1\n",
      "Running the initial version of the app\n",
      "Obviously, before you can update an app, you need to have an app deployed. You’re\n",
      "going to use a slightly modified version of the kubia NodeJS app you created in chap-\n",
      "ter 2 as your initial version. In case you don’t remember what it does, it’s a simple web-\n",
      "app that returns the pod’s hostname in the HTTP response. \n",
      "CREATING THE V1 APP\n",
      "You’ll change the app so it also returns its version number in the response, which will\n",
      "allow you to distinguish between the different versions you’re about to build. I’ve\n",
      "already built and pushed the app image to Docker Hub under luksa/kubia:v1. The\n",
      "following listing shows the app’s code.\n",
      "const http = require('http');\n",
      "const os = require('os');\n",
      "console.log(\"Kubia server starting...\");\n",
      "Listing 9.1\n",
      "The v1 version of our app: v1/app.js\n",
      "Service\n",
      "Pod: v1\n",
      "Pod: v1\n",
      "Replication\n",
      "Controller:\n",
      "v1\n",
      "v1\n",
      "Replication\n",
      "Controller:\n",
      "v2\n",
      "Pod: v2\n",
      "Service\n",
      "Pod: v2\n",
      "Pod: v2\n",
      "Pod: v2\n",
      "Service\n",
      "Pod: v1\n",
      "Pod: v1\n",
      "Pod: v1\n",
      "Service\n",
      "Pod: v1\n",
      "Pod: v2\n",
      "Pod: v2\n",
      "v2\n",
      "Replication\n",
      "Controller:\n",
      "v1\n",
      "v1\n",
      "Replication\n",
      "Controller:\n",
      "v2\n",
      "v2\n",
      "Replication\n",
      "Controller:\n",
      "v1\n",
      "Replication\n",
      "Controller:\n",
      "v2\n",
      "v2\n",
      "Replication\n",
      "Controller:\n",
      "v1\n",
      "v1\n",
      "v1\n",
      "Replication\n",
      "Controller:\n",
      "v2\n",
      "v2\n",
      "Figure 9.4\n",
      "A rolling update of pods using two ReplicationControllers\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 287, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "255\n",
      "Performing an automatic rolling update with a ReplicationController\n",
      "var handler = function(request, response) {\n",
      "  console.log(\"Received request from \" + request.connection.remoteAddress);\n",
      "  response.writeHead(200);\n",
      "  response.end(\"This is v1 running in pod \" + os.hostname() + \"\\n\");\n",
      "};\n",
      "var www = http.createServer(handler);\n",
      "www.listen(8080);\n",
      "RUNNING THE APP AND EXPOSING IT THROUGH A SERVICE USING A SINGLE YAML FILE\n",
      "To run your app, you’ll create a ReplicationController and a LoadBalancer Service to\n",
      "enable you to access the app externally. This time, rather than create these two\n",
      "resources separately, you’ll create a single YAML for both of them and post it to the\n",
      "Kubernetes API with a single kubectl create command. A YAML manifest can con-\n",
      "tain multiple objects delimited with a line containing three dashes, as shown in the\n",
      "following listing.\n",
      "apiVersion: v1\n",
      "kind: ReplicationController\n",
      "metadata:\n",
      "  name: kubia-v1\n",
      "spec:\n",
      "  replicas: 3\n",
      "  template:\n",
      "    metadata:\n",
      "      name: kubia\n",
      "      labels:                      \n",
      "        app: kubia                 \n",
      "    spec:\n",
      "      containers:\n",
      "      - image: luksa/kubia:v1     \n",
      "        name: nodejs\n",
      "---                         \n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: kubia\n",
      "spec:\n",
      "  type: LoadBalancer\n",
      "  selector:                                        \n",
      "    app: kubia                                     \n",
      "  ports:\n",
      "  - port: 80\n",
      "    targetPort: 8080\n",
      "The YAML defines a ReplicationController called kubia-v1 and a Service called\n",
      "kubia. Go ahead and post the YAML to Kubernetes. After a while, your three v1 pods\n",
      "and the load balancer should all be running, so you can look up the Service’s external\n",
      "IP and start hitting the service with curl, as shown in the following listing.\n",
      "Listing 9.2\n",
      "A YAML containing an RC and a Service: kubia-rc-and-service-v1.yaml\n",
      "The Service fronts all \n",
      "pods created by the \n",
      "ReplicationController.\n",
      "You’re creating a \n",
      "ReplicationController for \n",
      "pods running this image.\n",
      "YAML files can contain \n",
      "multiple resource \n",
      "definitions separated by \n",
      "a line with three dashes.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 288, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "256\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      "$ kubectl get svc kubia\n",
      "NAME      CLUSTER-IP     EXTERNAL-IP       PORT(S)         AGE\n",
      "kubia     10.3.246.195   130.211.109.222   80:32143/TCP    5m\n",
      "$ while true; do curl http://130.211.109.222; done\n",
      "This is v1 running in pod kubia-v1-qr192\n",
      "This is v1 running in pod kubia-v1-kbtsk\n",
      "This is v1 running in pod kubia-v1-qr192\n",
      "This is v1 running in pod kubia-v1-2321o\n",
      "...\n",
      "NOTE\n",
      "If you’re using Minikube or any other Kubernetes cluster where load\n",
      "balancer services aren’t supported, you can use the Service’s node port to\n",
      "access the app. This was explained in chapter 5.\n",
      "9.2.2\n",
      "Performing a rolling update with kubectl\n",
      "Next you’ll create version 2 of the app. To keep things simple, all you’ll do is change\n",
      "the response to say, “This is v2”:\n",
      "  response.end(\"This is v2 running in pod \" + os.hostname() + \"\\n\");\n",
      "This new version is available in the image luksa/kubia:v2 on Docker Hub, so you\n",
      "don’t need to build it yourself.\n",
      "Listing 9.3\n",
      "Getting the Service’s external IP and hitting the service in a loop with curl\n",
      "Pushing updates to the same image tag\n",
      "Modifying an app and pushing the changes to the same image tag isn’t a good idea,\n",
      "but we all tend to do that during development. If you’re modifying the latest tag,\n",
      "that’s not a problem, but when you’re tagging an image with a different tag (for exam-\n",
      "ple, tag v1 instead of latest), once the image is pulled by a worker node, the image\n",
      "will be stored on the node and not pulled again when a new pod using the same\n",
      "image is run (at least that’s the default policy for pulling images).\n",
      "That means any changes you make to the image won’t be picked up if you push them\n",
      "to the same tag. If a new pod is scheduled to the same node, the Kubelet will run the\n",
      "old version of the image. On the other hand, nodes that haven’t run the old version\n",
      "will pull and run the new image, so you might end up with two different versions of\n",
      "the pod running. To make sure this doesn’t happen, you need to set the container’s\n",
      "imagePullPolicy property to Always. \n",
      "You need to be aware that the default imagePullPolicy depends on the image tag.\n",
      "If a container refers to the latest tag (either explicitly or by not specifying the tag at\n",
      "all), imagePullPolicy defaults to Always, but if the container refers to any other\n",
      "tag, the policy defaults to IfNotPresent. \n",
      "When using a tag other than latest, you need to set the imagePullPolicy properly\n",
      "if you make changes to an image without changing the tag. Or better yet, make sure\n",
      "you always push changes to an image under a new tag.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 289, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "257\n",
      "Performing an automatic rolling update with a ReplicationController\n",
      "Keep the curl loop running and open another terminal, where you’ll get the rolling\n",
      "update started. To perform the update, you’ll run the kubectl rolling-update com-\n",
      "mand. All you need to do is tell it which ReplicationController you’re replacing, give a\n",
      "name for the new ReplicationController, and specify the new image you’d like to\n",
      "replace the original one with. The following listing shows the full command for per-\n",
      "forming the rolling update.\n",
      "$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2\n",
      "Created kubia-v2\n",
      "Scaling up kubia-v2 from 0 to 3, scaling down kubia-v1 from 3 to 0 (keep 3 \n",
      "pods available, don't exceed 4 pods)\n",
      "...\n",
      "Because you’re replacing ReplicationController kubia-v1 with one running version 2\n",
      "of your kubia app, you’d like the new ReplicationController to be called kubia-v2\n",
      "and use the luksa/kubia:v2 container image. \n",
      " When you run the command, a new ReplicationController called kubia-v2 is cre-\n",
      "ated immediately. The state of the system at this point is shown in figure 9.5.\n",
      "The new ReplicationController’s pod template references the luksa/kubia:v2 image\n",
      "and its initial desired replica count is set to 0, as you can see in the following listing.\n",
      "$ kubectl describe rc kubia-v2\n",
      "Name:       kubia-v2\n",
      "Namespace:  default\n",
      "Image(s):   luksa/kubia:v2          \n",
      "Selector:   app=kubia,deployment=757d16a0f02f6a5c387f2b5edb62b155\n",
      "Labels:     app=kubia            \n",
      "Replicas:   0 current / 0 desired    \n",
      "...\n",
      "Listing 9.4\n",
      "Initiating a rolling-update of a ReplicationController using kubectl\n",
      "Listing 9.5\n",
      "Describing the new ReplicationController created by the rolling update\n",
      "Pod: v1\n",
      "Pod: v1\n",
      "No v2 pods yet\n",
      "Pod: v1\n",
      "ReplicationController: kubia-v1\n",
      "Image: kubia/v1\n",
      "Replicas: 3\n",
      "ReplicationController: kubia-v2\n",
      "Image: kubia/v2\n",
      "Replicas: 0\n",
      "Figure 9.5\n",
      "The state of the system immediately after starting the rolling update\n",
      "The new \n",
      "ReplicationController \n",
      "refers to the v2 image.\n",
      "Initially, the desired \n",
      "number of replicas is zero.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 290, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "258\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      "UNDERSTANDING THE STEPS PERFORMED BY KUBECTL BEFORE THE ROLLING UPDATE COMMENCES\n",
      "kubectl created this ReplicationController by copying the kubia-v1 controller and\n",
      "changing the image in its pod template. If you look closely at the controller’s label\n",
      "selector, you’ll notice it has been modified, too. It includes not only a simple\n",
      "app=kubia label, but also an additional deployment label which the pods must have in\n",
      "order to be managed by this ReplicationController.\n",
      " You probably know this already, but this is necessary to avoid having both the new\n",
      "and the old ReplicationControllers operating on the same set of pods. But even if pods\n",
      "created by the new controller have the additional deployment label in addition to the\n",
      "app=kubia label, doesn’t this mean they’ll be selected by the first ReplicationControl-\n",
      "ler’s selector, because it’s set to app=kubia? \n",
      " Yes, that’s exactly what would happen, but there’s a catch. The rolling-update pro-\n",
      "cess has modified the selector of the first ReplicationController, as well:\n",
      "$ kubectl describe rc kubia-v1\n",
      "Name:       kubia-v1\n",
      "Namespace:  default\n",
      "Image(s):   luksa/kubia:v1\n",
      "Selector:   app=kubia,deployment=3ddd307978b502a5b975ed4045ae4964-orig \n",
      "Okay, but doesn’t this mean the first controller now sees zero pods matching its selec-\n",
      "tor, because the three pods previously created by it contain only the app=kubia label?\n",
      "No, because kubectl had also modified the labels of the live pods just before modify-\n",
      "ing the ReplicationController’s selector:\n",
      "$ kubectl get po --show-labels\n",
      "NAME            READY  STATUS   RESTARTS  AGE  LABELS\n",
      "kubia-v1-m33mv  1/1    Running  0         2m   app=kubia,deployment=3ddd...\n",
      "kubia-v1-nmzw9  1/1    Running  0         2m   app=kubia,deployment=3ddd...\n",
      "kubia-v1-cdtey  1/1    Running  0         2m   app=kubia,deployment=3ddd...\n",
      "If this is getting too complicated, examine figure 9.6, which shows the pods, their\n",
      "labels, and the two ReplicationControllers, along with their pod selectors.\n",
      "ReplicationController: kubia-v1\n",
      "Replicas: 3\n",
      "Selector: app=kubia,\n",
      "deployment=3ddd…\n",
      "ReplicationController: kubia-v2\n",
      "Replicas: 0\n",
      "Selector: app=kubia,\n",
      "deployment=757d...\n",
      "deployment: 3ddd...\n",
      "app: kubia\n",
      "Pod: v1\n",
      "deployment: 3ddd...\n",
      "app: kubia\n",
      "Pod: v1\n",
      "deployment: 3ddd...\n",
      "app: kubia\n",
      "Pod: v1\n",
      "Figure 9.6\n",
      "Detailed state of the old and new ReplicationControllers and pods at the start of a rolling \n",
      "update\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 291, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "259\n",
      "Performing an automatic rolling update with a ReplicationController\n",
      "kubectl had to do all this before even starting to scale anything up or down. Now\n",
      "imagine doing the rolling update manually. It’s easy to see yourself making a mistake\n",
      "here and possibly having the ReplicationController kill off all your pods—pods that\n",
      "are actively serving your production clients!\n",
      "REPLACING OLD PODS WITH NEW ONES BY SCALING THE TWO REPLICATIONCONTROLLERS\n",
      "After setting up all this, kubectl starts replacing pods by first scaling up the new\n",
      "controller to 1. The controller thus creates the first v2 pod. kubectl then scales\n",
      "down the old ReplicationController by 1. This is shown in the next two lines printed\n",
      "by kubectl:\n",
      "Scaling kubia-v2 up to 1\n",
      "Scaling kubia-v1 down to 2\n",
      "Because the Service is targeting all pods with the app=kubia label, you should start see-\n",
      "ing your curl requests redirected to the new v2 pod every few loop iterations:\n",
      "This is v2 running in pod kubia-v2-nmzw9      \n",
      "This is v1 running in pod kubia-v1-kbtsk\n",
      "This is v1 running in pod kubia-v1-2321o\n",
      "This is v2 running in pod kubia-v2-nmzw9      \n",
      "...\n",
      "Figure 9.7 shows the current state of the system.\n",
      "As kubectl continues with the rolling update, you start seeing a progressively bigger\n",
      "percentage of requests hitting v2 pods, as the update process deletes more of the v1\n",
      "pods and replaces them with those running your new image. Eventually, the original\n",
      "Requests hitting the pod \n",
      "running the new version\n",
      "ReplicationController: kubia-v1\n",
      "Replicas: 2\n",
      "Selector: app=kubia,\n",
      "deployment=3ddd…\n",
      "ReplicationController: kubia-v2\n",
      "Replicas: 1\n",
      "Selector: app=kubia,\n",
      "deployment=757d…\n",
      "deployment: 3ddd...\n",
      "app: kubia\n",
      "Pod: v1\n",
      "deployment: 3ddd...\n",
      "app: kubia\n",
      "Pod: v1\n",
      "deployment: 757d...\n",
      "app: kubia\n",
      "Pod: v2\n",
      "curl\n",
      "Service\n",
      "Selector: app=kubia\n",
      "Figure 9.7\n",
      "The Service is redirecting requests to both the old and new pods during the \n",
      "rolling update.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 292, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "260\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      "ReplicationController is scaled to zero, causing the last v1 pod to be deleted, which\n",
      "means the Service will now be backed by v2 pods only. At that point, kubectl will\n",
      "delete the original ReplicationController and the update process will be finished, as\n",
      "shown in the following listing.\n",
      "...\n",
      "Scaling kubia-v2 up to 2\n",
      "Scaling kubia-v1 down to 1\n",
      "Scaling kubia-v2 up to 3\n",
      "Scaling kubia-v1 down to 0\n",
      "Update succeeded. Deleting kubia-v1\n",
      "replicationcontroller \"kubia-v1\" rolling updated to \"kubia-v2\"\n",
      "You’re now left with only the kubia-v2 ReplicationController and three v2 pods. All\n",
      "throughout this update process, you’ve hit your service and gotten a response every\n",
      "time. You have, in fact, performed a rolling update with zero downtime. \n",
      "9.2.3\n",
      "Understanding why kubectl rolling-update is now obsolete\n",
      "At the beginning of this section, I mentioned an even better way of doing updates\n",
      "than through kubectl rolling-update. What’s so wrong with this process that a bet-\n",
      "ter one had to be introduced? \n",
      " Well, for starters, I, for one, don’t like Kubernetes modifying objects I’ve created.\n",
      "Okay, it’s perfectly fine for the scheduler to assign a node to my pods after I create\n",
      "them, but Kubernetes modifying the labels of my pods and the label selectors of my\n",
      "ReplicationControllers is something that I don’t expect and could cause me to go\n",
      "around the office yelling at my colleagues, “Who’s been messing with my controllers!?!?” \n",
      " But even more importantly, if you’ve paid close attention to the words I’ve used,\n",
      "you probably noticed that all this time I said explicitly that the kubectl client was the\n",
      "one performing all these steps of the rolling update. \n",
      " You can see this by turning on verbose logging with the --v option when triggering\n",
      "the rolling update:\n",
      "$ kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2 --v 6\n",
      "TIP\n",
      "Using the --v 6 option increases the logging level enough to let you see\n",
      "the requests kubectl is sending to the API server.\n",
      "Using this option, kubectl will print out each HTTP request it sends to the Kuberne-\n",
      "tes API server. You’ll see PUT requests to\n",
      "/api/v1/namespaces/default/replicationcontrollers/kubia-v1\n",
      "which is the RESTful URL representing your kubia-v1 ReplicationController resource.\n",
      "These requests are the ones scaling down your ReplicationController, which shows\n",
      "Listing 9.6\n",
      "The final steps performed by kubectl rolling-update\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 293, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "261\n",
      "Using Deployments for updating apps declaratively\n",
      "that the kubectl client is the one doing the scaling, instead of it being performed by\n",
      "the Kubernetes master. \n",
      "TIP\n",
      "Use the verbose logging option when running other kubectl commands,\n",
      "to learn more about the communication between kubectl and the API server. \n",
      "But why is it such a bad thing that the update process is being performed by the client\n",
      "instead of on the server? Well, in your case, the update went smoothly, but what if you\n",
      "lost network connectivity while kubectl was performing the update? The update pro-\n",
      "cess would be interrupted mid-way. Pods and ReplicationControllers would end up in\n",
      "an intermediate state.\n",
      " Another reason why performing an update like this isn’t as good as it could be is\n",
      "because it’s imperative. Throughout this book, I’ve stressed how Kubernetes is about\n",
      "you telling it the desired state of the system and having Kubernetes achieve that\n",
      "state on its own, by figuring out the best way to do it. This is how pods are deployed\n",
      "and how pods are scaled up and down. You never tell Kubernetes to add an addi-\n",
      "tional pod or remove an excess one—you change the number of desired replicas\n",
      "and that’s it.\n",
      " Similarly, you will also want to change the desired image tag in your pod defini-\n",
      "tions and have Kubernetes replace the pods with new ones running the new image.\n",
      "This is exactly what drove the introduction of a new resource called a Deployment,\n",
      "which is now the preferred way of deploying applications in Kubernetes. \n",
      "9.3\n",
      "Using Deployments for updating apps declaratively\n",
      "A Deployment is a higher-level resource meant for deploying applications and\n",
      "updating them declaratively, instead of doing it through a ReplicationController or\n",
      "a ReplicaSet, which are both considered lower-level concepts.\n",
      " When you create a Deployment, a ReplicaSet resource is created underneath\n",
      "(eventually more of them). As you may remember from chapter 4, ReplicaSets are a\n",
      "new generation of ReplicationControllers, and should be used instead of them. Replica-\n",
      "Sets replicate and manage pods, as well. When using a Deployment, the actual pods\n",
      "are created and managed by the Deployment’s ReplicaSets, not by the Deployment\n",
      "directly (the relationship is shown in figure 9.8).\n",
      "You might wonder why you’d want to complicate things by introducing another object\n",
      "on top of a ReplicationController or ReplicaSet, when they’re what suffices to keep a set\n",
      "of pod instances running. As the rolling update example in section 9.2 demonstrates,\n",
      "when updating the app, you need to introduce an additional ReplicationController and\n",
      "Pods\n",
      "ReplicaSet\n",
      "Deployment\n",
      "Figure 9.8\n",
      "A Deployment is backed \n",
      "by a ReplicaSet, which supervises the \n",
      "deployment’s pods.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 294, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "262\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      "coordinate the two controllers to dance around each other without stepping on each\n",
      "other’s toes. You need something coordinating this dance. A Deployment resource\n",
      "takes care of that (it’s not the Deployment resource itself, but the controller process\n",
      "running in the Kubernetes control plane that does that; but we’ll get to that in chap-\n",
      "ter 11).\n",
      " Using a Deployment instead of the lower-level constructs makes updating an app\n",
      "much easier, because you’re defining the desired state through the single Deployment\n",
      "resource and letting Kubernetes take care of the rest, as you’ll see in the next few pages.\n",
      "9.3.1\n",
      "Creating a Deployment\n",
      "Creating a Deployment isn’t that different from creating a ReplicationController. A\n",
      "Deployment is also composed of a label selector, a desired replica count, and a pod\n",
      "template. In addition to that, it also contains a field, which specifies a deployment\n",
      "strategy that defines how an update should be performed when the Deployment\n",
      "resource is modified.  \n",
      "CREATING A DEPLOYMENT MANIFEST\n",
      "Let’s see how to use the kubia-v1 ReplicationController example from earlier in this\n",
      "chapter and modify it so it describes a Deployment instead of a ReplicationController.\n",
      "As you’ll see, this requires only three trivial changes. The following listing shows the\n",
      "modified YAML.\n",
      "apiVersion: apps/v1beta1          \n",
      "kind: Deployment                  \n",
      "metadata:\n",
      "  name: kubia          \n",
      "spec:\n",
      "  replicas: 3\n",
      "  template:\n",
      "    metadata:\n",
      "      name: kubia\n",
      "      labels:\n",
      "        app: kubia\n",
      "    spec:\n",
      "      containers:\n",
      "      - image: luksa/kubia:v1\n",
      "        name: nodejs\n",
      "NOTE\n",
      "You’ll find an older version of the Deployment resource in extensions/\n",
      "v1beta1, and a newer one in apps/v1beta2 with different required fields and\n",
      "different defaults. Be aware that kubectl explain shows the older version.\n",
      "Because the ReplicationController from before was managing a specific version of the\n",
      "pods, you called it kubia-v1. A Deployment, on the other hand, is above that version\n",
      "stuff. At a given point in time, the Deployment can have multiple pod versions run-\n",
      "ning under its wing, so its name shouldn’t reference the app version.\n",
      "Listing 9.7\n",
      "A Deployment definition: kubia-deployment-v1.yaml\n",
      "Deployments are in the apps \n",
      "API group, version v1beta1.\n",
      "You’ve changed the kind \n",
      "from ReplicationController \n",
      "to Deployment.\n",
      "There’s no need to include \n",
      "the version in the name of \n",
      "the Deployment.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 295, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "263\n",
      "Using Deployments for updating apps declaratively\n",
      "CREATING THE DEPLOYMENT RESOURCE\n",
      "Before you create this Deployment, make sure you delete any ReplicationControllers\n",
      "and pods that are still running, but keep the kubia Service for now. You can use the\n",
      "--all switch to delete all those ReplicationControllers like this:\n",
      "$ kubectl delete rc --all\n",
      "You’re now ready to create the Deployment: \n",
      "$ kubectl create -f kubia-deployment-v1.yaml --record\n",
      "deployment \"kubia\" created\n",
      "TIP\n",
      "Be sure to include the --record command-line option when creating it.\n",
      "This records the command in the revision history, which will be useful later.\n",
      "DISPLAYING THE STATUS OF THE DEPLOYMENT ROLLOUT\n",
      "You can use the usual kubectl get deployment and the kubectl describe deployment\n",
      "commands to see details of the Deployment, but let me point you to an additional\n",
      "command, which is made specifically for checking a Deployment’s status:\n",
      "$ kubectl rollout status deployment kubia\n",
      "deployment kubia successfully rolled out\n",
      "According to this, the Deployment has been successfully rolled out, so you should see\n",
      "the three pod replicas up and running. Let’s see:\n",
      "$ kubectl get po\n",
      "NAME                     READY     STATUS    RESTARTS   AGE\n",
      "kubia-1506449474-otnnh   1/1       Running   0          14s\n",
      "kubia-1506449474-vmn7s   1/1       Running   0          14s\n",
      "kubia-1506449474-xis6m   1/1       Running   0          14s\n",
      "UNDERSTANDING HOW DEPLOYMENTS CREATE REPLICASETS, WHICH THEN CREATE THE PODS\n",
      "Take note of the names of these pods. Earlier, when you used a ReplicationController\n",
      "to create pods, their names were composed of the name of the controller plus a ran-\n",
      "domly generated string (for example, kubia-v1-m33mv). The three pods created by\n",
      "the Deployment include an additional numeric value in the middle of their names.\n",
      "What is that exactly?\n",
      " The number corresponds to the hashed value of the pod template in the Deploy-\n",
      "ment and the ReplicaSet managing these pods. As we said earlier, a Deployment\n",
      "doesn’t manage pods directly. Instead, it creates ReplicaSets and leaves the managing\n",
      "to them, so let’s look at the ReplicaSet created by your Deployment:\n",
      "$ kubectl get replicasets\n",
      "NAME               DESIRED   CURRENT   AGE\n",
      "kubia-1506449474   3         3         10s\n",
      "The ReplicaSet’s name also contains the hash value of its pod template. As you’ll see\n",
      "later, a Deployment creates multiple ReplicaSets—one for each version of the pod\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 296, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "264\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      "template. Using the hash value of the pod template like this allows the Deployment\n",
      "to always use the same (possibly existing) ReplicaSet for a given version of the pod\n",
      "template.\n",
      "ACCESSING THE PODS THROUGH THE SERVICE\n",
      "With the three replicas created by this ReplicaSet now running, you can use the Ser-\n",
      "vice you created a while ago to access them, because you made the new pods’ labels\n",
      "match the Service’s label selector. \n",
      " Up until this point, you probably haven’t seen a good-enough reason why you should\n",
      "use Deployments over ReplicationControllers. Luckily, creating a Deployment also hasn’t\n",
      "been any harder than creating a ReplicationController. Now, you’ll start doing things\n",
      "with this Deployment, which will make it clear why Deployments are superior. This will\n",
      "become clear in the next few moments, when you see how updating the app through\n",
      "a Deployment resource compares to updating it through a ReplicationController.\n",
      "9.3.2\n",
      "Updating a Deployment\n",
      "Previously, when you ran your app using a ReplicationController, you had to explicitly\n",
      "tell Kubernetes to perform the update by running kubectl rolling-update. You even\n",
      "had to specify the name for the new ReplicationController that should replace the old\n",
      "one. Kubernetes replaced all the original pods with new ones and deleted the original\n",
      "ReplicationController at the end of the process. During the process, you basically had\n",
      "to stay around, keeping your terminal open and waiting for kubectl to finish the roll-\n",
      "ing update. \n",
      " Now compare this to how you’re about to update a Deployment. The only thing\n",
      "you need to do is modify the pod template defined in the Deployment resource and\n",
      "Kubernetes will take all the steps necessary to get the actual system state to what’s\n",
      "defined in the resource. Similar to scaling a ReplicationController or ReplicaSet up or\n",
      "down, all you need to do is reference a new image tag in the Deployment’s pod tem-\n",
      "plate and leave it to Kubernetes to transform your system so it matches the new\n",
      "desired state.\n",
      "UNDERSTANDING THE AVAILABLE DEPLOYMENT STRATEGIES\n",
      "How this new state should be achieved is governed by the deployment strategy config-\n",
      "ured on the Deployment itself. The default strategy is to perform a rolling update (the\n",
      "strategy is called RollingUpdate). The alternative is the Recreate strategy, which\n",
      "deletes all the old pods at once and then creates new ones, similar to modifying a\n",
      "ReplicationController’s pod template and then deleting all the pods (we talked about\n",
      "this in section 9.1.1).\n",
      " The Recreate strategy causes all old pods to be deleted before the new ones are\n",
      "created. Use this strategy when your application doesn’t support running multiple ver-\n",
      "sions in parallel and requires the old version to be stopped completely before the\n",
      "new one is started. This strategy does involve a short period of time when your app\n",
      "becomes completely unavailable.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 297, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "265\n",
      "Using Deployments for updating apps declaratively\n",
      " The RollingUpdate strategy, on the other hand, removes old pods one by one,\n",
      "while adding new ones at the same time, keeping the application available throughout\n",
      "the whole process, and ensuring there’s no drop in its capacity to handle requests.\n",
      "This is the default strategy. The upper and lower limits for the number of pods above\n",
      "or below the desired replica count are configurable. You should use this strategy only\n",
      "when your app can handle running both the old and new version at the same time.\n",
      "SLOWING DOWN THE ROLLING UPDATE FOR DEMO PURPOSES\n",
      "In the next exercise, you’ll use the RollingUpdate strategy, but you need to slow down\n",
      "the update process a little, so you can see that the update is indeed performed in a\n",
      "rolling fashion. You can do that by setting the minReadySeconds attribute on the\n",
      "Deployment. We’ll explain what this attribute does by the end of this chapter. For\n",
      "now, set it to 10 seconds with the kubectl patch command.\n",
      "$ kubectl patch deployment kubia -p '{\"spec\": {\"minReadySeconds\": 10}}'\n",
      "\"kubia\" patched\n",
      "TIP\n",
      "The kubectl patch command is useful for modifying a single property\n",
      "or a limited number of properties of a resource without having to edit its defi-\n",
      "nition in a text editor.\n",
      "You used the patch command to change the spec of the Deployment. This doesn’t\n",
      "cause any kind of update to the pods, because you didn’t change the pod template.\n",
      "Changing other Deployment properties, like the desired replica count or the deploy-\n",
      "ment strategy, also doesn’t trigger a rollout, because it doesn’t affect the existing indi-\n",
      "vidual pods in any way.\n",
      "TRIGGERING THE ROLLING UPDATE\n",
      "If you’d like to track the update process as it progresses, first run the curl loop again\n",
      "in another terminal to see what’s happening with the requests (don’t forget to replace\n",
      "the IP with the actual external IP of your service):\n",
      "$ while true; do curl http://130.211.109.222; done\n",
      "To trigger the actual rollout, you’ll change the image used in the single pod container\n",
      "to luksa/kubia:v2. Instead of editing the whole YAML of the Deployment object or\n",
      "using the patch command to change the image, you’ll use the kubectl set image\n",
      "command, which allows changing the image of any resource that contains a container\n",
      "(ReplicationControllers, ReplicaSets, Deployments, and so on). You’ll use it to modify\n",
      "your Deployment like this:\n",
      "$ kubectl set image deployment kubia nodejs=luksa/kubia:v2\n",
      "deployment \"kubia\" image updated\n",
      "When you execute this command, you’re updating the kubia Deployment’s pod tem-\n",
      "plate so the image used in its nodejs container is changed to luksa/kubia:v2 (from\n",
      ":v1). This is shown in figure 9.9.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 298, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "266\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      "Ways of modifying Deployments and other resources\n",
      "Over the course of this book, you’ve learned several ways how to modify an existing\n",
      "object. Let’s list all of them together to refresh your memory.\n",
      "All these methods are equivalent as far as Deployments go. What they do is change\n",
      "the Deployment’s specification. This change then triggers the rollout process.\n",
      "Image registry\n",
      "Pod template\n",
      "Deployment\n",
      "kubectl set image…\n",
      "luksa/kubia:v2\n",
      "Container:\n",
      "nodejs\n",
      ":v1\n",
      ":v2\n",
      "Image registry\n",
      "Pod template\n",
      "Deployment\n",
      "Container:\n",
      "nodejs\n",
      ":v1\n",
      ":v2\n",
      "Figure 9.9\n",
      "Updating a Deployment’s pod template to point to a new image\n",
      "Table 9.1\n",
      "Modifying an existing resource in Kubernetes\n",
      "Method\n",
      "What it does\n",
      "kubectl edit\n",
      "Opens the object’s manifest in your default editor. After making \n",
      "changes, saving the file, and exiting the editor, the object is updated.\n",
      "Example: kubectl edit deployment kubia\n",
      "kubectl patch\n",
      "Modifies individual properties of an object.\n",
      "Example: kubectl patch deployment kubia -p '{\"spec\": \n",
      "{\"template\": {\"spec\": {\"containers\": [{\"name\": \n",
      "\"nodejs\", \"image\": \"luksa/kubia:v2\"}]}}}}'\n",
      "kubectl apply\n",
      "Modifies the object by applying property values from a full YAML or \n",
      "JSON file. If the object specified in the YAML/JSON doesn’t exist yet, \n",
      "it’s created. The file needs to contain the full definition of the \n",
      "resource (it can’t include only the fields you want to update, as is the \n",
      "case with kubectl patch).\n",
      "Example: kubectl apply -f kubia-deployment-v2.yaml\n",
      "kubectl replace\n",
      "Replaces the object with a new one from a YAML/JSON file. In con-\n",
      "trast to the apply command, this command requires the object to \n",
      "exist; otherwise it prints an error.\n",
      "Example: kubectl replace -f kubia-deployment-v2.yaml\n",
      "kubectl set image\n",
      "Changes the container image defined in a Pod, ReplicationControl-\n",
      "ler’s template, Deployment, DaemonSet, Job, or ReplicaSet.\n",
      "Example: kubectl set image deployment kubia \n",
      "nodejs=luksa/kubia:v2\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 299, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "267\n",
      "Using Deployments for updating apps declaratively\n",
      "If you’ve run the curl loop, you’ll see requests initially hitting only the v1 pods; then\n",
      "more and more of them hit the v2 pods until, finally, all of them hit only the remain-\n",
      "ing v2 pods, after all v1 pods are deleted. This works much like the rolling update per-\n",
      "formed by kubectl.\n",
      "UNDERSTANDING THE AWESOMENESS OF DEPLOYMENTS\n",
      "Let’s think about what has happened. By changing the pod template in your Deploy-\n",
      "ment resource, you’ve updated your app to a newer version—by changing a single\n",
      "field! \n",
      " The controllers running as part of the Kubernetes control plane then performed\n",
      "the update. The process wasn’t performed by the kubectl client, like it was when you\n",
      "used kubectl rolling-update. I don’t know about you, but I think that’s simpler than\n",
      "having to run a special command telling Kubernetes what to do and then waiting\n",
      "around for the process to be completed.\n",
      "NOTE\n",
      "Be aware that if the pod template in the Deployment references a\n",
      "ConfigMap (or a Secret), modifying the ConfigMap will not trigger an\n",
      "update. One way to trigger an update when you need to modify an app’s con-\n",
      "fig is to create a new ConfigMap and modify the pod template so it references\n",
      "the new ConfigMap.\n",
      "The events that occurred below the Deployment’s surface during the update are simi-\n",
      "lar to what happened during the kubectl rolling-update. An additional ReplicaSet\n",
      "was created and it was then scaled up slowly, while the previous ReplicaSet was scaled\n",
      "down to zero (the initial and final states are shown in figure 9.10).\n",
      "You can still see the old ReplicaSet next to the new one if you list them:\n",
      "$ kubectl get rs\n",
      "NAME               DESIRED   CURRENT   AGE\n",
      "kubia-1506449474   0         0         24m\n",
      "kubia-1581357123   3         3         23m\n",
      "Pods: v1\n",
      "ReplicaSet: v1\n",
      "Replicas: --\n",
      "Before\n",
      "After\n",
      "ReplicaSet: v2\n",
      "Replicas: ++\n",
      "Deployment\n",
      "Pods: v2\n",
      "ReplicaSet: v1\n",
      "ReplicaSet: v2\n",
      "Deployment\n",
      "Figure 9.10\n",
      "A Deployment at the start and end of a rolling update\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 300, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "268\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      "Similar to ReplicationControllers, all your new pods are now managed by the new\n",
      "ReplicaSet. Unlike before, the old ReplicaSet is still there, whereas the old Replication-\n",
      "Controller was deleted at the end of the rolling-update process. You’ll soon see what\n",
      "the purpose of this inactive ReplicaSet is. \n",
      " But you shouldn’t care about ReplicaSets here, because you didn’t create them\n",
      "directly. You created and operated only on the Deployment resource; the underlying\n",
      "ReplicaSets are an implementation detail. You’ll agree that managing a single Deploy-\n",
      "ment object is much easier compared to dealing with and keeping track of multiple\n",
      "ReplicationControllers. \n",
      " Although this difference may not be so apparent when everything goes well with a\n",
      "rollout, it becomes much more obvious when you hit a problem during the rollout\n",
      "process. Let’s simulate one problem right now.\n",
      "9.3.3\n",
      "Rolling back a deployment\n",
      "You’re currently running version v2 of your image, so you’ll need to prepare version 3\n",
      "first. \n",
      "CREATING VERSION 3 OF YOUR APP\n",
      "In version 3, you’ll introduce a bug that makes your app handle only the first four\n",
      "requests properly. All requests from the fifth request onward will return an internal\n",
      "server error (HTTP status code 500). You’ll simulate this by adding an if statement at\n",
      "the beginning of the handler function. The following listing shows the new code, with\n",
      "all required changes shown in bold.\n",
      "const http = require('http');\n",
      "const os = require('os');\n",
      "var requestCount = 0;\n",
      "console.log(\"Kubia server starting...\");\n",
      "var handler = function(request, response) {\n",
      "  console.log(\"Received request from \" + request.connection.remoteAddress);\n",
      "  if (++requestCount >= 5) {\n",
      "    response.writeHead(500);\n",
      "    response.end(\"Some internal error has occurred! This is pod \" + \n",
      "os.hostname() + \"\\n\");\n",
      "    return;\n",
      "  }\n",
      "  response.writeHead(200);\n",
      "  response.end(\"This is v3 running in pod \" + os.hostname() + \"\\n\");\n",
      "};\n",
      "var www = http.createServer(handler);\n",
      "www.listen(8080); \n",
      "As you can see, on the fifth and all subsequent requests, the code returns a 500 error\n",
      "with the message “Some internal error has occurred...”\n",
      "Listing 9.8\n",
      "Version 3 of our app (a broken version): v3/app.js\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 301, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "269\n",
      "Using Deployments for updating apps declaratively\n",
      "DEPLOYING VERSION 3\n",
      "I’ve made the v3 version of the image available as luksa/kubia:v3. You’ll deploy this\n",
      "new version by changing the image in the Deployment specification again: \n",
      "$ kubectl set image deployment kubia nodejs=luksa/kubia:v3\n",
      "deployment \"kubia\" image updated\n",
      "You can follow the progress of the rollout with kubectl rollout status:\n",
      "$ kubectl rollout status deployment kubia\n",
      "Waiting for rollout to finish: 1 out of 3 new replicas have been updated...\n",
      "Waiting for rollout to finish: 2 out of 3 new replicas have been updated...\n",
      "Waiting for rollout to finish: 1 old replicas are pending termination...\n",
      "deployment \"kubia\" successfully rolled out\n",
      "The new version is now live. As the following listing shows, after a few requests, your\n",
      "web clients start receiving errors.\n",
      "$ while true; do curl http://130.211.109.222; done\n",
      "This is v3 running in pod kubia-1914148340-lalmx\n",
      "This is v3 running in pod kubia-1914148340-bz35w\n",
      "This is v3 running in pod kubia-1914148340-w0voh\n",
      "...\n",
      "This is v3 running in pod kubia-1914148340-w0voh\n",
      "Some internal error has occurred! This is pod kubia-1914148340-bz35w\n",
      "This is v3 running in pod kubia-1914148340-w0voh\n",
      "Some internal error has occurred! This is pod kubia-1914148340-lalmx\n",
      "This is v3 running in pod kubia-1914148340-w0voh\n",
      "Some internal error has occurred! This is pod kubia-1914148340-lalmx\n",
      "Some internal error has occurred! This is pod kubia-1914148340-bz35w\n",
      "Some internal error has occurred! This is pod kubia-1914148340-w0voh\n",
      "UNDOING A ROLLOUT\n",
      "You can’t have your users experiencing internal server errors, so you need to do some-\n",
      "thing about it fast. In section 9.3.6 you’ll see how to block bad rollouts automatically,\n",
      "but for now, let’s see what you can do about your bad rollout manually. Luckily,\n",
      "Deployments make it easy to roll back to the previously deployed version by telling\n",
      "Kubernetes to undo the last rollout of a Deployment:\n",
      "$ kubectl rollout undo deployment kubia\n",
      "deployment \"kubia\" rolled back\n",
      "This rolls the Deployment back to the previous revision. \n",
      "TIP\n",
      "The undo command can also be used while the rollout process is still in\n",
      "progress to essentially abort the rollout. Pods already created during the roll-\n",
      "out process are removed and replaced with the old ones again.\n",
      "Listing 9.9\n",
      "Hitting your broken version 3\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 302, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "270\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      "DISPLAYING A DEPLOYMENT’S ROLLOUT HISTORY\n",
      "Rolling back a rollout is possible because Deployments keep a revision history. As\n",
      "you’ll see later, the history is stored in the underlying ReplicaSets. When a rollout\n",
      "completes, the old ReplicaSet isn’t deleted, and this enables rolling back to any revi-\n",
      "sion, not only the previous one. The revision history can be displayed with the\n",
      "kubectl rollout history command:\n",
      "$ kubectl rollout history deployment kubia\n",
      "deployments \"kubia\":\n",
      "REVISION    CHANGE-CAUSE\n",
      "2           kubectl set image deployment kubia nodejs=luksa/kubia:v2\n",
      "3           kubectl set image deployment kubia nodejs=luksa/kubia:v3\n",
      "Remember the --record command-line option you used when creating the Deploy-\n",
      "ment? Without it, the CHANGE-CAUSE column in the revision history would be empty,\n",
      "making it much harder to figure out what’s behind each revision.\n",
      "ROLLING BACK TO A SPECIFIC DEPLOYMENT REVISION\n",
      "You can roll back to a specific revision by specifying the revision in the undo com-\n",
      "mand. For example, if you want to roll back to the first version, you’d execute the fol-\n",
      "lowing command:\n",
      "$ kubectl rollout undo deployment kubia --to-revision=1\n",
      "Remember the inactive ReplicaSet left over when you modified the Deployment the\n",
      "first time? The ReplicaSet represents the first revision of your Deployment. All Replica-\n",
      "Sets created by a Deployment represent the complete revision history, as shown in fig-\n",
      "ure 9.11. Each ReplicaSet stores the complete information of the Deployment at that\n",
      "specific revision, so you shouldn’t delete it manually. If you do, you’ll lose that specific\n",
      "revision from the Deployment’s history, preventing you from rolling back to it.\n",
      "But having old ReplicaSets cluttering your ReplicaSet list is not ideal, so the length of\n",
      "the revision history is limited by the revisionHistoryLimit property on the Deploy-\n",
      "ment resource. It defaults to two, so normally only the current and the previous revision\n",
      "are shown in the history (and only the current and the previous ReplicaSet are pre-\n",
      "served). Older ReplicaSets are deleted automatically. \n",
      "Deployment\n",
      "v1 ReplicaSet\n",
      "ReplicaSet\n",
      "Pods: v1\n",
      "ReplicaSet\n",
      "ReplicaSet\n",
      "ReplicaSet\n",
      "Revision 2\n",
      "Revision 4\n",
      "Revision 3\n",
      "Revision 1\n",
      "Revision history\n",
      "Current revision\n",
      "Figure 9.11\n",
      "A Deployment’s ReplicaSets also act as its revision history.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 303, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "271\n",
      "Using Deployments for updating apps declaratively\n",
      "NOTE\n",
      "The extensions/v1beta1 version of Deployments doesn’t have a default\n",
      "revisionHistoryLimit, whereas the default in version apps/v1beta2 is 10.\n",
      "9.3.4\n",
      "Controlling the rate of the rollout\n",
      "When you performed the rollout to v3 and tracked its progress with the kubectl\n",
      "rollout status command, you saw that first a new pod was created, and when it\n",
      "became available, one of the old pods was deleted and another new pod was created.\n",
      "This continued until there were no old pods left. The way new pods are created and\n",
      "old ones are deleted is configurable through two additional properties of the rolling\n",
      "update strategy. \n",
      "INTRODUCING THE MAXSURGE AND MAXUNAVAILABLE PROPERTIES OF THE ROLLING UPDATE STRATEGY\n",
      "Two properties affect how many pods are replaced at once during a Deployment’s roll-\n",
      "ing update. They are maxSurge and maxUnavailable and can be set as part of the\n",
      "rollingUpdate sub-property of the Deployment’s strategy attribute, as shown in\n",
      "the following listing.\n",
      "spec:\n",
      "  strategy:\n",
      "    rollingUpdate:\n",
      "      maxSurge: 1\n",
      "      maxUnavailable: 0\n",
      "    type: RollingUpdate\n",
      "What these properties do is explained in table 9.2.\n",
      "Because the desired replica count in your case was three, and both these properties\n",
      "default to 25%, maxSurge allowed the number of all pods to reach four, and\n",
      "Listing 9.10\n",
      "Specifying parameters for the rollingUpdate strategy\n",
      "Table 9.2\n",
      "Properties for configuring the rate of the rolling update\n",
      "Property\n",
      "What it does\n",
      "maxSurge\n",
      "Determines how many pod instances you allow to exist above the desired replica \n",
      "count configured on the Deployment. It defaults to 25%, so there can be at most \n",
      "25% more pod instances than the desired count. If the desired replica count is \n",
      "set to four, there will never be more than five pod instances running at the same \n",
      "time during an update. When converting a percentage to an absolute number, \n",
      "the number is rounded up. Instead of a percentage, the value can also be an \n",
      "absolute value (for example, one or two additional pods can be allowed).\n",
      "maxUnavailable\n",
      "Determines how many pod instances can be unavailable relative to the desired \n",
      "replica count during the update. It also defaults to 25%, so the number of avail-\n",
      "able pod instances must never fall below 75% of the desired replica count. Here, \n",
      "when converting a percentage to an absolute number, the number is rounded \n",
      "down. If the desired replica count is set to four and the percentage is 25%, only \n",
      "one pod can be unavailable. There will always be at least three pod instances \n",
      "available to serve requests during the whole rollout. As with maxSurge, you can \n",
      "also specify an absolute value instead of a percentage.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 304, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "272\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      "maxUnavailable disallowed having any unavailable pods (in other words, three pods\n",
      "had to be available at all times). This is shown in figure 9.12.\n",
      "UNDERSTANDING THE MAXUNAVAILABLE PROPERTY\n",
      "The extensions/v1beta1 version of Deployments uses different defaults—it sets both\n",
      "maxSurge and maxUnavailable to 1 instead of 25%. In the case of three replicas, max-\n",
      "Surge is the same as before, but maxUnavailable is different (1 instead of 0). This\n",
      "makes the rollout process unwind a bit differently, as shown in figure 9.13.\n",
      "v1\n",
      "Number\n",
      "of pods\n",
      "3\n",
      "4\n",
      "2\n",
      "1\n",
      "Time\n",
      "v1\n",
      "3 available\n",
      "1 unavailable\n",
      "Create\n",
      "one\n",
      "v2 pod\n",
      "4 available\n",
      "3 available\n",
      "1 unavailable\n",
      "4 available\n",
      "3 available\n",
      "1 unavailable\n",
      "maxSurge = 1\n",
      "maxUnavailable = 0\n",
      "Desired replica count = 3\n",
      "3 available\n",
      "v2\n",
      "v1\n",
      "v1\n",
      "v2\n",
      "v2\n",
      "v1\n",
      "v1\n",
      "v1\n",
      "v1\n",
      "v1\n",
      "v1\n",
      "v1\n",
      "v1\n",
      "v1\n",
      "v1\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v1\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "4 available\n",
      "Wait\n",
      "until\n",
      "it’s\n",
      "available\n",
      "Delete\n",
      "one v1\n",
      "pod and\n",
      "create one\n",
      "v2 pod\n",
      "Wait\n",
      "until\n",
      "it’s\n",
      "available\n",
      "Delete\n",
      "one v1\n",
      "pod and\n",
      "create one\n",
      "v2 pod\n",
      "Wait\n",
      "until\n",
      "it’s\n",
      "available\n",
      "Delete\n",
      "last\n",
      "v1 pod\n",
      "Figure 9.12\n",
      "Rolling update of a Deployment with three replicas and default maxSurge and maxUnavailable \n",
      "v1\n",
      "Number\n",
      "of pods\n",
      "3\n",
      "4\n",
      "2\n",
      "1\n",
      "Time\n",
      "v1\n",
      "2 available\n",
      "2 unavailable\n",
      "4 available\n",
      "2 available\n",
      "1 unavailable\n",
      "3 available\n",
      "maxSurge = 1\n",
      "maxUnavailable = 1\n",
      "Desired replica count = 3\n",
      "v1\n",
      "v1\n",
      "v1\n",
      "v1\n",
      "v1\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "v2\n",
      "Wait until\n",
      "both are\n",
      "available\n",
      "Delete\n",
      "two v1\n",
      "pods and\n",
      "create one\n",
      "v2 pod\n",
      "Delete v1\n",
      "pod and\n",
      "create two\n",
      "v2 pods\n",
      "Wait\n",
      "until it’s\n",
      "available\n",
      "Figure 9.13\n",
      "Rolling update of a Deployment with the maxSurge=1 and maxUnavailable=1\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 305, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "273\n",
      "Using Deployments for updating apps declaratively\n",
      "In this case, one replica can be unavailable, so if the desired replica count is three,\n",
      "only two of them need to be available. That’s why the rollout process immediately\n",
      "deletes one pod and creates two new ones. This ensures two pods are available and\n",
      "that the maximum number of pods isn’t exceeded (the maximum is four in this\n",
      "case—three plus one from maxSurge). As soon as the two new pods are available, the\n",
      "two remaining old pods are deleted.\n",
      " This is a bit hard to grasp, especially since the maxUnavailable property leads you\n",
      "to believe that that’s the maximum number of unavailable pods that are allowed. If\n",
      "you look at the previous figure closely, you’ll see two unavailable pods in the second\n",
      "column even though maxUnavailable is set to 1. \n",
      " It’s important to keep in mind that maxUnavailable is relative to the desired\n",
      "replica count. If the replica count is set to three and maxUnavailable is set to one,\n",
      "that means that the update process must always keep at least two (3 minus 1) pods\n",
      "available, while the number of pods that aren’t available can exceed one.\n",
      "9.3.5\n",
      "Pausing the rollout process\n",
      "After the bad experience with version 3 of your app, imagine you’ve now fixed the bug\n",
      "and pushed version 4 of your image. You’re a little apprehensive about rolling it out\n",
      "across all your pods the way you did before. What you want is to run a single v4 pod\n",
      "next to your existing v2 pods and see how it behaves with only a fraction of all your\n",
      "users. Then, once you’re sure everything’s okay, you can replace all the old pods with\n",
      "new ones. \n",
      " You could achieve this by running an additional pod either directly or through an\n",
      "additional Deployment, ReplicationController, or ReplicaSet, but you do have another\n",
      "option available on the Deployment itself. A Deployment can also be paused during\n",
      "the rollout process. This allows you to verify that everything is fine with the new ver-\n",
      "sion before proceeding with the rest of the rollout.\n",
      "PAUSING THE ROLLOUT\n",
      "I’ve prepared the v4 image, so go ahead and trigger the rollout by changing the image\n",
      "to luksa/kubia:v4, but then immediately (within a few seconds) pause the rollout:\n",
      "$ kubectl set image deployment kubia nodejs=luksa/kubia:v4\n",
      "deployment \"kubia\" image updated\n",
      "$ kubectl rollout pause deployment kubia\n",
      "deployment \"kubia\" paused\n",
      "A single new pod should have been created, but all original pods should also still be\n",
      "running. Once the new pod is up, a part of all requests to the service will be redirected\n",
      "to the new pod. This way, you’ve effectively run a canary release. A canary release is a\n",
      "technique for minimizing the risk of rolling out a bad version of an application and it\n",
      "affecting all your users. Instead of rolling out the new version to everyone, you replace\n",
      "only one or a small number of old pods with new ones. This way only a small number\n",
      "of users will initially hit the new version. You can then verify whether the new version\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 306, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "274\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      "is working fine or not and then either continue the rollout across all remaining pods\n",
      "or roll back to the previous version. \n",
      "RESUMING THE ROLLOUT\n",
      "In your case, by pausing the rollout process, only a small portion of client requests will\n",
      "hit your v4 pod, while most will still hit the v3 pods. Once you’re confident the new\n",
      "version works as it should, you can resume the deployment to replace all the old pods\n",
      "with new ones:\n",
      "$ kubectl rollout resume deployment kubia\n",
      "deployment \"kubia\" resumed\n",
      "Obviously, having to pause the deployment at an exact point in the rollout process\n",
      "isn’t what you want to do. In the future, a new upgrade strategy may do that automati-\n",
      "cally, but currently, the proper way of performing a canary release is by using two dif-\n",
      "ferent Deployments and scaling them appropriately. \n",
      "USING THE PAUSE FEATURE TO PREVENT ROLLOUTS\n",
      "Pausing a Deployment can also be used to prevent updates to the Deployment from\n",
      "kicking off the rollout process, allowing you to make multiple changes to the Deploy-\n",
      "ment and starting the rollout only when you’re done making all the necessary changes.\n",
      "Once you’re ready for changes to take effect, you resume the Deployment and the\n",
      "rollout process will start.\n",
      "NOTE\n",
      "If a Deployment is paused, the undo command won’t undo it until you\n",
      "resume the Deployment.\n",
      "9.3.6\n",
      "Blocking rollouts of bad versions\n",
      "Before you conclude this chapter, we need to discuss one more property of the Deploy-\n",
      "ment resource. Remember the minReadySeconds property you set on the Deployment\n",
      "at the beginning of section 9.3.2? You used it to slow down the rollout, so you could see\n",
      "it was indeed performing a rolling update and not replacing all the pods at once. The\n",
      "main function of minReadySeconds is to prevent deploying malfunctioning versions, not\n",
      "slowing down a deployment for fun. \n",
      "UNDERSTANDING THE APPLICABILITY OF MINREADYSECONDS\n",
      "The minReadySeconds property specifies how long a newly created pod should be\n",
      "ready before the pod is treated as available. Until the pod is available, the rollout pro-\n",
      "cess will not continue (remember the maxUnavailable property?). A pod is ready\n",
      "when readiness probes of all its containers return a success. If a new pod isn’t func-\n",
      "tioning properly and its readiness probe starts failing before minReadySeconds have\n",
      "passed, the rollout of the new version will effectively be blocked.\n",
      " You used this property to slow down your rollout process by having Kubernetes\n",
      "wait 10 seconds after a pod was ready before continuing with the rollout. Usually,\n",
      "you’d set minReadySeconds to something much higher to make sure pods keep report-\n",
      "ing they’re ready after they’ve already started receiving actual traffic. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 307, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "275\n",
      "Using Deployments for updating apps declaratively\n",
      " Although you should obviously test your pods both in a test and in a staging envi-\n",
      "ronment before deploying them into production, using minReadySeconds is like an\n",
      "airbag that saves your app from making a big mess after you’ve already let a buggy ver-\n",
      "sion slip into production. \n",
      " With a properly configured readiness probe and a proper minReadySeconds set-\n",
      "ting, Kubernetes would have prevented us from deploying the buggy v3 version ear-\n",
      "lier. Let me show you how.\n",
      "DEFINING A READINESS PROBE TO PREVENT OUR V3 VERSION FROM BEING ROLLED OUT FULLY\n",
      "You’re going to deploy version v3 again, but this time, you’ll have the proper readi-\n",
      "ness probe defined on the pod. Your Deployment is currently at version v4, so before\n",
      "you start, roll back to version v2 again so you can pretend this is the first time you’re\n",
      "upgrading to v3. If you wish, you can go straight from v4 to v3, but the text that fol-\n",
      "lows assumes you returned to v2 first.\n",
      " Unlike before, where you only updated the image in the pod template, you’re now\n",
      "also going to introduce a readiness probe for the container at the same time. Up until\n",
      "now, because there was no explicit readiness probe defined, the container and the\n",
      "pod were always considered ready, even if the app wasn’t truly ready or was returning\n",
      "errors. There was no way for Kubernetes to know that the app was malfunctioning and\n",
      "shouldn’t be exposed to clients. \n",
      " To change the image and introduce the readiness probe at once, you’ll use the\n",
      "kubectl apply command. You’ll use the following YAML to update the deployment\n",
      "(you’ll store it as kubia-deployment-v3-with-readinesscheck.yaml), as shown in\n",
      "the following listing.\n",
      "apiVersion: apps/v1beta1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: kubia\n",
      "spec:\n",
      "  replicas: 3\n",
      "  minReadySeconds: 10           \n",
      "  strategy:\n",
      "    rollingUpdate:\n",
      "      maxSurge: 1                  \n",
      "      maxUnavailable: 0         \n",
      "    type: RollingUpdate\n",
      "  template:\n",
      "    metadata:\n",
      "      name: kubia\n",
      "      labels:\n",
      "        app: kubia\n",
      "    spec:\n",
      "      containers:\n",
      "      - image: luksa/kubia:v3\n",
      "Listing 9.11\n",
      "Deployment with a readiness probe: kubia-deployment-v3-with-\n",
      "readinesscheck.yaml\n",
      "You’re keeping \n",
      "minReadySeconds \n",
      "set to 10.\n",
      "You’re keeping maxUnavailable \n",
      "set to 0 to make the deployment \n",
      "replace pods one by one\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 308, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "276\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      "        name: nodejs\n",
      "        readinessProbe:\n",
      "          periodSeconds: 1       \n",
      "          httpGet:                  \n",
      "            path: /                 \n",
      "            port: 8080              \n",
      "UPDATING A DEPLOYMENT WITH KUBECTL APPLY\n",
      "To update the Deployment this time, you’ll use kubectl apply like this:\n",
      "$ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml \n",
      "deployment \"kubia\" configured\n",
      "The apply command updates the Deployment with everything that’s defined in the\n",
      "YAML file. It not only updates the image but also adds the readiness probe definition\n",
      "and anything else you’ve added or modified in the YAML. If the new YAML also con-\n",
      "tains the replicas field, which doesn’t match the number of replicas on the existing\n",
      "Deployment, the apply operation will also scale the Deployment, which isn’t usually\n",
      "what you want. \n",
      "TIP\n",
      "To keep the desired replica count unchanged when updating a Deploy-\n",
      "ment with kubectl apply, don’t include the replicas field in the YAML. \n",
      "Running the apply command will kick off the update process, which you can again\n",
      "follow with the rollout status command:\n",
      "$ kubectl rollout status deployment kubia\n",
      "Waiting for rollout to finish: 1 out of 3 new replicas have been updated...\n",
      "Because the status says one new pod has been created, your service should be hitting it\n",
      "occasionally, right? Let’s see:\n",
      "$ while true; do curl http://130.211.109.222; done\n",
      "This is v2 running in pod kubia-1765119474-jvslk\n",
      "This is v2 running in pod kubia-1765119474-jvslk\n",
      "This is v2 running in pod kubia-1765119474-xk5g3\n",
      "This is v2 running in pod kubia-1765119474-pmb26\n",
      "This is v2 running in pod kubia-1765119474-pmb26\n",
      "This is v2 running in pod kubia-1765119474-xk5g3\n",
      "...\n",
      "Nope, you never hit the v3 pod. Why not? Is it even there? List the pods:\n",
      "$ kubectl get po\n",
      "NAME                     READY     STATUS    RESTARTS   AGE\n",
      "kubia-1163142519-7ws0i   0/1       Running   0          30s\n",
      "kubia-1765119474-jvslk   1/1       Running   0          9m\n",
      "kubia-1765119474-pmb26   1/1       Running   0          9m\n",
      "kubia-1765119474-xk5g3   1/1       Running   0          8m\n",
      "You’re defining a readiness probe \n",
      "that will be executed every second.\n",
      "The readiness probe will \n",
      "perform an HTTP GET request \n",
      "against our container.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 309, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "277\n",
      "Using Deployments for updating apps declaratively\n",
      "Aha! There’s your problem (or as you’ll learn soon, your blessing)! The pod is shown\n",
      "as not ready, but I guess you’ve been expecting that, right? What has happened?\n",
      "UNDERSTANDING HOW A READINESS PROBE PREVENTS BAD VERSIONS FROM BEING ROLLED OUT\n",
      "As soon as your new pod starts, the readiness probe starts being hit every second (you\n",
      "set the probe’s interval to one second in the pod spec). On the fifth request the readi-\n",
      "ness probe began failing, because your app starts returning HTTP status code 500\n",
      "from the fifth request onward. \n",
      " As a result, the pod is removed as an endpoint from the service (see figure 9.14).\n",
      "By the time you start hitting the service in the curl loop, the pod has already been\n",
      "marked as not ready. This explains why you never hit the new pod with curl. And\n",
      "that’s exactly what you want, because you don’t want clients to hit a pod that’s not\n",
      "functioning properly.\n",
      "But what about the rollout process? The rollout status command shows only one\n",
      "new replica has started. Thankfully, the rollout process will not continue, because the\n",
      "new pod will never become available. To be considered available, it needs to be ready\n",
      "for at least 10 seconds. Until it’s available, the rollout process will not create any new\n",
      "pods, and it also won’t remove any original pods because you’ve set the maxUnavailable\n",
      "property to 0. \n",
      "Service\n",
      "curl\n",
      "Pod: v2\n",
      "Pod: v2\n",
      "Pod: v3\n",
      "(unhealthy)\n",
      "Pod: v2\n",
      "ReplicaSet: v2\n",
      "Replicas: 3\n",
      "Deployment\n",
      "Replicas: 3\n",
      "rollingUpdate:\n",
      "maxSurge: 1\n",
      "maxUnavailable: 0\n",
      "ReplicaSet: v3\n",
      "Replicas: 1\n",
      "Requests are not forwarded\n",
      "to v3 pod because of failed\n",
      "readiness probe\n",
      "Figure 9.14\n",
      "Deployment blocked by a failing readiness probe in the new pod\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 310, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "278\n",
      "CHAPTER 9\n",
      "Deployments: updating applications declaratively\n",
      " The fact that the deployment is stuck is a good thing, because if it had continued\n",
      "replacing the old pods with the new ones, you’d end up with a completely non-working\n",
      "service, like you did when you first rolled out version 3, when you weren’t using the\n",
      "readiness probe. But now, with the readiness probe in place, there was virtually no\n",
      "negative impact on your users. A few users may have experienced the internal server\n",
      "error, but that’s not as big of a problem as if the rollout had replaced all pods with the\n",
      "faulty version 3.\n",
      "TIP\n",
      "If you only define the readiness probe without setting minReadySeconds\n",
      "properly, new pods are considered available immediately when the first invo-\n",
      "cation of the readiness probe succeeds. If the readiness probe starts failing\n",
      "shortly after, the bad version is rolled out across all pods. Therefore, you\n",
      "should set minReadySeconds appropriately.\n",
      "CONFIGURING A DEADLINE FOR THE ROLLOUT\n",
      "By default, after the rollout can’t make any progress in 10 minutes, it’s considered as\n",
      "failed. If you use the kubectl describe deployment command, you’ll see it display a\n",
      "ProgressDeadlineExceeded condition, as shown in the following listing.\n",
      "$ kubectl describe deploy kubia\n",
      "Name:                   kubia\n",
      "...\n",
      "Conditions:\n",
      "  Type          Status  Reason\n",
      "  ----          ------  ------\n",
      "  Available     True    MinimumReplicasAvailable\n",
      "  Progressing   False   ProgressDeadlineExceeded   \n",
      "The time after which the Deployment is considered failed is configurable through the\n",
      "progressDeadlineSeconds property in the Deployment spec.\n",
      "NOTE\n",
      "The extensions/v1beta1 version of Deployments doesn’t set a deadline.\n",
      "ABORTING A BAD ROLLOUT\n",
      "Because the rollout will never continue, the only thing to do now is abort the rollout\n",
      "by undoing it:\n",
      "$ kubectl rollout undo deployment kubia\n",
      "deployment \"kubia\" rolled back\n",
      "NOTE\n",
      "In future versions, the rollout will be aborted automatically when the\n",
      "time specified in progressDeadlineSeconds is exceeded.\n",
      "Listing 9.12\n",
      "Seeing the conditions of a Deployment with kubectl describe\n",
      "The Deployment \n",
      "took too long to \n",
      "make progress.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 311, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "279\n",
      "Summary\n",
      "9.4\n",
      "Summary\n",
      "This chapter has shown you how to make your life easier by using a declarative\n",
      "approach to deploying and updating applications in Kubernetes. Now that you’ve\n",
      "read this chapter, you should know how to\n",
      "Perform a rolling update of pods managed by a ReplicationController\n",
      "Create Deployments instead of lower-level ReplicationControllers or ReplicaSets\n",
      "Update your pods by editing the pod template in the Deployment specification\n",
      "Roll back a Deployment either to the previous revision or to any earlier revision\n",
      "still listed in the revision history\n",
      "Abort a Deployment mid-way\n",
      "Pause a Deployment to inspect how a single instance of the new version behaves\n",
      "in production before allowing additional pod instances to replace the old ones\n",
      "Control the rate of the rolling update through maxSurge and maxUnavailable\n",
      "properties\n",
      "Use minReadySeconds and readiness probes to have the rollout of a faulty ver-\n",
      "sion blocked automatically\n",
      "In addition to these Deployment-specific tasks, you also learned how to\n",
      "Use three dashes as a separator to define multiple resources in a single YAML file\n",
      "Turn on kubectl’s verbose logging to see exactly what it’s doing behind the\n",
      "curtains\n",
      "You now know how to deploy and manage sets of pods created from the same pod\n",
      "template and thus share the same persistent storage. You even know how to update\n",
      "them declaratively. But what about running sets of pods, where each instance needs to\n",
      "use its own persistent storage? We haven’t looked at that yet. That’s the subject of our\n",
      "next chapter.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 312, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "280\n",
      "StatefulSets:\n",
      "deploying replicated\n",
      "stateful applications\n",
      "You now know how to run both single-instance and replicated stateless pods,\n",
      "and even stateful pods utilizing persistent storage. You can run several repli-\n",
      "cated web-server pod instances and you can run a single database pod instance\n",
      "that uses persistent storage, provided either through plain pod volumes or through\n",
      "PersistentVolumes bound by a PersistentVolumeClaim. But can you employ a\n",
      "ReplicaSet to replicate the database pod?\n",
      "This chapter covers\n",
      "Deploying stateful clustered applications\n",
      "Providing separate storage for each instance of \n",
      "a replicated pod\n",
      "Guaranteeing a stable name and hostname for \n",
      "pod replicas\n",
      "Starting and stopping pod replicas in a \n",
      "predictable order\n",
      "Discovering peers through DNS SRV records\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 313, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "281\n",
      "Replicating stateful pods\n",
      "10.1\n",
      "Replicating stateful pods\n",
      "ReplicaSets create multiple pod replicas from a single pod template. These replicas\n",
      "don’t differ from each other, apart from their name and IP address. If the pod tem-\n",
      "plate includes a volume, which refers to a specific PersistentVolumeClaim, all replicas\n",
      "of the ReplicaSet will use the exact same PersistentVolumeClaim and therefore the\n",
      "same PersistentVolume bound by the claim (shown in figure 10.1).\n",
      "Because the reference to the claim is in the pod template, which is used to stamp out\n",
      "multiple pod replicas, you can’t make each replica use its own separate Persistent-\n",
      "VolumeClaim. You can’t use a ReplicaSet to run a distributed data store, where each\n",
      "instance needs its own separate storage—at least not by using a single ReplicaSet. To\n",
      "be honest, none of the API objects you’ve seen so far make running such a data store\n",
      "possible. You need something else. \n",
      "10.1.1 Running multiple replicas with separate storage for each\n",
      "How does one run multiple replicas of a pod and have each pod use its own storage\n",
      "volume? ReplicaSets create exact copies (replicas) of a pod; therefore you can’t use\n",
      "them for these types of pods. What can you use?\n",
      "CREATING PODS MANUALLY\n",
      "You could create pods manually and have each of them use its own PersistentVolume-\n",
      "Claim, but because no ReplicaSet looks after them, you’d need to manage them man-\n",
      "ually and recreate them when they disappear (as in the event of a node failure).\n",
      "Therefore, this isn’t a viable option.\n",
      "USING ONE REPLICASET PER POD INSTANCE\n",
      "Instead of creating pods directly, you could create multiple ReplicaSets—one for each\n",
      "pod with each ReplicaSet’s desired replica count set to one, and each ReplicaSet’s pod\n",
      "template referencing a dedicated PersistentVolumeClaim (as shown in figure 10.2).\n",
      " Although this takes care of the automatic rescheduling in case of node failures or\n",
      "accidental pod deletions, it’s much more cumbersome compared to having a single\n",
      "ReplicaSet. For example, think about how you’d scale the pods in that case. You\n",
      "Persistent\n",
      "Volume\n",
      "Claim\n",
      "Persistent\n",
      "Volume\n",
      "ReplicaSet\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "Figure 10.1\n",
      "All pods from the same ReplicaSet always use the same \n",
      "PersistentVolumeClaim and PersistentVolume.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 314, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "282\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      "couldn’t change the desired replica count—you’d have to create additional Replica-\n",
      "Sets instead. \n",
      " Using multiple ReplicaSets is therefore not the best solution. But could you maybe\n",
      "use a single ReplicaSet and have each pod instance keep its own persistent state, even\n",
      "though they’re all using the same storage volume? \n",
      "USING MULTIPLE DIRECTORIES IN THE SAME VOLUME\n",
      "A trick you can use is to have all pods use the same PersistentVolume, but then have a\n",
      "separate file directory inside that volume for each pod (this is shown in figure 10.3).\n",
      "Because you can’t configure pod replicas differently from a single pod template, you\n",
      "can’t tell each instance what directory it should use, but you can make each instance\n",
      "automatically select (and possibly also create) a data directory that isn’t being used\n",
      "by any other instance at that time. This solution does require coordination between\n",
      "the instances, and isn’t easy to do correctly. It also makes the shared storage volume\n",
      "the bottleneck.\n",
      "10.1.2 Providing a stable identity for each pod\n",
      "In addition to storage, certain clustered applications also require that each instance\n",
      "has a long-lived stable identity. Pods can be killed from time to time and replaced with\n",
      "PVC A1\n",
      "PV A1\n",
      "ReplicaSet A1\n",
      "Pod A1-xyz\n",
      "PVC A2\n",
      "PV A2\n",
      "ReplicaSet A2\n",
      "Pod A2-xzy\n",
      "PVC A3\n",
      "PV A3\n",
      "ReplicaSet A3\n",
      "Pod A3-zyx\n",
      "Figure 10.2\n",
      "Using one ReplicaSet for each pod instance\n",
      "Persistent\n",
      "Volume\n",
      "Claim\n",
      "PersistentVolume\n",
      "ReplicaSet\n",
      "Pod\n",
      "Pod\n",
      "Pod\n",
      "App\n",
      "App\n",
      "App\n",
      "/data/1/\n",
      "/data/3/\n",
      "/data/2/\n",
      "Figure 10.3\n",
      "Working around the shared storage problem by having the app \n",
      "in each pod use a different file directory \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 315, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "283\n",
      "Replicating stateful pods\n",
      "new ones. When a ReplicaSet replaces a pod, the new pod is a completely new pod\n",
      "with a new hostname and IP, although the data in its storage volume may be that of\n",
      "the killed pod. For certain apps, starting up with the old instance’s data but with a\n",
      "completely new network identity may cause problems.\n",
      " Why do certain apps mandate a stable network identity? This requirement is\n",
      "fairly common in distributed stateful applications. Certain apps require the adminis-\n",
      "trator to list all the other cluster members and their IP addresses (or hostnames) in\n",
      "each member’s configuration file. But in Kubernetes, every time a pod is resched-\n",
      "uled, the new pod gets both a new hostname and a new IP address, so the whole\n",
      "application cluster would have to be reconfigured every time one of its members is\n",
      "rescheduled. \n",
      "USING A DEDICATED SERVICE FOR EACH POD INSTANCE\n",
      "A trick you can use to work around this problem is to provide a stable network address\n",
      "for cluster members by creating a dedicated Kubernetes Service for each individual\n",
      "member. Because service IPs are stable, you can then point to each member through\n",
      "its service IP (rather than the pod IP) in the configuration. \n",
      " This is similar to creating a ReplicaSet for each member to provide them with indi-\n",
      "vidual storage, as described previously. Combining these two techniques results in the\n",
      "setup shown in figure 10.4 (an additional service covering all the cluster members is\n",
      "also shown, because you usually need one for clients of the cluster).\n",
      "The solution is not only ugly, but it still doesn’t solve everything. The individual pods\n",
      "can’t know which Service they are exposed through (and thus can’t know their stable\n",
      "IP), so they can’t self-register in other pods using that IP. \n",
      "PVC A1\n",
      "PV A1\n",
      "ReplicaSet A1\n",
      "Pod A1-xzy\n",
      "Service A1\n",
      "Service A\n",
      "PVC A2\n",
      "PV A2\n",
      "ReplicaSet A2\n",
      "Pod A2-xzy\n",
      "Service A2\n",
      "PVC A3\n",
      "PV A3\n",
      "ReplicaSet A3\n",
      "Pod A3-zyx\n",
      "Service A3\n",
      "Figure 10.4\n",
      "Using one \n",
      "Service and ReplicaSet per \n",
      "pod to provide a stable \n",
      "network address and an \n",
      "individual volume for each \n",
      "pod, respectively\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 316, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "284\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      " Luckily, Kubernetes saves us from resorting to such complex solutions. The proper\n",
      "clean and simple way of running these special types of applications in Kubernetes is\n",
      "through a StatefulSet. \n",
      "10.2\n",
      "Understanding StatefulSets\n",
      "Instead of using a ReplicaSet to run these types of pods, you create a StatefulSet\n",
      "resource, which is specifically tailored to applications where instances of the applica-\n",
      "tion must be treated as non-fungible individuals, with each one having a stable name\n",
      "and state. \n",
      "10.2.1 Comparing StatefulSets with ReplicaSets\n",
      "To understand the purpose of StatefulSets, it’s best to compare them to ReplicaSets or\n",
      "ReplicationControllers. But first let me explain them with a little analogy that’s widely\n",
      "used in the field.\n",
      "UNDERSTANDING STATEFUL PODS WITH THE PETS VS. CATTLE ANALOGY\n",
      "You may have already heard of the pets vs. cattle analogy. If not, let me explain it. We\n",
      "can treat our apps either as pets or as cattle. \n",
      "NOTE\n",
      "StatefulSets were initially called PetSets. That name comes from the\n",
      "pets vs. cattle analogy explained here.\n",
      "We tend to treat our app instances as pets, where we give each instance a name and\n",
      "take care of each instance individually. But it’s usually better to treat instances as cattle\n",
      "and not pay special attention to each individual instance. This makes it easy to replace\n",
      "unhealthy instances without giving it a second thought, similar to the way a farmer\n",
      "replaces unhealthy cattle. \n",
      " Instances of a stateless app, for example, behave much like heads of cattle. It\n",
      "doesn’t matter if an instance dies—you can create a new instance and people won’t\n",
      "notice the difference. \n",
      " On the other hand, with stateful apps, an app instance is more like a pet. When a\n",
      "pet dies, you can’t go buy a new one and expect people not to notice. To replace a lost\n",
      "pet, you need to find a new one that looks and behaves exactly like the old one. In the\n",
      "case of apps, this means the new instance needs to have the same state and identity as\n",
      "the old one.\n",
      "COMPARING STATEFULSETS WITH REPLICASETS OR REPLICATIONCONTROLLERS\n",
      "Pod replicas managed by a ReplicaSet or ReplicationController are much like cattle.\n",
      "Because they’re mostly stateless, they can be replaced with a completely new pod\n",
      "replica at any time. Stateful pods require a different approach. When a stateful pod\n",
      "instance dies (or the node it’s running on fails), the pod instance needs to be resur-\n",
      "rected on another node, but the new instance needs to get the same name, network\n",
      "identity, and state as the one it’s replacing. This is what happens when the pods are\n",
      "managed through a StatefulSet. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "239\n",
      "height\n",
      "115\n",
      "PIX BUFFER SIZE\n",
      "82455\n",
      "Original IMG_BUFFER_SIZE\n",
      "82455\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011480>\n",
      "width\n",
      "13\n",
      "height\n",
      "7\n",
      "PIX BUFFER SIZE\n",
      "273\n",
      "Original IMG_BUFFER_SIZE\n",
      "273\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011480>\n",
      "width\n",
      "239\n",
      "height\n",
      "8\n",
      "PIX BUFFER SIZE\n",
      "5736\n",
      "Original IMG_BUFFER_SIZE\n",
      "5736\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011480>\n",
      "width\n",
      "205\n",
      "height\n",
      "90\n",
      "PIX BUFFER SIZE\n",
      "55350\n",
      "Original IMG_BUFFER_SIZE\n",
      "55350\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011480>\n",
      "width\n",
      "239\n",
      "height\n",
      "115\n",
      "PIX BUFFER SIZE\n",
      "82455\n",
      "Original IMG_BUFFER_SIZE\n",
      "82455\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011480>\n",
      "width\n",
      "11\n",
      "height\n",
      "7\n",
      "PIX BUFFER SIZE\n",
      "231\n",
      "Original IMG_BUFFER_SIZE\n",
      "231\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011480>\n",
      "width\n",
      "239\n",
      "height\n",
      "124\n",
      "PIX BUFFER SIZE\n",
      "88908\n",
      "Original IMG_BUFFER_SIZE\n",
      "88908\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011480>\n",
      "page_image_dict\n",
      "{'page': 317, 'img_cnt': 7, 'img_npy_lst': []}\n",
      "285\n",
      "Understanding StatefulSets\n",
      " A StatefulSet makes sure pods are rescheduled in such a way that they retain their\n",
      "identity and state. It also allows you to easily scale the number of pets up and down. A\n",
      "StatefulSet, like a ReplicaSet, has a desired replica count field that determines how\n",
      "many pets you want running at that time. Similar to ReplicaSets, pods are created from\n",
      "a pod template specified as part of the StatefulSet (remember the cookie-cutter anal-\n",
      "ogy?). But unlike pods created by ReplicaSets, pods created by the StatefulSet aren’t\n",
      "exact replicas of each other. Each can have its own set of volumes—in other words,\n",
      "storage (and thus persistent state)—which differentiates it from its peers. Pet pods\n",
      "also have a predictable (and stable) identity instead of each new pod instance getting\n",
      "a completely random one. \n",
      "10.2.2 Providing a stable network identity\n",
      "Each pod created by a StatefulSet is assigned an ordinal index (zero-based), which\n",
      "is then used to derive the pod’s name and hostname, and to attach stable storage to\n",
      "the pod. The names of the pods are thus predictable, because each pod’s name is\n",
      "derived from the StatefulSet’s name and the ordinal index of the instance. Rather\n",
      "than the pods having random names, they’re nicely organized, as shown in the next\n",
      "figure.\n",
      "INTRODUCING THE GOVERNING SERVICE\n",
      "But it’s not all about the pods having a predictable name and hostname. Unlike regu-\n",
      "lar pods, stateful pods sometimes need to be addressable by their hostname, whereas\n",
      "stateless pods usually don’t. After all, each stateless pod is like any other. When you\n",
      "need one, you pick any one of them. But with stateful pods, you usually want to oper-\n",
      "ate on a specific pod from the group, because they differ from each other (they hold\n",
      "different state, for example). \n",
      " For this reason, a StatefulSet requires you to create a corresponding governing\n",
      "headless Service that’s used to provide the actual network identity to each pod.\n",
      "Through this Service, each pod gets its own DNS entry, so its peers and possibly other\n",
      "clients in the cluster can address the pod by its hostname. For example, if the govern-\n",
      "ing Service belongs to the default namespace and is called foo, and one of the pods\n",
      "ReplicaSet A\n",
      "Pod A-fewrb\n",
      "Pod A-jwqec\n",
      "Pod A-dsfwx\n",
      "StatefulSet A\n",
      "Pod A-1\n",
      "Pod A-2\n",
      "Pod A-0\n",
      "Figure 10.5\n",
      "Pods created by a StatefulSet have predictable names (and hostnames), \n",
      "unlike those created by a ReplicaSet\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "228\n",
      "height\n",
      "104\n",
      "PIX BUFFER SIZE\n",
      "71136\n",
      "Original IMG_BUFFER_SIZE\n",
      "71136\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011180>\n",
      "width\n",
      "12\n",
      "height\n",
      "6\n",
      "PIX BUFFER SIZE\n",
      "216\n",
      "Original IMG_BUFFER_SIZE\n",
      "216\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011180>\n",
      "width\n",
      "228\n",
      "height\n",
      "104\n",
      "PIX BUFFER SIZE\n",
      "71136\n",
      "Original IMG_BUFFER_SIZE\n",
      "71136\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011180>\n",
      "width\n",
      "196\n",
      "height\n",
      "104\n",
      "PIX BUFFER SIZE\n",
      "61152\n",
      "Original IMG_BUFFER_SIZE\n",
      "61152\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011180>\n",
      "width\n",
      "6\n",
      "height\n",
      "12\n",
      "PIX BUFFER SIZE\n",
      "216\n",
      "Original IMG_BUFFER_SIZE\n",
      "216\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011180>\n",
      "page_image_dict\n",
      "{'page': 318, 'img_cnt': 5, 'img_npy_lst': []}\n",
      "286\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      "is called A-0, you can reach the pod through its fully qualified domain name, which\n",
      "is a-0.foo.default.svc.cluster.local. You can’t do that with pods managed by a\n",
      "ReplicaSet.\n",
      " Additionally, you can also use DNS to look up all the StatefulSet’s pods’ names by\n",
      "looking up SRV records for the foo.default.svc.cluster.local domain. We’ll\n",
      "explain SRV records in section 10.4 and learn how they’re used to discover members\n",
      "of a StatefulSet.\n",
      "REPLACING LOST PETS\n",
      "When a pod instance managed by a StatefulSet disappears (because the node the pod\n",
      "was running on has failed, it was evicted from the node, or someone deleted the pod\n",
      "object manually), the StatefulSet makes sure it’s replaced with a new instance—similar\n",
      "to how ReplicaSets do it. But in contrast to ReplicaSets, the replacement pod gets the\n",
      "same name and hostname as the pod that has disappeared (this distinction between\n",
      "ReplicaSets and StatefulSets is illustrated in figure 10.6).\n",
      "Node 1\n",
      "Node 2\n",
      "Node 1\n",
      "Node 2\n",
      "ReplicaSet B\n",
      "ReplicaSet B\n",
      "StatefulSet\n",
      "StatefulSet A\n",
      "Pod A-0\n",
      "Pod A-1\n",
      "Pod A-0\n",
      "Pod A-0\n",
      "Pod A-1\n",
      "Node 1 fails\n",
      "StatefulSet A\n",
      "Node 1\n",
      "Node 2\n",
      "Node 1\n",
      "Node 2\n",
      "ReplicaSet\n",
      "Node 1 fails\n",
      "Pod B-fdawr\n",
      "Pod B-jkbde\n",
      "Pod B-fdawr\n",
      "Pod B-rsqkw\n",
      "Pod B-jkbde\n",
      "Figure 10.6\n",
      "A StatefulSet replaces a lost pod with a new one with the same identity, whereas a \n",
      "ReplicaSet replaces it with a completely new unrelated pod.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 319, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "287\n",
      "Understanding StatefulSets\n",
      "The new pod isn’t necessarily scheduled to the same node, but as you learned early\n",
      "on, what node a pod runs on shouldn’t matter. This holds true even for stateful pods.\n",
      "Even if the pod is scheduled to a different node, it will still be available and reachable\n",
      "under the same hostname as before. \n",
      "SCALING A STATEFULSET\n",
      "Scaling the StatefulSet creates a new pod instance with the next unused ordinal index.\n",
      "If you scale up from two to three instances, the new instance will get index 2 (the exist-\n",
      "ing instances obviously have indexes 0 and 1). \n",
      " The nice thing about scaling down a StatefulSet is the fact that you always know\n",
      "what pod will be removed. Again, this is also in contrast to scaling down a ReplicaSet,\n",
      "where you have no idea what instance will be deleted, and you can’t even specify\n",
      "which one you want removed first (but this feature may be introduced in the future).\n",
      "Scaling down a StatefulSet always removes the instances with the highest ordinal index\n",
      "first (shown in figure 10.7). This makes the effects of a scale-down predictable.\n",
      "Because certain stateful applications don’t handle rapid scale-downs nicely, Stateful-\n",
      "Sets scale down only one pod instance at a time. A distributed data store, for example,\n",
      "may lose data if multiple nodes go down at the same time. For example, if a replicated\n",
      "data store is configured to store two copies of each data entry, in cases where two\n",
      "nodes go down at the same time, a data entry would be lost if it was stored on exactly\n",
      "those two nodes. If the scale-down was sequential, the distributed data store has time\n",
      "to create an additional replica of the data entry somewhere else to replace the (single)\n",
      "lost copy.\n",
      " For this exact reason, StatefulSets also never permit scale-down operations if any of\n",
      "the instances are unhealthy. If an instance is unhealthy, and you scale down by one at\n",
      "the same time, you’ve effectively lost two cluster members at once.\n",
      "10.2.3 Providing stable dedicated storage to each stateful instance\n",
      "You’ve seen how StatefulSets ensure stateful pods have a stable identity, but what\n",
      "about storage? Each stateful pod instance needs to use its own storage, plus if a state-\n",
      "ful pod is rescheduled (replaced with a new instance but with the same identity as\n",
      "before), the new instance must have the same storage attached to it. How do Stateful-\n",
      "Sets achieve this?\n",
      "Pod\n",
      "A-0\n",
      "Pod\n",
      "A-1\n",
      "Pod\n",
      "A-2\n",
      "StatefulSet A\n",
      "Replicas: 3\n",
      "Pod\n",
      "A-0\n",
      "Pod\n",
      "A-1\n",
      "Pod\n",
      "A-2\n",
      "StatefulSet A\n",
      "Replicas: 2\n",
      "Pod\n",
      "A-0\n",
      "Pod\n",
      "A-1\n",
      "StatefulSet A\n",
      "Replicas: 1\n",
      "Scale down\n",
      "Scale down\n",
      "Figure 10.7\n",
      "Scaling down a StatefulSet always removes the pod with the highest ordinal index first.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 320, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "288\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      " Obviously, storage for stateful pods needs to be persistent and decoupled from\n",
      "the pods. In chapter 6 you learned about PersistentVolumes and PersistentVolume-\n",
      "Claims, which allow persistent storage to be attached to a pod by referencing the\n",
      "PersistentVolumeClaim in the pod by name. Because PersistentVolumeClaims map\n",
      "to PersistentVolumes one-to-one, each pod of a StatefulSet needs to reference a dif-\n",
      "ferent PersistentVolumeClaim to have its own separate PersistentVolume. Because\n",
      "all pod instances are stamped from the same pod template, how can they each refer\n",
      "to a different PersistentVolumeClaim? And who creates these claims? Surely you’re\n",
      "not expected to create as many PersistentVolumeClaims as the number of pods you\n",
      "plan to have in the StatefulSet upfront? Of course not.\n",
      "TEAMING UP POD TEMPLATES WITH VOLUME CLAIM TEMPLATES\n",
      "The StatefulSet has to create the PersistentVolumeClaims as well, the same way it’s cre-\n",
      "ating the pods. For this reason, a StatefulSet can also have one or more volume claim\n",
      "templates, which enable it to stamp out PersistentVolumeClaims along with each pod\n",
      "instance (see figure 10.8).\n",
      "The PersistentVolumes for the claims can either be provisioned up-front by an admin-\n",
      "istrator or just in time through dynamic provisioning of PersistentVolumes, as explained\n",
      "at the end of chapter 6. \n",
      "UNDERSTANDING THE CREATION AND DELETION OF PERSISTENTVOLUMECLAIMS\n",
      "Scaling up a StatefulSet by one creates two or more API objects (the pod and one or\n",
      "more PersistentVolumeClaims referenced by the pod). Scaling down, however, deletes\n",
      "only the pod, leaving the claims alone. The reason for this is obvious, if you consider\n",
      "what happens when a claim is deleted. After a claim is deleted, the PersistentVolume it\n",
      "was bound to gets recycled or deleted and its contents are lost. \n",
      " Because stateful pods are meant to run stateful applications, which implies that the\n",
      "data they store in the volume is important, deleting the claim on scale-down of a Stateful-\n",
      "Set could be catastrophic—especially since triggering a scale-down is as simple as\n",
      "decreasing the replicas field of the StatefulSet. For this reason, you’re required to\n",
      "delete PersistentVolumeClaims manually to release the underlying PersistentVolume.\n",
      "PVC A-0\n",
      "PV\n",
      "Pod A-0\n",
      "PVC A-1\n",
      "PV\n",
      "Pod A-1\n",
      "PVC A-2\n",
      "PV\n",
      "Pod A-2\n",
      "StatefulSet A\n",
      "Pod\n",
      "template\n",
      "Volume claim\n",
      "template\n",
      "Figure 10.8\n",
      "A StatefulSet creates both pods and PersistentVolumeClaims.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 321, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "289\n",
      "Understanding StatefulSets\n",
      "REATTACHING THE PERSISTENTVOLUMECLAIM TO THE NEW INSTANCE OF THE SAME POD\n",
      "The fact that the PersistentVolumeClaim remains after a scale-down means a subse-\n",
      "quent scale-up can reattach the same claim along with the bound PersistentVolume\n",
      "and its contents to the new pod instance (shown in figure 10.9). If you accidentally\n",
      "scale down a StatefulSet, you can undo the mistake by scaling up again and the new\n",
      "pod will get the same persisted state again (as well as the same name).\n",
      "10.2.4 Understanding StatefulSet guarantees\n",
      "As you’ve seen so far, StatefulSets behave differently from ReplicaSets or Replication-\n",
      "Controllers. But this doesn’t end with the pods having a stable identity and storage.\n",
      "StatefulSets also have different guarantees regarding their pods. \n",
      "UNDERSTANDING THE IMPLICATIONS OF STABLE IDENTITY AND STORAGE\n",
      "While regular, stateless pods are fungible, stateful pods aren’t. We’ve already seen how\n",
      "a stateful pod is always replaced with an identical pod (one having the same name and\n",
      "hostname, using the same persistent storage, and so on). This happens when Kuber-\n",
      "netes sees that the old pod is no longer there (for example, when you delete the pod\n",
      "manually). \n",
      " But what if Kubernetes can’t be sure about the state of the pod? If it creates a\n",
      "replacement pod with the same identity, two instances of the app with the same iden-\n",
      "tity might be running in the system. The two would also be bound to the same storage,\n",
      "Pod\n",
      "A-0\n",
      "Pod\n",
      "A-1\n",
      "StatefulSet A\n",
      "Replicas: 2\n",
      "Scale\n",
      "down\n",
      "Scale\n",
      "up\n",
      "New pod instance created\n",
      "with same identity as before\n",
      "PVC is\n",
      "re-attached\n",
      "PVC\n",
      "A-0\n",
      "PV\n",
      "PVC\n",
      "A-1\n",
      "PV\n",
      "Pod\n",
      "A-0\n",
      "StatefulSet A\n",
      "Replicas: 1\n",
      "PVC\n",
      "A-0\n",
      "PV\n",
      "PVC\n",
      "A-1\n",
      "PV\n",
      "Pod\n",
      "A-0\n",
      "Pod has been deleted\n",
      "Pod\n",
      "A-1\n",
      "StatefulSet A\n",
      "Replicas: 2\n",
      "PVC\n",
      "A-0\n",
      "PV\n",
      "PVC\n",
      "A-1\n",
      "PVC has not\n",
      "been deleted\n",
      "PV\n",
      "Figure 10.9\n",
      "StatefulSets don’t delete PersistentVolumeClaims when scaling down; then they \n",
      "reattach them when scaling back up.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 322, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "290\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      "so two processes with the same identity would be writing over the same files. With pods\n",
      "managed by a ReplicaSet, this isn’t a problem, because the apps are obviously made to\n",
      "work on the same files. Also, ReplicaSets create pods with a randomly generated iden-\n",
      "tity, so there’s no way for two processes to run with the same identity. \n",
      "INTRODUCING STATEFULSET’S AT-MOST-ONE SEMANTICS\n",
      "Kubernetes must thus take great care to ensure two stateful pod instances are never\n",
      "running with the same identity and are bound to the same PersistentVolumeClaim. A\n",
      "StatefulSet must guarantee at-most-one semantics for stateful pod instances. \n",
      " This means a StatefulSet must be absolutely certain that a pod is no longer run-\n",
      "ning before it can create a replacement pod. This has a big effect on how node fail-\n",
      "ures are handled. We’ll demonstrate this later in the chapter. Before we can do that,\n",
      "however, you need to create a StatefulSet and see how it behaves. You’ll also learn a\n",
      "few more things about them along the way.\n",
      "10.3\n",
      "Using a StatefulSet\n",
      "To properly show StatefulSets in action, you’ll build your own little clustered data\n",
      "store. Nothing fancy—more like a data store from the Stone Age. \n",
      "10.3.1 Creating the app and container image\n",
      "You’ll use the kubia app you’ve used throughout the book as your starting point. You’ll\n",
      "expand it so it allows you to store and retrieve a single data entry on each pod instance. \n",
      " The important parts of the source code of your data store are shown in the follow-\n",
      "ing listing.\n",
      "...\n",
      "const dataFile = \"/var/data/kubia.txt\";\n",
      "...\n",
      "var handler = function(request, response) {\n",
      "  if (request.method == 'POST') {                \n",
      "    var file = fs.createWriteStream(dataFile);                     \n",
      "    file.on('open', function (fd) {                                \n",
      "      request.pipe(file);                                          \n",
      "      console.log(\"New data has been received and stored.\");       \n",
      "      response.writeHead(200);                                     \n",
      "      response.end(\"Data stored on pod \" + os.hostname() + \"\\n\");  \n",
      "    });\n",
      "  } else {                                       \n",
      "    var data = fileExists(dataFile)                                \n",
      "      ? fs.readFileSync(dataFile, 'utf8')                          \n",
      "      : \"No data posted yet\";                                      \n",
      "    response.writeHead(200);                                       \n",
      "    response.write(\"You've hit \" + os.hostname() + \"\\n\");          \n",
      "    response.end(\"Data stored on this pod: \" + data + \"\\n\");       \n",
      "  }\n",
      "};\n",
      "Listing 10.1\n",
      "A simple stateful app: kubia-pet-image/app.js\n",
      "On POST \n",
      "requests, store \n",
      "the request’s \n",
      "body into a \n",
      "data file.\n",
      "On GET (and all \n",
      "other types of) \n",
      "requests, return \n",
      "your hostname \n",
      "and the contents \n",
      "of the data file.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 323, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "291\n",
      "Using a StatefulSet\n",
      "var www = http.createServer(handler);\n",
      "www.listen(8080);\n",
      "Whenever the app receives a POST request, it writes the data it receives in the body of\n",
      "the request to the file /var/data/kubia.txt. Upon a GET request, it returns the host-\n",
      "name and the stored data (contents of the file). Simple enough, right? This is the first\n",
      "version of your app. It’s not clustered yet, but it’s enough to get you started. You’ll\n",
      "expand the app later in the chapter.\n",
      " The Dockerfile for building the container image is shown in the following listing\n",
      "and hasn’t changed from before.\n",
      "FROM node:7\n",
      "ADD app.js /app.js\n",
      "ENTRYPOINT [\"node\", \"app.js\"]\n",
      "Go ahead and build the image now, or use the one I pushed to docker.io/luksa/kubia-pet.\n",
      "10.3.2 Deploying the app through a StatefulSet\n",
      "To deploy your app, you’ll need to create two (or three) different types of objects:\n",
      "PersistentVolumes for storing your data files (you’ll need to create these only if\n",
      "the cluster doesn’t support dynamic provisioning of PersistentVolumes).\n",
      "A governing Service required by the StatefulSet.\n",
      "The StatefulSet itself.\n",
      "For each pod instance, the StatefulSet will create a PersistentVolumeClaim that will\n",
      "bind to a PersistentVolume. If your cluster supports dynamic provisioning, you don’t\n",
      "need to create any PersistentVolumes manually (you can skip the next section). If it\n",
      "doesn’t, you’ll need to create them as explained in the next section. \n",
      "CREATING THE PERSISTENT VOLUMES\n",
      "You’ll need three PersistentVolumes, because you’ll be scaling the StatefulSet up to\n",
      "three replicas. You must create more if you plan on scaling the StatefulSet up more\n",
      "than that.\n",
      " If you’re using Minikube, deploy the PersistentVolumes defined in the Chapter06/\n",
      "persistent-volumes-hostpath.yaml file in the book’s code archive. \n",
      " If you’re using Google Kubernetes Engine, you’ll first need to create the actual\n",
      "GCE Persistent Disks like this:\n",
      "$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-a\n",
      "$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-b\n",
      "$ gcloud compute disks create --size=1GiB --zone=europe-west1-b pv-c\n",
      "NOTE\n",
      "Make sure to create the disks in the same zone that your nodes are\n",
      "running in.\n",
      "Listing 10.2\n",
      "Dockerfile for the stateful app: kubia-pet-image/Dockerfile\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 324, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "292\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      "Then create the PersistentVolumes from the persistent-volumes-gcepd.yaml file,\n",
      "which is shown in the following listing.\n",
      "kind: List                     \n",
      "apiVersion: v1\n",
      "items:\n",
      "- apiVersion: v1\n",
      "  kind: PersistentVolume       \n",
      "  metadata:\n",
      "    name: pv-a                \n",
      "  spec:\n",
      "    capacity:\n",
      "      storage: 1Mi            \n",
      "    accessModes:\n",
      "      - ReadWriteOnce\n",
      "    persistentVolumeReclaimPolicy: Recycle     \n",
      "    gcePersistentDisk:         \n",
      "      pdName: pv-a             \n",
      "      fsType: nfs4                         \n",
      "- apiVersion: v1\n",
      "  kind: PersistentVolume\n",
      "  metadata:\n",
      "    name: pv-b\n",
      " ...\n",
      "NOTE\n",
      "In the previous chapter you specified multiple resources in the same\n",
      "YAML by delimiting them with a three-dash line. Here you’re using a differ-\n",
      "ent approach by defining a List object and listing the resources as items of\n",
      "the object. Both methods are equivalent.\n",
      "This manifest creates PersistentVolumes called pv-a, pv-b, and pv-c. They use GCE Per-\n",
      "sistent Disks as the underlying storage mechanism, so they’re not appropriate for clus-\n",
      "ters that aren’t running on Google Kubernetes Engine or Google Compute Engine. If\n",
      "you’re running the cluster elsewhere, you must modify the PersistentVolume definition\n",
      "and use an appropriate volume type, such as NFS (Network File System), or similar.\n",
      "CREATING THE GOVERNING SERVICE\n",
      "As explained earlier, before deploying a StatefulSet, you first need to create a headless\n",
      "Service, which will be used to provide the network identity for your stateful pods. The\n",
      "following listing shows the Service manifest.\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: kubia       \n",
      "spec:\n",
      "  clusterIP: None    \n",
      "Listing 10.3\n",
      "Three PersistentVolumes: persistent-volumes-gcepd.yaml\n",
      "Listing 10.4\n",
      "Headless service to be used in the StatefulSet: kubia-service-headless.yaml\n",
      "File describes a list \n",
      "of three persistent \n",
      "volumes\n",
      "Persistent volumes’ names \n",
      "are pv-a, pv-b, and pv-c\n",
      "Capacity of each persistent \n",
      "volume is 1 Mebibyte\n",
      "When the volume \n",
      "is released by the \n",
      "claim, it’s recycled \n",
      "to be used again.\n",
      "The volume uses a GCE \n",
      "Persistent Disk as the underlying \n",
      "storage mechanism.\n",
      "Name of the \n",
      "Service\n",
      "The StatefulSet’s governing \n",
      "Service must be headless.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 325, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "293\n",
      "Using a StatefulSet\n",
      "  selector:           \n",
      "    app: kubia        \n",
      "  ports:\n",
      "  - name: http\n",
      "    port: 80\n",
      "You’re setting the clusterIP field to None, which makes this a headless Service. It will\n",
      "enable peer discovery between your pods (you’ll need this later). Once you create the\n",
      "Service, you can move on to creating the actual StatefulSet.\n",
      "CREATING THE STATEFULSET MANIFEST\n",
      "Now you can finally create the StatefulSet. The following listing shows the manifest.\n",
      "apiVersion: apps/v1beta1\n",
      "kind: StatefulSet\n",
      "metadata:\n",
      "  name: kubia\n",
      "spec:\n",
      "  serviceName: kubia\n",
      "  replicas: 2\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:                  \n",
      "        app: kubia             \n",
      "    spec:\n",
      "      containers:\n",
      "      - name: kubia\n",
      "        image: luksa/kubia-pet\n",
      "        ports:\n",
      "        - name: http\n",
      "          containerPort: 8080\n",
      "        volumeMounts:\n",
      "        - name: data                  \n",
      "          mountPath: /var/data        \n",
      "  volumeClaimTemplates:\n",
      "  - metadata:                  \n",
      "      name: data               \n",
      "    spec:                      \n",
      "      resources:               \n",
      "        requests:              \n",
      "          storage: 1Mi         \n",
      "      accessModes:             \n",
      "      - ReadWriteOnce          \n",
      "The StatefulSet manifest isn’t that different from ReplicaSet or Deployment manifests\n",
      "you’ve created so far. What’s new is the volumeClaimTemplates list. In it, you’re defin-\n",
      "ing one volume claim template called data, which will be used to create a Persistent-\n",
      "VolumeClaim for each pod. As you may remember from chapter 6, a pod references a\n",
      "claim by including a persistentVolumeClaim volume in the manifest. In the previous\n",
      "Listing 10.5\n",
      "StatefulSet manifest: kubia-statefulset.yaml\n",
      "All pods with the app=kubia \n",
      "label belong to this service.\n",
      "Pods created by the StatefulSet \n",
      "will have the app=kubia label.\n",
      "The container inside the pod will \n",
      "mount the pvc volume at this path.\n",
      "The PersistentVolumeClaims \n",
      "will be created from this \n",
      "template.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 326, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "294\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      "pod template, you’ll find no such volume. The StatefulSet adds it to the pod specifica-\n",
      "tion automatically and configures the volume to be bound to the claim the StatefulSet\n",
      "created for the specific pod.\n",
      "CREATING THE STATEFULSET\n",
      "You’ll create the StatefulSet now:\n",
      "$ kubectl create -f kubia-statefulset.yaml \n",
      "statefulset \"kubia\" created\n",
      "Now, list your pods:\n",
      "$ kubectl get po\n",
      "NAME      READY     STATUS              RESTARTS   AGE\n",
      "kubia-0   0/1       ContainerCreating   0          1s\n",
      "Notice anything strange? Remember how a ReplicationController or a ReplicaSet cre-\n",
      "ates all the pod instances at the same time? Your StatefulSet is configured to create\n",
      "two replicas, but it created a single pod. \n",
      " Don’t worry, nothing is wrong. The second pod will be created only after the first\n",
      "one is up and ready. StatefulSets behave this way because certain clustered stateful\n",
      "apps are sensitive to race conditions if two or more cluster members come up at the\n",
      "same time, so it’s safer to bring each member up fully before continuing to bring up\n",
      "the rest.\n",
      " List the pods again to see how the pod creation is progressing:\n",
      "$ kubectl get po\n",
      "NAME      READY     STATUS              RESTARTS   AGE\n",
      "kubia-0   1/1       Running             0          8s\n",
      "kubia-1   0/1       ContainerCreating   0          2s\n",
      "See, the first pod is now running, and the second one has been created and is being\n",
      "started. \n",
      "EXAMINING THE GENERATED STATEFUL POD\n",
      "Let’s take a closer look at the first pod’s spec in the following listing to see how the\n",
      "StatefulSet has constructed the pod from the pod template and the PersistentVolume-\n",
      "Claim template.\n",
      "$ kubectl get po kubia-0 -o yaml\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  ...\n",
      "spec:\n",
      "  containers:\n",
      "  - image: luksa/kubia-pet\n",
      "    ...\n",
      "Listing 10.6\n",
      "A stateful pod created by the StatefulSet\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 327, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "295\n",
      "Using a StatefulSet\n",
      "    volumeMounts:\n",
      "    - mountPath: /var/data           \n",
      "      name: data                     \n",
      "    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n",
      "      name: default-token-r2m41\n",
      "      readOnly: true\n",
      "  ...\n",
      "  volumes:\n",
      "  - name: data                       \n",
      "    persistentVolumeClaim:           \n",
      "      claimName: data-kubia-0            \n",
      "  - name: default-token-r2m41\n",
      "    secret:\n",
      "      secretName: default-token-r2m41\n",
      "The PersistentVolumeClaim template was used to create the PersistentVolumeClaim\n",
      "and the volume inside the pod, which refers to the created PersistentVolumeClaim. \n",
      "EXAMINING THE GENERATED PERSISTENTVOLUMECLAIMS\n",
      "Now list the generated PersistentVolumeClaims to confirm they were created:\n",
      "$ kubectl get pvc\n",
      "NAME           STATUS    VOLUME    CAPACITY   ACCESSMODES   AGE\n",
      "data-kubia-0   Bound     pv-c      0                        37s\n",
      "data-kubia-1   Bound     pv-a      0                        37s\n",
      "The names of the generated PersistentVolumeClaims are composed of the name\n",
      "defined in the volumeClaimTemplate and the name of each pod. You can examine the\n",
      "claims’ YAML to see that they match the template.\n",
      "10.3.3 Playing with your pods\n",
      "With the nodes of your data store cluster now running, you can start exploring it. You\n",
      "can’t communicate with your pods through the Service you created because it’s head-\n",
      "less. You’ll need to connect to individual pods directly (or create a regular Service, but\n",
      "that wouldn’t allow you to talk to a specific pod).\n",
      " You’ve already seen ways to connect to a pod directly: by piggybacking on another\n",
      "pod and running curl inside it, by using port-forwarding, and so on. This time, you’ll\n",
      "try another option. You’ll use the API server as a proxy to the pods. \n",
      "COMMUNICATING WITH PODS THROUGH THE API SERVER\n",
      "One useful feature of the API server is the ability to proxy connections directly to indi-\n",
      "vidual pods. If you want to perform requests against your kubia-0 pod, you hit the fol-\n",
      "lowing URL:\n",
      "<apiServerHost>:<port>/api/v1/namespaces/default/pods/kubia-0/proxy/<path>\n",
      "Because the API server is secured, sending requests to pods through the API server is\n",
      "cumbersome (among other things, you need to pass the authorization token in each\n",
      "request). Luckily, in chapter 8 you learned how to use kubectl proxy to talk to the\n",
      "The volume mount, as \n",
      "specified in the manifest\n",
      "The volume created \n",
      "by the StatefulSet\n",
      "The claim referenced \n",
      "by this volume\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 328, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "296\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      "API server without having to deal with authentication and SSL certificates. Run the\n",
      "proxy again:\n",
      "$ kubectl proxy\n",
      "Starting to serve on 127.0.0.1:8001\n",
      "Now, because you’ll be talking to the API server through the kubectl proxy, you’ll use\n",
      "localhost:8001 rather than the actual API server host and port. You’ll send a request to\n",
      "the kubia-0 pod like this:\n",
      "$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\n",
      "You've hit kubia-0\n",
      "Data stored on this pod: No data posted yet\n",
      "The response shows that the request was indeed received and handled by the app run-\n",
      "ning in your pod kubia-0. \n",
      "NOTE\n",
      "If you receive an empty response, make sure you haven’t left out that\n",
      "last slash character at the end of the URL (or make sure curl follows redirects\n",
      "by using its -L option). \n",
      "Because you’re communicating with the pod through the API server, which you’re\n",
      "connecting to through the kubectl proxy, the request went through two different\n",
      "proxies (the first was the kubectl proxy and the other was the API server, which prox-\n",
      "ied the request to the pod). For a clearer picture, examine figure 10.10.\n",
      "The request you sent to the pod was a GET request, but you can also send POST\n",
      "requests through the API server. This is done by sending a POST request to the same\n",
      "proxy URL as the one you sent the GET request to. \n",
      " When your app receives a POST request, it stores whatever’s in the request body\n",
      "into a local file. Send a POST request to the kubia-0 pod:\n",
      "$ curl -X POST -d \"Hey there! This greeting was submitted to kubia-0.\"\n",
      "➥ localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\n",
      "Data stored on pod kubia-0\n",
      "kubectl proxy\n",
      "curl\n",
      "GET localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\n",
      "GET 192.168.99.100:8443/api/v1/namespaces/default/pods/kubia-0/proxy/\n",
      "Authorization: Bearer <token>\n",
      "GET 172.17.0.3:8080/\n",
      "API server\n",
      "Pod: kubia-0\n",
      "192.168.99.100\n",
      "172.17.0.3\n",
      "localhost\n",
      "Figure 10.10\n",
      "Connecting to a pod through both the kubectl proxy and API server proxy\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 329, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "297\n",
      "Using a StatefulSet\n",
      "The data you sent should now be stored in that pod. Let’s see if it returns the stored\n",
      "data when you perform a GET request again:\n",
      "$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\n",
      "You've hit kubia-0\n",
      "Data stored on this pod: Hey there! This greeting was submitted to kubia-0.\n",
      "Okay, so far so good. Now let’s see what the other cluster node (the kubia-1 pod)\n",
      "says:\n",
      "$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/\n",
      "You've hit kubia-1\n",
      "Data stored on this pod: No data posted yet\n",
      "As expected, each node has its own state. But is that state persisted? Let’s find out.\n",
      "DELETING A STATEFUL POD TO SEE IF THE RESCHEDULED POD IS REATTACHED TO THE SAME STORAGE\n",
      "You’re going to delete the kubia-0 pod and wait for it to be rescheduled. Then you’ll\n",
      "see if it’s still serving the same data as before:\n",
      "$ kubectl delete po kubia-0\n",
      "pod \"kubia-0\" deleted\n",
      "If you list the pods, you’ll see that the pod is terminating: \n",
      "$ kubectl get po\n",
      "NAME      READY     STATUS        RESTARTS   AGE\n",
      "kubia-0   1/1       Terminating   0          3m\n",
      "kubia-1   1/1       Running       0          3m\n",
      "As soon as it terminates successfully, a new pod with the same name is created by the\n",
      "StatefulSet:\n",
      "$ kubectl get po\n",
      "NAME      READY     STATUS              RESTARTS   AGE\n",
      "kubia-0   0/1       ContainerCreating   0          6s\n",
      "kubia-1   1/1       Running             0          4m\n",
      "$ kubectl get po\n",
      "NAME      READY     STATUS    RESTARTS   AGE\n",
      "kubia-0   1/1       Running   0          9s\n",
      "kubia-1   1/1       Running   0          4m\n",
      "Let me remind you again that this new pod may be scheduled to any node in the clus-\n",
      "ter, not necessarily the same node that the old pod was scheduled to. The old pod’s\n",
      "whole identity (the name, hostname, and the storage) is effectively moved to the new\n",
      "node (as shown in figure 10.11). If you’re using Minikube, you can’t see this because it\n",
      "only runs a single node, but in a multi-node cluster, you may see the pod scheduled to\n",
      "a different node than before.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "13\n",
      "height\n",
      "3\n",
      "PIX BUFFER SIZE\n",
      "117\n",
      "Original IMG_BUFFER_SIZE\n",
      "117\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011d80>\n",
      "width\n",
      "231\n",
      "height\n",
      "104\n",
      "PIX BUFFER SIZE\n",
      "72072\n",
      "Original IMG_BUFFER_SIZE\n",
      "72072\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011d80>\n",
      "page_image_dict\n",
      "{'page': 330, 'img_cnt': 2, 'img_npy_lst': []}\n",
      "298\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      "With the new pod now running, let’s check to see if it has the exact same identity as in\n",
      "its previous incarnation. The pod’s name is the same, but what about the hostname\n",
      "and persistent data? You can ask the pod itself to confirm:\n",
      "$ curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/\n",
      "You've hit kubia-0\n",
      "Data stored on this pod: Hey there! This greeting was submitted to kubia-0.\n",
      "The pod’s response shows that both the hostname and the data are the same as before,\n",
      "confirming that a StatefulSet always replaces a deleted pod with what’s effectively the\n",
      "exact same pod. \n",
      "SCALING A STATEFULSET\n",
      "Scaling down a StatefulSet and scaling it back up after an extended time period\n",
      "should be no different than deleting a pod and having the StatefulSet recreate it\n",
      "immediately. Remember that scaling down a StatefulSet only deletes the pods, but\n",
      "leaves the PersistentVolumeClaims untouched. I’ll let you try scaling down the State-\n",
      "fulSet yourself and confirm this behavior. \n",
      " The key thing to remember is that scaling down (and up) is performed gradu-\n",
      "ally—similar to how individual pods are created when the StatefulSet is created ini-\n",
      "tially. When scaling down by more than one instance, the pod with the highest ordinal\n",
      "number is deleted first. Only after the pod terminates completely is the pod with the\n",
      "second highest ordinal number deleted. \n",
      "EXPOSING STATEFUL PODS THROUGH A REGULAR, NON-HEADLESS SERVICE\n",
      "Before you move on to the last part of this chapter, you’re going to add a proper, non-\n",
      "headless Service in front of your pods, because clients usually connect to the pods\n",
      "through a Service rather than connecting directly.\n",
      "Node 1\n",
      "Pod: kubia-0\n",
      "Pod: kubia-1\n",
      "Delete kubia-0\n",
      "Storage\n",
      "Storage\n",
      "Storage\n",
      "Pod: kubia-1\n",
      "Storage\n",
      "Node 1\n",
      "kubia-0 rescheduled\n",
      "Node 1\n",
      "Node 2\n",
      "Node 2\n",
      "Node 2\n",
      "Storage\n",
      "Pod: kubia-1\n",
      "Storage\n",
      "Pod: kubia-0\n",
      "Figure 10.11\n",
      "A stateful pod may be rescheduled to a different node, but it retains the name, hostname, and storage.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 331, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "299\n",
      "Discovering peers in a StatefulSet\n",
      " You know how to create the Service by now, but in case you don’t, the following list-\n",
      "ing shows the manifest.\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: kubia-public\n",
      "spec:\n",
      "  selector:\n",
      "    app: kubia\n",
      "  ports:\n",
      "  - port: 80\n",
      "    targetPort: 8080\n",
      "Because this isn’t an externally exposed Service (it’s a regular ClusterIP Service, not\n",
      "a NodePort or a LoadBalancer-type Service), you can only access it from inside the\n",
      "cluster. You’ll need a pod to access it from, right? Not necessarily.\n",
      "CONNECTING TO CLUSTER-INTERNAL SERVICES THROUGH THE API SERVER\n",
      "Instead of using a piggyback pod to access the service from inside the cluster, you can\n",
      "use the same proxy feature provided by the API server to access the service the way\n",
      "you’ve accessed individual pods.\n",
      " The URI path for proxy-ing requests to Services is formed like this:\n",
      "/api/v1/namespaces/<namespace>/services/<service name>/proxy/<path>\n",
      "Therefore, you can run curl on your local machine and access the service through the\n",
      "kubectl proxy like this (you ran kubectl proxy earlier and it should still be running):\n",
      "$ curl localhost:8001/api/v1/namespaces/default/services/kubia-\n",
      "➥ public/proxy/\n",
      "You've hit kubia-1\n",
      "Data stored on this pod: No data posted yet\n",
      "Likewise, clients (inside the cluster) can use the kubia-public service for storing to\n",
      "and reading data from your clustered data store. Of course, each request lands on a\n",
      "random cluster node, so you’ll get the data from a random node each time. You’ll\n",
      "improve this next.\n",
      "10.4\n",
      "Discovering peers in a StatefulSet\n",
      "We still need to cover one more important thing. An important requirement of clus-\n",
      "tered apps is peer discovery—the ability to find other members of the cluster. Each\n",
      "member of a StatefulSet needs to easily find all the other members. Sure, it could do\n",
      "that by talking to the API server, but one of Kubernetes’ aims is to expose features that\n",
      "help keep applications completely Kubernetes-agnostic. Having apps talk to the Kuber-\n",
      "netes API is therefore undesirable.\n",
      "Listing 10.7\n",
      "A regular Service for accessing the stateful pods: kubia-service-public.yaml\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 332, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "300\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      " How can a pod discover its peers without talking to the API? Is there an existing,\n",
      "well-known technology you can use that makes this possible? How about the Domain\n",
      "Name System (DNS)? Depending on how much you know about DNS, you probably\n",
      "understand what an A, CNAME, or MX record is used for. Other lesser-known types of\n",
      "DNS records also exist. One of them is the SRV record.\n",
      "INTRODUCING SRV RECORDS\n",
      "SRV records are used to point to hostnames and ports of servers providing a specific\n",
      "service. Kubernetes creates SRV records to point to the hostnames of the pods back-\n",
      "ing a headless service. \n",
      " You’re going to list the SRV records for your stateful pods by running the dig DNS\n",
      "lookup tool inside a new temporary pod. This is the command you’ll use:\n",
      "$ kubectl run -it srvlookup --image=tutum/dnsutils --rm \n",
      "➥ --restart=Never -- dig SRV kubia.default.svc.cluster.local\n",
      "The command runs a one-off pod (--restart=Never) called srvlookup, which is\n",
      "attached to the console (-it) and is deleted as soon as it terminates (--rm). The\n",
      "pod runs a single container from the tutum/dnsutils image and runs the following\n",
      "command:\n",
      "dig SRV kubia.default.svc.cluster.local\n",
      "The following listing shows what the command prints out.\n",
      "...\n",
      ";; ANSWER SECTION:\n",
      "k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-0.kubia.default.svc.cluster.local.\n",
      "k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-1.kubia.default.svc.cluster.local.\n",
      ";; ADDITIONAL SECTION:\n",
      "kubia-0.kubia.default.svc.cluster.local. 30 IN A 172.17.0.4\n",
      "kubia-1.kubia.default.svc.cluster.local. 30 IN A 172.17.0.6\n",
      "...\n",
      "NOTE\n",
      "I’ve had to shorten the actual name to get records to fit into a single\n",
      "line, so kubia.d.s.c.l is actually kubia.default.svc.cluster.local.\n",
      "The ANSWER SECTION shows two SRV records pointing to the two pods backing your head-\n",
      "less service. Each pod also gets its own A record, as shown in ADDITIONAL SECTION.\n",
      " For a pod to get a list of all the other pods of a StatefulSet, all you need to do is\n",
      "perform an SRV DNS lookup. In Node.js, for example, the lookup is performed\n",
      "like this:\n",
      "dns.resolveSrv(\"kubia.default.svc.cluster.local\", callBackFunction);\n",
      "You’ll use this command in your app to enable each pod to discover its peers.\n",
      "Listing 10.8\n",
      "Listing DNS SRV records of your headless Service\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 333, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "301\n",
      "Discovering peers in a StatefulSet\n",
      "NOTE\n",
      "The order of the returned SRV records is random, because they all have\n",
      "the same priority. Don’t expect to always see kubia-0 listed before kubia-1.\n",
      "10.4.1 Implementing peer discovery through DNS\n",
      "Your Stone Age data store isn’t clustered yet. Each data store node runs completely\n",
      "independently of all the others—no communication exists between them. You’ll get\n",
      "them talking to each other next.\n",
      " Data posted by clients connecting to your data store cluster through the kubia-\n",
      "public Service lands on a random cluster node. The cluster can store multiple data\n",
      "entries, but clients currently have no good way to see all those entries. Because ser-\n",
      "vices forward requests to pods randomly, a client would need to perform many\n",
      "requests until it hit all the pods if it wanted to get the data from all the pods. \n",
      " You can improve this by having the node respond with data from all the cluster\n",
      "nodes. To do this, the node needs to find all its peers. You’re going to use what you\n",
      "learned about StatefulSets and SRV records to do this.\n",
      " You’ll modify your app’s source code as shown in the following listing (the full\n",
      "source is available in the book’s code archive; the listing shows only the important\n",
      "parts).\n",
      "...\n",
      "const dns = require('dns');\n",
      "const dataFile = \"/var/data/kubia.txt\";\n",
      "const serviceName = \"kubia.default.svc.cluster.local\";\n",
      "const port = 8080;\n",
      "...\n",
      "var handler = function(request, response) {\n",
      "  if (request.method == 'POST') {\n",
      "    ...\n",
      "  } else {\n",
      "    response.writeHead(200);\n",
      "    if (request.url == '/data') {\n",
      "      var data = fileExists(dataFile) \n",
      "        ? fs.readFileSync(dataFile, 'utf8') \n",
      "        : \"No data posted yet\";\n",
      "      response.end(data);\n",
      "    } else {\n",
      "      response.write(\"You've hit \" + os.hostname() + \"\\n\");\n",
      "      response.write(\"Data stored in the cluster:\\n\");\n",
      "      dns.resolveSrv(serviceName, function (err, addresses) {    \n",
      "        if (err) {\n",
      "          response.end(\"Could not look up DNS SRV records: \" + err);\n",
      "          return;\n",
      "        }\n",
      "        var numResponses = 0;\n",
      "        if (addresses.length == 0) {\n",
      "          response.end(\"No peers discovered.\");\n",
      "        } else {\n",
      "Listing 10.9\n",
      "Discovering peers in a sample app: kubia-pet-peers-image/app.js\n",
      "The app \n",
      "performs a DNS \n",
      "lookup to obtain \n",
      "SRV records.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 334, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "302\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      "          addresses.forEach(function (item) {                   \n",
      "            var requestOptions = {\n",
      "              host: item.name, \n",
      "              port: port, \n",
      "              path: '/data'\n",
      "            };\n",
      "            httpGet(requestOptions, function (returnedData) {   \n",
      "              numResponses++;\n",
      "              response.write(\"- \" + item.name + \": \" + returnedData);\n",
      "              response.write(\"\\n\");\n",
      "              if (numResponses == addresses.length) {\n",
      "                response.end();\n",
      "              }\n",
      "            });\n",
      "          });\n",
      "        }\n",
      "      });\n",
      "    }\n",
      "  }\n",
      "};\n",
      "...\n",
      "Figure 10.12 shows what happens when a GET request is received by your app. The\n",
      "server that receives the request first performs a lookup of SRV records for the head-\n",
      "less kubia service and then sends a GET request to each of the pods backing the ser-\n",
      "vice (even to itself, which obviously isn’t necessary, but I wanted to keep the code as\n",
      "simple as possible). It then returns a list of all the nodes along with the data stored on\n",
      "each of them.\n",
      "The container image containing this new version of the app is available at docker.io/\n",
      "luksa/kubia-pet-peers.\n",
      "10.4.2 Updating a StatefulSet\n",
      "Your StatefulSet is already running, so let’s see how to update its pod template so the\n",
      "pods use the new image. You’ll also set the replica count to 3 at the same time. To\n",
      "Each pod \n",
      "pointed to by \n",
      "an SRV record is \n",
      "then contacted \n",
      "to get its data.\n",
      "curl\n",
      "DNS\n",
      "1. GET /\n",
      "4. GET /data\n",
      "5. GET /data\n",
      "2. SRV lookup\n",
      "6. Return collated data\n",
      "kubia-0\n",
      "kubia-1\n",
      "kubia-2\n",
      "3. GET /data\n",
      "Figure 10.12\n",
      "The operation of your simplistic distributed data store\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 335, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "303\n",
      "Discovering peers in a StatefulSet\n",
      "update the StatefulSet, use the kubectl edit command (the patch command would\n",
      "be another option):\n",
      "$ kubectl edit statefulset kubia\n",
      "This opens the StatefulSet definition in your default editor. In the definition, change\n",
      "spec.replicas to 3 and modify the spec.template.spec.containers.image attri-\n",
      "bute so it points to the new image (luksa/kubia-pet-peers instead of luksa/kubia-\n",
      "pet). Save the file and exit the editor to update the StatefulSet. Two replicas were\n",
      "running previously, so you should now see an additional replica called kubia-2 start-\n",
      "ing. List the pods to confirm:\n",
      "$ kubectl get po\n",
      "NAME      READY     STATUS              RESTARTS   AGE\n",
      "kubia-0   1/1       Running             0          25m\n",
      "kubia-1   1/1       Running             0          26m\n",
      "kubia-2   0/1       ContainerCreating   0          4s\n",
      "The new pod instance is running the new image. But what about the existing two rep-\n",
      "licas? Judging from their age, they don’t seem to have been updated. This is expected,\n",
      "because initially, StatefulSets were more like ReplicaSets and not like Deployments,\n",
      "so they don’t perform a rollout when the template is modified. You need to delete\n",
      "the replicas manually and the StatefulSet will bring them up again based on the new\n",
      "template:\n",
      "$ kubectl delete po kubia-0 kubia-1\n",
      "pod \"kubia-0\" deleted\n",
      "pod \"kubia-1\" deleted\n",
      "NOTE\n",
      "Starting from Kubernetes version 1.7, StatefulSets support rolling\n",
      "updates the same way Deployments and DaemonSets do. See the StatefulSet’s\n",
      "spec.updateStrategy field documentation using kubectl explain for more\n",
      "information.\n",
      "10.4.3 Trying out your clustered data store\n",
      "Once the two pods are up, you can see if your shiny new Stone Age data store works as\n",
      "expected. Post a few requests to the cluster, as shown in the following listing.\n",
      "$ curl -X POST -d \"The sun is shining\" \\\n",
      "➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/\n",
      "Data stored on pod kubia-1\n",
      "$ curl -X POST -d \"The weather is sweet\" \\\n",
      "➥ localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/\n",
      "Data stored on pod kubia-0\n",
      "Now, read the stored data, as shown in the following listing.\n",
      "Listing 10.10\n",
      "Writing to the clustered data store through the service\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 336, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "304\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      "$ curl localhost:8001/api/v1/namespaces/default/services\n",
      "➥ /kubia-public/proxy/\n",
      "You've hit kubia-2\n",
      "Data stored on each cluster node:\n",
      "- kubia-0.kubia.default.svc.cluster.local: The weather is sweet\n",
      "- kubia-1.kubia.default.svc.cluster.local: The sun is shining\n",
      "- kubia-2.kubia.default.svc.cluster.local: No data posted yet\n",
      "Nice! When a client request reaches one of your cluster nodes, it discovers all its\n",
      "peers, gathers data from them, and sends all the data back to the client. Even if you\n",
      "scale the StatefulSet up or down, the pod servicing the client’s request can always find\n",
      "all the peers running at that time. \n",
      " The app itself isn’t that useful, but I hope you found it a fun way to show how\n",
      "instances of a replicated stateful app can discover their peers and handle horizontal\n",
      "scaling with ease.\n",
      "10.5\n",
      "Understanding how StatefulSets deal with node \n",
      "failures\n",
      "In section 10.2.4 we stated that Kubernetes must be absolutely sure that a stateful\n",
      "pod is no longer running before creating its replacement. When a node fails\n",
      "abruptly, Kubernetes can’t know the state of the node or its pods. It can’t know\n",
      "whether the pods are no longer running, or if they still are and are possibly even still\n",
      "reachable, and it’s only the Kubelet that has stopped reporting the node’s state to\n",
      "the master.\n",
      " Because a StatefulSet guarantees that there will never be two pods running with\n",
      "the same identity and storage, when a node appears to have failed, the StatefulSet can-\n",
      "not and should not create a replacement pod until it knows for certain that the pod is\n",
      "no longer running. \n",
      " It can only know that when the cluster administrator tells it so. To do that, the\n",
      "admin needs to either delete the pod or delete the whole node (doing so then deletes\n",
      "all the pods scheduled to the node).\n",
      " As your final exercise in this chapter, you’ll look at what happens to StatefulSets\n",
      "and their pods when one of the cluster nodes gets disconnected from the network.\n",
      "10.5.1 Simulating a node’s disconnection from the network \n",
      "As in chapter 4, you’ll simulate the node disconnecting from the network by shutting\n",
      "down the node’s eth0 network interface. Because this example requires multiple\n",
      "nodes, you can’t run it on Minikube. You’ll use Google Kubernetes Engine instead.\n",
      "SHUTTING DOWN THE NODE’S NETWORK ADAPTER\n",
      "To shut down a node’s eth0 interface, you need to ssh into one of the nodes like this:\n",
      "$ gcloud compute ssh gke-kubia-default-pool-32a2cac8-m0g1\n",
      "Listing 10.11\n",
      "Reading from the data store\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 337, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "305\n",
      "Understanding how StatefulSets deal with node failures\n",
      "Then, inside the node, run the following command:\n",
      "$ sudo ifconfig eth0 down\n",
      "Your ssh session will stop working, so you’ll need to open another terminal to continue.\n",
      "CHECKING THE NODE’S STATUS AS SEEN BY THE KUBERNETES MASTER\n",
      "With the node’s network interface down, the Kubelet running on the node can no\n",
      "longer contact the Kubernetes API server and let it know that the node and all its pods\n",
      "are still running.\n",
      " After a while, the control plane will mark the node as NotReady. You can see this\n",
      "when listing nodes, as the following listing shows.\n",
      "$ kubectl get node\n",
      "NAME                                   STATUS     AGE       VERSION\n",
      "gke-kubia-default-pool-32a2cac8-596v   Ready      16m       v1.6.2\n",
      "gke-kubia-default-pool-32a2cac8-m0g1   NotReady   16m       v1.6.2\n",
      "gke-kubia-default-pool-32a2cac8-sgl7   Ready      16m       v1.6.2\n",
      "Because the control plane is no longer getting status updates from the node, the\n",
      "status of all pods on that node is Unknown. This is shown in the pod list in the follow-\n",
      "ing listing.\n",
      "$ kubectl get po\n",
      "NAME      READY     STATUS    RESTARTS   AGE\n",
      "kubia-0   1/1       Unknown   0          15m\n",
      "kubia-1   1/1       Running   0          14m\n",
      "kubia-2   1/1       Running   0          13m\n",
      "As you can see, the kubia-0 pod’s status is no longer known because the pod was (and\n",
      "still is) running on the node whose network interface you shut down.\n",
      "UNDERSTANDING WHAT HAPPENS TO PODS WHOSE STATUS IS UNKNOWN\n",
      "If the node were to come back online and report its and its pod statuses again, the pod\n",
      "would again be marked as Running. But if the pod’s status remains unknown for more\n",
      "than a few minutes (this time is configurable), the pod is automatically evicted from\n",
      "the node. This is done by the master (the Kubernetes control plane). It evicts the pod\n",
      "by deleting the pod resource. \n",
      " When the Kubelet sees that the pod has been marked for deletion, it starts ter-\n",
      "minating the pod. In your case, the Kubelet can no longer reach the master (because\n",
      "you disconnected the node from the network), which means the pod will keep\n",
      "running.\n",
      "Listing 10.12\n",
      "Observing a failed node’s status change to NotReady\n",
      "Listing 10.13\n",
      "Observing the pod’s status change after its node becomes NotReady\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 338, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "306\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      " Let’s examine the current situation. Use kubectl describe to display details about\n",
      "the kubia-0 pod, as shown in the following listing.\n",
      "$ kubectl describe po kubia-0\n",
      "Name:        kubia-0\n",
      "Namespace:   default\n",
      "Node:        gke-kubia-default-pool-32a2cac8-m0g1/10.132.0.2\n",
      "...\n",
      "Status:      Terminating (expires Tue, 23 May 2017 15:06:09 +0200)\n",
      "Reason:      NodeLost\n",
      "Message:     Node gke-kubia-default-pool-32a2cac8-m0g1 which was \n",
      "             running pod kubia-0 is unresponsive\n",
      "The pod is shown as Terminating, with NodeLost listed as the reason for the termina-\n",
      "tion. The message says the node is considered lost because it’s unresponsive.\n",
      "NOTE\n",
      "What’s shown here is the control plane’s view of the world. In reality,\n",
      "the pod’s container is still running perfectly fine. It isn’t terminating at all.\n",
      "10.5.2 Deleting the pod manually\n",
      "You know the node isn’t coming back, but you need all three pods running to handle\n",
      "clients properly. You need to get the kubia-0 pod rescheduled to a healthy node. As\n",
      "mentioned earlier, you need to delete the node or the pod manually. \n",
      "DELETING THE POD IN THE USUAL WAY\n",
      "Delete the pod the way you’ve always deleted pods:\n",
      "$ kubectl delete po kubia-0\n",
      "pod \"kubia-0\" deleted\n",
      "All done, right? By deleting the pod, the StatefulSet should immediately create a\n",
      "replacement pod, which will get scheduled to one of the remaining nodes. List the\n",
      "pods again to confirm: \n",
      "$ kubectl get po\n",
      "NAME      READY     STATUS    RESTARTS   AGE\n",
      "kubia-0   1/1       Unknown   0          15m\n",
      "kubia-1   1/1       Running   0          14m\n",
      "kubia-2   1/1       Running   0          13m\n",
      "That’s strange. You deleted the pod a moment ago and kubectl said it had deleted it.\n",
      "Why is the same pod still there? \n",
      "NOTE\n",
      "The kubia-0 pod in the listing isn’t a new pod with the same name—\n",
      "this is clear by looking at the AGE column. If it were new, its age would be\n",
      "merely a few seconds.\n",
      "Listing 10.14\n",
      "Displaying details of the pod with the unknown status\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 339, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "307\n",
      "Summary\n",
      "UNDERSTANDING WHY THE POD ISN’T DELETED\n",
      "The pod was marked for deletion even before you deleted it. That’s because the con-\n",
      "trol plane itself already deleted it (in order to evict it from the node). \n",
      " If you look at listing 10.14 again, you’ll see that the pod’s status is Terminating.\n",
      "The pod was already marked for deletion earlier and will be removed as soon as the\n",
      "Kubelet on its node notifies the API server that the pod’s containers have terminated.\n",
      "Because the node’s network is down, this will never happen. \n",
      "FORCIBLY DELETING THE POD\n",
      "The only thing you can do is tell the API server to delete the pod without waiting for\n",
      "the Kubelet to confirm that the pod is no longer running. You do that like this:\n",
      "$ kubectl delete po kubia-0 --force --grace-period 0\n",
      "warning: Immediate deletion does not wait for confirmation that the running \n",
      "resource has been terminated. The resource may continue to run on the \n",
      "cluster indefinitely.\n",
      "pod \"kubia-0\" deleted\n",
      "You need to use both the --force and --grace-period 0 options. The warning dis-\n",
      "played by kubectl notifies you of what you did. If you list the pods again, you’ll finally\n",
      "see a new kubia-0 pod created:\n",
      "$ kubectl get po\n",
      "NAME          READY     STATUS              RESTARTS   AGE\n",
      "kubia-0       0/1       ContainerCreating   0          8s\n",
      "kubia-1       1/1       Running             0          20m\n",
      "kubia-2       1/1       Running             0          19m\n",
      "WARNING\n",
      "Don’t delete stateful pods forcibly unless you know the node is no\n",
      "longer running or is unreachable (and will remain so forever). \n",
      "Before continuing, you may want to bring the node you disconnected back online.\n",
      "You can do that by restarting the node through the GCE web console or in a terminal\n",
      "by issuing the following command:\n",
      "$ gcloud compute instances reset <node name>\n",
      "10.6\n",
      "Summary\n",
      "This concludes the chapter on using StatefulSets to deploy stateful apps. This chapter\n",
      "has shown you how to\n",
      "Give replicated pods individual storage\n",
      "Provide a stable identity to a pod\n",
      "Create a StatefulSet and a corresponding headless governing Service\n",
      "Scale and update a StatefulSet\n",
      "Discover other members of the StatefulSet through DNS\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 340, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "308\n",
      "CHAPTER 10\n",
      "StatefulSets: deploying replicated stateful applications\n",
      "Connect to other members through their host names\n",
      "Forcibly delete stateful pods\n",
      "Now that you know the major building blocks you can use to have Kubernetes run and\n",
      "manage your apps, we can look more closely at how it does that. In the next chapter,\n",
      "you’ll learn about the individual components that control the Kubernetes cluster and\n",
      "keep your apps running.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 341, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "309\n",
      "Understanding\n",
      "Kubernetes internals\n",
      "By reading this book up to this point, you’ve become familiar with what Kubernetes\n",
      "has to offer and what it does. But so far, I’ve intentionally not spent much time\n",
      "explaining exactly how it does all this because, in my opinion, it makes no sense to\n",
      "go into details of how a system works until you have a good understanding of what\n",
      "the system does. That’s why we haven’t talked about exactly how a pod is scheduled\n",
      "or how the various controllers running inside the Controller Manager make deployed\n",
      "resources come to life. Because you now know most resources that can be deployed in\n",
      "Kubernetes, it’s time to dive into how they’re implemented.\n",
      "This chapter covers\n",
      "What components make up a Kubernetes cluster\n",
      "What each component does and how it does it\n",
      "How creating a Deployment object results in a \n",
      "running pod\n",
      "What a running pod is\n",
      "How the network between pods works\n",
      "How Kubernetes Services work\n",
      "How high-availability is achieved\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 342, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "310\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "11.1\n",
      "Understanding the architecture\n",
      "Before you look at how Kubernetes does what it does, let’s take a closer look at the\n",
      "components that make up a Kubernetes cluster. In chapter 1, you saw that a Kuberne-\n",
      "tes cluster is split into two parts:\n",
      "The Kubernetes Control Plane\n",
      "The (worker) nodes\n",
      "Let’s look more closely at what these two parts do and what’s running inside them.\n",
      "COMPONENTS OF THE CONTROL PLANE\n",
      "The Control Plane is what controls and makes the whole cluster function. To refresh\n",
      "your memory, the components that make up the Control Plane are\n",
      "The etcd distributed persistent storage\n",
      "The API server\n",
      "The Scheduler\n",
      "The Controller Manager\n",
      "These components store and manage the state of the cluster, but they aren’t what runs\n",
      "the application containers. \n",
      "COMPONENTS RUNNING ON THE WORKER NODES\n",
      "The task of running your containers is up to the components running on each\n",
      "worker node:\n",
      "The Kubelet\n",
      "The Kubernetes Service Proxy (kube-proxy)\n",
      "The Container Runtime (Docker, rkt, or others)\n",
      "ADD-ON COMPONENTS\n",
      "Beside the Control Plane components and the components running on the nodes, a\n",
      "few add-on components are required for the cluster to provide everything discussed\n",
      "so far. This includes\n",
      "The Kubernetes DNS server\n",
      "The Dashboard\n",
      "An Ingress controller\n",
      "Heapster, which we’ll talk about in chapter 14\n",
      "The Container Network Interface network plugin (we’ll explain it later in this\n",
      "chapter)\n",
      "11.1.1 The distributed nature of Kubernetes components\n",
      "The previously mentioned components all run as individual processes. The compo-\n",
      "nents and their inter-dependencies are shown in figure 11.1.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 343, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "311\n",
      "Understanding the architecture\n",
      "To get all the features Kubernetes provides, all these components need to be running.\n",
      "But several can also perform useful work individually without the other components.\n",
      "You’ll see how as we examine each of them.\n",
      "HOW THESE COMPONENTS COMMUNICATE\n",
      "Kubernetes system components communicate only with the API server. They don’t\n",
      "talk to each other directly. The API server is the only component that communicates\n",
      "with etcd. None of the other components communicate with etcd directly, but instead\n",
      "modify the cluster state by talking to the API server.\n",
      " Connections between the API server and the other components are almost always\n",
      "initiated by the components, as shown in figure 11.1. But the API server does connect\n",
      "to the Kubelet when you use kubectl to fetch logs, use kubectl attach to connect to\n",
      "a running container, or use the kubectl port-forward command.\n",
      "NOTE\n",
      "The kubectl attach command is similar to kubectl exec, but it attaches\n",
      "to the main process running in the container instead of running an addi-\n",
      "tional one.\n",
      "RUNNING MULTIPLE INSTANCES OF INDIVIDUAL COMPONENTS\n",
      "Although the components on the worker nodes all need to run on the same node,\n",
      "the components of the Control Plane can easily be split across multiple servers. There\n",
      "Checking the status of the Control Plane components\n",
      "The API server exposes an API resource called ComponentStatus, which shows the\n",
      "health status of each Control Plane component. You can list the components and\n",
      "their statuses with kubectl:\n",
      "$ kubectl get componentstatuses\n",
      "NAME                 STATUS    MESSAGE              ERROR\n",
      "scheduler            Healthy   ok\n",
      "controller-manager   Healthy   ok\n",
      "etcd-0               Healthy   {\"health\": \"true\"}\n",
      "Control Plane (master node)\n",
      "Worker node(s)\n",
      "etcd\n",
      "API server\n",
      "kube-proxy\n",
      "Kubelet\n",
      "Scheduler\n",
      "Controller\n",
      "Manager\n",
      "Controller\n",
      "Runtime\n",
      "Figure 11.1\n",
      "Kubernetes \n",
      "components of the Control \n",
      "Plane and the worker nodes\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 344, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "312\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "can be more than one instance of each Control Plane component running to ensure\n",
      "high availability. While multiple instances of etcd and API server can be active at the\n",
      "same time and do perform their jobs in parallel, only a single instance of the Sched-\n",
      "uler and the Controller Manager may be active at a given time—with the others in\n",
      "standby mode.\n",
      "HOW COMPONENTS ARE RUN\n",
      "The Control Plane components, as well as kube-proxy, can either be deployed on the\n",
      "system directly or they can run as pods (as shown in listing 11.1). You may be surprised\n",
      "to hear this, but it will all make sense later when we talk about the Kubelet. \n",
      " The Kubelet is the only component that always runs as a regular system compo-\n",
      "nent, and it’s the Kubelet that then runs all the other components as pods. To run the\n",
      "Control Plane components as pods, the Kubelet is also deployed on the master. The\n",
      "next listing shows pods in the kube-system namespace in a cluster created with\n",
      "kubeadm, which is explained in appendix B.\n",
      "$ kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName \n",
      "➥ --sort-by spec.nodeName -n kube-system\n",
      "POD                              NODE\n",
      "kube-controller-manager-master   master      \n",
      "kube-dns-2334855451-37d9k        master      \n",
      "etcd-master                      master      \n",
      "kube-apiserver-master            master      \n",
      "kube-scheduler-master            master      \n",
      "kube-flannel-ds-tgj9k            node1      \n",
      "kube-proxy-ny3xm                 node1      \n",
      "kube-flannel-ds-0eek8            node2      \n",
      "kube-proxy-sp362                 node2      \n",
      "kube-flannel-ds-r5yf4            node3      \n",
      "kube-proxy-og9ac                 node3      \n",
      "As you can see in the listing, all the Control Plane components are running as pods on\n",
      "the master node. There are three worker nodes, and each one runs the kube-proxy\n",
      "and a Flannel pod, which provides the overlay network for the pods (we’ll talk about\n",
      "Flannel later). \n",
      "TIP\n",
      "As shown in the listing, you can tell kubectl to display custom columns\n",
      "with the -o custom-columns option and sort the resource list with --sort-by.\n",
      "Now, let’s look at each of the components up close, starting with the lowest level com-\n",
      "ponent of the Control Plane—the persistent storage.\n",
      "11.1.2 How Kubernetes uses etcd\n",
      "All the objects you’ve created throughout this book—Pods, ReplicationControllers,\n",
      "Services, Secrets, and so on—need to be stored somewhere in a persistent manner so\n",
      "their manifests survive API server restarts and failures. For this, Kubernetes uses etcd,\n",
      "Listing 11.1\n",
      "Kubernetes components running as pods\n",
      "etcd, API server, Scheduler, \n",
      "Controller Manager, and \n",
      "the DNS server are running \n",
      "on the master.\n",
      "The three nodes each run \n",
      "a Kube Proxy pod and a \n",
      "Flannel networking pod.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 345, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "313\n",
      "Understanding the architecture\n",
      "which is a fast, distributed, and consistent key-value store. Because it’s distributed,\n",
      "you can run more than one etcd instance to provide both high availability and bet-\n",
      "ter performance.\n",
      " The only component that talks to etcd directly is the Kubernetes API server. All\n",
      "other components read and write data to etcd indirectly through the API server. This\n",
      "brings a few benefits, among them a more robust optimistic locking system as well as\n",
      "validation; and, by abstracting away the actual storage mechanism from all the other\n",
      "components, it’s much simpler to replace it in the future. It’s worth emphasizing that\n",
      "etcd is the only place Kubernetes stores cluster state and metadata.\n",
      "HOW RESOURCES ARE STORED IN ETCD\n",
      "As I’m writing this, Kubernetes can use either etcd version 2 or version 3, but version 3\n",
      "is now recommended because of improved performance. etcd v2 stores keys in a hier-\n",
      "archical key space, which makes key-value pairs similar to files in a file system. Each\n",
      "key in etcd is either a directory, which contains other keys, or is a regular key with a\n",
      "corresponding value. etcd v3 doesn’t support directories, but because the key format\n",
      "remains the same (keys can include slashes), you can still think of them as being\n",
      "grouped into directories. Kubernetes stores all its data in etcd under /registry. The\n",
      "following listing shows a list of keys stored under /registry.\n",
      "$ etcdctl ls /registry\n",
      "/registry/configmaps\n",
      "/registry/daemonsets\n",
      "/registry/deployments\n",
      "/registry/events\n",
      "/registry/namespaces\n",
      "/registry/pods\n",
      "...\n",
      "About optimistic concurrency control\n",
      "Optimistic concurrency control (sometimes referred to as optimistic locking) is a\n",
      "method where instead of locking a piece of data and preventing it from being read or\n",
      "updated while the lock is in place, the piece of data includes a version number. Every\n",
      "time the data is updated, the version number increases. When updating the data, the\n",
      "version number is checked to see if it has increased between the time the client read\n",
      "the data and the time it submits the update. If this happens, the update is rejected\n",
      "and the client must re-read the new data and try to update it again. \n",
      "The result is that when two clients try to update the same data entry, only the first\n",
      "one succeeds.\n",
      "All Kubernetes resources include a metadata.resourceVersion field, which clients\n",
      "need to pass back to the API server when updating an object. If the version doesn’t\n",
      "match the one stored in etcd, the API server rejects the update.\n",
      "Listing 11.2\n",
      "Top-level entries stored in etcd by Kubernetes\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 346, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "314\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "You’ll recognize that these keys correspond to the resource types you learned about in\n",
      "the previous chapters. \n",
      "NOTE\n",
      "If you’re using v3 of the etcd API, you can’t use the ls command to see\n",
      "the contents of a directory. Instead, you can list all keys that start with a given\n",
      "prefix with etcdctl get /registry --prefix=true.\n",
      "The following listing shows the contents of the /registry/pods directory.\n",
      "$ etcdctl ls /registry/pods\n",
      "/registry/pods/default\n",
      "/registry/pods/kube-system\n",
      "As you can infer from the names, these two entries correspond to the default and the\n",
      "kube-system namespaces, which means pods are stored per namespace. The follow-\n",
      "ing listing shows the entries in the /registry/pods/default directory.\n",
      "$ etcdctl ls /registry/pods/default\n",
      "/registry/pods/default/kubia-159041347-xk0vc\n",
      "/registry/pods/default/kubia-159041347-wt6ga\n",
      "/registry/pods/default/kubia-159041347-hp2o5\n",
      "Each entry corresponds to an individual pod. These aren’t directories, but key-value\n",
      "entries. The following listing shows what’s stored in one of them.\n",
      "$ etcdctl get /registry/pods/default/kubia-159041347-wt6ga\n",
      "{\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"kubia-159041347-wt6ga\",\n",
      "\"generateName\":\"kubia-159041347-\",\"namespace\":\"default\",\"selfLink\":...\n",
      "You’ll recognize that this is nothing other than a pod definition in JSON format. The\n",
      "API server stores the complete JSON representation of a resource in etcd. Because of\n",
      "etcd’s hierarchical key space, you can think of all the stored resources as JSON files in\n",
      "a filesystem. Simple, right?\n",
      "WARNING\n",
      "Prior to Kubernetes version 1.7, the JSON manifest of a Secret\n",
      "resource was also stored like this (it wasn’t encrypted). If someone got direct\n",
      "access to etcd, they knew all your Secrets. From version 1.7, Secrets are\n",
      "encrypted and thus stored much more securely.\n",
      "ENSURING THE CONSISTENCY AND VALIDITY OF STORED OBJECTS\n",
      "Remember Google’s Borg and Omega systems mentioned in chapter 1, which are\n",
      "what Kubernetes is based on? Like Kubernetes, Omega also uses a centralized store to\n",
      "hold the state of the cluster, but in contrast, multiple Control Plane components\n",
      "access the store directly. All these components need to make sure they all adhere to\n",
      "Listing 11.3\n",
      "Keys in the /registry/pods directory\n",
      "Listing 11.4\n",
      "etcd entries for pods in the default namespace\n",
      "Listing 11.5\n",
      "An etcd entry representing a pod\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 347, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "315\n",
      "Understanding the architecture\n",
      "the same optimistic locking mechanism to handle conflicts properly. A single compo-\n",
      "nent not adhering fully to the mechanism may lead to inconsistent data. \n",
      " Kubernetes improves this by requiring all other Control Plane components to go\n",
      "through the API server. This way updates to the cluster state are always consistent, because\n",
      "the optimistic locking mechanism is implemented in a single place, so less chance exists,\n",
      "if any, of error. The API server also makes sure that the data written to the store is always\n",
      "valid and that changes to the data are only performed by authorized clients. \n",
      "ENSURING CONSISTENCY WHEN ETCD IS CLUSTERED\n",
      "For ensuring high availability, you’ll usually run more than a single instance of etcd.\n",
      "Multiple etcd instances will need to remain consistent. Such a distributed system\n",
      "needs to reach a consensus on what the actual state is. etcd uses the RAFT consensus\n",
      "algorithm to achieve this, which ensures that at any given moment, each node’s state is\n",
      "either what the majority of the nodes agrees is the current state or is one of the previ-\n",
      "ously agreed upon states. \n",
      " Clients connecting to different nodes of an etcd cluster will either see the actual\n",
      "current state or one of the states from the past (in Kubernetes, the only etcd client is\n",
      "the API server, but there may be multiple instances). \n",
      " The consensus algorithm requires a majority (or quorum) for the cluster to progress\n",
      "to the next state. As a result, if the cluster splits into two disconnected groups of nodes,\n",
      "the state in the two groups can never diverge, because to transition from the previous\n",
      "state to the new one, there needs to be more than half of the nodes taking part in\n",
      "the state change. If one group contains the majority of all nodes, the other one obvi-\n",
      "ously doesn’t. The first group can modify the cluster state, whereas the other one can’t.\n",
      "When the two groups reconnect, the second group can catch up with the state in the\n",
      "first group (see figure 11.2).\n",
      "Clients(s)\n",
      "Clients(s)\n",
      "Clients(s)\n",
      "etcd-0\n",
      "etcd-1\n",
      "etcd-2\n",
      "The nodes know\n",
      "there are three nodes\n",
      "in the etcd cluster.\n",
      "etcd-0\n",
      "etcd-1\n",
      "These two nodes know\n",
      "they still have quorum\n",
      "and can accept state\n",
      "changes from clients.\n",
      "etcd-2\n",
      "This node knows it does\n",
      "not have quorum and\n",
      "should therefore not\n",
      "allow state changes.\n",
      "Network\n",
      "split\n",
      "Figure 11.2\n",
      "In a split-brain scenario, only the side which still has the majority (quorum) accepts \n",
      "state changes.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 348, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "316\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "WHY THE NUMBER OF ETCD INSTANCES SHOULD BE AN ODD NUMBER\n",
      "etcd is usually deployed with an odd number of instances. I’m sure you’d like to know\n",
      "why. Let’s compare having two vs. having one instance. Having two instances requires\n",
      "both instances to be present to have a majority. If either of them fails, the etcd cluster\n",
      "can’t transition to a new state because no majority exists. Having two instances is worse\n",
      "than having only a single instance. By having two, the chance of the whole cluster fail-\n",
      "ing has increased by 100%, compared to that of a single-node cluster failing. \n",
      " The same applies when comparing three vs. four etcd instances. With three instances,\n",
      "one instance can fail and a majority (of two) still exists. With four instances, you need\n",
      "three nodes for a majority (two aren’t enough). In both three- and four-instance clus-\n",
      "ters, only a single instance may fail. But when running four instances, if one fails, a\n",
      "higher possibility exists of an additional instance of the three remaining instances fail-\n",
      "ing (compared to a three-node cluster with one failed node and two remaining nodes).\n",
      " Usually, for large clusters, an etcd cluster of five or seven nodes is sufficient. It can\n",
      "handle a two- or a three-node failure, respectively, which suffices in almost all situations. \n",
      "11.1.3 What the API server does\n",
      "The Kubernetes API server is the central component used by all other components\n",
      "and by clients, such as kubectl. It provides a CRUD (Create, Read, Update, Delete)\n",
      "interface for querying and modifying the cluster state over a RESTful API. It stores\n",
      "that state in etcd.\n",
      " In addition to providing a consistent way of storing objects in etcd, it also performs\n",
      "validation of those objects, so clients can’t store improperly configured objects (which\n",
      "they could if they were writing to the store directly). Along with validation, it also han-\n",
      "dles optimistic locking, so changes to an object are never overridden by other clients\n",
      "in the event of concurrent updates.\n",
      " One of the API server’s clients is the command-line tool kubectl you’ve been\n",
      "using from the beginning of the book. When creating a resource from a JSON file, for\n",
      "example, kubectl posts the file’s contents to the API server through an HTTP POST\n",
      "request. Figure 11.3 shows what happens inside the API server when it receives the\n",
      "request. This is explained in more detail in the next few paragraphs.\n",
      "API server\n",
      "etcd\n",
      "Authentication\n",
      "plugin 1\n",
      "Authentication\n",
      "plugin 2\n",
      "Authentication\n",
      "plugin 3\n",
      "Client\n",
      "(\n",
      ")\n",
      "kubectl\n",
      "HTTP POST\n",
      "request\n",
      "Authorization\n",
      "plugin 1\n",
      "Authorization\n",
      "plugin 2\n",
      "Authorization\n",
      "plugin 3\n",
      "Admission\n",
      "control plugin 1\n",
      "Admission\n",
      "control plugin 2\n",
      "Admission\n",
      "control plugin 3\n",
      "Resource\n",
      "validation\n",
      "Figure 11.3\n",
      "The operation of the API server\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 349, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "317\n",
      "Understanding the architecture\n",
      "AUTHENTICATING THE CLIENT WITH AUTHENTICATION PLUGINS\n",
      "First, the API server needs to authenticate the client sending the request. This is per-\n",
      "formed by one or more authentication plugins configured in the API server. The API\n",
      "server calls these plugins in turn, until one of them determines who is sending the\n",
      "request. It does this by inspecting the HTTP request. \n",
      " Depending on the authentication method, the user can be extracted from the cli-\n",
      "ent’s certificate or an HTTP header, such as Authorization, which you used in chap-\n",
      "ter 8. The plugin extracts the client’s username, user ID, and groups the user belongs\n",
      "to. This data is then used in the next stage, which is authorization.\n",
      "AUTHORIZING THE CLIENT WITH AUTHORIZATION PLUGINS\n",
      "Besides authentication plugins, the API server is also configured to use one or more\n",
      "authorization plugins. Their job is to determine whether the authenticated user can\n",
      "perform the requested action on the requested resource. For example, when creating\n",
      "pods, the API server consults all authorization plugins in turn, to determine whether\n",
      "the user can create pods in the requested namespace. As soon as a plugin says the user\n",
      "can perform the action, the API server progresses to the next stage.\n",
      "VALIDATING AND/OR MODIFYING THE RESOURCE IN THE REQUEST WITH ADMISSION CONTROL PLUGINS\n",
      "If the request is trying to create, modify, or delete a resource, the request is sent\n",
      "through Admission Control. Again, the server is configured with multiple Admission\n",
      "Control plugins. These plugins can modify the resource for different reasons. They\n",
      "may initialize fields missing from the resource specification to the configured default\n",
      "values or even override them. They may even modify other related resources, which\n",
      "aren’t in the request, and can also reject a request for whatever reason. The resource\n",
      "passes through all Admission Control plugins.\n",
      "NOTE\n",
      "When the request is only trying to read data, the request doesn’t go\n",
      "through the Admission Control.\n",
      "Examples of Admission Control plugins include\n",
      "\n",
      "AlwaysPullImages—Overrides the pod’s imagePullPolicy to Always, forcing\n",
      "the image to be pulled every time the pod is deployed.\n",
      "\n",
      "ServiceAccount—Applies the default service account to pods that don’t specify\n",
      "it explicitly.\n",
      "\n",
      "NamespaceLifecycle—Prevents creation of pods in namespaces that are in the\n",
      "process of being deleted, as well as in non-existing namespaces.\n",
      "\n",
      "ResourceQuota—Ensures pods in a certain namespace only use as much CPU\n",
      "and memory as has been allotted to the namespace. We’ll learn more about this\n",
      "in chapter 14.\n",
      "You’ll find a list of additional Admission Control plugins in the Kubernetes documen-\n",
      "tation at https:/\n",
      "/kubernetes.io/docs/admin/admission-controllers/.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 350, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "318\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "VALIDATING THE RESOURCE AND STORING IT PERSISTENTLY\n",
      "After letting the request pass through all the Admission Control plugins, the API server\n",
      "then validates the object, stores it in etcd, and returns a response to the client.\n",
      "11.1.4 Understanding how the API server notifies clients of resource \n",
      "changes\n",
      "The API server doesn’t do anything else except what we’ve discussed. For example, it\n",
      "doesn’t create pods when you create a ReplicaSet resource and it doesn’t manage the\n",
      "endpoints of a service. That’s what controllers in the Controller Manager do. \n",
      " But the API server doesn’t even tell these controllers what to do. All it does is\n",
      "enable those controllers and other components to observe changes to deployed\n",
      "resources. A Control Plane component can request to be notified when a resource is\n",
      "created, modified, or deleted. This enables the component to perform whatever task\n",
      "it needs in response to a change of the cluster metadata.\n",
      " Clients watch for changes by opening an HTTP connection to the API server.\n",
      "Through this connection, the client will then receive a stream of modifications to the\n",
      "watched objects. Every time an object is updated, the server sends the new version of\n",
      "the object to all connected clients watching the object. Figure 11.4 shows how clients\n",
      "can watch for changes to pods and how a change to one of the pods is stored into etcd\n",
      "and then relayed to all clients watching pods at that moment.\n",
      "One of the API server’s clients is the kubectl tool, which also supports watching\n",
      "resources. For example, when deploying a pod, you don’t need to constantly poll the list\n",
      "of pods by repeatedly executing kubectl get pods. Instead, you can use the --watch\n",
      "flag and be notified of each creation, modification, or deletion of a pod, as shown in\n",
      "the following listing.\n",
      "$ kubectl get pods --watch\n",
      "NAME                    READY     STATUS              RESTARTS   AGE\n",
      "Listing 11.6\n",
      "Watching a pod being created and then deleted\n",
      "Various\n",
      "clients\n",
      "kubectl\n",
      "API server\n",
      "1. GET /.../pods?watch=true\n",
      "2. POST /.../pods/pod-xyz\n",
      "5. Send updated object\n",
      "to all watchers\n",
      "3. Update object\n",
      "in etcd\n",
      "4. Modiﬁcation\n",
      "notiﬁcation\n",
      "etcd\n",
      "Figure 11.4\n",
      "When an object is updated, the API server sends the updated object to all interested \n",
      "watchers.\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 351, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "319\n",
      "Understanding the architecture\n",
      "kubia-159041347-14j3i   0/1       Pending             0          0s\n",
      "kubia-159041347-14j3i   0/1       Pending             0          0s\n",
      "kubia-159041347-14j3i   0/1       ContainerCreating   0          1s\n",
      "kubia-159041347-14j3i   0/1       Running             0          3s\n",
      "kubia-159041347-14j3i   1/1       Running             0          5s\n",
      "kubia-159041347-14j3i   1/1       Terminating         0          9s\n",
      "kubia-159041347-14j3i   0/1       Terminating         0          17s\n",
      "kubia-159041347-14j3i   0/1       Terminating         0          17s\n",
      "kubia-159041347-14j3i   0/1       Terminating         0          17s\n",
      "You can even have kubectl print out the whole YAML on each watch event like this:\n",
      "$ kubectl get pods -o yaml --watch\n",
      "The watch mechanism is also used by the Scheduler, which is the next Control Plane\n",
      "component you’re going to learn more about.\n",
      "11.1.5 Understanding the Scheduler\n",
      "You’ve already learned that you don’t usually specify which cluster node a pod should\n",
      "run on. This is left to the Scheduler. From afar, the operation of the Scheduler looks\n",
      "simple. All it does is wait for newly created pods through the API server’s watch mech-\n",
      "anism and assign a node to each new pod that doesn’t already have the node set. \n",
      " The Scheduler doesn’t instruct the selected node (or the Kubelet running on that\n",
      "node) to run the pod. All the Scheduler does is update the pod definition through the\n",
      "API server. The API server then notifies the Kubelet (again, through the watch mech-\n",
      "anism described previously) that the pod has been scheduled. As soon as the Kubelet\n",
      "on the target node sees the pod has been scheduled to its node, it creates and runs the\n",
      "pod’s containers.\n",
      " Although a coarse-grained view of the scheduling process seems trivial, the actual\n",
      "task of selecting the best node for the pod isn’t that simple. Sure, the simplest\n",
      "Scheduler could pick a random node and not care about the pods already running on\n",
      "that node. On the other side of the spectrum, the Scheduler could use advanced tech-\n",
      "niques such as machine learning to anticipate what kind of pods are about to be\n",
      "scheduled in the next minutes or hours and schedule pods to maximize future hard-\n",
      "ware utilization without requiring any rescheduling of existing pods. Kubernetes’\n",
      "default Scheduler falls somewhere in between. \n",
      "UNDERSTANDING THE DEFAULT SCHEDULING ALGORITHM\n",
      "The selection of a node can be broken down into two parts, as shown in figure 11.5:\n",
      "Filtering the list of all nodes to obtain a list of acceptable nodes the pod can be\n",
      "scheduled to.\n",
      "Prioritizing the acceptable nodes and choosing the best one. If multiple nodes\n",
      "have the highest score, round-robin is used to ensure pods are deployed across\n",
      "all of them evenly.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "183\n",
      "height\n",
      "124\n",
      "PIX BUFFER SIZE\n",
      "68076\n",
      "Original IMG_BUFFER_SIZE\n",
      "68076\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011e40>\n",
      "page_image_dict\n",
      "{'page': 352, 'img_cnt': 1, 'img_npy_lst': []}\n",
      "320\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "FINDING ACCEPTABLE NODES\n",
      "To determine which nodes are acceptable for the pod, the Scheduler passes each\n",
      "node through a list of configured predicate functions. These check various things\n",
      "such as\n",
      "Can the node fulfill the pod’s requests for hardware resources? You’ll learn how\n",
      "to specify them in chapter 14.\n",
      "Is the node running out of resources (is it reporting a memory or a disk pres-\n",
      "sure condition)? \n",
      "If the pod requests to be scheduled to a specific node (by name), is this the node?\n",
      "Does the node have a label that matches the node selector in the pod specifica-\n",
      "tion (if one is defined)?\n",
      "If the pod requests to be bound to a specific host port (discussed in chapter 13),\n",
      "is that port already taken on this node or not? \n",
      "If the pod requests a certain type of volume, can this volume be mounted for\n",
      "this pod on this node, or is another pod on the node already using the same\n",
      "volume?\n",
      "Does the pod tolerate the taints of the node? Taints and tolerations are explained\n",
      "in chapter 16.\n",
      "Does the pod specify node and/or pod affinity or anti-affinity rules? If yes,\n",
      "would scheduling the pod to this node break those rules? This is also explained\n",
      "in chapter 16.\n",
      "All these checks must pass for the node to be eligible to host the pod. After perform-\n",
      "ing these checks on every node, the Scheduler ends up with a subset of the nodes. Any\n",
      "of these nodes could run the pod, because they have enough available resources for\n",
      "the pod and conform to all requirements you’ve specified in the pod definition.\n",
      "SELECTING THE BEST NODE FOR THE POD\n",
      "Even though all these nodes are acceptable and can run the pod, several may be a\n",
      "better choice than others. Suppose you have a two-node cluster. Both nodes are eli-\n",
      "gible, but one is already running 10 pods, while the other, for whatever reason, isn’t\n",
      "running any pods right now. It’s obvious the Scheduler should favor the second\n",
      "node in this case. \n",
      "Node 1\n",
      "Node 2\n",
      "Node 3\n",
      "Node 4\n",
      "Node 5\n",
      "...\n",
      "Find acceptable\n",
      "nodes\n",
      "Node 1\n",
      "Node 2\n",
      "Node 3\n",
      "Node 4\n",
      "Node 5\n",
      "...\n",
      "Prioritize nodes\n",
      "and select the\n",
      "top one\n",
      "Node 3\n",
      "Node 1\n",
      "Node 4\n",
      "Figure 11.5\n",
      "The Scheduler finds acceptable nodes for a pod and then selects the best node \n",
      "for the pod.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 353, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "321\n",
      "Understanding the architecture\n",
      " Or is it? If these two nodes are provided by the cloud infrastructure, it may be bet-\n",
      "ter to schedule the pod to the first node and relinquish the second node back to the\n",
      "cloud provider to save money. \n",
      "ADVANCED SCHEDULING OF PODS\n",
      "Consider another example. Imagine having multiple replicas of a pod. Ideally, you’d\n",
      "want them spread across as many nodes as possible instead of having them all sched-\n",
      "uled to a single one. Failure of that node would cause the service backed by those\n",
      "pods to become unavailable. But if the pods were spread across different nodes, a sin-\n",
      "gle node failure would barely leave a dent in the service’s capacity. \n",
      " Pods belonging to the same Service or ReplicaSet are spread across multiple nodes\n",
      "by default. It’s not guaranteed that this is always the case. But you can force pods to be\n",
      "spread around the cluster or kept close together by defining pod affinity and anti-\n",
      "affinity rules, which are explained in chapter 16. \n",
      " Even these two simple cases show how complex scheduling can be, because it\n",
      "depends on a multitude of factors. Because of this, the Scheduler can either be config-\n",
      "ured to suit your specific needs or infrastructure specifics, or it can even be replaced\n",
      "with a custom implementation altogether. You could also run a Kubernetes cluster\n",
      "without a Scheduler, but then you’d have to perform the scheduling manually.\n",
      "USING MULTIPLE SCHEDULERS\n",
      "Instead of running a single Scheduler in the cluster, you can run multiple Schedulers.\n",
      "Then, for each pod, you specify the Scheduler that should schedule this particular\n",
      "pod by setting the schedulerName property in the pod spec.\n",
      " Pods without this property set are scheduled using the default Scheduler, and so\n",
      "are pods with schedulerName set to default-scheduler. All other pods are ignored by\n",
      "the default Scheduler, so they need to be scheduled either manually or by another\n",
      "Scheduler watching for such pods. \n",
      " You can implement your own Schedulers and deploy them in the cluster, or you\n",
      "can deploy an additional instance of Kubernetes’ Scheduler with different configura-\n",
      "tion options.\n",
      "11.1.6 Introducing the controllers running in the Controller Manager\n",
      "As previously mentioned, the API server doesn’t do anything except store resources in\n",
      "etcd and notify clients about the change. The Scheduler only assigns a node to the\n",
      "pod, so you need other active components to make sure the actual state of the system\n",
      "converges toward the desired state, as specified in the resources deployed through the\n",
      "API server. This work is done by controllers running inside the Controller Manager. \n",
      " The single Controller Manager process currently combines a multitude of control-\n",
      "lers performing various reconciliation tasks. Eventually those controllers will be split\n",
      "up into separate processes, enabling you to replace each one with a custom imple-\n",
      "mentation if necessary. The list of these controllers includes the\n",
      "Replication Manager (a controller for ReplicationController resources)\n",
      "ReplicaSet, DaemonSet, and Job controllers\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 354, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "322\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "Deployment controller\n",
      "StatefulSet controller\n",
      "Node controller\n",
      "Service controller\n",
      "Endpoints controller\n",
      "Namespace controller\n",
      "PersistentVolume controller\n",
      "Others\n",
      "What each of these controllers does should be evident from its name. From the list,\n",
      "you can tell there’s a controller for almost every resource you can create. Resources\n",
      "are descriptions of what should be running in the cluster, whereas the controllers are\n",
      "the active Kubernetes components that perform actual work as a result of the deployed\n",
      "resources.\n",
      "UNDERSTANDING WHAT CONTROLLERS DO AND HOW THEY DO IT\n",
      "Controllers do many different things, but they all watch the API server for changes to\n",
      "resources (Deployments, Services, and so on) and perform operations for each change,\n",
      "whether it’s a creation of a new object or an update or deletion of an existing object.\n",
      "Most of the time, these operations include creating other resources or updating the\n",
      "watched resources themselves (to update the object’s status, for example).\n",
      " In general, controllers run a reconciliation loop, which reconciles the actual state\n",
      "with the desired state (specified in the resource’s spec section) and writes the new\n",
      "actual state to the resource’s status section. Controllers use the watch mechanism to\n",
      "be notified of changes, but because using watches doesn’t guarantee the controller\n",
      "won’t miss an event, they also perform a re-list operation periodically to make sure\n",
      "they haven’t missed anything.\n",
      " Controllers never talk to each other directly. They don’t even know any other con-\n",
      "trollers exist. Each controller connects to the API server and, through the watch\n",
      "mechanism described in section 11.1.3, asks to be notified when a change occurs in\n",
      "the list of resources of any type the controller is responsible for. \n",
      " We’ll briefly look at what each of the controllers does, but if you’d like an in-depth\n",
      "view of what they do, I suggest you look at their source code directly. The sidebar\n",
      "explains how to get started.\n",
      "A few pointers on exploring the controllers’ source code\n",
      "If you’re interested in seeing exactly how these controllers operate, I strongly encour-\n",
      "age you to browse through their source code. To make it easier, here are a few tips:\n",
      "The source code for the controllers is available at https:/\n",
      "/github.com/kubernetes/\n",
      "kubernetes/blob/master/pkg/controller.\n",
      "Each controller usually has a constructor in which it creates an Informer, which is\n",
      "basically a listener that gets called every time an API object gets updated. Usually,\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 355, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "323\n",
      "Understanding the architecture\n",
      "THE REPLICATION MANAGER\n",
      "The controller that makes ReplicationController resources come to life is called the\n",
      "Replication Manager. We talked about how ReplicationControllers work in chapter 4.\n",
      "It’s not the ReplicationControllers that do the actual work, but the Replication Man-\n",
      "ager. Let’s quickly review what the controller does, because this will help you under-\n",
      "stand the rest of the controllers.\n",
      " In chapter 4, we said that the operation of a ReplicationController could be\n",
      "thought of as an infinite loop, where in each iteration, the controller finds the num-\n",
      "ber of pods matching its pod selector and compares the number to the desired replica\n",
      "count. \n",
      " Now that you know how the API server can notify clients through the watch\n",
      "mechanism, it’s clear that the controller doesn’t poll the pods in every iteration, but\n",
      "is instead notified by the watch mechanism of each change that may affect the\n",
      "desired replica count or the number of matched pods (see figure 11.6). Any such\n",
      "changes trigger the controller to recheck the desired vs. actual replica count and act\n",
      "accordingly.\n",
      " You already know that when too few pod instances are running, the Replication-\n",
      "Controller runs additional instances. But it doesn’t actually run them itself. It creates\n",
      "an Informer listens for changes to a specific type of resource. Looking at the con-\n",
      "structor will show you which resources the controller is watching.\n",
      "Next, go look for the worker() method. In it, you’ll find the method that gets invoked\n",
      "each time the controller needs to do something. The actual function is often stored\n",
      "in a field called syncHandler or something similar. This field is also initialized in the\n",
      "constructor, so that’s where you’ll find the name of the function that gets called. That\n",
      "function is the place where all the magic happens.\n",
      "Controller Manager\n",
      "Watches\n",
      "Creates and\n",
      "deletes\n",
      "Replication\n",
      "Manager\n",
      "API server\n",
      "ReplicationController\n",
      "resources\n",
      "Pod resources\n",
      "Other resources\n",
      "Figure 11.6\n",
      "The Replication Manager watches for changes to API \n",
      "objects.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 356, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "324\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "new Pod manifests, posts them to the API server, and lets the Scheduler and the\n",
      "Kubelet do their job of scheduling and running the pod.\n",
      " The Replication Manager performs its work by manipulating Pod API objects\n",
      "through the API server. This is how all controllers operate.\n",
      "THE REPLICASET, THE DAEMONSET, AND THE JOB CONTROLLERS\n",
      "The ReplicaSet controller does almost the same thing as the Replication Manager\n",
      "described previously, so we don’t have much to add here. The DaemonSet and Job\n",
      "controllers are similar. They create Pod resources from the pod template defined in\n",
      "their respective resources. Like the Replication Manager, these controllers don’t run\n",
      "the pods, but post Pod definitions to the API server, letting the Kubelet create their\n",
      "containers and run them.\n",
      "THE DEPLOYMENT CONTROLLER\n",
      "The Deployment controller takes care of keeping the actual state of a deployment in\n",
      "sync with the desired state specified in the corresponding Deployment API object. \n",
      " The Deployment controller performs a rollout of a new version each time a\n",
      "Deployment object is modified (if the modification should affect the deployed pods).\n",
      "It does this by creating a ReplicaSet and then appropriately scaling both the old and\n",
      "the new ReplicaSet based on the strategy specified in the Deployment, until all the old\n",
      "pods have been replaced with new ones. It doesn’t create any pods directly.\n",
      "THE STATEFULSET CONTROLLER\n",
      "The StatefulSet controller, similarly to the ReplicaSet controller and other related\n",
      "controllers, creates, manages, and deletes Pods according to the spec of a StatefulSet\n",
      "resource. But while those other controllers only manage Pods, the StatefulSet control-\n",
      "ler also instantiates and manages PersistentVolumeClaims for each Pod instance.\n",
      "THE NODE CONTROLLER\n",
      "The Node controller manages the Node resources, which describe the cluster’s worker\n",
      "nodes. Among other things, a Node controller keeps the list of Node objects in sync\n",
      "with the actual list of machines running in the cluster. It also monitors each node’s\n",
      "health and evicts pods from unreachable nodes.\n",
      " The Node controller isn’t the only component making changes to Node objects.\n",
      "They’re also changed by the Kubelet, and can obviously also be modified by users\n",
      "through REST API calls. \n",
      "THE SERVICE CONTROLLER\n",
      "In chapter 5, when we talked about Services, you learned that a few different types\n",
      "exist. One of them was the LoadBalancer service, which requests a load balancer from\n",
      "the infrastructure to make the service available externally. The Service controller is\n",
      "the one requesting and releasing a load balancer from the infrastructure, when a\n",
      "LoadBalancer-type Service is created or deleted.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 357, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "325\n",
      "Understanding the architecture\n",
      "THE ENDPOINTS CONTROLLER\n",
      "You’ll remember that Services aren’t linked directly to pods, but instead contain a list\n",
      "of endpoints (IPs and ports), which is created and updated either manually or auto-\n",
      "matically according to the pod selector defined on the Service. The Endpoints con-\n",
      "troller is the active component that keeps the endpoint list constantly updated with\n",
      "the IPs and ports of pods matching the label selector.\n",
      " As figure 11.7 shows, the controller watches both Services and Pods. When\n",
      "Services are added or updated or Pods are added, updated, or deleted, it selects Pods\n",
      "matching the Service’s pod selector and adds their IPs and ports to the Endpoints\n",
      "resource. Remember, the Endpoints object is a standalone object, so the controller\n",
      "creates it if necessary. Likewise, it also deletes the Endpoints object when the Service is\n",
      "deleted.\n",
      "THE NAMESPACE CONTROLLER\n",
      "Remember namespaces (we talked about them in chapter 3)? Most resources belong\n",
      "to a specific namespace. When a Namespace resource is deleted, all the resources in\n",
      "that namespace must also be deleted. This is what the Namespace controller does.\n",
      "When it’s notified of the deletion of a Namespace object, it deletes all the resources\n",
      "belonging to the namespace through the API server. \n",
      "THE PERSISTENTVOLUME CONTROLLER\n",
      "In chapter 6 you learned about PersistentVolumes and PersistentVolumeClaims.\n",
      "Once a user creates a PersistentVolumeClaim, Kubernetes must find an appropriate\n",
      "PersistentVolume and bind it to the claim. This is performed by the PersistentVolume\n",
      "controller. \n",
      " When a PersistentVolumeClaim pops up, the controller finds the best match for\n",
      "the claim by selecting the smallest PersistentVolume with the access mode matching\n",
      "the one requested in the claim and the declared capacity above the capacity requested\n",
      "Controller Manager\n",
      "Watches\n",
      "Creates, modiﬁes,\n",
      "and deletes\n",
      "Endpoints\n",
      "controller\n",
      "API server\n",
      "Service resources\n",
      "Pod resources\n",
      "Endpoints resources\n",
      "Figure 11.7\n",
      "The Endpoints controller watches Service and Pod resources, \n",
      "and manages Endpoints.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 358, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "326\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "in the claim. It does this by keeping an ordered list of PersistentVolumes for each\n",
      "access mode by ascending capacity and returning the first volume from the list.\n",
      " Then, when the user deletes the PersistentVolumeClaim, the volume is unbound\n",
      "and reclaimed according to the volume’s reclaim policy (left as is, deleted, or emptied).\n",
      "CONTROLLER WRAP-UP\n",
      "You should now have a good feel for what each controller does and how controllers\n",
      "work in general. Again, all these controllers operate on the API objects through the\n",
      "API server. They don’t communicate with the Kubelets directly or issue any kind of\n",
      "instructions to them. In fact, they don’t even know Kubelets exist. After a controller\n",
      "updates a resource in the API server, the Kubelets and Kubernetes Service Proxies,\n",
      "also oblivious of the controllers’ existence, perform their work, such as spinning up a\n",
      "pod’s containers and attaching network storage to them, or in the case of services, set-\n",
      "ting up the actual load balancing across pods. \n",
      " The Control Plane handles one part of the operation of the whole system, so to\n",
      "fully understand how things unfold in a Kubernetes cluster, you also need to under-\n",
      "stand what the Kubelet and the Kubernetes Service Proxy do. We’ll learn that next.\n",
      "11.1.7 What the Kubelet does\n",
      "In contrast to all the controllers, which are part of the Kubernetes Control Plane and\n",
      "run on the master node(s), the Kubelet and the Service Proxy both run on the worker\n",
      "nodes, where the actual pods containers run. What does the Kubelet do exactly?\n",
      "UNDERSTANDING THE KUBELET’S JOB\n",
      "In a nutshell, the Kubelet is the component responsible for everything running on a\n",
      "worker node. Its initial job is to register the node it’s running on by creating a Node\n",
      "resource in the API server. Then it needs to continuously monitor the API server for\n",
      "Pods that have been scheduled to the node, and start the pod’s containers. It does this\n",
      "by telling the configured container runtime (which is Docker, CoreOS’ rkt, or some-\n",
      "thing else) to run a container from a specific container image. The Kubelet then con-\n",
      "stantly monitors running containers and reports their status, events, and resource\n",
      "consumption to the API server. \n",
      " The Kubelet is also the component that runs the container liveness probes, restart-\n",
      "ing containers when the probes fail. Lastly, it terminates containers when their Pod is\n",
      "deleted from the API server and notifies the server that the pod has terminated.\n",
      "RUNNING STATIC PODS WITHOUT THE API SERVER\n",
      "Although the Kubelet talks to the Kubernetes API server and gets the pod manifests\n",
      "from there, it can also run pods based on pod manifest files in a specific local direc-\n",
      "tory as shown in figure 11.8. This feature is used to run the containerized versions of\n",
      "the Control Plane components as pods, as you saw in the beginning of the chapter.\n",
      " Instead of running Kubernetes system components natively, you can put their pod\n",
      "manifests into the Kubelet’s manifest directory and have the Kubelet run and manage\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 359, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "327\n",
      "Understanding the architecture\n",
      "them. You can also use the same method to run your custom system containers, but\n",
      "doing it through a DaemonSet is the recommended method.\n",
      "11.1.8 The role of the Kubernetes Service Proxy\n",
      "Beside the Kubelet, every worker node also runs the kube-proxy, whose purpose is to\n",
      "make sure clients can connect to the services you define through the Kubernetes API.\n",
      "The kube-proxy makes sure connections to the service IP and port end up at one of\n",
      "the pods backing that service (or other, non-pod service endpoints). When a service is\n",
      "backed by more than one pod, the proxy performs load balancing across those pods. \n",
      "WHY IT’S CALLED A PROXY\n",
      "The initial implementation of the kube-proxy was the userspace proxy. It used an\n",
      "actual server process to accept connections and proxy them to the pods. To inter-\n",
      "cept connections destined to the service IPs, the proxy configured iptables rules\n",
      "(iptables is the tool for managing the Linux kernel’s packet filtering features) to\n",
      "redirect the connections to the proxy server. A rough diagram of the userspace proxy\n",
      "mode is shown in figure 11.9.\n",
      "Container Runtime\n",
      "(Docker, rkt, ...)\n",
      "Kubelet\n",
      "API server\n",
      "Worker node\n",
      "Runs, monitors,\n",
      "and manages\n",
      "containers\n",
      "Pod resource\n",
      "Container A\n",
      "Container B\n",
      "Container A\n",
      "Container B\n",
      "Container C\n",
      "Pod manifest (ﬁle)\n",
      "Local manifest directory\n",
      "Container C\n",
      "Figure 11.8\n",
      "The Kubelet runs pods based on pod specs from the API server and a local file directory.\n",
      "Client\n",
      "kube-proxy\n",
      "Conﬁgures\n",
      ":\n",
      "iptables\n",
      "redirect through proxy server\n",
      "iptables\n",
      "Pod\n",
      "Figure 11.9\n",
      "The userspace proxy mode\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 360, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "328\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "The kube-proxy got its name because it was an actual proxy, but the current, much\n",
      "better performing implementation only uses iptables rules to redirect packets to a\n",
      "randomly selected backend pod without passing them through an actual proxy server.\n",
      "This mode is called the iptables proxy mode and is shown in figure 11.10.\n",
      "The major difference between these two modes is whether packets pass through the\n",
      "kube-proxy and must be handled in user space, or whether they’re handled only by\n",
      "the Kernel (in kernel space). This has a major impact on performance. \n",
      " Another smaller difference is that the userspace proxy mode balanced connec-\n",
      "tions across pods in a true round-robin fashion, while the iptables proxy mode\n",
      "doesn’t—it selects pods randomly. When only a few clients use a service, they may not\n",
      "be spread evenly across pods. For example, if a service has two backing pods but only\n",
      "five or so clients, don’t be surprised if you see four clients connect to pod A and only\n",
      "one client connect to pod B. With a higher number of clients or pods, this problem\n",
      "isn’t so apparent.\n",
      " You’ll learn exactly how iptables proxy mode works in section 11.5. \n",
      "11.1.9 Introducing Kubernetes add-ons\n",
      "We’ve now discussed the core components that make a Kubernetes cluster work. But\n",
      "in the beginning of the chapter, we also listed a few add-ons, which although not\n",
      "always required, enable features such as DNS lookup of Kubernetes services, exposing\n",
      "multiple HTTP services through a single external IP address, the Kubernetes web\n",
      "dashboard, and so on.\n",
      "HOW ADD-ONS ARE DEPLOYED\n",
      "These components are available as add-ons and are deployed as pods by submitting\n",
      "YAML manifests to the API server, the way you’ve been doing throughout the book.\n",
      "Some of these components are deployed through a Deployment resource or a Repli-\n",
      "cationController resource, and some through a DaemonSet. \n",
      " For example, as I’m writing this, in Minikube, the Ingress controller and the\n",
      "dashboard add-ons are deployed as ReplicationControllers, as shown in the follow-\n",
      "ing listing.\n",
      " \n",
      "Client\n",
      "Conﬁgures\n",
      ":\n",
      "iptables\n",
      "redirect straight to pod\n",
      "(no proxy server in-between)\n",
      "iptables\n",
      "Pod\n",
      "kube-proxy\n",
      "Figure 11.10\n",
      "The iptables proxy mode\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 361, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "329\n",
      "Understanding the architecture\n",
      "$ kubectl get rc -n kube-system\n",
      "NAME                       DESIRED   CURRENT   READY     AGE\n",
      "default-http-backend       1         1         1         6d\n",
      "kubernetes-dashboard       1         1         1         6d\n",
      "nginx-ingress-controller   1         1         1         6d\n",
      "The DNS add-on is deployed as a Deployment, as shown in the following listing.\n",
      "$ kubectl get deploy -n kube-system\n",
      "NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\n",
      "kube-dns   1         1         1            1           6d\n",
      "Let’s see how DNS and the Ingress controllers work.\n",
      "HOW THE DNS SERVER WORKS\n",
      "All the pods in the cluster are configured to use the cluster’s internal DNS server by\n",
      "default. This allows pods to easily look up services by name or even the pod’s IP\n",
      "addresses in the case of headless services.\n",
      " The DNS server pod is exposed through the kube-dns service, allowing the pod to\n",
      "be moved around the cluster, like any other pod. The service’s IP address is specified\n",
      "as the nameserver in the /etc/resolv.conf file inside every container deployed in the\n",
      "cluster. The kube-dns pod uses the API server’s watch mechanism to observe changes\n",
      "to Services and Endpoints and updates its DNS records with every change, allowing its\n",
      "clients to always get (fairly) up-to-date DNS information. I say fairly because during\n",
      "the time between the update of the Service or Endpoints resource and the time the\n",
      "DNS pod receives the watch notification, the DNS records may be invalid.\n",
      "HOW (MOST) INGRESS CONTROLLERS WORK\n",
      "Unlike the DNS add-on, you’ll find a few different implementations of Ingress con-\n",
      "trollers, but most of them work in the same way. An Ingress controller runs a reverse\n",
      "proxy server (like Nginx, for example), and keeps it configured according to the\n",
      "Ingress, Service, and Endpoints resources defined in the cluster. The controller thus\n",
      "needs to observe those resources (again, through the watch mechanism) and change\n",
      "the proxy server’s config every time one of them changes. \n",
      " Although the Ingress resource’s definition points to a Service, Ingress controllers\n",
      "forward traffic to the service’s pod directly instead of going through the service IP.\n",
      "This affects the preservation of client IPs when external clients connect through the\n",
      "Ingress controller, which makes them preferred over Services in certain use cases.\n",
      "USING OTHER ADD-ONS\n",
      "You’ve seen how both the DNS server and the Ingress controller add-ons are similar to\n",
      "the controllers running in the Controller Manager, except that they also accept client\n",
      "connections instead of only observing and modifying resources through the API server. \n",
      "Listing 11.7\n",
      "Add-ons deployed with ReplicationControllers in Minikube\n",
      "Listing 11.8\n",
      "The kube-dns Deployment \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 362, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "330\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      " Other add-ons are similar. They all need to observe the cluster state and perform\n",
      "the necessary actions when that changes. We’ll introduce a few other add-ons in this\n",
      "and the remaining chapters.\n",
      "11.1.10Bringing it all together\n",
      "You’ve now learned that the whole Kubernetes system is composed of relatively small,\n",
      "loosely coupled components with good separation of concerns. The API server, the\n",
      "Scheduler, the individual controllers running inside the Controller Manager, the\n",
      "Kubelet, and the kube-proxy all work together to keep the actual state of the system\n",
      "synchronized with what you specify as the desired state. \n",
      " For example, submitting a pod manifest to the API server triggers a coordinated\n",
      "dance of various Kubernetes components, which eventually results in the pod’s con-\n",
      "tainers running. You’ll learn how this dance unfolds in the next section. \n",
      "11.2\n",
      "How controllers cooperate\n",
      "You now know about all the components that a Kubernetes cluster is comprised of.\n",
      "Now, to solidify your understanding of how Kubernetes works, let’s go over what hap-\n",
      "pens when a Pod resource is created. Because you normally don’t create Pods directly,\n",
      "you’re going to create a Deployment resource instead and see everything that must\n",
      "happen for the pod’s containers to be started.\n",
      "11.2.1 Understanding which components are involved\n",
      "Even before you start the whole process, the controllers, the Scheduler, and the\n",
      "Kubelet are watching the API server for changes to their respective resource types.\n",
      "This is shown in figure 11.11. The components depicted in the figure will each play a\n",
      "part in the process you’re about to trigger. The diagram doesn’t include etcd, because\n",
      "it’s hidden behind the API server, and you can think of the API server as the place\n",
      "where objects are stored.\n",
      "Master node\n",
      "Controller Manager\n",
      "Watches\n",
      "Deployment\n",
      "controller\n",
      "Scheduler\n",
      "ReplicaSet\n",
      "controller\n",
      "API server\n",
      "Deployments\n",
      "Pods\n",
      "ReplicaSets\n",
      "Watches\n",
      "Watches\n",
      "Node X\n",
      "Watches\n",
      "Docker\n",
      "Kubelet\n",
      "Figure 11.11\n",
      "Kubernetes components watching API objects through the API server\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 363, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "331\n",
      "How controllers cooperate\n",
      "11.2.2 The chain of events\n",
      "Imagine you prepared the YAML file containing the Deployment manifest and you’re\n",
      "about to submit it to Kubernetes through kubectl. kubectl sends the manifest to the\n",
      "Kubernetes API server in an HTTP POST request. The API server validates the Deploy-\n",
      "ment specification, stores it in etcd, and returns a response to kubectl. Now a chain\n",
      "of events starts to unfold, as shown in figure 11.12.\n",
      "THE DEPLOYMENT CONTROLLER CREATES THE REPLICASET\n",
      "All API server clients watching the list of Deployments through the API server’s watch\n",
      "mechanism are notified of the newly created Deployment resource immediately after\n",
      "it’s created. One of those clients is the Deployment controller, which, as we discussed\n",
      "earlier, is the active component responsible for handling Deployments. \n",
      " As you may remember from chapter 9, a Deployment is backed by one or more\n",
      "ReplicaSets, which then create the actual pods. As a new Deployment object is\n",
      "detected by the Deployment controller, it creates a ReplicaSet for the current speci-\n",
      "fication of the Deployment. This involves creating a new ReplicaSet resource\n",
      "through the Kubernetes API. The Deployment controller doesn’t deal with individ-\n",
      "ual pods at all.\n",
      "Master node\n",
      "Controller\n",
      "Manager\n",
      "2. Notiﬁcation\n",
      "through watch\n",
      "3. Creates\n",
      "ReplicaSet\n",
      "4. Notiﬁcation\n",
      "5. Creates pod\n",
      "6. Notiﬁcation\n",
      "through watch\n",
      "7. Assigns pod to node\n",
      "1. Creates Deployment\n",
      "resource\n",
      "Deployment\n",
      "controller\n",
      "Scheduler\n",
      "kubectl\n",
      "ReplicaSet\n",
      "controller\n",
      "API server\n",
      "Deployment A\n",
      "Deployments\n",
      "ReplicaSets\n",
      "Pod A\n",
      "Pods\n",
      "ReplicaSet A\n",
      "Node X\n",
      "8. Notiﬁcation\n",
      "through watch\n",
      "9. Tells Docker to\n",
      "run containers\n",
      "Docker\n",
      "10. Runs\n",
      "containers\n",
      "Container(s)\n",
      "Kubelet\n",
      "Figure 11.12\n",
      "The chain of events that unfolds when a Deployment resource is posted to the API server\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 364, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "332\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "THE REPLICASET CONTROLLER CREATES THE POD RESOURCES\n",
      "The newly created ReplicaSet is then picked up by the ReplicaSet controller, which\n",
      "watches for creations, modifications, and deletions of ReplicaSet resources in the\n",
      "API server. The controller takes into consideration the replica count and pod selec-\n",
      "tor defined in the ReplicaSet and verifies whether enough existing Pods match\n",
      "the selector.\n",
      " The controller then creates the Pod resources based on the pod template in the\n",
      "ReplicaSet (the pod template was copied over from the Deployment when the Deploy-\n",
      "ment controller created the ReplicaSet). \n",
      "THE SCHEDULER ASSIGNS A NODE TO THE NEWLY CREATED PODS\n",
      "These newly created Pods are now stored in etcd, but they each still lack one import-\n",
      "ant thing—they don’t have an associated node yet. Their nodeName attribute isn’t set.\n",
      "The Scheduler watches for Pods like this, and when it encounters one, chooses the\n",
      "best node for the Pod and assigns the Pod to the node. The Pod’s definition now\n",
      "includes the name of the node it should be running on.\n",
      " Everything so far has been happening in the Kubernetes Control Plane. None of\n",
      "the controllers that have taken part in this whole process have done anything tangible\n",
      "except update the resources through the API server. \n",
      "THE KUBELET RUNS THE POD’S CONTAINERS\n",
      "The worker nodes haven’t done anything up to this point. The pod’s containers\n",
      "haven’t been started yet. The images for the pod’s containers haven’t even been down-\n",
      "loaded yet. \n",
      " But with the Pod now scheduled to a specific node, the Kubelet on that node can\n",
      "finally get to work. The Kubelet, watching for changes to Pods on the API server, sees a\n",
      "new Pod scheduled to its node, so it inspects the Pod definition and instructs Docker,\n",
      "or whatever container runtime it’s using, to start the pod’s containers. The container\n",
      "runtime then runs the containers.\n",
      "11.2.3 Observing cluster events\n",
      "Both the Control Plane components and the Kubelet emit events to the API server as\n",
      "they perform these actions. They do this by creating Event resources, which are like\n",
      "any other Kubernetes resource. You’ve already seen events pertaining to specific\n",
      "resources every time you used kubectl describe to inspect those resources, but you\n",
      "can also retrieve events directly with kubectl get events.\n",
      " Maybe it’s me, but using kubectl get to inspect events is painful, because they’re\n",
      "not shown in proper temporal order. Instead, if an event occurs multiple times, the\n",
      "event is displayed only once, showing when it was first seen, when it was last seen, and\n",
      "the number of times it occurred. Luckily, watching events with the --watch option is\n",
      "much easier on the eyes and useful for seeing what’s happening in the cluster. \n",
      " The following listing shows the events emitted in the process described previously\n",
      "(some columns have been removed and the output is edited heavily to make it legible\n",
      "in the limited space on the page).\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 365, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "333\n",
      "Understanding what a running pod is\n",
      "$ kubectl get events --watch\n",
      "    NAME             KIND         REASON              SOURCE \n",
      "... kubia            Deployment   ScalingReplicaSet   deployment-controller  \n",
      "                     ➥ Scaled up replica set kubia-193 to 3\n",
      "... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  \n",
      "                     ➥ Created pod: kubia-193-w7ll2\n",
      "... kubia-193-tpg6j  Pod          Scheduled           default-scheduler   \n",
      "                     ➥ Successfully assigned kubia-193-tpg6j to node1\n",
      "... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  \n",
      "                     ➥ Created pod: kubia-193-39590\n",
      "... kubia-193        ReplicaSet   SuccessfulCreate    replicaset-controller  \n",
      "                     ➥ Created pod: kubia-193-tpg6j\n",
      "... kubia-193-39590  Pod          Scheduled           default-scheduler  \n",
      "                     ➥ Successfully assigned kubia-193-39590 to node2\n",
      "... kubia-193-w7ll2  Pod          Scheduled           default-scheduler  \n",
      "                     ➥ Successfully assigned kubia-193-w7ll2 to node2\n",
      "... kubia-193-tpg6j  Pod          Pulled              kubelet, node1  \n",
      "                     ➥ Container image already present on machine\n",
      "... kubia-193-tpg6j  Pod          Created             kubelet, node1  \n",
      "                     ➥ Created container with id 13da752\n",
      "... kubia-193-39590  Pod          Pulled              kubelet, node2  \n",
      "                     ➥ Container image already present on machine\n",
      "... kubia-193-tpg6j  Pod          Started             kubelet, node1  \n",
      "                     ➥ Started container with id 13da752\n",
      "... kubia-193-w7ll2  Pod          Pulled              kubelet, node2  \n",
      "                     ➥ Container image already present on machine\n",
      "... kubia-193-39590  Pod          Created             kubelet, node2  \n",
      "                     ➥ Created container with id 8850184\n",
      "...\n",
      "As you can see, the SOURCE column shows the controller performing the action, and\n",
      "the NAME and KIND columns show the resource the controller is acting on. The REASON\n",
      "column and the MESSAGE column (shown in every second line) give more details\n",
      "about what the controller has done.\n",
      "11.3\n",
      "Understanding what a running pod is\n",
      "With the pod now running, let’s look more closely at what a running pod even is. If a\n",
      "pod contains a single container, do you think that the Kubelet just runs this single\n",
      "container, or is there more to it?\n",
      " You’ve run several pods throughout this book. If you’re the investigative type, you\n",
      "may have already snuck a peek at what exactly Docker ran when you created a pod. If\n",
      "not, let me explain what you’d see.\n",
      " Imagine you run a single container pod. Let’s say you create an Nginx pod:\n",
      "$ kubectl run nginx --image=nginx\n",
      "deployment \"nginx\" created\n",
      "You can now ssh into the worker node running the pod and inspect the list of run-\n",
      "ning Docker containers. I’m using Minikube to test this out, so to ssh into the single\n",
      "Listing 11.9\n",
      "Watching events emitted by the controllers\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 366, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "334\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "node, I use minikube ssh. If you’re using GKE, you can ssh into a node with gcloud\n",
      "compute ssh <node name>.\n",
      " Once you’re inside the node, you can list all the running containers with docker\n",
      "ps, as shown in the following listing.\n",
      "docker@minikubeVM:~$ docker ps\n",
      "CONTAINER ID   IMAGE                  COMMAND                 CREATED\n",
      "c917a6f3c3f7   nginx                  \"nginx -g 'daemon off\"  4 seconds ago \n",
      "98b8bf797174   gcr.io/.../pause:3.0   \"/pause\"                7 seconds ago\n",
      "...\n",
      "NOTE\n",
      "I’ve removed irrelevant information from the previous listing—this\n",
      "includes both columns and rows. I’ve also removed all the other running con-\n",
      "tainers. If you’re trying this out yourself, pay attention to the two containers\n",
      "that were created a few seconds ago. \n",
      "As expected, you see the Nginx container, but also an additional container. Judging\n",
      "from the COMMAND column, this additional container isn’t doing anything (the con-\n",
      "tainer’s command is \"pause\"). If you look closely, you’ll see that this container was\n",
      "created a few seconds before the Nginx container. What’s its role?\n",
      " This pause container is the container that holds all the containers of a pod\n",
      "together. Remember how all containers of a pod share the same network and other\n",
      "Linux namespaces? The pause container is an infrastructure container whose sole\n",
      "purpose is to hold all these namespaces. All other user-defined containers of the pod\n",
      "then use the namespaces of the pod infrastructure container (see figure 11.13).\n",
      "Actual application containers may die and get restarted. When such a container starts\n",
      "up again, it needs to become part of the same Linux namespaces as before. The infra-\n",
      "structure container makes this possible since its lifecycle is tied to that of the pod—the\n",
      "container runs from the time the pod is scheduled until the pod is deleted. If the\n",
      "infrastructure pod is killed in the meantime, the Kubelet recreates it and all the pod’s\n",
      "containers.\n",
      "Listing 11.10\n",
      "Listing running Docker containers\n",
      "Pod\n",
      "Container A\n",
      "Container A\n",
      "Pod infrastructure\n",
      "container\n",
      "Container B\n",
      "Container B\n",
      "Uses Linux\n",
      "namespaces from\n",
      "Uses Linux\n",
      "namespaces from\n",
      "Figure 11.13\n",
      "A two-container pod results in three running containers \n",
      "sharing the same Linux namespaces.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 367, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "335\n",
      "Inter-pod networking\n",
      "11.4\n",
      "Inter-pod networking\n",
      "By now, you know that each pod gets its own unique IP address and can communicate\n",
      "with all other pods through a flat, NAT-less network. How exactly does Kubernetes\n",
      "achieve this? In short, it doesn’t. The network is set up by the system administrator or\n",
      "by a Container Network Interface (CNI) plugin, not by Kubernetes itself. \n",
      "11.4.1 What the network must be like\n",
      "Kubernetes doesn’t require you to use a specific networking technology, but it does\n",
      "mandate that the pods (or to be more precise, their containers) can communicate\n",
      "with each other, regardless if they’re running on the same worker node or not. The\n",
      "network the pods use to communicate must be such that the IP address a pod sees as\n",
      "its own is the exact same address that all other pods see as the IP address of the pod in\n",
      "question. \n",
      " Look at figure 11.14. When pod A connects to (sends a network packet to) pod B,\n",
      "the source IP pod B sees must be the same IP that pod A sees as its own. There should\n",
      "be no network address translation (NAT) performed in between—the packet sent by\n",
      "pod A must reach pod B with both the source and destination address unchanged.\n",
      "This is important, because it makes networking for applications running inside pods\n",
      "simple and exactly as if they were running on machines connected to the same net-\n",
      "work switch. The absence of NAT between pods enables applications running inside\n",
      "them to self-register in other pods. \n",
      "Node 1\n",
      "Pod A\n",
      "IP: 10.1.1.1\n",
      "srcIP: 10.1.1.1\n",
      "dstIP: 10.1.2.1\n",
      "srcIP: 10.1.1.1\n",
      "dstIP: 10.1.2.1\n",
      "Packet\n",
      "Node 2\n",
      "Pod B\n",
      "IP: 10.1.2.1\n",
      "srcIP: 10.1.1.1\n",
      "dstIP: 10.1.2.1\n",
      "Packet\n",
      "Network\n",
      "No NAT (IPs\n",
      "are preserved)\n",
      "Figure 11.14\n",
      "Kubernetes mandates pods are connected through a NAT-less \n",
      "network.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 368, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "336\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      " For example, say you have a client pod X and pod Y, which provides a kind of noti-\n",
      "fication service to all pods that register with it. Pod X connects to pod Y and tells it,\n",
      "“Hey, I’m pod X, available at IP 1.2.3.4; please send updates to me at this IP address.”\n",
      "The pod providing the service can connect to the first pod by using the received\n",
      "IP address. \n",
      " The requirement for NAT-less communication between pods also extends to pod-\n",
      "to-node and node-to-pod communication. But when a pod communicates with ser-\n",
      "vices out on the internet, the source IP of the packets the pod sends does need to be\n",
      "changed, because the pod’s IP is private. The source IP of outbound packets is\n",
      "changed to the host worker node’s IP address.\n",
      " Building a proper Kubernetes cluster involves setting up the networking according\n",
      "to these requirements. There are various methods and technologies available to do\n",
      "this, each with its own benefits or drawbacks in a given scenario. Because of this, we’re\n",
      "not going to go into specific technologies. Instead, let’s explain how inter-pod net-\n",
      "working works in general. \n",
      "11.4.2 Diving deeper into how networking works\n",
      "In section 11.3, we saw that a pod’s IP address and network namespace are set up and\n",
      "held by the infrastructure container (the pause container). The pod’s containers then\n",
      "use its network namespace. A pod’s network interface is thus whatever is set up in the\n",
      "infrastructure container. Let’s see how the interface is created and how it’s connected\n",
      "to the interfaces in all the other pods. Look at figure 11.15. We’ll discuss it next.\n",
      "ENABLING COMMUNICATION BETWEEN PODS ON THE SAME NODE\n",
      "Before the infrastructure container is started, a virtual Ethernet interface pair (a veth\n",
      "pair) is created for the container. One interface of the pair remains in the host’s\n",
      "namespace (you’ll see it listed as vethXXX when you run ifconfig on the node),\n",
      "whereas the other is moved into the container’s network namespace and renamed\n",
      "eth0. The two virtual interfaces are like two ends of a pipe (or like two network\n",
      "devices connected by an Ethernet cable)—what goes in on one side comes out on the\n",
      "other, and vice-versa. \n",
      "Node\n",
      "Pod A\n",
      "eth0\n",
      "10.1.1.1\n",
      "veth123\n",
      "Pod B\n",
      "eth0\n",
      "10.1.1.2\n",
      "veth234\n",
      "Bridge\n",
      "10.1.1.0/24\n",
      "This is pod A’s\n",
      "veth pair.\n",
      "This is pod B’s\n",
      "veth pair.\n",
      "Figure 11.15\n",
      "Pods on a node are \n",
      "connected to the same bridge through \n",
      "virtual Ethernet interface pairs.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 369, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "337\n",
      "Inter-pod networking\n",
      " The interface in the host’s network namespace is attached to a network bridge that\n",
      "the container runtime is configured to use. The eth0 interface in the container is\n",
      "assigned an IP address from the bridge’s address range. Anything that an application\n",
      "running inside the container sends to the eth0 network interface (the one in the con-\n",
      "tainer’s namespace), comes out at the other veth interface in the host’s namespace\n",
      "and is sent to the bridge. This means it can be received by any network interface that’s\n",
      "connected to the bridge. \n",
      " If pod A sends a network packet to pod B, the packet first goes through pod A’s\n",
      "veth pair to the bridge and then through pod B’s veth pair. All containers on a node\n",
      "are connected to the same bridge, which means they can all communicate with each\n",
      "other. But to enable communication between containers running on different nodes,\n",
      "the bridges on those nodes need to be connected somehow. \n",
      "ENABLING COMMUNICATION BETWEEN PODS ON DIFFERENT NODES\n",
      "You have many ways to connect bridges on different nodes. This can be done with\n",
      "overlay or underlay networks or by regular layer 3 routing, which we’ll look at next.\n",
      " You know pod IP addresses must be unique across the whole cluster, so the bridges\n",
      "across the nodes must use non-overlapping address ranges to prevent pods on differ-\n",
      "ent nodes from getting the same IP. In the example shown in figure 11.16, the bridge\n",
      "on node A is using the 10.1.1.0/24 IP range and the bridge on node B is using\n",
      "10.1.2.0/24, which ensures no IP address conflicts exist.\n",
      " Figure 11.16 shows that to enable communication between pods across two nodes\n",
      "with plain layer 3 networking, the node’s physical network interface needs to be con-\n",
      "nected to the bridge as well. Routing tables on node A need to be configured so all\n",
      "packets destined for 10.1.2.0/24 are routed to node B, whereas node B’s routing\n",
      "tables need to be configured so packets sent to 10.1.1.0/24 are routed to node A.\n",
      " With this type of setup, when a packet is sent by a container on one of the nodes\n",
      "to a container on the other node, the packet first goes through the veth pair, then\n",
      "Node A\n",
      "Pod A\n",
      "Network\n",
      "eth0\n",
      "10.1.1.1\n",
      "veth123\n",
      "Pod B\n",
      "eth0\n",
      "10.1.1.2\n",
      "veth234\n",
      "Bridge\n",
      "10.1.1.0/24\n",
      "eth0\n",
      "10.100.0.1\n",
      "Node B\n",
      "Pod C\n",
      "eth0\n",
      "10.1.2.1\n",
      "veth345\n",
      "Pod D\n",
      "eth0\n",
      "10.1.2.2\n",
      "veth456\n",
      "Bridge\n",
      "10.1.2.0/24\n",
      "eth0\n",
      "10.100.0.2\n",
      "Figure 11.16\n",
      "For pods on different nodes to communicate, the bridges need to be connected \n",
      "somehow.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 370, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "338\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "through the bridge to the node’s physical adapter, then over the wire to the other\n",
      "node’s physical adapter, through the other node’s bridge, and finally through the veth\n",
      "pair of the destination container.\n",
      " This works only when nodes are connected to the same network switch, without\n",
      "any routers in between; otherwise those routers would drop the packets because\n",
      "they refer to pod IPs, which are private. Sure, the routers in between could be con-\n",
      "figured to route packets between the nodes, but this becomes increasingly difficult\n",
      "and error-prone as the number of routers between the nodes increases. Because of\n",
      "this, it’s easier to use a Software Defined Network (SDN), which makes the nodes\n",
      "appear as though they’re connected to the same network switch, regardless of the\n",
      "actual underlying network topology, no matter how complex it is. Packets sent\n",
      "from the pod are encapsulated and sent over the network to the node running the\n",
      "other pod, where they are de-encapsulated and delivered to the pod in their origi-\n",
      "nal form.\n",
      "11.4.3 Introducing the Container Network Interface\n",
      "To make it easier to connect containers into a network, a project called Container\n",
      "Network Interface (CNI) was started. The CNI allows Kubernetes to be configured to\n",
      "use any CNI plugin that’s out there. These plugins include\n",
      "Calico\n",
      "Flannel\n",
      "Romana\n",
      "Weave Net \n",
      "And others\n",
      "We’re not going to go into the details of these plugins; if you want to learn more about\n",
      "them, refer to https:/\n",
      "/kubernetes.io/docs/concepts/cluster-administration/addons/.\n",
      " Installing a network plugin isn’t difficult. You only need to deploy a YAML con-\n",
      "taining a DaemonSet and a few other supporting resources. This YAML is provided\n",
      "on each plugin’s project page. As you can imagine, the DaemonSet is used to deploy\n",
      "a network agent on all cluster nodes. It then ties into the CNI interface on the node,\n",
      "but be aware that the Kubelet needs to be started with --network-plugin=cni to\n",
      "use CNI. \n",
      "11.5\n",
      "How services are implemented\n",
      "In chapter 5 you learned about Services, which allow exposing a set of pods at a long-\n",
      "lived, stable IP address and port. In order to focus on what Services are meant for and\n",
      "how they can be used, we intentionally didn’t go into how they work. But to truly\n",
      "understand Services and have a better feel for where to look when things don’t behave\n",
      "the way you expect, you need to understand how they are implemented. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 371, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "339\n",
      "How services are implemented\n",
      "11.5.1 Introducing the kube-proxy\n",
      "Everything related to Services is handled by the kube-proxy process running on each\n",
      "node. Initially, the kube-proxy was an actual proxy waiting for connections and for\n",
      "each incoming connection, opening a new connection to one of the pods. This was\n",
      "called the userspace proxy mode. Later, a better-performing iptables proxy mode\n",
      "replaced it. This is now the default, but you can still configure Kubernetes to use the\n",
      "old mode if you want.\n",
      " Before we continue, let’s quickly review a few things about Services, which are rele-\n",
      "vant for understanding the next few paragraphs.\n",
      " We’ve learned that each Service gets its own stable IP address and port. Clients\n",
      "(usually pods) use the service by connecting to this IP address and port. The IP\n",
      "address is virtual—it’s not assigned to any network interfaces and is never listed as\n",
      "either the source or the destination IP address in a network packet when the packet\n",
      "leaves the node. A key detail of Services is that they consist of an IP and port pair (or\n",
      "multiple IP and port pairs in the case of multi-port Services), so the service IP by itself\n",
      "doesn’t represent anything. That’s why you can’t ping them. \n",
      "11.5.2 How kube-proxy uses iptables\n",
      "When a service is created in the API server, the virtual IP address is assigned to it\n",
      "immediately. Soon afterward, the API server notifies all kube-proxy agents running on\n",
      "the worker nodes that a new Service has been created. Then, each kube-proxy makes\n",
      "that service addressable on the node it’s running on. It does this by setting up a few\n",
      "iptables rules, which make sure each packet destined for the service IP/port pair is\n",
      "intercepted and its destination address modified, so the packet is redirected to one of\n",
      "the pods backing the service. \n",
      " Besides watching the API server for changes to Services, kube-proxy also watches\n",
      "for changes to Endpoints objects. We talked about them in chapter 5, but let me\n",
      "refresh your memory, as it’s easy to forget they even exist, because you rarely create\n",
      "them manually. An Endpoints object holds the IP/port pairs of all the pods that back\n",
      "the service (an IP/port pair can also point to something other than a pod). That’s\n",
      "why the kube-proxy must also watch all Endpoints objects. After all, an Endpoints\n",
      "object changes every time a new backing pod is created or deleted, and when the\n",
      "pod’s readiness status changes or the pod’s labels change and it falls in or out of scope\n",
      "of the service. \n",
      " Now let’s see how kube-proxy enables clients to connect to those pods through the\n",
      "Service. This is shown in figure 11.17.\n",
      " The figure shows what the kube-proxy does and how a packet sent by a client pod\n",
      "reaches one of the pods backing the Service. Let’s examine what happens to the\n",
      "packet when it’s sent by the client pod (pod A in the figure). \n",
      " The packet’s destination is initially set to the IP and port of the Service (in the\n",
      "example, the Service is at 172.30.0.1:80). Before being sent to the network, the\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 372, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "340\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "packet is first handled by node A’s kernel according to the iptables rules set up on\n",
      "the node. \n",
      " The kernel checks if the packet matches any of those iptables rules. One of them\n",
      "says that if any packet has the destination IP equal to 172.30.0.1 and destination port\n",
      "equal to 80, the packet’s destination IP and port should be replaced with the IP and\n",
      "port of a randomly selected pod. \n",
      " The packet in the example matches that rule and so its destination IP/port is\n",
      "changed. In the example, pod B2 was randomly selected, so the packet’s destination\n",
      "IP is changed to 10.1.2.1 (pod B2’s IP) and the port to 8080 (the target port specified\n",
      "in the Service spec). From here on, it’s exactly as if the client pod had sent the packet\n",
      "to pod B directly instead of through the service. \n",
      " It’s slightly more complicated than that, but that’s the most important part you\n",
      "need to understand.\n",
      " \n",
      "Node A\n",
      "Node B\n",
      "API server\n",
      "Pod A\n",
      "Pod B1\n",
      "Pod B2\n",
      "Pod B3\n",
      "Packet X\n",
      "Source:\n",
      "10.1.1.1\n",
      "Destination:\n",
      "172.30.0.1:80\n",
      "10.1.2.1:8080\n",
      "iptables\n",
      "Service B\n",
      "172.30.0.1:80\n",
      "Conﬁgures\n",
      "iptables\n",
      "Packet X\n",
      "Source:\n",
      "10.1.1.1\n",
      "Destination:\n",
      "172.30.0.1:80\n",
      "kube-proxy\n",
      "Endpoints B\n",
      "Pod A\n",
      "IP: 10.1.1.1\n",
      "Pod B1\n",
      "IP: 10.1.1.2\n",
      "Pod B2\n",
      "IP: 10.1.2.1\n",
      "Pod B3\n",
      "IP: 10.1.2.2\n",
      "Watches for changes to\n",
      "services and endpoints\n",
      "Figure 11.17\n",
      "Network packets sent to a Service’s virtual IP/port pair are \n",
      "modified and redirected to a randomly selected backend pod.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 373, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "341\n",
      "Running highly available clusters\n",
      "11.6\n",
      "Running highly available clusters\n",
      "One of the reasons for running apps inside Kubernetes is to keep them running with-\n",
      "out interruption with no or limited manual intervention in case of infrastructure\n",
      "failures. For running services without interruption it’s not only the apps that need to\n",
      "be up all the time, but also the Kubernetes Control Plane components. We’ll look at\n",
      "what’s involved in achieving high availability next.\n",
      "11.6.1 Making your apps highly available\n",
      "When running apps in Kubernetes, the various controllers make sure your app keeps\n",
      "running smoothly and at the specified scale even when nodes fail. To ensure your app\n",
      "is highly available, you only need to run them through a Deployment resource and\n",
      "configure an appropriate number of replicas; everything else is taken care of by\n",
      "Kubernetes. \n",
      "RUNNING MULTIPLE INSTANCES TO REDUCE THE LIKELIHOOD OF DOWNTIME\n",
      "This requires your apps to be horizontally scalable, but even if that’s not the case in\n",
      "your app, you should still use a Deployment with its replica count set to one. If the\n",
      "replica becomes unavailable, it will be replaced with a new one quickly, although that\n",
      "doesn’t happen instantaneously. It takes time for all the involved controllers to notice\n",
      "that a node has failed, create the new pod replica, and start the pod’s containers.\n",
      "There will inevitably be a short period of downtime in between. \n",
      "USING LEADER-ELECTION FOR NON-HORIZONTALLY SCALABLE APPS\n",
      "To avoid the downtime, you need to run additional inactive replicas along with the\n",
      "active one and use a fast-acting lease or leader-election mechanism to make sure only\n",
      "one is active. In case you’re unfamiliar with leader election, it’s a way for multiple app\n",
      "instances running in a distributed environment to come to an agreement on which is\n",
      "the leader. That leader is either the only one performing tasks, while all others are\n",
      "waiting for the leader to fail and then becoming leaders themselves, or they can all be\n",
      "active, with the leader being the only instance performing writes, while all the others\n",
      "are providing read-only access to their data, for example. This ensures two instances\n",
      "are never doing the same job, if that would lead to unpredictable system behavior due\n",
      "to race conditions.\n",
      " The mechanism doesn’t need to be incorporated into the app itself. You can use a\n",
      "sidecar container that performs all the leader-election operations and signals the\n",
      "main container when it should become active. You’ll find an example of leader elec-\n",
      "tion in Kubernetes at https:/\n",
      "/github.com/kubernetes/contrib/tree/master/election.\n",
      " Ensuring your apps are highly available is relatively simple, because Kubernetes\n",
      "takes care of almost everything. But what if Kubernetes itself fails? What if the servers\n",
      "running the Kubernetes Control Plane components go down? How are those compo-\n",
      "nents made highly available?\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 374, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "342\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "11.6.2 Making Kubernetes Control Plane components highly available\n",
      "In the beginning of this chapter, you learned about the few components that make up\n",
      "a Kubernetes Control Plane. To make Kubernetes highly available, you need to run\n",
      "multiple master nodes, which run multiple instances of the following components:\n",
      "etcd, which is the distributed data store where all the API objects are kept\n",
      "API server\n",
      "Controller Manager, which is the process in which all the controllers run\n",
      "Scheduler\n",
      "Without going into the actual details of how to install and run these components, let’s\n",
      "see what’s involved in making each of these components highly available. Figure 11.18\n",
      "shows an overview of a highly available cluster.\n",
      "RUNNING AN ETCD CLUSTER\n",
      "Because etcd was designed as a distributed system, one of its key features is the ability\n",
      "to run multiple etcd instances, so making it highly available is no big deal. All you\n",
      "need to do is run it on an appropriate number of machines (three, five, or seven, as\n",
      "explained earlier in the chapter) and make them aware of each other. You do this by\n",
      "including the list of all the other instances in every instance’s configuration. For\n",
      "example, when starting an instance, you specify the IPs and ports where the other etcd\n",
      "instances can be reached. \n",
      " etcd will replicate data across all its instances, so a failure of one of the nodes when\n",
      "running a three-machine cluster will still allow the cluster to accept both read and\n",
      "write operations. To increase the fault tolerance to more than a single node, you need\n",
      "to run five or seven etcd nodes, which would allow the cluster to handle two or three\n",
      "Node 1\n",
      "Kubelet\n",
      "Node 2\n",
      "Kubelet\n",
      "Node 3\n",
      "Kubelet\n",
      "Node 4\n",
      "Kubelet\n",
      "Node 5\n",
      "Kubelet\n",
      "...\n",
      "Node N\n",
      "Kubelet\n",
      "Load\n",
      "balancer\n",
      "Master 3\n",
      "etcd\n",
      "API server\n",
      "Scheduler\n",
      "Controller\n",
      "Manager\n",
      "[standing-by]\n",
      "[standing-by]\n",
      "Master 2\n",
      "etcd\n",
      "API server\n",
      "Scheduler\n",
      "Controller\n",
      "Manager\n",
      "[standing-by]\n",
      "[standing-by]\n",
      "Master 1\n",
      "etcd\n",
      "API server\n",
      "Scheduler\n",
      "Controller\n",
      "Manager\n",
      "[active]\n",
      "[active]\n",
      "Figure 11.18\n",
      "A highly-available cluster with three master nodes\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 375, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "343\n",
      "Running highly available clusters\n",
      "node failures, respectively. Having more than seven etcd instances is almost never nec-\n",
      "essary and begins impacting performance.\n",
      "RUNNING MULTIPLE INSTANCES OF THE API SERVER\n",
      "Making the API server highly available is even simpler. Because the API server is (almost\n",
      "completely) stateless (all the data is stored in etcd, but the API server does cache it), you\n",
      "can run as many API servers as you need, and they don’t need to be aware of each other\n",
      "at all. Usually, one API server is collocated with every etcd instance. By doing this, the\n",
      "etcd instances don’t need any kind of load balancer in front of them, because every API\n",
      "server instance only talks to the local etcd instance. \n",
      " The API servers, on the other hand, do need to be fronted by a load balancer, so\n",
      "clients (kubectl, but also the Controller Manager, Scheduler, and all the Kubelets)\n",
      "always connect only to the healthy API server instances. \n",
      "ENSURING HIGH AVAILABILITY OF THE CONTROLLERS AND THE SCHEDULER\n",
      "Compared to the API server, where multiple replicas can run simultaneously, run-\n",
      "ning multiple instances of the Controller Manager or the Scheduler isn’t as simple.\n",
      "Because controllers and the Scheduler all actively watch the cluster state and act when\n",
      "it changes, possibly modifying the cluster state further (for example, when the desired\n",
      "replica count on a ReplicaSet is increased by one, the ReplicaSet controller creates an\n",
      "additional pod), running multiple instances of each of those components would\n",
      "result in all of them performing the same action. They’d be racing each other, which\n",
      "could cause undesired effects (creating two new pods instead of one, as mentioned in\n",
      "the previous example).\n",
      " For this reason, when running multiple instances of these components, only one\n",
      "instance may be active at any given time. Luckily, this is all taken care of by the compo-\n",
      "nents themselves (this is controlled with the --leader-elect option, which defaults to\n",
      "true). Each individual component will only be active when it’s the elected leader. Only\n",
      "the leader performs actual work, whereas all other instances are standing by and waiting\n",
      "for the current leader to fail. When it does, the remaining instances elect a new leader,\n",
      "which then takes over the work. This mechanism ensures that two components are never\n",
      "operating at the same time and doing the same work (see figure 11.19).\n",
      "Master 3\n",
      "Scheduler\n",
      "Controller\n",
      "Manager\n",
      "[standing-by]\n",
      "[standing-by]\n",
      "Master 1\n",
      "Scheduler\n",
      "Controller\n",
      "Manager\n",
      "[active]\n",
      "[active]\n",
      "Master 2\n",
      "Scheduler\n",
      "Controller\n",
      "Manager\n",
      "[standing-by]\n",
      "[standing-by]\n",
      "Only the controllers in\n",
      "this Controller Manager\n",
      "are reacting to API\n",
      "resources being created,\n",
      "updated, and deleted.\n",
      "These Controller Managers\n",
      "and Schedulers aren’t doing\n",
      "anything except waiting to\n",
      "become leaders.\n",
      "Only this Scheduler\n",
      "is scheduling pods.\n",
      "Figure 11.19\n",
      "Only a single Controller Manager and a single Scheduler are active; others are standing by.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 376, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "344\n",
      "CHAPTER 11\n",
      "Understanding Kubernetes internals\n",
      "The Controller Manager and Scheduler can run collocated with the API server and\n",
      "etcd, or they can run on separate machines. When collocated, they can talk to the\n",
      "local API server directly; otherwise they connect to the API servers through the load\n",
      "balancer.\n",
      "UNDERSTANDING THE LEADER ELECTION MECHANISM USED IN CONTROL PLANE COMPONENTS\n",
      "What I find most interesting here is that these components don’t need to talk to each\n",
      "other directly to elect a leader. The leader election mechanism works purely by creat-\n",
      "ing a resource in the API server. And it’s not even a special kind of resource—the End-\n",
      "points resource is used to achieve this (abused is probably a more appropriate term).\n",
      " There’s nothing special about using an Endpoints object to do this. It’s used\n",
      "because it has no side effects as long as no Service with the same name exists. Any\n",
      "other resource could be used (in fact, the leader election mechanism will soon use\n",
      "ConfigMaps instead of Endpoints). \n",
      " I’m sure you’re interested in how a resource can be used for this purpose. Let’s\n",
      "take the Scheduler, for example. All instances of the Scheduler try to create (and later\n",
      "update) an Endpoints resource called kube-scheduler. You’ll find it in the kube-\n",
      "system namespace, as the following listing shows.\n",
      "$ kubectl get endpoints kube-scheduler -n kube-system -o yaml\n",
      "apiVersion: v1\n",
      "kind: Endpoints\n",
      "metadata:\n",
      "  annotations:\n",
      "    control-plane.alpha.kubernetes.io/leader: '{\"holderIdentity\":\n",
      "      ➥ \"minikube\",\"leaseDurationSeconds\":15,\"acquireTime\":\n",
      "      ➥ \"2017-05-27T18:54:53Z\",\"renewTime\":\"2017-05-28T13:07:49Z\",\n",
      "      ➥ \"leaderTransitions\":0}'\n",
      "  creationTimestamp: 2017-05-27T18:54:53Z\n",
      "  name: kube-scheduler\n",
      "  namespace: kube-system\n",
      "  resourceVersion: \"654059\"\n",
      "  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler\n",
      "  uid: f847bd14-430d-11e7-9720-080027f8fa4e\n",
      "subsets: []\n",
      "The control-plane.alpha.kubernetes.io/leader annotation is the important part.\n",
      "As you can see, it contains a field called holderIdentity, which holds the name of the\n",
      "current leader. The first instance that succeeds in putting its name there becomes\n",
      "the leader. Instances race each other to do that, but there’s always only one winner.\n",
      " Remember the optimistic concurrency we explained earlier? That’s what ensures\n",
      "that if multiple instances try to write their name into the resource only one of them\n",
      "succeeds. Based on whether the write succeeded or not, each instance knows whether\n",
      "it is or it isn’t the leader. \n",
      " Once becoming the leader, it must periodically update the resource (every two sec-\n",
      "onds by default), so all other instances know that it’s still alive. When the leader fails,\n",
      "Listing 11.11\n",
      "The kube-scheduler Endpoints resource used for leader-election\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 377, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "345\n",
      "Summary\n",
      "other instances see that the resource hasn’t been updated for a while, and try to become\n",
      "the leader by writing their own name to the resource. Simple, right?\n",
      "11.7\n",
      "Summary\n",
      "Hopefully, this has been an interesting chapter that has improved your knowledge of\n",
      "the inner workings of Kubernetes. This chapter has shown you\n",
      "What components make up a Kubernetes cluster and what each component is\n",
      "responsible for\n",
      "How the API server, Scheduler, various controllers running in the Controller\n",
      "Manager, and the Kubelet work together to bring a pod to life\n",
      "How the infrastructure container binds together all the containers of a pod\n",
      "How pods communicate with other pods running on the same node through\n",
      "the network bridge, and how those bridges on different nodes are connected,\n",
      "so pods running on different nodes can talk to each other\n",
      "How the kube-proxy performs load balancing across pods in the same service by\n",
      "configuring iptables rules on the node\n",
      "How multiple instances of each component of the Control Plane can be run to\n",
      "make the cluster highly available\n",
      "Next, we’ll look at how to secure the API server and, by extension, the cluster as a whole.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 378, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "346\n",
      "Securing the\n",
      "Kubernetes API server\n",
      "In chapter 8 you learned how applications running in pods can talk to the API\n",
      "server to retrieve or change the state of resources deployed in the cluster. To\n",
      "authenticate with the API server, you used the ServiceAccount token mounted into\n",
      "the pod. In this chapter, you’ll learn what ServiceAccounts are and how to config-\n",
      "ure their permissions, as well as permissions for other subjects using the cluster. \n",
      "12.1\n",
      "Understanding authentication\n",
      "In the previous chapter, we said the API server can be configured with one or more\n",
      "authentication plugins (and the same is true for authorization plugins). When a\n",
      "request is received by the API server, it goes through the list of authentication\n",
      "This chapter covers\n",
      "Understanding authentication\n",
      "What ServiceAccounts are and why they’re used\n",
      "Understanding the role-based access control \n",
      "(RBAC) plugin\n",
      "Using Roles and RoleBindings\n",
      "Using ClusterRoles and ClusterRoleBindings\n",
      "Understanding the default roles and bindings\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 379, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "347\n",
      "Understanding authentication\n",
      "plugins, so they can each examine the request and try to determine who’s sending the\n",
      "request. The first plugin that can extract that information from the request returns\n",
      "the username, user ID, and the groups the client belongs to back to the API server\n",
      "core. The API server stops invoking the remaining authentication plugins and contin-\n",
      "ues onto the authorization phase. \n",
      " Several authentication plugins are available. They obtain the identity of the client\n",
      "using the following methods:\n",
      "From the client certificate\n",
      "From an authentication token passed in an HTTP header\n",
      "Basic HTTP authentication\n",
      "Others\n",
      "The authentication plugins are enabled through command-line options when starting\n",
      "the API server. \n",
      "12.1.1 Users and groups\n",
      "An authentication plugin returns the username and group(s) of the authenticated\n",
      "user. Kubernetes doesn’t store that information anywhere; it uses it to verify whether\n",
      "the user is authorized to perform an action or not.\n",
      "UNDERSTANDING USERS\n",
      "Kubernetes distinguishes between two kinds of clients connecting to the API server:\n",
      "Actual humans (users)\n",
      "Pods (more specifically, applications running inside them)\n",
      "Both these types of clients are authenticated using the aforementioned authentication\n",
      "plugins. Users are meant to be managed by an external system, such as a Single Sign\n",
      "On (SSO) system, but the pods use a mechanism called service accounts, which are cre-\n",
      "ated and stored in the cluster as ServiceAccount resources. In contrast, no resource\n",
      "represents user accounts, which means you can’t create, update, or delete users through\n",
      "the API server. \n",
      " We won’t go into any details of how to manage users, but we will explore Service-\n",
      "Accounts in detail, because they’re essential for running pods. For more informa-\n",
      "tion on how to configure the cluster for authentication of human users, cluster\n",
      "administrators should refer to the Kubernetes Cluster Administrator guide at http:/\n",
      "/\n",
      "kubernetes.io/docs/admin.\n",
      "UNDERSTANDING GROUPS\n",
      "Both human users and ServiceAccounts can belong to one or more groups. We’ve said\n",
      "that the authentication plugin returns groups along with the username and user ID.\n",
      "Groups are used to grant permissions to several users at once, instead of having to\n",
      "grant them to individual users. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 380, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "348\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      " Groups returned by the plugin are nothing but strings, representing arbitrary\n",
      "group names, but built-in groups have special meaning:\n",
      "The system:unauthenticated group is used for requests where none of the\n",
      "authentication plugins could authenticate the client.\n",
      "The system:authenticated group is automatically assigned to a user who was\n",
      "authenticated successfully.\n",
      "The system:serviceaccounts group encompasses all ServiceAccounts in the\n",
      "system.\n",
      "The system:serviceaccounts:<namespace> includes all ServiceAccounts in a\n",
      "specific namespace.\n",
      "12.1.2 Introducing ServiceAccounts\n",
      "Let’s explore ServiceAccounts up close. You’ve already learned that the API server\n",
      "requires clients to authenticate themselves before they’re allowed to perform opera-\n",
      "tions on the server. And you’ve already seen how pods can authenticate by sending the\n",
      "contents of the file/var/run/secrets/kubernetes.io/serviceaccount/token, which\n",
      "is mounted into each container’s filesystem through a secret volume.\n",
      " But what exactly does that file represent? Every pod is associated with a Service-\n",
      "Account, which represents the identity of the app running in the pod. The token file\n",
      "holds the ServiceAccount’s authentication token. When an app uses this token to con-\n",
      "nect to the API server, the authentication plugin authenticates the ServiceAccount\n",
      "and passes the ServiceAccount’s username back to the API server core. Service-\n",
      "Account usernames are formatted like this:\n",
      "system:serviceaccount:<namespace>:<service account name>\n",
      "The API server passes this username to the configured authorization plugins, which\n",
      "determine whether the action the app is trying to perform is allowed to be performed\n",
      "by the ServiceAccount.\n",
      " ServiceAccounts are nothing more than a way for an application running inside a\n",
      "pod to authenticate itself with the API server. As already mentioned, applications do\n",
      "that by passing the ServiceAccount’s token in the request.\n",
      "UNDERSTANDING THE SERVICEACCOUNT RESOURCE\n",
      "ServiceAccounts are resources just like Pods, Secrets, ConfigMaps, and so on, and are\n",
      "scoped to individual namespaces. A default ServiceAccount is automatically created\n",
      "for each namespace (that’s the one your pods have used all along). \n",
      " You can list ServiceAccounts like you do other resources:\n",
      "$ kubectl get sa\n",
      "NAME      SECRETS   AGE\n",
      "default   1         1d\n",
      "NOTE\n",
      "The shorthand for serviceaccount is sa.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 381, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "349\n",
      "Understanding authentication\n",
      "As you can see, the current namespace only contains the default ServiceAccount. Addi-\n",
      "tional ServiceAccounts can be added when required. Each pod is associated with exactly\n",
      "one ServiceAccount, but multiple pods can use the same ServiceAccount. As you can\n",
      "see in figure 12.1, a pod can only use a ServiceAccount from the same namespace.\n",
      "UNDERSTANDING HOW SERVICEACCOUNTS TIE INTO AUTHORIZATION\n",
      "You can assign a ServiceAccount to a pod by specifying the account’s name in the pod\n",
      "manifest. If you don’t assign it explicitly, the pod will use the default ServiceAccount\n",
      "in the namespace.\n",
      " By assigning different ServiceAccounts to pods, you can control which resources\n",
      "each pod has access to. When a request bearing the authentication token is received\n",
      "by the API server, the server uses the token to authenticate the client sending the\n",
      "request and then determines whether or not the related ServiceAccount is allowed to\n",
      "perform the requested operation. The API server obtains this information from the\n",
      "system-wide authorization plugin configured by the cluster administrator. One of\n",
      "the available authorization plugins is the role-based access control (RBAC) plugin,\n",
      "which is discussed later in this chapter. From Kubernetes version 1.6 on, the RBAC\n",
      "plugin is the plugin most clusters should use.\n",
      "12.1.3 Creating ServiceAccounts\n",
      "We’ve said every namespace contains its own default ServiceAccount, but additional\n",
      "ones can be created if necessary. But why should you bother with creating Service-\n",
      "Accounts instead of using the default one for all your pods? \n",
      " The obvious reason is cluster security. Pods that don’t need to read any cluster\n",
      "metadata should run under a constrained account that doesn’t allow them to retrieve\n",
      "or modify any resources deployed in the cluster. Pods that need to retrieve resource\n",
      "metadata should run under a ServiceAccount that only allows reading those objects’\n",
      "metadata, whereas pods that need to modify those objects should run under their own\n",
      "ServiceAccount allowing modifications of API objects. \n",
      "Pod\n",
      "Namespace: foo\n",
      "Service-\n",
      "Account:\n",
      "default\n",
      "Pod\n",
      "Pod\n",
      "Namespace: baz\n",
      "Pod\n",
      "Namespace: bar\n",
      "Pod\n",
      "Pod\n",
      "Not possible\n",
      "Service-\n",
      "Account:\n",
      "default\n",
      "Another\n",
      "Service-\n",
      "Account\n",
      "Multiple pods using the\n",
      "same ServiceAccount\n",
      "Figure 12.1\n",
      "Each pod is associated with a single ServiceAccount in the pod’s namespace.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 382, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "350\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      " Let’s see how you can create additional ServiceAccounts, how they relate to Secrets,\n",
      "and how you can assign them to your pods.\n",
      "CREATING A SERVICEACCOUNT\n",
      "Creating a ServiceAccount is incredibly easy, thanks to the dedicated kubectl create\n",
      "serviceaccount command. Let’s create a new ServiceAccount called foo:\n",
      "$ kubectl create serviceaccount foo\n",
      "serviceaccount \"foo\" created\n",
      "Now, you can inspect the ServiceAccount with the describe command, as shown in\n",
      "the following listing.\n",
      "$ kubectl describe sa foo\n",
      "Name:               foo\n",
      "Namespace:          default\n",
      "Labels:             <none>\n",
      "Image pull secrets: <none>             \n",
      "Mountable secrets:  foo-token-qzq7j    \n",
      "Tokens:             foo-token-qzq7j    \n",
      "You can see that a custom token Secret has been created and associated with the\n",
      "ServiceAccount. If you look at the Secret’s data with kubectl describe secret foo-\n",
      "token-qzq7j, you’ll see it contains the same items (the CA certificate, namespace, and\n",
      "token) as the default ServiceAccount’s token does (the token itself will obviously be\n",
      "different), as shown in the following listing.\n",
      "$ kubectl describe secret foo-token-qzq7j\n",
      "...\n",
      "ca.crt:         1066 bytes\n",
      "namespace:      7 bytes\n",
      "token:          eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\n",
      "NOTE\n",
      "You’ve probably heard of JSON Web Tokens (JWT). The authentica-\n",
      "tion tokens used in ServiceAccounts are JWT tokens.\n",
      "UNDERSTANDING A SERVICEACCOUNT’S MOUNTABLE SECRETS\n",
      "The token is shown in the Mountable secrets list when you inspect a ServiceAccount\n",
      "with kubectl describe. Let me explain what that list represents. In chapter 7 you\n",
      "learned how to create Secrets and mount them inside a pod. By default, a pod can\n",
      "mount any Secret it wants. But the pod’s ServiceAccount can be configured to only\n",
      "Listing 12.1\n",
      "Inspecting a ServiceAccount with kubectl describe\n",
      "Listing 12.2\n",
      "Inspecting the custom ServiceAccount’s Secret\n",
      "These will be added \n",
      "automatically to all pods \n",
      "using this ServiceAccount.\n",
      "Pods using this ServiceAccount \n",
      "can only mount these Secrets if \n",
      "mountable Secrets are enforced.\n",
      "Authentication token(s). \n",
      "The first one is mounted \n",
      "inside the container.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 383, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "351\n",
      "Understanding authentication\n",
      "allow the pod to mount Secrets that are listed as mountable Secrets on the Service-\n",
      "Account. To enable this feature, the ServiceAccount must contain the following anno-\n",
      "tation: kubernetes.io/enforce-mountable-secrets=\"true\". \n",
      " If the ServiceAccount is annotated with this annotation, any pods using it can mount\n",
      "only the ServiceAccount’s mountable Secrets—they can’t use any other Secret.\n",
      "UNDERSTANDING A SERVICEACCOUNT’S IMAGE PULL SECRETS\n",
      "A ServiceAccount can also contain a list of image pull Secrets, which we examined in\n",
      "chapter 7. In case you don’t remember, they are Secrets that hold the credentials for\n",
      "pulling container images from a private image repository. \n",
      " The following listing shows an example of a ServiceAccount definition, which\n",
      "includes the image pull Secret you created in chapter 7.\n",
      "apiVersion: v1\n",
      "kind: ServiceAccount\n",
      "metadata:\n",
      "  name: my-service-account\n",
      "imagePullSecrets:\n",
      "- name: my-dockerhub-secret\n",
      "A ServiceAccount’s image pull Secrets behave slightly differently than its mountable\n",
      "Secrets. Unlike mountable Secrets, they don’t determine which image pull Secrets a\n",
      "pod can use, but which ones are added automatically to all pods using the Service-\n",
      "Account. Adding image pull Secrets to a ServiceAccount saves you from having to add\n",
      "them to each pod individually. \n",
      "12.1.4 Assigning a ServiceAccount to a pod\n",
      "After you create additional ServiceAccounts, you need to assign them to pods. This is\n",
      "done by setting the name of the ServiceAccount in the spec.serviceAccountName\n",
      "field in the pod definition. \n",
      "NOTE\n",
      "A pod’s ServiceAccount must be set when creating the pod. It can’t be\n",
      "changed later. \n",
      "CREATING A POD WHICH USES A CUSTOM SERVICEACCOUNT\n",
      "In chapter 8 you deployed a pod that ran a container based on the tutum/curl image\n",
      "and an ambassador container alongside it. You used it to explore the API server’s\n",
      "REST interface. The ambassador container ran the kubectl proxy process, which\n",
      "used the pod’s ServiceAccount’s token to authenticate with the API server. \n",
      " You can now modify the pod so it uses the foo ServiceAccount you created minutes\n",
      "ago. The next listing shows the pod definition.\n",
      " \n",
      " \n",
      "Listing 12.3\n",
      "ServiceAccount with an image pull Secret: sa-image-pull-secrets.yaml\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 384, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "352\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: curl-custom-sa\n",
      "spec:\n",
      "  serviceAccountName: foo           \n",
      "  containers:\n",
      "  - name: main\n",
      "    image: tutum/curl\n",
      "    command: [\"sleep\", \"9999999\"]\n",
      "  - name: ambassador                  \n",
      "    image: luksa/kubectl-proxy:1.6.2\n",
      "To confirm that the custom ServiceAccount’s token is mounted into the two contain-\n",
      "ers, you can print the contents of the token as shown in the following listing.\n",
      "$ kubectl exec -it curl-custom-sa -c main \n",
      "➥ cat /var/run/secrets/kubernetes.io/serviceaccount/token\n",
      "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\n",
      "You can see the token is the one from the foo ServiceAccount by comparing the token\n",
      "string in listing 12.5 with the one in listing 12.2. \n",
      "USING THE CUSTOM SERVICEACCOUNT’S TOKEN TO TALK TO THE API SERVER\n",
      "Let’s see if you can talk to the API server using this token. As mentioned previously,\n",
      "the ambassador container uses the token when talking to the server, so you can test\n",
      "the token by going through the ambassador, which listens on localhost:8001, as\n",
      "shown in the following listing.\n",
      "$ kubectl exec -it curl-custom-sa -c main curl localhost:8001/api/v1/pods\n",
      "{\n",
      "  \"kind\": \"PodList\",\n",
      "  \"apiVersion\": \"v1\",\n",
      "  \"metadata\": {\n",
      "    \"selfLink\": \"/api/v1/pods\",\n",
      "    \"resourceVersion\": \"433895\"\n",
      "  },\n",
      "  \"items\": [\n",
      "  ...\n",
      "Okay, you got back a proper response from the server, which means the custom\n",
      "ServiceAccount is allowed to list pods. This may be because your cluster doesn’t use\n",
      "the RBAC authorization plugin, or you gave all ServiceAccounts full permissions, as\n",
      "instructed in chapter 8. \n",
      "Listing 12.4\n",
      "Pod using a non-default ServiceAccount: curl-custom-sa.yaml\n",
      "Listing 12.5\n",
      "Inspecting the token mounted into the pod’s container(s)\n",
      "Listing 12.6\n",
      "Talking to the API server with a custom ServiceAccount\n",
      "This pod uses the \n",
      "foo ServiceAccount \n",
      "instead of the default.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 385, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "353\n",
      "Securing the cluster with role-based access control\n",
      " When your cluster isn’t using proper authorization, creating and using additional\n",
      "ServiceAccounts doesn’t make much sense, since even the default ServiceAccount is\n",
      "allowed to do anything. The only reason to use ServiceAccounts in that case is to\n",
      "enforce mountable Secrets or to provide image pull Secrets through the Service-\n",
      "Account, as explained earlier. \n",
      " But creating additional ServiceAccounts is practically a must when you use the\n",
      "RBAC authorization plugin, which we’ll explore next.\n",
      "12.2\n",
      "Securing the cluster with role-based access control\n",
      "Starting with Kubernetes version 1.6.0, cluster security was ramped up considerably. In\n",
      "earlier versions, if you managed to acquire the authentication token from one of the\n",
      "pods, you could use it to do anything you want in the cluster. If you google around,\n",
      "you’ll find demos showing how a path traversal (or directory traversal) attack (where clients\n",
      "can retrieve files located outside of the web server’s web root directory) can be used to\n",
      "get the token and use it to run your malicious pods in an insecure Kubernetes cluster.\n",
      " But in version 1.8.0, the RBAC authorization plugin graduated to GA (General\n",
      "Availability) and is now enabled by default on many clusters (for example, when\n",
      "deploying a cluster with kubadm, as described in appendix B). RBAC prevents unau-\n",
      "thorized users from viewing or modifying the cluster state. The default Service-\n",
      "Account isn’t allowed to view cluster state, let alone modify it in any way, unless you\n",
      "grant it additional privileges. To write apps that communicate with the Kubernetes\n",
      "API server (as described in chapter 8), you need to understand how to manage\n",
      "authorization through RBAC-specific resources.\n",
      "NOTE\n",
      "In addition to RBAC, Kubernetes also includes other authorization\n",
      "plugins, such as the Attribute-based access control (ABAC) plugin, a Web-\n",
      "Hook plugin and custom plugin implementations. RBAC is the standard,\n",
      "though.\n",
      "12.2.1 Introducing the RBAC authorization plugin\n",
      "The Kubernetes API server can be configured to use an authorization plugin to check\n",
      "whether an action is allowed to be performed by the user requesting the action. Because\n",
      "the API server exposes a REST interface, users perform actions by sending HTTP\n",
      "requests to the server. Users authenticate themselves by including credentials in the\n",
      "request (an authentication token, username and password, or a client certificate).\n",
      "UNDERSTANDING ACTIONS\n",
      "But what actions are there? As you know, REST clients send GET, POST, PUT, DELETE,\n",
      "and other types of HTTP requests to specific URL paths, which represent specific\n",
      "REST resources. In Kubernetes, those resources are Pods, Services, Secrets, and so on.\n",
      "Here are a few examples of actions in Kubernetes:\n",
      "Get Pods\n",
      "Create Services\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 386, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "354\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      "Update Secrets\n",
      "And so on\n",
      "The verbs in those examples (get, create, update) map to HTTP methods (GET, POST,\n",
      "PUT) performed by the client (the complete mapping is shown in table 12.1). The\n",
      "nouns (Pods, Service, Secrets) obviously map to Kubernetes resources. \n",
      " An authorization plugin such as RBAC, which runs inside the API server, deter-\n",
      "mines whether a client is allowed to perform the requested verb on the requested\n",
      "resource or not.\n",
      "NOTE\n",
      "The additional verb use is used for PodSecurityPolicy resources, which\n",
      "are explained in the next chapter.\n",
      "Besides applying security permissions to whole resource types, RBAC rules can also\n",
      "apply to specific instances of a resource (for example, a Service called myservice).\n",
      "And later you’ll see that permissions can also apply to non-resource URL paths,\n",
      "because not every path the API server exposes maps to a resource (such as the /api\n",
      "path itself or the server health information at /healthz). \n",
      "UNDERSTANDING THE RBAC PLUGIN\n",
      "The RBAC authorization plugin, as the name suggests, uses user roles as the key factor\n",
      "in determining whether the user may perform the action or not. A subject (which may\n",
      "be a human, a ServiceAccount, or a group of users or ServiceAccounts) is associated\n",
      "with one or more roles and each role is allowed to perform certain verbs on certain\n",
      "resources. \n",
      " If a user has multiple roles, they may do anything that any of their roles allows\n",
      "them to do. If none of the user’s roles contains a permission to, for example, update\n",
      "Secrets, the API server will prevent the user from performing PUT or PATCH requests\n",
      "on Secrets.\n",
      " Managing authorization through the RBAC plugin is simple. It’s all done by creat-\n",
      "ing four RBAC-specific Kubernetes resources, which we’ll look at next.\n",
      "Table 12.1\n",
      "Mapping HTTP methods to authorization verbs\n",
      "HTTP method\n",
      "Verb for single resource\n",
      "Verb for collection\n",
      "GET, HEAD\n",
      "get (and watch for watching)\n",
      "list (and watch)\n",
      "POST\n",
      "create\n",
      "n/a\n",
      "PUT\n",
      "update\n",
      "n/a\n",
      "PATCH\n",
      "patch\n",
      "n/a\n",
      "DELETE\n",
      "delete\n",
      "deletecollection\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 387, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "355\n",
      "Securing the cluster with role-based access control\n",
      "12.2.2 Introducing RBAC resources\n",
      "The RBAC authorization rules are configured through four resources, which can be\n",
      "grouped into two groups:\n",
      "Roles and ClusterRoles, which specify which verbs can be performed on which\n",
      "resources.\n",
      "RoleBindings and ClusterRoleBindings, which bind the above roles to specific\n",
      "users, groups, or ServiceAccounts.\n",
      "Roles define what can be done, while bindings define who can do it (this is shown in\n",
      "figure 12.2).\n",
      "The distinction between a Role and a ClusterRole, or between a RoleBinding and a\n",
      "ClusterRoleBinding, is that the Role and RoleBinding are namespaced resources,\n",
      "whereas the ClusterRole and ClusterRoleBinding are cluster-level resources (not\n",
      "namespaced). This is depicted in figure 12.3.\n",
      " As you can see from the figure, multiple RoleBindings can exist in a single name-\n",
      "space (this is also true for Roles). Likewise, multiple ClusterRoleBindings and Cluster-\n",
      "Roles can be created. Another thing shown in the figure is that although RoleBindings\n",
      "are namespaced, they can also reference ClusterRoles, which aren’t. \n",
      " The best way to learn about these four resources and what their effects are is by try-\n",
      "ing them out in a hands-on exercise. You’ll do that now.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "What?\n",
      "Role\n",
      "Binding\n",
      "Some\n",
      "resources\n",
      "Other\n",
      "resources\n",
      "Role\n",
      "Doesn’t allow\n",
      "doing anything\n",
      "with other resources\n",
      "User A\n",
      "Who?\n",
      "Admins group\n",
      "Allows users\n",
      "to access\n",
      "Service-\n",
      "Account:\n",
      "x\n",
      "Figure 12.2\n",
      "Roles grant permissions, whereas RoleBindings bind Roles to subjects.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 388, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "356\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      "SETTING UP YOUR EXERCISE\n",
      "Before you can explore how RBAC resources affect what you can do through the API\n",
      "server, you need to make sure RBAC is enabled in your cluster. First, ensure you’re\n",
      "using at least version 1.6 of Kubernetes and that the RBAC plugin is the only config-\n",
      "ured authorization plugin. There can be multiple plugins enabled in parallel and if\n",
      "one of them allows an action to be performed, the action is allowed.\n",
      "NOTE\n",
      "If you’re using GKE 1.6 or 1.7, you need to explicitly disable legacy autho-\n",
      "rization by creating the cluster with the --no-enable-legacy-authorization\n",
      "option. If you’re using Minikube, you also may need to enable RBAC by start-\n",
      "ing Minikube with --extra-config=apiserver.Authorization.Mode=RBAC\n",
      "If you followed the instructions on how to disable RBAC in chapter 8, now’s the time\n",
      "to re-enable it by running the following command:\n",
      "$ kubectl delete clusterrolebinding permissive-binding\n",
      "To try out RBAC, you’ll run a pod through which you’ll try to talk to the API server,\n",
      "the way you did in chapter 8. But this time you’ll run two pods in different namespaces\n",
      "to see how per-namespace security behaves.\n",
      " In the examples in chapter 8, you ran two containers to demonstrate how an appli-\n",
      "cation in one container uses the other container to talk to the API server. This time,\n",
      "you’ll run a single container (based on the kubectl-proxy image) and use kubectl\n",
      "exec to run curl inside that container directly. The proxy will take care of authentica-\n",
      "tion and HTTPS, so you can focus on the authorization aspect of API server security.\n",
      "Namespace C\n",
      "Namespaced\n",
      "resources\n",
      "Cluster-level\n",
      "resources\n",
      "RoleBinding\n",
      "RoleBinding\n",
      "Role\n",
      "Namespace B\n",
      "Namespaced\n",
      "resources\n",
      "RoleBinding\n",
      "Role\n",
      "Namespace A\n",
      "Namespaced\n",
      "resources\n",
      "RoleBinding\n",
      "Role\n",
      "Cluster scope (resources that aren’t namespaced)\n",
      "ClusterRoleBinding\n",
      "ClusterRole\n",
      "Figure 12.3\n",
      "Roles and RoleBindings are namespaced; ClusterRoles and ClusterRoleBindings aren’t.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 389, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "357\n",
      "Securing the cluster with role-based access control\n",
      "CREATING THE NAMESPACES AND RUNNING THE PODS\n",
      "You’re going to create one pod in namespace foo and the other one in namespace\n",
      "bar, as shown in the following listing.\n",
      "$ kubectl create ns foo\n",
      "namespace \"foo\" created\n",
      "$ kubectl run test --image=luksa/kubectl-proxy -n foo\n",
      "deployment \"test\" created\n",
      "$ kubectl create ns bar\n",
      "namespace \"bar\" created\n",
      "$ kubectl run test --image=luksa/kubectl-proxy -n bar\n",
      "deployment \"test\" created\n",
      "Now open two terminals and use kubectl exec to run a shell inside each of the two\n",
      "pods (one in each terminal). For example, to run the shell in the pod in namespace\n",
      "foo, first get the name of the pod:\n",
      "$ kubectl get po -n foo\n",
      "NAME                   READY     STATUS    RESTARTS   AGE\n",
      "test-145485760-ttq36   1/1       Running   0          1m\n",
      "Then use the name in the kubectl exec command:\n",
      "$ kubectl exec -it test-145485760-ttq36 -n foo sh\n",
      "/ #\n",
      "Do the same in the other terminal, but for the pod in the bar namespace.\n",
      "LISTING SERVICES FROM YOUR PODS\n",
      "To verify that RBAC is enabled and preventing the pod from reading cluster state, use\n",
      "curl to list Services in the foo namespace:\n",
      "/ # curl localhost:8001/api/v1/namespaces/foo/services\n",
      "User \"system:serviceaccount:foo:default\" cannot list services in the \n",
      "namespace \"foo\".\n",
      "You’re connecting to localhost:8001, which is where the kubectl proxy process is\n",
      "listening (as explained in chapter 8). The process received your request and sent it to\n",
      "the API server while authenticating as the default ServiceAccount in the foo name-\n",
      "space (as evident from the API server’s response). \n",
      " The API server responded that the ServiceAccount isn’t allowed to list Services in\n",
      "the foo namespace, even though the pod is running in that same namespace. You’re\n",
      "seeing RBAC in action. The default permissions for a ServiceAccount don’t allow it to\n",
      "list or modify any resources. Now, let’s learn how to allow the ServiceAccount to do\n",
      "that. First, you’ll need to create a Role resource.\n",
      "Listing 12.7\n",
      "Running test pods in different namespaces\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 390, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "358\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      "12.2.3 Using Roles and RoleBindings\n",
      "A Role resource defines what actions can be taken on which resources (or, as\n",
      "explained earlier, which types of HTTP requests can be performed on which RESTful\n",
      "resources). The following listing defines a Role, which allows users to get and list\n",
      "Services in the foo namespace.\n",
      "apiVersion: rbac.authorization.k8s.io/v1\n",
      "kind: Role\n",
      "metadata:\n",
      "  namespace: foo            \n",
      "  name: service-reader\n",
      "rules:\n",
      "- apiGroups: [\"\"]            \n",
      "  verbs: [\"get\", \"list\"]     \n",
      "  resources: [\"services\"]   \n",
      "WARNING\n",
      "The plural form must be used when specifying resources.\n",
      "This Role resource will be created in the foo namespace. In chapter 8, you learned that\n",
      "each resource type belongs to an API group, which you specify in the apiVersion field\n",
      "(along with the version) in the resource’s manifest. In a Role definition, you need to spec-\n",
      "ify the apiGroup for the resources listed in each rule included in the definition. If you’re\n",
      "allowing access to resources belonging to different API groups, you use multiple rules.\n",
      "NOTE\n",
      "In the example, you’re allowing access to all Service resources, but you\n",
      "could also limit access only to specific Service instances by specifying their\n",
      "names through an additional resourceNames field.\n",
      "Figure 12.4 shows the Role, its verbs and resources, and the namespace it will be cre-\n",
      "ated in.\n",
      "Listing 12.8\n",
      "A definition of a Role: service-reader.yaml\n",
      "Roles are namespaced (if namespace is \n",
      "omitted, the current namespace is used).\n",
      "Services are resources in the core apiGroup, \n",
      "which has no name – hence the \"\".\n",
      "Getting individual Services (by name) \n",
      "and listing all of them is allowed.\n",
      "This rule pertains to services \n",
      "(plural name must be used!).\n",
      "Allows getting\n",
      "Allows listing\n",
      "Services\n",
      "Role:\n",
      "service-reader\n",
      "Services\n",
      "Namespace: foo\n",
      "Namespace: bar\n",
      "Does not allow users to\n",
      "get or list Services in\n",
      "other namespaces\n",
      "Figure 12.4\n",
      "The service-reader Role allows getting and listing Services in the foo namespace.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 391, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "359\n",
      "Securing the cluster with role-based access control\n",
      "CREATING A ROLE\n",
      "Create the previous Role in the foo namespace now:\n",
      "$ kubectl create -f service-reader.yaml -n foo\n",
      "role \"service-reader\" created\n",
      "NOTE\n",
      "The -n option is shorthand for --namespace.\n",
      "Note that if you’re using GKE, the previous command may fail because you don’t have\n",
      "cluster-admin rights. To grant the rights, run the following command:\n",
      "$ kubectl create clusterrolebinding cluster-admin-binding \n",
      "➥ --clusterrole=cluster-admin --user=your.email@address.com\n",
      "Instead of creating the service-reader Role from a YAML file, you could also create\n",
      "it with the special kubectl create role command. Let’s use this method to create the\n",
      "Role in the bar namespace:\n",
      "$ kubectl create role service-reader --verb=get --verb=list \n",
      "➥ --resource=services -n bar\n",
      "role \"service-reader\" created\n",
      "These two Roles will allow you to list Services in the foo and bar namespaces from\n",
      "within your two pods (running in the foo and bar namespace, respectively). But cre-\n",
      "ating the two Roles isn’t enough (you can check by executing the curl command\n",
      "again). You need to bind each of the Roles to the ServiceAccounts in their respec-\n",
      "tive namespaces. \n",
      "BINDING A ROLE TO A SERVICEACCOUNT\n",
      "A Role defines what actions can be performed, but it doesn’t specify who can perform\n",
      "them. To do that, you must bind the Role to a subject, which can be a user, a Service-\n",
      "Account, or a group (of users or ServiceAccounts).\n",
      " Binding Roles to subjects is achieved by creating a RoleBinding resource. To bind\n",
      "the Role to the default ServiceAccount, run the following command:\n",
      "$ kubectl create rolebinding test --role=service-reader \n",
      "➥ --serviceaccount=foo:default -n foo\n",
      "rolebinding \"test\" created\n",
      "The command should be self-explanatory. You’re creating a RoleBinding, which binds\n",
      "the service-reader Role to the default ServiceAccount in namespace foo. You’re cre-\n",
      "ating the RoleBinding in namespace foo. The RoleBinding and the referenced Service-\n",
      "Account and Role are shown in figure 12.5.\n",
      "NOTE\n",
      "To bind a Role to a user instead of a ServiceAccount, use the --user\n",
      "argument to specify the username. To bind it to a group, use --group.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 392, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "360\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      "The following listing shows the YAML of the RoleBinding you created.\n",
      "$ kubectl get rolebinding test -n foo -o yaml\n",
      "apiVersion: rbac.authorization.k8s.io/v1\n",
      "kind: RoleBinding\n",
      "metadata:\n",
      "  name: test\n",
      "  namespace: foo\n",
      "  ...\n",
      "roleRef:\n",
      "  apiGroup: rbac.authorization.k8s.io\n",
      "  kind: Role                         \n",
      "  name: service-reader               \n",
      "subjects:\n",
      "- kind: ServiceAccount       \n",
      "  name: default              \n",
      "  namespace: foo             \n",
      "As you can see, a RoleBinding always references a single Role (as evident from the\n",
      "roleRef property), but can bind the Role to multiple subjects (for example, one or\n",
      "more ServiceAccounts and any number of users or groups). Because this RoleBinding\n",
      "binds the Role to the ServiceAccount the pod in namespace foo is running under, you\n",
      "can now list Services from within that pod.\n",
      "/ # curl localhost:8001/api/v1/namespaces/foo/services\n",
      "{\n",
      "  \"kind\": \"ServiceList\",\n",
      "  \"apiVersion\": \"v1\",\n",
      "  \"metadata\": {\n",
      "    \"selfLink\": \"/api/v1/namespaces/foo/services\",\n",
      "Listing 12.9\n",
      "A RoleBinding referencing a Role\n",
      "Listing 12.10\n",
      "Getting Services from the API server\n",
      "Namespace: foo\n",
      "Role:\n",
      "service-reader\n",
      "Get, list\n",
      "Default ServiceAccount\n",
      "is allowed to get and list\n",
      "services in this namespace\n",
      "Services\n",
      "RoleBinding:\n",
      "test\n",
      "Service-\n",
      "Account:\n",
      "default\n",
      "Figure 12.5\n",
      "The test RoleBinding binds the default ServiceAccount with the \n",
      "service-reader Role.\n",
      "This RoleBinding references \n",
      "the service-reader Role.\n",
      "And binds it to the \n",
      "default ServiceAccount \n",
      "in the foo namespace.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 393, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "361\n",
      "Securing the cluster with role-based access control\n",
      "    \"resourceVersion\": \"24906\"\n",
      "  },\n",
      "  \"items\": []     \n",
      "}\n",
      "INCLUDING SERVICEACCOUNTS FROM OTHER NAMESPACES IN A ROLEBINDING\n",
      "The pod in namespace bar can’t list the Services in its own namespace, and obviously\n",
      "also not those in the foo namespace. But you can edit your RoleBinding in the foo\n",
      "namespace and add the other pod’s ServiceAccount, even though it’s in a different\n",
      "namespace. Run the following command:\n",
      "$ kubectl edit rolebinding test -n foo\n",
      "Then add the following lines to the list of subjects, as shown in the following listing.\n",
      "subjects:\n",
      "- kind: ServiceAccount\n",
      "  name: default          \n",
      "  namespace: bar         \n",
      "Now you can also list Services in the foo namespace from inside the pod running in\n",
      "the bar namespace. Run the same command as in listing 12.10, but do it in the other\n",
      "terminal, where you’re running the shell in the other pod.\n",
      " Before moving on to ClusterRoles and ClusterRoleBindings, let’s summarize\n",
      "what RBAC resources you currently have. You have a RoleBinding in namespace\n",
      "foo, which references the service-reader Role (also in the foo namespace) and\n",
      "binds the default ServiceAccounts in both the foo and the bar namespaces, as\n",
      "depicted in figure 12.6.\n",
      "Listing 12.11\n",
      "Referencing a ServiceAccount from another namespace\n",
      "The list of items is empty, \n",
      "because no Services exist.\n",
      "You’re referencing the default \n",
      "ServiceAccount in the bar namespace.\n",
      "Namespace: foo\n",
      "Role:\n",
      "service-reader\n",
      "Get, list\n",
      "Both ServiceAccounts are\n",
      "allowed to get and list Services\n",
      "in the foo namespace\n",
      "Services\n",
      "Namespace: bar\n",
      "RoleBinding:\n",
      "test\n",
      "Service-\n",
      "Account:\n",
      "default\n",
      "Service-\n",
      "Account:\n",
      "default\n",
      "Figure 12.6\n",
      "A RoleBinding binding ServiceAccounts from different namespaces to the same Role.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 394, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "362\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      "12.2.4 Using ClusterRoles and ClusterRoleBindings\n",
      "Roles and RoleBindings are namespaced resources, meaning they reside in and apply\n",
      "to resources in a single namespace, but, as we saw, RoleBindings can refer to Service-\n",
      "Accounts from other namespaces, too. \n",
      " In addition to these namespaced resources, two cluster-level RBAC resources also\n",
      "exist: ClusterRole and ClusterRoleBinding. They’re not namespaced. Let’s see why\n",
      "you need them.\n",
      " A regular Role only allows access to resources in the same namespace the Role is\n",
      "in. If you want to allow someone access to resources across different namespaces, you\n",
      "have to create a Role and RoleBinding in every one of those namespaces. If you want\n",
      "to extend this to all namespaces (this is something a cluster administrator would prob-\n",
      "ably need), you need to create the same Role and RoleBinding in each namespace.\n",
      "When creating an additional namespace, you have to remember to create the two\n",
      "resources there as well. \n",
      " As you’ve learned throughout the book, certain resources aren’t namespaced at\n",
      "all (this includes Nodes, PersistentVolumes, Namespaces, and so on). We’ve also\n",
      "mentioned the API server exposes some URL paths that don’t represent resources\n",
      "(/healthz for example). Regular Roles can’t grant access to those resources or non-\n",
      "resource URLs, but ClusterRoles can.\n",
      " A ClusterRole is a cluster-level resource for allowing access to non-namespaced\n",
      "resources or non-resource URLs or used as a common role to be bound inside individ-\n",
      "ual namespaces, saving you from having to redefine the same role in each of them.\n",
      "ALLOWING ACCESS TO CLUSTER-LEVEL RESOURCES\n",
      "As mentioned, a ClusterRole can be used to allow access to cluster-level resources.\n",
      "Let’s look at how to allow your pod to list PersistentVolumes in your cluster. First,\n",
      "you’ll create a ClusterRole called pv-reader:\n",
      "$ kubectl create clusterrole pv-reader --verb=get,list \n",
      "➥ --resource=persistentvolumes\n",
      "clusterrole \"pv-reader\" created\n",
      "The ClusterRole’s YAML is shown in the following listing.\n",
      "$ kubectl get clusterrole pv-reader -o yaml\n",
      "apiVersion: rbac.authorization.k8s.io/v1\n",
      "kind: ClusterRole\n",
      "metadata:                                       \n",
      "  name: pv-reader                               \n",
      "  resourceVersion: \"39932\"                      \n",
      "  selfLink: ...                                 \n",
      "  uid: e9ac1099-30e2-11e7-955c-080027e6b159     \n",
      "Listing 12.12\n",
      "A ClusterRole definition\n",
      "ClusterRoles aren’t \n",
      "namespaced, hence \n",
      "no namespace field.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 395, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "363\n",
      "Securing the cluster with role-based access control\n",
      "rules:\n",
      "- apiGroups:                      \n",
      "  - \"\"                            \n",
      "  resources:                      \n",
      "  - persistentvolumes             \n",
      "  verbs:                          \n",
      "  - get                           \n",
      "  - list                          \n",
      "Before you bind this ClusterRole to your pod’s ServiceAccount, verify whether the pod\n",
      "can list PersistentVolumes. Run the following command in the first terminal, where\n",
      "you’re running the shell inside the pod in the foo namespace:\n",
      "/ # curl localhost:8001/api/v1/persistentvolumes\n",
      "User \"system:serviceaccount:foo:default\" cannot list persistentvolumes at the \n",
      "cluster scope.\n",
      "NOTE\n",
      "The URL contains no namespace, because PersistentVolumes aren’t\n",
      "namespaced. \n",
      "As expected, the default ServiceAccount can’t list PersistentVolumes. You need to\n",
      "bind the ClusterRole to your ServiceAccount to allow it to do that. ClusterRoles can\n",
      "be bound to subjects with regular RoleBindings, so you’ll create a RoleBinding now:\n",
      "$ kubectl create rolebinding pv-test --clusterrole=pv-reader \n",
      "➥ --serviceaccount=foo:default -n foo\n",
      "rolebinding \"pv-test\" created\n",
      "Can you list PersistentVolumes now?\n",
      "/ # curl localhost:8001/api/v1/persistentvolumes\n",
      "User \"system:serviceaccount:foo:default\" cannot list persistentvolumes at the \n",
      "cluster scope.\n",
      "Hmm, that’s strange. Let’s examine the RoleBinding’s YAML in the following listing.\n",
      "Can you tell what (if anything) is wrong with it?\n",
      "$ kubectl get rolebindings pv-test -o yaml\n",
      "apiVersion: rbac.authorization.k8s.io/v1\n",
      "kind: RoleBinding\n",
      "metadata:\n",
      "  name: pv-test\n",
      "  namespace: foo\n",
      "  ...\n",
      "roleRef:\n",
      "  apiGroup: rbac.authorization.k8s.io\n",
      "  kind: ClusterRole              \n",
      "  name: pv-reader                \n",
      "Listing 12.13\n",
      "A RoleBinding referencing a ClusterRole\n",
      "In this case, the \n",
      "rules are exactly \n",
      "like those in a \n",
      "regular Role.\n",
      "The binding references the \n",
      "pv-reader ClusterRole.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 396, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "364\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      "subjects:\n",
      "- kind: ServiceAccount          \n",
      "  name: default                 \n",
      "  namespace: foo                \n",
      "The YAML looks perfectly fine. You’re referencing the correct ClusterRole and the\n",
      "correct ServiceAccount, as shown in figure 12.7, so what’s wrong?\n",
      "Although you can create a RoleBinding and have it reference a ClusterRole when you\n",
      "want to enable access to namespaced resources, you can’t use the same approach for\n",
      "cluster-level (non-namespaced) resources. To grant access to cluster-level resources,\n",
      "you must always use a ClusterRoleBinding.\n",
      " Luckily, creating a ClusterRoleBinding isn’t that different from creating a Role-\n",
      "Binding, but you’ll clean up and delete the RoleBinding first:\n",
      "$ kubectl delete rolebinding pv-test\n",
      "rolebinding \"pv-test\" deleted\n",
      "Now create the ClusterRoleBinding:\n",
      "$ kubectl create clusterrolebinding pv-test --clusterrole=pv-reader \n",
      "➥ --serviceaccount=foo:default\n",
      "clusterrolebinding \"pv-test\" created\n",
      "As you can see, you replaced rolebinding with clusterrolebinding in the command\n",
      "and didn’t (need to) specify the namespace. Figure 12.8 shows what you have now.\n",
      " Let’s see if you can list PersistentVolumes now:\n",
      "/ # curl localhost:8001/api/v1/persistentvolumes\n",
      "{\n",
      "  \"kind\": \"PersistentVolumeList\",\n",
      "  \"apiVersion\": \"v1\",\n",
      "...\n",
      "The bound subject is the \n",
      "default ServiceAccount in \n",
      "the foo namespace.\n",
      "Namespace: foo\n",
      "Cluster-level resources\n",
      "ClusterRole:\n",
      "pv-reader\n",
      "Get, list\n",
      "Persistent\n",
      "Volumes\n",
      "RoleBinding:\n",
      "pv-test\n",
      "Default ServiceAccount\n",
      "is unable to get and list\n",
      "PersistentVolumes\n",
      "Service-\n",
      "Account:\n",
      "default\n",
      "Figure 12.7\n",
      "A RoleBinding referencing a ClusterRole doesn’t grant access to cluster-\n",
      "level resources.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 397, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "365\n",
      "Securing the cluster with role-based access control\n",
      "You can! It turns out you must use a ClusterRole and a ClusterRoleBinding when\n",
      "granting access to cluster-level resources.\n",
      "TIP\n",
      "Remember that a RoleBinding can’t grant access to cluster-level resources,\n",
      "even if it references a ClusterRoleBinding.\n",
      "ALLOWING ACCESS TO NON-RESOURCE URLS\n",
      "We’ve mentioned that the API server also exposes non-resource URLs. Access to these\n",
      "URLs must also be granted explicitly; otherwise the API server will reject the client’s\n",
      "request. Usually, this is done for you automatically through the system:discovery\n",
      "ClusterRole and the identically named ClusterRoleBinding, which appear among\n",
      "other predefined ClusterRoles and ClusterRoleBindings (we’ll explore them in sec-\n",
      "tion 12.2.5). \n",
      " Let’s inspect the system:discovery ClusterRole shown in the following listing.\n",
      "$ kubectl get clusterrole system:discovery -o yaml\n",
      "apiVersion: rbac.authorization.k8s.io/v1\n",
      "kind: ClusterRole\n",
      "metadata:\n",
      "  name: system:discovery\n",
      "  ...\n",
      "rules:\n",
      "- nonResourceURLs:      \n",
      "  - /api                \n",
      "  - /api/*              \n",
      "  - /apis               \n",
      "  - /apis/*             \n",
      "  - /healthz            \n",
      "  - /swaggerapi         \n",
      "  - /swaggerapi/*       \n",
      "  - /version            \n",
      "Listing 12.14\n",
      "The default system:discovery ClusterRole\n",
      "Namespace: foo\n",
      "Cluster-level resources\n",
      "ClusterRole:\n",
      "pv-reader\n",
      "Get, list\n",
      "Persistent\n",
      "Volumes\n",
      "ClusterRoleBinding:\n",
      "pv-test\n",
      "Default ServiceAccount in\n",
      "foo namespace is now allowed\n",
      "to get and list PersistentVolumes\n",
      "Service-\n",
      "Account:\n",
      "default\n",
      "Figure 12.8\n",
      "A ClusterRoleBinding and ClusterRole must be used to grant access to cluster-\n",
      "level resources.\n",
      "Instead of referring \n",
      "to resources, this rule \n",
      "refers to non-resource \n",
      "URLs.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 398, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "366\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      "  verbs:             \n",
      "  - get              \n",
      "You can see this ClusterRole refers to URLs instead of resources (field nonResource-\n",
      "URLs is used instead of the resources field). The verbs field only allows the GET HTTP\n",
      "method to be used on these URLs.\n",
      "NOTE\n",
      "For non-resource URLs, plain HTTP verbs such as post, put, and\n",
      "patch are used instead of create or update. The verbs need to be specified in\n",
      "lowercase.\n",
      "As with cluster-level resources, ClusterRoles for non-resource URLs must be bound\n",
      "with a ClusterRoleBinding. Binding them with a RoleBinding won’t have any effect.\n",
      "The system:discovery ClusterRole has a corresponding system:discovery Cluster-\n",
      "RoleBinding, so let’s see what’s in it by examining the following listing.\n",
      "$ kubectl get clusterrolebinding system:discovery -o yaml\n",
      "apiVersion: rbac.authorization.k8s.io/v1\n",
      "kind: ClusterRoleBinding\n",
      "metadata:\n",
      "  name: system:discovery\n",
      "  ...\n",
      "roleRef:\n",
      "  apiGroup: rbac.authorization.k8s.io\n",
      "  kind: ClusterRole                           \n",
      "  name: system:discovery                      \n",
      "subjects:\n",
      "- apiGroup: rbac.authorization.k8s.io\n",
      "  kind: Group                                 \n",
      "  name: system:authenticated                  \n",
      "- apiGroup: rbac.authorization.k8s.io\n",
      "  kind: Group                                 \n",
      "  name: system:unauthenticated                \n",
      "The YAML shows the ClusterRoleBinding refers to the system:discovery ClusterRole,\n",
      "as expected. It’s bound to two groups, system:authenticated and system:unauthenti-\n",
      "cated, which makes it bound to all users. This means absolutely everyone can access\n",
      "the URLs listed in the ClusterRole. \n",
      "NOTE\n",
      "Groups are in the domain of the authentication plugin. When a\n",
      "request is received by the API server, it calls the authentication plugin to\n",
      "obtain the list of groups the user belongs to. This information is then used\n",
      "in authorization.\n",
      "You can confirm this by accessing the /api URL path from inside the pod (through\n",
      "the kubectl proxy, which means you’ll be authenticated as the pod’s ServiceAccount)\n",
      "Listing 12.15\n",
      "The default system:discovery ClusterRoleBinding\n",
      "Only the HTTP GET method \n",
      "is allowed for these URLs.\n",
      "This ClusterRoleBinding references \n",
      "the system:discovery ClusterRole.\n",
      "It binds the ClusterRole \n",
      "to all authenticated and \n",
      "unauthenticated users \n",
      "(that is, everyone).\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 399, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "367\n",
      "Securing the cluster with role-based access control\n",
      "and from your local machine, without specifying any authentication tokens (making\n",
      "you an unauthenticated user):\n",
      "$ curl https://$(minikube ip):8443/api -k\n",
      "{\n",
      "  \"kind\": \"APIVersions\",\n",
      "  \"versions\": [\n",
      "  ...\n",
      "You’ve now used ClusterRoles and ClusterRoleBindings to grant access to cluster-level\n",
      "resources and non-resource URLs. Now let’s look at how ClusterRoles can be used\n",
      "with namespaced RoleBindings to grant access to namespaced resources in the Role-\n",
      "Binding’s namespace.\n",
      "USING CLUSTERROLES TO GRANT ACCESS TO RESOURCES IN SPECIFIC NAMESPACES\n",
      "ClusterRoles don’t always need to be bound with cluster-level ClusterRoleBindings.\n",
      "They can also be bound with regular, namespaced RoleBindings. You’ve already\n",
      "started looking at predefined ClusterRoles, so let’s look at another one called view,\n",
      "which is shown in the following listing.\n",
      "$ kubectl get clusterrole view -o yaml\n",
      "apiVersion: rbac.authorization.k8s.io/v1\n",
      "kind: ClusterRole\n",
      "metadata:\n",
      "  name: view\n",
      "  ...\n",
      "rules:\n",
      "- apiGroups:\n",
      "  - \"\"\n",
      "  resources:                           \n",
      "  - configmaps                         \n",
      "  - endpoints                          \n",
      "  - persistentvolumeclaims             \n",
      "  - pods                               \n",
      "  - replicationcontrollers             \n",
      "  - replicationcontrollers/scale       \n",
      "  - serviceaccounts                    \n",
      "  - services                           \n",
      "  verbs:                \n",
      "  - get                 \n",
      "  - list                \n",
      "  - watch               \n",
      "...\n",
      "This ClusterRole has many rules. Only the first one is shown in the listing. The rule\n",
      "allows getting, listing, and watching resources like ConfigMaps, Endpoints, Persistent-\n",
      "VolumeClaims, and so on. These are namespaced resources, even though you’re\n",
      "looking at a ClusterRole (not a regular, namespaced Role). What exactly does this\n",
      "ClusterRole do?\n",
      "Listing 12.16\n",
      "The default view ClusterRole\n",
      "This rule applies to \n",
      "these resources (note: \n",
      "they’re all namespaced \n",
      "resources).\n",
      "As the ClusterRole’s name \n",
      "suggests, it only allows \n",
      "reading, not writing the \n",
      "resources listed. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 400, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "368\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      " It depends whether it’s bound with a ClusterRoleBinding or a RoleBinding (it can\n",
      "be bound with either). If you create a ClusterRoleBinding and reference the Cluster-\n",
      "Role in it, the subjects listed in the binding can view the specified resources across all\n",
      "namespaces. If, on the other hand, you create a RoleBinding, the subjects listed in the\n",
      "binding can only view resources in the namespace of the RoleBinding. You’ll try both\n",
      "options now.\n",
      " You’ll see how the two options affect your test pod’s ability to list pods. First, let’s\n",
      "see what happens before any bindings are in place:\n",
      "/ # curl localhost:8001/api/v1/pods\n",
      "User \"system:serviceaccount:foo:default\" cannot list pods at the cluster \n",
      "scope./ #\n",
      "/ # curl localhost:8001/api/v1/namespaces/foo/pods\n",
      "User \"system:serviceaccount:foo:default\" cannot list pods in the namespace \n",
      "\"foo\".\n",
      "With the first command, you’re trying to list pods across all namespaces. With the sec-\n",
      "ond, you’re trying to list pods in the foo namespace. The server doesn’t allow you to\n",
      "do either.\n",
      " Now, let’s see what happens when you create a ClusterRoleBinding and bind it to\n",
      "the pod’s ServiceAccount:\n",
      "$ kubectl create clusterrolebinding view-test --clusterrole=view \n",
      "➥ --serviceaccount=foo:default\n",
      "clusterrolebinding \"view-test\" created\n",
      "Can the pod now list pods in the foo namespace?\n",
      "/ # curl localhost:8001/api/v1/namespaces/foo/pods\n",
      "{\n",
      "  \"kind\": \"PodList\",\n",
      "  \"apiVersion\": \"v1\",\n",
      "  ...\n",
      "It can! Because you created a ClusterRoleBinding, it applies across all namespaces.\n",
      "The pod in namespace foo can list pods in the bar namespace as well:\n",
      "/ # curl localhost:8001/api/v1/namespaces/bar/pods\n",
      "{\n",
      "  \"kind\": \"PodList\",\n",
      "  \"apiVersion\": \"v1\",\n",
      "  ...\n",
      "Okay, the pod is allowed to list pods in a different namespace. It can also retrieve pods\n",
      "across all namespaces by hitting the /api/v1/pods URL path:\n",
      "/ # curl localhost:8001/api/v1/pods\n",
      "{\n",
      "  \"kind\": \"PodList\",\n",
      "  \"apiVersion\": \"v1\",\n",
      "  ...\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 401, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "369\n",
      "Securing the cluster with role-based access control\n",
      "As expected, the pod can get a list of all the pods in the cluster. To summarize, com-\n",
      "bining a ClusterRoleBinding with a ClusterRole referring to namespaced resources\n",
      "allows the pod to access namespaced resources in any namespace, as shown in fig-\n",
      "ure 12.9.\n",
      "Now, let’s see what happens if you replace the ClusterRoleBinding with a RoleBinding.\n",
      "First, delete the ClusterRoleBinding:\n",
      "$ kubectl delete clusterrolebinding view-test\n",
      "clusterrolebinding \"view-test\" deleted\n",
      "Next create a RoleBinding instead. Because a RoleBinding is namespaced, you need\n",
      "to specify the namespace you want to create it in. Create it in the foo namespace:\n",
      "$ kubectl create rolebinding view-test --clusterrole=view \n",
      "➥ --serviceaccount=foo:default -n foo\n",
      "rolebinding \"view-test\" created\n",
      "You now have a RoleBinding in the foo namespace, binding the default Service-\n",
      "Account in that same namespace with the view ClusterRole. What can your pod\n",
      "access now?\n",
      "/ # curl localhost:8001/api/v1/namespaces/foo/pods\n",
      "{\n",
      "  \"kind\": \"PodList\",\n",
      "  \"apiVersion\": \"v1\",\n",
      "  ...\n",
      "Namespace: foo\n",
      "Cluster-level\n",
      "resources\n",
      "Namespace: bar\n",
      "Pods\n",
      "Pods\n",
      "Default\n",
      "ServiceAccount\n",
      "in foo namespace\n",
      "is allowed to\n",
      "view pods in\n",
      "any namespace\n",
      "ClusterRole:\n",
      "view\n",
      "Allows getting,\n",
      "listing, watching\n",
      "ClusterRoleBinding:\n",
      "view-test\n",
      "Pods,\n",
      "Services,\n",
      "Endpoints,\n",
      "ConﬁgMaps,\n",
      "…\n",
      "Service-\n",
      "Account:\n",
      "default\n",
      "Figure 12.9\n",
      "A ClusterRoleBinding and ClusterRole grants permission to resources across all \n",
      "namespaces.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 402, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "370\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      "/ # curl localhost:8001/api/v1/namespaces/bar/pods\n",
      "User \"system:serviceaccount:foo:default\" cannot list pods in the namespace \n",
      "\"bar\".\n",
      "/ # curl localhost:8001/api/v1/pods\n",
      "User \"system:serviceaccount:foo:default\" cannot list pods at the cluster \n",
      "scope.\n",
      "As you can see, your pod can list pods in the foo namespace, but not in any other spe-\n",
      "cific namespace or across all namespaces. This is visualized in figure 12.10.\n",
      "SUMMARIZING ROLE, CLUSTERROLE, ROLEBINDING, AND CLUSTERROLEBINDING COMBINATIONS\n",
      "We’ve covered many different combinations and it may be hard for you to remember\n",
      "when to use each one. Let’s see if we can make sense of all these combinations by cat-\n",
      "egorizing them per specific use case. Refer to table 12.2.\n",
      "Table 12.2\n",
      "When to use specific combinations of role and binding types\n",
      "For accessing\n",
      "Role type to use\n",
      "Binding type to use\n",
      "Cluster-level resources (Nodes, PersistentVolumes, ...)\n",
      "ClusterRole\n",
      "ClusterRoleBinding\n",
      "Non-resource URLs (/api, /healthz, ...)\n",
      "ClusterRole\n",
      "ClusterRoleBinding\n",
      "Namespaced resources in any namespace (and \n",
      "across all namespaces)\n",
      "ClusterRole\n",
      "ClusterRoleBinding\n",
      "Namespaced resources in a specific namespace (reus-\n",
      "ing the same ClusterRole in multiple namespaces)\n",
      "ClusterRole\n",
      "RoleBinding\n",
      "Namespaced resources in a specific namespace \n",
      "(Role must be defined in each namespace)\n",
      "Role\n",
      "RoleBinding\n",
      "Namespace: foo\n",
      "Cluster-level resources\n",
      "Namespace: bar\n",
      "Pods\n",
      "Pods\n",
      "ClusterRole:\n",
      "view\n",
      "Allows getting,\n",
      "listing, watching\n",
      "RoleBinding:\n",
      "view-test\n",
      "Pods,\n",
      "Services,\n",
      "Endpoints,\n",
      "ConﬁgMaps,\n",
      "…\n",
      "Default ServiceAccount in\n",
      "foo namespace is only allowed\n",
      "to view pods in namespace foo,\n",
      "despite using a ClusterRole\n",
      "Service-\n",
      "Account:\n",
      "default\n",
      "Figure 12.10\n",
      "A RoleBinding referring to a ClusterRole only grants access to resources inside the \n",
      "RoleBinding’s namespace.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 403, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "371\n",
      "Securing the cluster with role-based access control\n",
      "Hopefully, the relationships between the four RBAC resources are much clearer\n",
      "now. Don’t worry if you still feel like you don’t yet grasp everything. Things may\n",
      "clear up as we explore the pre-configured ClusterRoles and ClusterRoleBindings in\n",
      "the next section.\n",
      "12.2.5 Understanding default ClusterRoles and ClusterRoleBindings\n",
      "Kubernetes comes with a default set of ClusterRoles and ClusterRoleBindings, which\n",
      "are updated every time the API server starts. This ensures all the default roles and\n",
      "bindings are recreated if you mistakenly delete them or if a newer version of Kuberne-\n",
      "tes uses a different configuration of cluster roles and bindings.\n",
      " You can see the default cluster roles and bindings in the following listing.\n",
      "$ kubectl get clusterrolebindings\n",
      "NAME                                           AGE\n",
      "cluster-admin                                  1d\n",
      "system:basic-user                              1d\n",
      "system:controller:attachdetach-controller      1d\n",
      "...\n",
      "system:controller:ttl-controller               1d\n",
      "system:discovery                               1d\n",
      "system:kube-controller-manager                 1d\n",
      "system:kube-dns                                1d\n",
      "system:kube-scheduler                          1d\n",
      "system:node                                    1d\n",
      "system:node-proxier                            1d\n",
      "$ kubectl get clusterroles\n",
      "NAME                                           AGE\n",
      "admin                                          1d\n",
      "cluster-admin                                  1d\n",
      "edit                                           1d\n",
      "system:auth-delegator                          1d\n",
      "system:basic-user                              1d\n",
      "system:controller:attachdetach-controller      1d\n",
      "...\n",
      "system:controller:ttl-controller               1d\n",
      "system:discovery                               1d\n",
      "system:heapster                                1d\n",
      "system:kube-aggregator                         1d\n",
      "system:kube-controller-manager                 1d\n",
      "system:kube-dns                                1d\n",
      "system:kube-scheduler                          1d\n",
      "system:node                                    1d\n",
      "system:node-bootstrapper                       1d\n",
      "system:node-problem-detector                   1d\n",
      "system:node-proxier                            1d\n",
      "system:persistent-volume-provisioner           1d\n",
      "view                                           1d\n",
      "Listing 12.17\n",
      "Listing all ClusterRoleBindings and ClusterRoles\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 404, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "372\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      "The most important roles are the view, edit, admin, and cluster-admin ClusterRoles.\n",
      "They’re meant to be bound to ServiceAccounts used by user-defined pods.\n",
      "ALLOWING READ-ONLY ACCESS TO RESOURCES WITH THE VIEW CLUSTERROLE\n",
      "You already used the default view ClusterRole in the previous example. It allows read-\n",
      "ing most resources in a namespace, except for Roles, RoleBindings, and Secrets. You’re\n",
      "probably wondering, why not Secrets? Because one of those Secrets might include an\n",
      "authentication token with greater privileges than those defined in the view Cluster-\n",
      "Role and could allow the user to masquerade as a different user to gain additional\n",
      "privileges (privilege escalation). \n",
      "ALLOWING MODIFYING RESOURCES WITH THE EDIT CLUSTERROLE\n",
      "Next is the edit ClusterRole, which allows you to modify resources in a namespace,\n",
      "but also allows both reading and modifying Secrets. It doesn’t, however, allow viewing\n",
      "or modifying Roles or RoleBindings—again, this is to prevent privilege escalation.\n",
      "GRANTING FULL CONTROL OF A NAMESPACE WITH THE ADMIN CLUSTERROLE\n",
      "Complete control of the resources in a namespace is granted in the admin Cluster-\n",
      "Role. Subjects with this ClusterRole can read and modify any resource in the name-\n",
      "space, except ResourceQuotas (we’ll learn what those are in chapter 14) and the\n",
      "Namespace resource itself. The main difference between the edit and the admin Cluster-\n",
      "Roles is in the ability to view and modify Roles and RoleBindings in the namespace.\n",
      "NOTE\n",
      "To prevent privilege escalation, the API server only allows users to cre-\n",
      "ate and update Roles if they already have all the permissions listed in that\n",
      "Role (and for the same scope). \n",
      "ALLOWING COMPLETE CONTROL WITH THE CLUSTER-ADMIN CLUSTERROLE \n",
      "Complete control of the Kubernetes cluster can be given by assigning the cluster-\n",
      "admin ClusterRole to a subject. As you’ve seen before, the admin ClusterRole doesn’t\n",
      "allow users to modify the namespace’s ResourceQuota objects or the Namespace\n",
      "resource itself. If you want to allow a user to do that, you need to create a RoleBinding\n",
      "that references the cluster-admin ClusterRole. This gives the user included in the\n",
      "RoleBinding complete control over all aspects of the namespace in which the Role-\n",
      "Binding is created.\n",
      " If you’ve paid attention, you probably already know how to give users complete\n",
      "control of all the namespaces in the cluster. Yes, by referencing the cluster-admin\n",
      "ClusterRole in a ClusterRoleBinding instead of a RoleBinding.\n",
      "UNDERSTANDING THE OTHER DEFAULT CLUSTERROLES\n",
      "The list of default ClusterRoles includes a large number of other ClusterRoles, which\n",
      "start with the system: prefix. These are meant to be used by the various Kubernetes\n",
      "components. Among them, you’ll find roles such as system:kube-scheduler, which\n",
      "is obviously used by the Scheduler, system:node, which is used by the Kubelets, and\n",
      "so on. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 405, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "373\n",
      "Summary\n",
      " Although the Controller Manager runs as a single pod, each controller running\n",
      "inside it can use a separate ClusterRole and ClusterRoleBinding (they’re prefixed\n",
      "with system: controller:). \n",
      " Each of these system ClusterRoles has a matching ClusterRoleBinding, which binds\n",
      "it to the user the system component authenticates as. The system:kube-scheduler\n",
      "ClusterRoleBinding, for example, assigns the identically named ClusterRole to the\n",
      "system:kube-scheduler user, which is the username the scheduler Authenticates as. \n",
      "12.2.6 Granting authorization permissions wisely\n",
      "By default, the default ServiceAccount in a namespace has no permissions other than\n",
      "those of an unauthenticated user (as you may remember from one of the previous\n",
      "examples, the system:discovery ClusterRole and associated binding allow anyone to\n",
      "make GET requests on a few non-resource URLs). Therefore, pods, by default, can’t\n",
      "even view cluster state. It’s up to you to grant them appropriate permissions to do that. \n",
      " Obviously, giving all your ServiceAccounts the cluster-admin ClusterRole is a\n",
      "bad idea. As is always the case with security, it’s best to give everyone only the permis-\n",
      "sions they need to do their job and not a single permission more (principle of least\n",
      "privilege).\n",
      "CREATING SPECIFIC SERVICEACCOUNTS FOR EACH POD\n",
      "It’s a good idea to create a specific ServiceAccount for each pod (or a set of pod rep-\n",
      "licas) and then associate it with a tailor-made Role (or a ClusterRole) through a\n",
      "RoleBinding (not a ClusterRoleBinding, because that would give the pod access to\n",
      "resources in other namespaces, which is probably not what you want). \n",
      " If one of your pods (the application running within it) only needs to read pods,\n",
      "while the other also needs to modify them, then create two different ServiceAccounts\n",
      "and make those pods use them by specifying the serviceAccountName property in the\n",
      "pod spec, as you learned in the first part of this chapter. Don’t add all the necessary\n",
      "permissions required by both pods to the default ServiceAccount in the namespace. \n",
      "EXPECTING YOUR APPS TO BE COMPROMISED\n",
      "Your aim is to reduce the possibility of an intruder getting hold of your cluster. Today’s\n",
      "complex apps contain many vulnerabilities. You should expect unwanted persons to\n",
      "eventually get their hands on the ServiceAccount’s authentication token, so you should\n",
      "always constrain the ServiceAccount to prevent them from doing any real damage.\n",
      "12.3\n",
      "Summary\n",
      "This chapter has given you a foundation on how to secure the Kubernetes API server.\n",
      "You learned the following:\n",
      "Clients of the API server include both human users and applications running\n",
      "in pods.\n",
      "Applications in pods are associated with a ServiceAccount. \n",
      "Both users and ServiceAccounts are associated with groups.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 406, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "374\n",
      "CHAPTER 12\n",
      "Securing the Kubernetes API server\n",
      "By default, pods run under the default ServiceAccount, which is created for\n",
      "each namespace automatically.\n",
      "Additional ServiceAccounts can be created manually and associated with a pod.\n",
      "ServiceAccounts can be configured to allow mounting only a constrained list of\n",
      "Secrets in a given pod.\n",
      "A ServiceAccount can also be used to attach image pull Secrets to pods, so you\n",
      "don’t need to specify the Secrets in every pod.\n",
      "Roles and ClusterRoles define what actions can be performed on which resources.\n",
      "RoleBindings and ClusterRoleBindings bind Roles and ClusterRoles to users,\n",
      "groups, and ServiceAccounts.\n",
      "Each cluster comes with default ClusterRoles and ClusterRoleBindings.\n",
      "In the next chapter, you’ll learn how to protect the cluster nodes from pods and how\n",
      "to isolate pods from each other by securing the network.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 407, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "375\n",
      "Securing cluster nodes\n",
      "and the network\n",
      "In the previous chapter, we talked about securing the API server. If an attacker\n",
      "gets access to the API server, they can run whatever they like by packaging their\n",
      "code into a container image and running it in a pod. But can they do any real\n",
      "damage? Aren’t containers isolated from other containers and from the node\n",
      "they’re running on? \n",
      " Not necessarily. In this chapter, you’ll learn how to allow pods to access the\n",
      "resources of the node they’re running on. You’ll also learn how to configure the\n",
      "cluster so users aren’t able to do whatever they want with their pods. Then, in\n",
      "This chapter covers\n",
      "Using the node’s default Linux namespaces \n",
      "in pods\n",
      "Running containers as different users\n",
      "Running privileged containers\n",
      "Adding or dropping a container’s kernel \n",
      "capabilities\n",
      "Defining security policies to limit what pods can do\n",
      "Securing the pod network\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 408, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "376\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "the last part of the chapter, you’ll also learn how to secure the network the pods use\n",
      "to communicate.\n",
      "13.1\n",
      "Using the host node’s namespaces in a pod\n",
      "Containers in a pod usually run under separate Linux namespaces, which isolate\n",
      "their processes from processes running in other containers or in the node’s default\n",
      "namespaces. \n",
      " For example, we learned that each pod gets its own IP and port space, because it\n",
      "uses its own network namespace. Likewise, each pod has its own process tree, because\n",
      "it has its own PID namespace, and it also uses its own IPC namespace, allowing only\n",
      "processes in the same pod to communicate with each other through the Inter-Process\n",
      "Communication mechanism (IPC).\n",
      "13.1.1 Using the node’s network namespace in a pod\n",
      "Certain pods (usually system pods) need to operate in the host’s default namespaces,\n",
      "allowing them to see and manipulate node-level resources and devices. For example, a\n",
      "pod may need to use the node’s network adapters instead of its own virtual network\n",
      "adapters. This can be achieved by setting the hostNetwork property in the pod spec\n",
      "to true.\n",
      " In that case, the pod gets to use the node’s network interfaces instead of having its\n",
      "own set, as shown in figure 13.1. This means the pod doesn’t get its own IP address and\n",
      "if it runs a process that binds to a port, the process will be bound to the node’s port.\n",
      "You can try running such a pod. The next listing shows an example pod manifest.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: pod-with-host-network\n",
      "Listing 13.1\n",
      "A pod using the node’s network namespace: pod-with-host-network.yaml\n",
      "Node\n",
      "Pod A\n",
      "Pod’s own network\n",
      "namespace\n",
      "eth0\n",
      "lo\n",
      "eth0\n",
      "docker0\n",
      "lo\n",
      "eth1\n",
      "Node’s default network\n",
      "namespace\n",
      "Pod B\n",
      "hostNetwork: true\n",
      "Figure 13.1\n",
      "A pod \n",
      "with hostNetwork: \n",
      "true uses the node’s \n",
      "network interfaces \n",
      "instead of its own.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 409, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "377\n",
      "Using the host node’s namespaces in a pod\n",
      "spec:\n",
      "  hostNetwork: true              \n",
      "  containers:\n",
      "  - name: main\n",
      "    image: alpine\n",
      "    command: [\"/bin/sleep\", \"999999\"]\n",
      "After you run the pod, you can use the following command to see that it’s indeed using\n",
      "the host’s network namespace (it sees all the host’s network adapters, for example).\n",
      "$ kubectl exec pod-with-host-network ifconfig\n",
      "docker0   Link encap:Ethernet  HWaddr 02:42:14:08:23:47\n",
      "          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0\n",
      "          ...\n",
      "eth0      Link encap:Ethernet  HWaddr 08:00:27:F8:FA:4E\n",
      "          inet addr:10.0.2.15  Bcast:10.0.2.255  Mask:255.255.255.0\n",
      "          ...\n",
      "lo        Link encap:Local Loopback\n",
      "          inet addr:127.0.0.1  Mask:255.0.0.0\n",
      "          ...\n",
      "veth1178d4f Link encap:Ethernet  HWaddr 1E:03:8D:D6:E1:2C\n",
      "          inet6 addr: fe80::1c03:8dff:fed6:e12c/64 Scope:Link\n",
      "          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n",
      "...\n",
      "When the Kubernetes Control Plane components are deployed as pods (such as when\n",
      "you deploy your cluster with kubeadm, as explained in appendix B), you’ll find that\n",
      "those pods use the hostNetwork option, effectively making them behave as if they\n",
      "weren’t running inside a pod.\n",
      "13.1.2 Binding to a host port without using the host’s network \n",
      "namespace\n",
      "A related feature allows pods to bind to a port in the node’s default namespace, but\n",
      "still have their own network namespace. This is done by using the hostPort property\n",
      "in one of the container’s ports defined in the spec.containers.ports field.\n",
      " Don’t confuse pods using hostPort with pods exposed through a NodePort service.\n",
      "They’re two different things, as explained in figure 13.2.\n",
      " The first thing you’ll notice in the figure is that when a pod is using a hostPort, a\n",
      "connection to the node’s port is forwarded directly to the pod running on that node,\n",
      "whereas with a NodePort service, a connection to the node’s port is forwarded to a\n",
      "randomly selected pod (possibly on another node). The other difference is that with\n",
      "pods using a hostPort, the node’s port is only bound on nodes that run such pods,\n",
      "whereas NodePort services bind the port on all nodes, even on those that don’t run\n",
      "such a pod (as on node 3 in the figure).\n",
      "Listing 13.2\n",
      "Network interfaces in a pod using the host’s network namespace\n",
      "Using the host node’s \n",
      "network namespace\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 410, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "378\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "It’s important to understand that if a pod is using a specific host port, only one\n",
      "instance of the pod can be scheduled to each node, because two processes can’t bind\n",
      "to the same host port. The Scheduler takes this into account when scheduling pods, so\n",
      "it doesn’t schedule multiple pods to the same node, as shown in figure 13.3. If you\n",
      "have three nodes and want to deploy four pod replicas, only three will be scheduled\n",
      "(one pod will remain Pending).\n",
      "Node 1\n",
      "Pod 1\n",
      "Two pods using\n",
      "hostPort\n",
      "Port\n",
      "8080\n",
      "Port\n",
      "9000\n",
      "Node 2\n",
      "Pod 2\n",
      "Port\n",
      "8080\n",
      "Port\n",
      "9000\n",
      "Node 3\n",
      "Node 1\n",
      "Pod 1\n",
      "Two pods under\n",
      "the same\n",
      "NodePort\n",
      "service\n",
      "Port\n",
      "8080\n",
      "Node 2\n",
      "Pod 2\n",
      "Port\n",
      "8080\n",
      "Node 3\n",
      "Port\n",
      "88\n",
      "Port\n",
      "88\n",
      "Port\n",
      "88\n",
      "Service\n",
      "(\n",
      ")\n",
      "iptables\n",
      "Service\n",
      "(\n",
      ")\n",
      "iptables\n",
      "Service\n",
      "(\n",
      ")\n",
      "iptables\n",
      "Figure 13.2\n",
      "Difference between pods using a hostPort and pods behind a NodePort service.\n",
      "Node 1\n",
      "Pod 1\n",
      "Port\n",
      "8080\n",
      "Host\n",
      "port\n",
      "9000\n",
      "Host\n",
      "port\n",
      "9000\n",
      "Pod 2\n",
      "Port\n",
      "8080\n",
      "Node 2\n",
      "Pod 3\n",
      "Port\n",
      "8080\n",
      "Host\n",
      "port\n",
      "9000\n",
      "Node 3\n",
      "Pod 4\n",
      "Port\n",
      "8080\n",
      "Cannot be scheduled to the same\n",
      "node, because the port is already bound\n",
      "Only a single\n",
      "replica per node\n",
      "Figure 13.3\n",
      "If a host port is used, only a single pod instance can be scheduled to a node.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 411, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "379\n",
      "Using the host node’s namespaces in a pod\n",
      "Let’s see how to define the hostPort in a pod’s YAML definition. The following listing\n",
      "shows the YAML to run your kubia pod and bind it to the node’s port 9000.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: kubia-hostport\n",
      "spec:\n",
      "  containers:\n",
      "  - image: luksa/kubia\n",
      "    name: kubia\n",
      "    ports:\n",
      "    - containerPort: 8080    \n",
      "      hostPort: 9000        \n",
      "      protocol: TCP\n",
      "After you create this pod, you can access it through port 9000 of the node it’s sched-\n",
      "uled to. If you have multiple nodes, you’ll see you can’t access the pod through that\n",
      "port on the other nodes. \n",
      "NOTE\n",
      "If you’re trying this on GKE, you need to configure the firewall prop-\n",
      "erly using gcloud compute firewall-rules, the way you did in chapter 5.\n",
      "The hostPort feature is primarily used for exposing system services, which are\n",
      "deployed to every node using DaemonSets. Initially, people also used it to ensure two\n",
      "replicas of the same pod were never scheduled to the same node, but now you have a\n",
      "better way of achieving this—it’s explained in chapter 16.\n",
      "13.1.3 Using the node’s PID and IPC namespaces\n",
      "Similar to the hostNetwork option are the hostPID and hostIPC pod spec properties.\n",
      "When you set them to true, the pod’s containers will use the node’s PID and IPC\n",
      "namespaces, allowing processes running in the containers to see all the other pro-\n",
      "cesses on the node or communicate with them through IPC, respectively. See the fol-\n",
      "lowing listing for an example.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: pod-with-host-pid-and-ipc\n",
      "spec:\n",
      "  hostPID: true                    \n",
      "  hostIPC: true                     \n",
      "  containers:\n",
      "  - name: main\n",
      "    image: alpine\n",
      "    command: [\"/bin/sleep\", \"999999\"]\n",
      "Listing 13.3\n",
      "Binding a pod to a port in the node’s port space: kubia-hostport.yaml\n",
      "Listing 13.4\n",
      "Using the host’s PID and IPC namespaces: pod-with-host-pid-and-ipc.yaml\n",
      "The container can be \n",
      "reached on port 8080 \n",
      "of the pod’s IP.\n",
      "It can also be reached \n",
      "on port 9000 of the \n",
      "node it’s deployed on.\n",
      "You want the pod to \n",
      "use the host’s PID \n",
      "namespace.\n",
      "You also want the \n",
      "pod to use the host’s \n",
      "IPC namespace.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 412, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "380\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "You’ll remember that pods usually see only their own processes, but if you run this pod\n",
      "and then list the processes from within its container, you’ll see all the processes run-\n",
      "ning on the host node, not only the ones running in the container, as shown in the\n",
      "following listing.\n",
      "$ kubectl exec pod-with-host-pid-and-ipc ps aux\n",
      "PID   USER     TIME   COMMAND\n",
      "    1 root       0:01 /usr/lib/systemd/systemd --switched-root --system ...\n",
      "    2 root       0:00 [kthreadd]\n",
      "    3 root       0:00 [ksoftirqd/0]\n",
      "    5 root       0:00 [kworker/0:0H]\n",
      "    6 root       0:00 [kworker/u2:0]\n",
      "    7 root       0:00 [migration/0]\n",
      "    8 root       0:00 [rcu_bh]\n",
      "    9 root       0:00 [rcu_sched]\n",
      "   10 root       0:00 [watchdog/0]\n",
      "...\n",
      "By setting the hostIPC property to true, processes in the pod’s containers can also\n",
      "communicate with all the other processes running on the node, through Inter-Process\n",
      "Communication.\n",
      "13.2\n",
      "Configuring the container’s security context\n",
      "Besides allowing the pod to use the host’s Linux namespaces, other security-related\n",
      "features can also be configured on the pod and its container through the security-\n",
      "Context properties, which can be specified under the pod spec directly and inside the\n",
      "spec of individual containers.\n",
      "UNDERSTANDING WHAT’S CONFIGURABLE IN THE SECURITY CONTEXT\n",
      "Configuring the security context allows you to do various things:\n",
      "Specify the user (the user’s ID) under which the process in the container will run.\n",
      "Prevent the container from running as root (the default user a container runs\n",
      "as is usually defined in the container image itself, so you may want to prevent\n",
      "containers from running as root).\n",
      "Run the container in privileged mode, giving it full access to the node’s kernel.\n",
      "Configure fine-grained privileges, by adding or dropping capabilities—in con-\n",
      "trast to giving the container all possible permissions by running it in privi-\n",
      "leged mode.\n",
      "Set SELinux (Security Enhanced Linux) options to strongly lock down a\n",
      "container.\n",
      "Prevent the process from writing to the container’s filesystem.\n",
      "We’ll explore these options next. \n",
      "Listing 13.5\n",
      "Processes visible in a pod with hostPID: true\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 413, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "381\n",
      "Configuring the container’s security context\n",
      "RUNNING A POD WITHOUT SPECIFYING A SECURITY CONTEXT\n",
      "First, run a pod with the default security context options (by not specifying them at\n",
      "all), so you can see how it behaves compared to pods with a custom security context:\n",
      "$ kubectl run pod-with-defaults --image alpine --restart Never \n",
      "➥  -- /bin/sleep 999999\n",
      "pod \"pod-with-defaults\" created\n",
      "Let’s see what user and group ID the container is running as, and which groups it\n",
      "belongs to. You can see this by running the id command inside the container:\n",
      "$ kubectl exec pod-with-defaults id\n",
      "uid=0(root) gid=0(root) groups=0(root), 1(bin), 2(daemon), 3(sys), 4(adm), \n",
      "6(disk), 10(wheel), 11(floppy), 20(dialout), 26(tape), 27(video)\n",
      "The container is running as user ID (uid) 0, which is root, and group ID (gid) 0 (also\n",
      "root). It’s also a member of multiple other groups. \n",
      "NOTE\n",
      "What user the container runs as is specified in the container image. In\n",
      "a Dockerfile, this is done using the USER directive. If omitted, the container\n",
      "runs as root.\n",
      "Now, you’ll run a pod where the container runs as a different user.\n",
      "13.2.1 Running a container as a specific user\n",
      "To run a pod under a different user ID than the one that’s baked into the container\n",
      "image, you’ll need to set the pod’s securityContext.runAsUser property. You’ll\n",
      "make the container run as user guest, whose user ID in the alpine container image is\n",
      "405, as shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: pod-as-user-guest\n",
      "spec:\n",
      "  containers:\n",
      "  - name: main\n",
      "    image: alpine\n",
      "    command: [\"/bin/sleep\", \"999999\"]\n",
      "    securityContext:\n",
      "      runAsUser: 405      \n",
      "Now, to see the effect of the runAsUser property, run the id command in this new\n",
      "pod, the way you did before:\n",
      "$ kubectl exec pod-as-user-guest id\n",
      "uid=405(guest) gid=100(users)\n",
      "Listing 13.6\n",
      "Running containers as a specific user: pod-as-user-guest.yaml\n",
      "You need to specify a user ID, not \n",
      "a username (id 405 corresponds \n",
      "to the guest user).\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 414, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "382\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "As requested, the container is running as the guest user. \n",
      "13.2.2 Preventing a container from running as root\n",
      "What if you don’t care what user the container runs as, but you still want to prevent it\n",
      "from running as root? \n",
      " Imagine having a pod deployed with a container image that was built with a USER\n",
      "daemon directive in the Dockerfile, which makes the container run under the daemon\n",
      "user. What if an attacker gets access to your image registry and pushes a different\n",
      "image under the same tag? The attacker’s image is configured to run as the root user.\n",
      "When Kubernetes schedules a new instance of your pod, the Kubelet will download\n",
      "the attacker’s image and run whatever code they put into it. \n",
      " Although containers are mostly isolated from the host system, running their pro-\n",
      "cesses as root is still considered a bad practice. For example, when a host directory is\n",
      "mounted into the container, if the process running in the container is running as\n",
      "root, it has full access to the mounted directory, whereas if it’s running as non-root,\n",
      "it won’t. \n",
      " To prevent the attack scenario described previously, you can specify that the pod’s\n",
      "container needs to run as a non-root user, as shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: pod-run-as-non-root\n",
      "spec:\n",
      "  containers:\n",
      "  - name: main\n",
      "    image: alpine\n",
      "    command: [\"/bin/sleep\", \"999999\"]\n",
      "    securityContext:                   \n",
      "      runAsNonRoot: true               \n",
      "If you deploy this pod, it gets scheduled, but is not allowed to run:\n",
      "$ kubectl get po pod-run-as-non-root\n",
      "NAME                 READY  STATUS                                                  \n",
      "pod-run-as-non-root  0/1    container has runAsNonRoot and image will run \n",
      "                            ➥  as root\n",
      "Now, if anyone tampers with your container images, they won’t get far.\n",
      "13.2.3 Running pods in privileged mode\n",
      "Sometimes pods need to do everything that the node they’re running on can do, such\n",
      "as use protected system devices or other kernel features, which aren’t accessible to\n",
      "regular containers. \n",
      "Listing 13.7\n",
      "Preventing containers from running as root: pod-run-as-non-root.yaml\n",
      "This container will only \n",
      "be allowed to run as a \n",
      "non-root user.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 415, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "383\n",
      "Configuring the container’s security context\n",
      " An example of such a pod is the kube-proxy pod, which needs to modify the node’s\n",
      "iptables rules to make services work, as was explained in chapter 11. If you follow the\n",
      "instructions in appendix B and deploy a cluster with kubeadm, you’ll see every cluster\n",
      "node runs a kube-proxy pod and you can examine its YAML specification to see all the\n",
      "special features it’s using. \n",
      " To get full access to the node’s kernel, the pod’s container runs in privileged\n",
      "mode. This is achieved by setting the privileged property in the container’s security-\n",
      "Context property to true. You’ll create a privileged pod from the YAML in the follow-\n",
      "ing listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: pod-privileged\n",
      "spec:\n",
      "  containers:\n",
      "  - name: main\n",
      "    image: alpine\n",
      "    command: [\"/bin/sleep\", \"999999\"]\n",
      "    securityContext:\n",
      "      privileged: true     \n",
      "Go ahead and deploy this pod, so you can compare it with the non-privileged pod you\n",
      "ran earlier. \n",
      " If you’re familiar with Linux, you may know it has a special file directory called /dev,\n",
      "which contains device files for all the devices on the system. These aren’t regular files on\n",
      "disk, but are special files used to communicate with devices. Let’s see what devices are\n",
      "visible in the non-privileged container you deployed earlier (the pod-with-defaults\n",
      "pod), by listing files in its /dev directory, as shown in the following listing.\n",
      "$ kubectl exec -it pod-with-defaults ls /dev\n",
      "core             null             stderr           urandom\n",
      "fd               ptmx             stdin            zero\n",
      "full             pts              stdout\n",
      "fuse             random           termination-log\n",
      "mqueue           shm              tty\n",
      "The listing shows all the devices. The list is fairly short. Now, compare this with the fol-\n",
      "lowing listing, which shows the device files your privileged pod can see.\n",
      "$ kubectl exec -it pod-privileged ls /dev\n",
      "autofs              snd                 tty46\n",
      "bsg                 sr0                 tty47\n",
      "Listing 13.8\n",
      "A pod with a privileged container: pod-privileged.yaml\n",
      "Listing 13.9\n",
      "List of available devices in a non-privileged pod\n",
      "Listing 13.10\n",
      "List of available devices in a privileged pod\n",
      "This container will \n",
      "run in privileged \n",
      "mode\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 416, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "384\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "btrfs-control       stderr              tty48\n",
      "core                stdin               tty49\n",
      "cpu                 stdout              tty5\n",
      "cpu_dma_latency     termination-log     tty50\n",
      "fd                  tty                 tty51\n",
      "full                tty0                tty52\n",
      "fuse                tty1                tty53\n",
      "hpet                tty10               tty54\n",
      "hwrng               tty11               tty55\n",
      "...                 ...                 ...\n",
      "I haven’t included the whole list, because it’s too long for the book, but it’s evident\n",
      "that the device list is much longer than before. In fact, the privileged container sees\n",
      "all the host node’s devices. This means it can use any device freely. \n",
      " For example, I had to use privileged mode like this when I wanted a pod running\n",
      "on a Raspberry Pi to control LEDs connected it.\n",
      "13.2.4 Adding individual kernel capabilities to a container\n",
      "In the previous section, you saw one way of giving a container unlimited power. In the\n",
      "old days, traditional UNIX implementations only distinguished between privileged\n",
      "and unprivileged processes, but for many years, Linux has supported a much more\n",
      "fine-grained permission system through kernel capabilities.\n",
      " Instead of making a container privileged and giving it unlimited permissions, a\n",
      "much safer method (from a security perspective) is to give it access only to the kernel\n",
      "features it really requires. Kubernetes allows you to add capabilities to each container\n",
      "or drop part of them, which allows you to fine-tune the container’s permissions and\n",
      "limit the impact of a potential intrusion by an attacker.\n",
      " For example, a container usually isn’t allowed to change the system time (the hard-\n",
      "ware clock’s time). You can confirm this by trying to set the time in your pod-with-\n",
      "defaults pod:\n",
      "$ kubectl exec -it pod-with-defaults -- date +%T -s \"12:00:00\"\n",
      "date: can't set date: Operation not permitted\n",
      "If you want to allow the container to change the system time, you can add a capabil-\n",
      "ity called CAP_SYS_TIME to the container’s capabilities list, as shown in the follow-\n",
      "ing listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: pod-add-settime-capability\n",
      "spec:\n",
      "  containers:\n",
      "  - name: main\n",
      "    image: alpine\n",
      "Listing 13.11\n",
      "Adding the CAP_SYS_TIME capability: pod-add-settime-capability.yaml\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 417, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "385\n",
      "Configuring the container’s security context\n",
      "    command: [\"/bin/sleep\", \"999999\"]\n",
      "    securityContext:                     \n",
      "      capabilities:                      \n",
      "        add:                  \n",
      "        - SYS_TIME            \n",
      "NOTE\n",
      "Linux kernel capabilities are usually prefixed with CAP_. But when\n",
      "specifying them in a pod spec, you must leave out the prefix.\n",
      "If you run the same command in this new pod’s container, the system time is changed\n",
      "successfully:\n",
      "$ kubectl exec -it pod-add-settime-capability -- date +%T -s \"12:00:00\"\n",
      "12:00:00\n",
      "$ kubectl exec -it pod-add-settime-capability -- date\n",
      "Sun May  7 12:00:03 UTC 2017\n",
      "WARNING\n",
      "If you try this yourself, be aware that it may cause your worker\n",
      "node to become unusable. In Minikube, although the system time was auto-\n",
      "matically reset back by the Network Time Protocol (NTP) daemon, I had to\n",
      "reboot the VM to schedule new pods. \n",
      "You can confirm the node’s time has been changed by checking the time on the node\n",
      "running the pod. In my case, I’m using Minikube, so I have only one node and I can\n",
      "get its time like this:\n",
      "$ minikube ssh date\n",
      "Sun May  7 12:00:07 UTC 2017\n",
      "Adding capabilities like this is a much better way than giving a container full privileges\n",
      "with privileged: true. Admittedly, it does require you to know and understand what\n",
      "each capability does.\n",
      "TIP\n",
      "You’ll find the list of Linux kernel capabilities in the Linux man pages.\n",
      "13.2.5 Dropping capabilities from a container\n",
      "You’ve seen how to add capabilities, but you can also drop capabilities that may oth-\n",
      "erwise be available to the container. For example, the default capabilities given to a\n",
      "container include the CAP_CHOWN capability, which allows processes to change the\n",
      "ownership of files in the filesystem. \n",
      " You can see that’s the case by changing the ownership of the /tmp directory in\n",
      "your pod-with-defaults pod to the guest user, for example:\n",
      "$ kubectl exec pod-with-defaults chown guest /tmp\n",
      "$ kubectl exec pod-with-defaults -- ls -la / | grep tmp\n",
      "drwxrwxrwt    2 guest    root             6 May 25 15:18 tmp\n",
      "Capabilities are added or dropped \n",
      "under the securityContext property.\n",
      "You’re adding the \n",
      "SYS_TIME capability.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 418, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "386\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "To prevent the container from doing that, you need to drop the capability by listing it\n",
      "under the container’s securityContext.capabilities.drop property, as shown in\n",
      "the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: pod-drop-chown-capability\n",
      "spec:\n",
      "  containers:\n",
      "  - name: main\n",
      "    image: alpine\n",
      "    command: [\"/bin/sleep\", \"999999\"]\n",
      "    securityContext:\n",
      "      capabilities:\n",
      "        drop:                   \n",
      "        - CHOWN                 \n",
      "By dropping the CHOWN capability, you’re not allowed to change the owner of the /tmp\n",
      "directory in this pod:\n",
      "$ kubectl exec pod-drop-chown-capability chown guest /tmp\n",
      "chown: /tmp: Operation not permitted\n",
      "You’re almost done exploring the container’s security context options. Let’s look at\n",
      "one more.\n",
      "13.2.6 Preventing processes from writing to the container’s filesystem\n",
      "You may want to prevent the processes running in the container from writing to the\n",
      "container’s filesystem, and only allow them to write to mounted volumes. You’d want\n",
      "to do that mostly for security reasons. \n",
      " Let’s imagine you’re running a PHP application with a hidden vulnerability, allow-\n",
      "ing an attacker to write to the filesystem. The PHP files are added to the container\n",
      "image at build time and are served from the container’s filesystem. Because of the vul-\n",
      "nerability, the attacker can modify those files and inject them with malicious code. \n",
      " These types of attacks can be thwarted by preventing the container from writing to\n",
      "its filesystem, where the app’s executable code is normally stored. This is done by set-\n",
      "ting the container’s securityContext.readOnlyRootFilesystem property to true, as\n",
      "shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: pod-with-readonly-filesystem\n",
      "Listing 13.12\n",
      "Dropping a capability from a container: pod-drop-chown-capability.yaml\n",
      "Listing 13.13\n",
      "A container with a read-only filesystem: pod-with-readonly-filesystem.yaml\n",
      "You’re not allowing this container \n",
      "to change file ownership.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 419, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "387\n",
      "Configuring the container’s security context\n",
      "spec:\n",
      "  containers:\n",
      "  - name: main\n",
      "    image: alpine\n",
      "    command: [\"/bin/sleep\", \"999999\"]\n",
      "    securityContext:                      \n",
      "      readOnlyRootFilesystem: true        \n",
      "    volumeMounts:                      \n",
      "    - name: my-volume                  \n",
      "      mountPath: /volume               \n",
      "      readOnly: false                  \n",
      "  volumes:\n",
      "  - name: my-volume\n",
      "    emptyDir:\n",
      "When you deploy this pod, the container is running as root, which has write permis-\n",
      "sions to the / directory, but trying to write a file there fails:\n",
      "$ kubectl exec -it pod-with-readonly-filesystem touch /new-file\n",
      "touch: /new-file: Read-only file system\n",
      "On the other hand, writing to the mounted volume is allowed:\n",
      "$ kubectl exec -it pod-with-readonly-filesystem touch /volume/newfile\n",
      "$ kubectl exec -it pod-with-readonly-filesystem -- ls -la /volume/newfile\n",
      "-rw-r--r--    1 root     root       0 May  7 19:11 /mountedVolume/newfile\n",
      "As shown in the example, when you make the container’s filesystem read-only, you’ll\n",
      "probably want to mount a volume in every directory the application writes to (for\n",
      "example, logs, on-disk caches, and so on).\n",
      "TIP\n",
      "To increase security, when running pods in production, set their con-\n",
      "tainer’s readOnlyRootFilesystem property to true.\n",
      "SETTING SECURITY CONTEXT OPTIONS AT THE POD LEVEL\n",
      "In all these examples, you’ve set the security context of an individual container. Sev-\n",
      "eral of these options can also be set at the pod level (through the pod.spec.security-\n",
      "Context property). They serve as a default for all the pod’s containers but can be\n",
      "overridden at the container level. The pod-level security context also allows you to set\n",
      "additional properties, which we’ll explain next.\n",
      "13.2.7 Sharing volumes when containers run as different users\n",
      "In chapter 6, we explained how volumes are used to share data between the pod’s\n",
      "containers. You had no trouble writing files in one container and reading them in\n",
      "the other. \n",
      " But this was only because both containers were running as root, giving them full\n",
      "access to all the files in the volume. Now imagine using the runAsUser option we\n",
      "explained earlier. You may need to run the two containers as two different users (per-\n",
      "haps you’re using two third-party container images, where each one runs its process\n",
      "This container’s filesystem \n",
      "can’t be written to...\n",
      "...but writing to /volume is \n",
      "allowed, becase a volume \n",
      "is mounted there.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 420, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "388\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "under its own specific user). If those two containers use a volume to share files, they\n",
      "may not necessarily be able to read or write files of one another. \n",
      " That’s why Kubernetes allows you to specify supplemental groups for all the pods\n",
      "running in the container, allowing them to share files, regardless of the user IDs\n",
      "they’re running as. This is done using the following two properties:\n",
      "\n",
      "fsGroup\n",
      "\n",
      "supplementalGroups\n",
      "What they do is best explained in an example, so let’s see how to use them in a pod\n",
      "and then see what their effect is. The next listing describes a pod with two containers\n",
      "sharing the same volume.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: pod-with-shared-volume-fsgroup\n",
      "spec:\n",
      "  securityContext:                       \n",
      "    fsGroup: 555                         \n",
      "    supplementalGroups: [666, 777]       \n",
      "  containers:\n",
      "  - name: first\n",
      "    image: alpine\n",
      "    command: [\"/bin/sleep\", \"999999\"]\n",
      "    securityContext:                     \n",
      "      runAsUser: 1111                    \n",
      "    volumeMounts:                               \n",
      "    - name: shared-volume                       \n",
      "      mountPath: /volume\n",
      "      readOnly: false\n",
      "  - name: second\n",
      "    image: alpine\n",
      "    command: [\"/bin/sleep\", \"999999\"]\n",
      "    securityContext:                     \n",
      "      runAsUser: 2222                    \n",
      "    volumeMounts:                               \n",
      "    - name: shared-volume                       \n",
      "      mountPath: /volume\n",
      "      readOnly: false\n",
      "  volumes:                                      \n",
      "  - name: shared-volume                         \n",
      "    emptyDir:\n",
      "After you create this pod, run a shell in its first container and see what user and group\n",
      "IDs the container is running as:\n",
      "$ kubectl exec -it pod-with-shared-volume-fsgroup -c first sh\n",
      "/ $ id\n",
      "uid=1111 gid=0(root) groups=555,666,777\n",
      "Listing 13.14\n",
      "fsGroup & supplementalGroups: pod-with-shared-volume-fsgroup.yaml\n",
      "The fsGroup and supplementalGroups \n",
      "are defined in the security context at \n",
      "the pod level.\n",
      "The first container \n",
      "runs as user ID 1111.\n",
      "Both containers \n",
      "use the same \n",
      "volume\n",
      "The second\n",
      "container\n",
      "runs as user\n",
      "ID 2222.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 421, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "389\n",
      "Restricting the use of security-related features in pods\n",
      "The id command shows the container is running with user ID 1111, as specified in the\n",
      "pod definition. The effective group ID is 0(root), but group IDs 555, 666, and 777 are\n",
      "also associated with the user. \n",
      " In the pod definition, you set fsGroup to 555. Because of this, the mounted volume\n",
      "will be owned by group ID 555, as shown here:\n",
      "/ $ ls -l / | grep volume\n",
      "drwxrwsrwx    2 root     555              6 May 29 12:23 volume\n",
      "If you create a file in the mounted volume’s directory, the file is owned by user ID\n",
      "1111 (that’s the user ID the container is running as) and by group ID 555:\n",
      "/ $ echo foo > /volume/foo\n",
      "/ $ ls -l /volume\n",
      "total 4\n",
      "-rw-r--r--    1 1111     555              4 May 29 12:25 foo\n",
      "This is different from how ownership is otherwise set up for newly created files. Usu-\n",
      "ally, the user’s effective group ID, which is 0 in your case, is used when a user creates\n",
      "files. You can see this by creating a file in the container’s filesystem instead of in the\n",
      "volume:\n",
      "/ $ echo foo > /tmp/foo\n",
      "/ $ ls -l /tmp\n",
      "total 4\n",
      "-rw-r--r--    1 1111     root             4 May 29 12:41 foo\n",
      "As you can see, the fsGroup security context property is used when the process cre-\n",
      "ates files in a volume (but this depends on the volume plugin used), whereas the\n",
      "supplementalGroups property defines a list of additional group IDs the user is asso-\n",
      "ciated with. \n",
      " This concludes this section about the configuration of the container’s security con-\n",
      "text. Next, we’ll see how a cluster administrator can restrict users from doing so.\n",
      "13.3\n",
      "Restricting the use of security-related features in pods\n",
      "The examples in the previous sections have shown how a person deploying pods can\n",
      "do whatever they want on any cluster node, by deploying a privileged pod to the\n",
      "node, for example. Obviously, a mechanism must prevent users from doing part or\n",
      "all of what’s been explained. The cluster admin can restrict the use of the previously\n",
      "described security-related features by creating one or more PodSecurityPolicy\n",
      "resources.\n",
      "13.3.1 Introducing the PodSecurityPolicy resource\n",
      "PodSecurityPolicy is a cluster-level (non-namespaced) resource, which defines what\n",
      "security-related features users can or can’t use in their pods. The job of upholding\n",
      "the policies configured in PodSecurityPolicy resources is performed by the\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 422, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "390\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "PodSecurityPolicy admission control plugin running in the API server (we explained\n",
      "admission control plugins in chapter 11).\n",
      "NOTE\n",
      "The PodSecurityPolicy admission control plugin may not be enabled\n",
      "in your cluster. Before running the following examples, ensure it’s enabled. If\n",
      "you’re using Minikube, refer to the next sidebar.\n",
      "When someone posts a pod resource to the API server, the PodSecurityPolicy admis-\n",
      "sion control plugin validates the pod definition against the configured PodSecurity-\n",
      "Policies. If the pod conforms to the cluster’s policies, it’s accepted and stored into\n",
      "etcd; otherwise it’s rejected immediately. The plugin may also modify the pod\n",
      "resource according to defaults configured in the policy.\n",
      "UNDERSTANDING WHAT A PODSECURITYPOLICY CAN DO\n",
      "A PodSecurityPolicy resource defines things like the following:\n",
      "Whether a pod can use the host’s IPC, PID, or Network namespaces\n",
      "Which host ports a pod can bind to\n",
      "What user IDs a container can run as\n",
      "Whether a pod with privileged containers can be created\n",
      "Enabling RBAC and PodSecurityPolicy admission control in Minikube\n",
      "I’m using Minikube version v0.19.0 to run these examples. That version doesn’t\n",
      "enable either the PodSecurityPolicy admission control plugin or RBAC authorization,\n",
      "which is required in part of the exercises. One exercise also requires authenticating\n",
      "as a different user, so you’ll also need to enable the basic authentication plugin\n",
      "where users are defined in a file.\n",
      "To run Minikube with all these plugins enabled, you may need to use this (or a similar)\n",
      "command, depending on the version you’re using: \n",
      "$ minikube start --extra-config apiserver.Authentication.PasswordFile.\n",
      "➥ BasicAuthFile=/etc/kubernetes/passwd --extra-config=apiserver.\n",
      "➥ Authorization.Mode=RBAC --extra-config=apiserver.GenericServerRun\n",
      "➥ Options.AdmissionControl=NamespaceLifecycle,LimitRanger,Service\n",
      "➥ Account,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,\n",
      "➥ DefaultTolerationSeconds,PodSecurityPolicy\n",
      "The API server won’t start up until you create the password file you specified in the\n",
      "command line options. This is how to create the file:\n",
      "$ cat <<EOF | minikube ssh sudo tee /etc/kubernetes/passwd\n",
      "password,alice,1000,basic-user\n",
      "password,bob,2000,privileged-user\n",
      "EOF\n",
      "You’ll find a shell script that runs both commands in the book’s code archive in\n",
      "Chapter13/minikube-with-rbac-and-psp-enabled.sh.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 423, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "391\n",
      "Restricting the use of security-related features in pods\n",
      "Which kernel capabilities are allowed, which are added by default and which are\n",
      "always dropped\n",
      "What SELinux labels a container can use\n",
      "Whether a container can use a writable root filesystem or not\n",
      "Which filesystem groups the container can run as\n",
      "Which volume types a pod can use\n",
      "If you’ve read this chapter up to this point, everything but the last item in the previous\n",
      "list should be familiar. The last item should also be fairly clear. \n",
      "EXAMINING A SAMPLE PODSECURITYPOLICY\n",
      "The following listing shows a sample PodSecurityPolicy, which prevents pods from\n",
      "using the host’s IPC, PID, and Network namespaces, and prevents running privileged\n",
      "containers and the use of most host ports (except ports from 10000-11000 and 13000-\n",
      "14000). The policy doesn’t set any constraints on what users, groups, or SELinux\n",
      "groups the container can run as.\n",
      "apiVersion: extensions/v1beta1\n",
      "kind: PodSecurityPolicy\n",
      "metadata:\n",
      "  name: default\n",
      "spec:\n",
      "  hostIPC: false                 \n",
      "  hostPID: false                 \n",
      "  hostNetwork: false             \n",
      "  hostPorts:                         \n",
      "  - min: 10000                       \n",
      "    max: 11000                       \n",
      "  - min: 13000                       \n",
      "    max: 14000                       \n",
      "  privileged: false              \n",
      "  readOnlyRootFilesystem: true   \n",
      "  runAsUser:                      \n",
      "    rule: RunAsAny                \n",
      "  fsGroup:                        \n",
      "    rule: RunAsAny                \n",
      "  supplementalGroups:             \n",
      "    rule: RunAsAny                \n",
      "  seLinux:                      \n",
      "    rule: RunAsAny              \n",
      "  volumes:                  \n",
      "  - '*'                     \n",
      "Most of the options specified in the example should be self-explanatory, especially if\n",
      "you’ve read the previous sections. After this PodSecurityPolicy resource is posted to\n",
      "Listing 13.15\n",
      "An example PodSecurityPolicy: pod-security-policy.yaml\n",
      "Containers aren’t \n",
      "allowed to use the \n",
      "host’s IPC, PID, or \n",
      "network namespace.\n",
      "They can only bind to host ports \n",
      "10000 to 11000 (inclusive) or \n",
      "host ports 13000 to 14000.\n",
      "Containers cannot run \n",
      "in privileged mode.\n",
      "Containers are forced to run \n",
      "with a read-only root filesystem.\n",
      "Containers can \n",
      "run as any user \n",
      "and any group.\n",
      "They can also use any \n",
      "SELinux groups they want.\n",
      "All volume types can \n",
      "be used in pods.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 424, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "392\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "the cluster, the API server will no longer allow you to deploy the privileged pod used\n",
      "earlier. For example\n",
      "$ kubectl create -f pod-privileged.yaml\n",
      "Error from server (Forbidden): error when creating \"pod-privileged.yaml\":\n",
      "pods \"pod-privileged\" is forbidden: unable to validate against any pod \n",
      "security policy: [spec.containers[0].securityContext.privileged: Invalid \n",
      "value: true: Privileged containers are not allowed]\n",
      "Likewise, you can no longer deploy pods that want to use the host’s PID, IPC, or Net-\n",
      "work namespace. Also, because you set readOnlyRootFilesystem to true in the pol-\n",
      "icy, the container filesystems in all pods will be read-only (containers can only write\n",
      "to volumes).\n",
      "13.3.2 Understanding runAsUser, fsGroup, and supplementalGroups \n",
      "policies\n",
      "The policy in the previous example doesn’t impose any limits on which users and\n",
      "groups containers can run as, because you’ve used the RunAsAny rule for the runAs-\n",
      "User, fsGroup, and supplementalGroups fields. If you want to constrain the list of\n",
      "allowed user or group IDs, you change the rule to MustRunAs and specify the range of\n",
      "allowed IDs. \n",
      "USING THE MUSTRUNAS RULE\n",
      "Let’s look at an example. To only allow containers to run as user ID 2 and constrain the\n",
      "default filesystem group and supplemental group IDs to be anything from 2–10 or 20–\n",
      "30 (all inclusive), you’d include the following snippet in the PodSecurityPolicy resource.\n",
      "  runAsUser:\n",
      "    rule: MustRunAs\n",
      "    ranges:\n",
      "    - min: 2                \n",
      "      max: 2                \n",
      "  fsGroup:\n",
      "    rule: MustRunAs\n",
      "    ranges:\n",
      "    - min: 2                \n",
      "      max: 10               \n",
      "    - min: 20               \n",
      "      max: 30               \n",
      "  supplementalGroups:\n",
      "    rule: MustRunAs\n",
      "    ranges:\n",
      "    - min: 2                \n",
      "      max: 10               \n",
      "    - min: 20               \n",
      "      max: 30               \n",
      "Listing 13.16\n",
      "Specifying IDs containers must run as: psp-must-run-as.yaml\n",
      "Add a single range with min equal \n",
      "to max to set one specific ID.\n",
      "Multiple ranges are \n",
      "supported—here, \n",
      "group IDs can be 2–10 \n",
      "or 20–30 (inclusive).\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 425, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "393\n",
      "Restricting the use of security-related features in pods\n",
      "If the pod spec tries to set either of those fields to a value outside of these ranges, the\n",
      "pod will not be accepted by the API server. To try this, delete the previous PodSecurity-\n",
      "Policy and create the new one from the psp-must-run-as.yaml file. \n",
      "NOTE\n",
      "Changing the policy has no effect on existing pods, because PodSecurity-\n",
      "Policies are enforced only when creating or updating pods.\n",
      "DEPLOYING A POD WITH RUNASUSER OUTSIDE OF THE POLICY’S RANGE\n",
      "If you try deploying the pod-as-user-guest.yaml file from earlier, which says the con-\n",
      "tainer should run as user ID 405, the API server rejects the pod:\n",
      "$ kubectl create -f pod-as-user-guest.yaml\n",
      "Error from server (Forbidden): error when creating \"pod-as-user-guest.yaml\"\n",
      ": pods \"pod-as-user-guest\" is forbidden: unable to validate against any pod \n",
      "security policy: [securityContext.runAsUser: Invalid value: 405: UID on \n",
      "container main does not match required range.  Found 405, allowed: [{2 2}]]\n",
      "Okay, that was obvious. But what happens if you deploy a pod without setting the runAs-\n",
      "User property, but the user ID is baked into the container image (using the USER direc-\n",
      "tive in the Dockerfile)?\n",
      "DEPLOYING A POD WITH A CONTAINER IMAGE WITH AN OUT-OF-RANGE USER ID\n",
      "I’ve created an alternative image for the Node.js app you’ve used throughout the\n",
      "book. The image is configured so that the container will run as user ID 5. The Docker-\n",
      "file for the image is shown in the following listing.\n",
      "FROM node:7\n",
      "ADD app.js /app.js\n",
      "USER 5                         \n",
      "ENTRYPOINT [\"node\", \"app.js\"]\n",
      "I pushed the image to Docker Hub as luksa/kubia-run-as-user-5. If I deploy a pod\n",
      "with that image, the API server doesn’t reject it:\n",
      "$ kubectl run run-as-5 --image luksa/kubia-run-as-user-5 --restart Never\n",
      "pod \"run-as-5\" created\n",
      "Unlike before, the API server accepted the pod and the Kubelet has run its container.\n",
      "Let’s see what user ID the container is running as:\n",
      "$ kubectl exec run-as-5 -- id\n",
      "uid=2(bin) gid=2(bin) groups=2(bin)\n",
      "As you can see, the container is running as user ID 2, which is the ID you specified in\n",
      "the PodSecurityPolicy. The PodSecurityPolicy can be used to override the user ID\n",
      "hardcoded into a container image.\n",
      "Listing 13.17\n",
      "Dockerfile with a USER directive: kubia-run-as-user-5/Dockerfile\n",
      "Containers run from \n",
      "this image will run \n",
      "as user ID 5.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 426, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "394\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "USING THE MUSTRUNASNONROOT RULE IN THE RUNASUSER FIELD\n",
      "For the runAsUser field an additional rule can be used: MustRunAsNonRoot. As the\n",
      "name suggests, it prevents users from deploying containers that run as root. Either the\n",
      "container spec must specify a runAsUser field, which can’t be zero (zero is the root\n",
      "user’s ID), or the container image itself must run as a non-zero user ID. We explained\n",
      "why this is a good thing earlier.\n",
      "13.3.3 Configuring allowed, default, and disallowed capabilities\n",
      "As you learned, containers can run in privileged mode or not, and you can define a\n",
      "more fine-grained permission configuration by adding or dropping Linux kernel\n",
      "capabilities in each container. Three fields influence which capabilities containers can\n",
      "or cannot use:\n",
      "\n",
      "allowedCapabilities\n",
      "\n",
      "defaultAddCapabilities\n",
      "\n",
      "requiredDropCapabilities\n",
      "We’ll look at an example first, and then discuss what each of the three fields does. The\n",
      "following listing shows a snippet of a PodSecurityPolicy resource defining three fields\n",
      "related to capabilities.\n",
      "apiVersion: extensions/v1beta1 \n",
      "kind: PodSecurityPolicy\n",
      "spec:\n",
      "  allowedCapabilities:          \n",
      "  - SYS_TIME                    \n",
      "  defaultAddCapabilities:         \n",
      "  - CHOWN                         \n",
      "  requiredDropCapabilities:     \n",
      "  - SYS_ADMIN                   \n",
      "  - SYS_MODULE                  \n",
      "  ...\n",
      "NOTE\n",
      "The SYS_ADMIN capability allows a range of administrative operations,\n",
      "and the SYS_MODULE capability allows loading and unloading of Linux kernel\n",
      "modules.\n",
      "SPECIFYING WHICH CAPABILITIES CAN BE ADDED TO A CONTAINER\n",
      "The allowedCapabilities field is used to specify which capabilities pod authors can\n",
      "add in the securityContext.capabilities field in the container spec. In one of the\n",
      "previous examples, you added the SYS_TIME capability to your container. If the Pod-\n",
      "SecurityPolicy admission control plugin had been enabled, you wouldn’t have been\n",
      "able to add that capability, unless it was specified in the PodSecurityPolicy as shown\n",
      "in listing 13.18.\n",
      "Listing 13.18\n",
      "Specifying capabilities in a PodSecurityPolicy: psp-capabilities.yaml\n",
      "Allow containers to \n",
      "add the SYS_TIME \n",
      "capability.\n",
      "Automatically add the CHOWN \n",
      "capability to every container.\n",
      "Require containers to \n",
      "drop the SYS_ADMIN and \n",
      "SYS_MODULE capabilities.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 427, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "395\n",
      "Restricting the use of security-related features in pods\n",
      "ADDING CAPABILITIES TO ALL CONTAINERS\n",
      "All capabilities listed under the defaultAddCapabilities field will be added to\n",
      "every deployed pod’s containers. If a user doesn’t want certain containers to have\n",
      "those capabilities, they need to explicitly drop them in the specs of those containers.\n",
      " The example in listing 13.18 enables the automatic addition of the CAP_CHOWN capa-\n",
      "bility to every container, thus allowing processes running in the container to change the\n",
      "ownership of files in the container (with the chown command, for example).\n",
      "DROPPING CAPABILITIES FROM A CONTAINER\n",
      "The final field in this example is requiredDropCapabilities. I must admit, this was a\n",
      "somewhat strange name for me at first, but it’s not that complicated. The capabilities\n",
      "listed in this field are dropped automatically from every container (the PodSecurity-\n",
      "Policy Admission Control plugin will add them to every container’s security-\n",
      "Context.capabilities.drop field). \n",
      " If a user tries to create a pod where they explicitly add one of the capabilities listed\n",
      "in the policy’s requiredDropCapabilities field, the pod is rejected:\n",
      "$ kubectl create -f pod-add-sysadmin-capability.yaml\n",
      "Error from server (Forbidden): error when creating \"pod-add-sysadmin-\n",
      "capability.yaml\": pods \"pod-add-sysadmin-capability\" is forbidden: unable \n",
      "to validate against any pod security policy: [capabilities.add: Invalid \n",
      "value: \"SYS_ADMIN\": capability may not be added]\n",
      "13.3.4 Constraining the types of volumes pods can use\n",
      "The last thing a PodSecurityPolicy resource can do is define which volume types users\n",
      "can add to their pods. At the minimum, a PodSecurityPolicy should allow using at\n",
      "least the emptyDir, configMap, secret, downwardAPI, and the persistentVolume-\n",
      "Claim volumes. The pertinent part of such a PodSecurityPolicy resource is shown in\n",
      "the following listing.\n",
      "kind: PodSecurityPolicy\n",
      "spec:\n",
      "  volumes:\n",
      "  - emptyDir\n",
      "  - configMap\n",
      "  - secret\n",
      "  - downwardAPI\n",
      "  - persistentVolumeClaim\n",
      "If multiple PodSecurityPolicy resources are in place, pods can use any volume type\n",
      "defined in any of the policies (the union of all volumes lists is used).\n",
      "Listing 13.19\n",
      "A PSP snippet allowing the use of only certain volume types: \n",
      "psp-volumes.yaml\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 428, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "396\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "13.3.5 Assigning different PodSecurityPolicies to different users \n",
      "and groups\n",
      "We mentioned that a PodSecurityPolicy is a cluster-level resource, which means it\n",
      "can’t be stored in and applied to a specific namespace. Does that mean it always\n",
      "applies across all namespaces? No, because that would make them relatively unus-\n",
      "able. After all, system pods must often be allowed to do things that regular pods\n",
      "shouldn’t.\n",
      " Assigning different policies to different users is done through the RBAC mecha-\n",
      "nism described in the previous chapter. The idea is to create as many policies as you\n",
      "need and make them available to individual users or groups by creating ClusterRole\n",
      "resources and pointing them to the individual policies by name. By binding those\n",
      "ClusterRoles to specific users or groups with ClusterRoleBindings, when the Pod-\n",
      "SecurityPolicy Admission Control plugin needs to decide whether to admit a pod defi-\n",
      "nition or not, it will only consider the policies accessible to the user creating the pod. \n",
      " You’ll see how to do this in the next exercise. You’ll start by creating an additional\n",
      "PodSecurityPolicy.\n",
      "CREATING A PODSECURITYPOLICY ALLOWING PRIVILEGED CONTAINERS TO BE DEPLOYED\n",
      "You’ll create a special PodSecurityPolicy that will allow privileged users to create pods\n",
      "with privileged containers. The following listing shows the policy’s definition.\n",
      "apiVersion: extensions/v1beta1\n",
      "kind: PodSecurityPolicy\n",
      "metadata:\n",
      "  name: privileged          \n",
      "spec:\n",
      "  privileged: true        \n",
      "  runAsUser:\n",
      "    rule: RunAsAny\n",
      "  fsGroup:\n",
      "    rule: RunAsAny\n",
      "  supplementalGroups:\n",
      "    rule: RunAsAny\n",
      "  seLinux:\n",
      "    rule: RunAsAny\n",
      "  volumes:\n",
      "  - '*'\n",
      "After you post this policy to the API server, you have two policies in the cluster:\n",
      "$ kubectl get psp\n",
      "NAME         PRIV    CAPS   SELINUX    RUNASUSER   FSGROUP    ...  \n",
      "default      false   []     RunAsAny   RunAsAny    RunAsAny   ...\n",
      "privileged   true    []     RunAsAny   RunAsAny    RunAsAny   ...\n",
      "NOTE\n",
      "The shorthand for PodSecurityPolicy is psp.\n",
      "Listing 13.20\n",
      "A PodSecurityPolicy for privileged users: psp-privileged.yaml\n",
      "The name of this \n",
      "policy is \"privileged.”\n",
      "It allows running \n",
      "privileged containers.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 429, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "397\n",
      "Restricting the use of security-related features in pods\n",
      "As you can see in the PRIV column, the default policy doesn’t allow running privi-\n",
      "leged containers, whereas the privileged policy does. Because you’re currently\n",
      "logged in as a cluster-admin, you can see all the policies. When creating pods, if any\n",
      "policy allows you to deploy a pod with certain features, the API server will accept\n",
      "your pod.\n",
      " Now imagine two additional users are using your cluster: Alice and Bob. You want\n",
      "Alice to only deploy restricted (non-privileged) pods, but you want to allow Bob to\n",
      "also deploy privileged pods. You do this by making sure Alice can only use the default\n",
      "PodSecurityPolicy, while allowing Bob to use both.\n",
      "USING RBAC TO ASSIGN DIFFERENT PODSECURITYPOLICIES TO DIFFERENT USERS\n",
      "In the previous chapter, you used RBAC to grant users access to only certain resource\n",
      "types, but I mentioned that access can be granted to specific resource instances by ref-\n",
      "erencing them by name. That’s what you’ll use to make users use different Pod-\n",
      "SecurityPolicy resources.\n",
      " First, you’ll create two ClusterRoles, each allowing the use of one of the policies.\n",
      "You’ll call the first one psp-default and in it allow the use of the default Pod-\n",
      "SecurityPolicy resource. You can use kubectl create clusterrole to do that:\n",
      "$ kubectl create clusterrole psp-default --verb=use \n",
      "➥  --resource=podsecuritypolicies --resource-name=default\n",
      "clusterrole \"psp-default\" created\n",
      "NOTE\n",
      "You’re using the special verb use instead of get, list, watch, or similar.\n",
      "As you can see, you’re referring to a specific instance of a PodSecurityPolicy resource by\n",
      "using the --resource-name option. Now, create another ClusterRole called psp-\n",
      "privileged, pointing to the privileged policy:\n",
      "$ kubectl create clusterrole psp-privileged --verb=use\n",
      "➥  --resource=podsecuritypolicies --resource-name=privileged\n",
      "clusterrole \"psp-privileged\" created\n",
      "Now, you need to bind these two policies to users. As you may remember from the pre-\n",
      "vious chapter, if you’re binding a ClusterRole that grants access to cluster-level\n",
      "resources (which is what PodSecurityPolicy resources are), you need to use a Cluster-\n",
      "RoleBinding instead of a (namespaced) RoleBinding. \n",
      " You’re going to bind the psp-default ClusterRole to all authenticated users, not\n",
      "only to Alice. This is necessary because otherwise no one could create any pods,\n",
      "because the Admission Control plugin would complain that no policy is in place.\n",
      "Authenticated users all belong to the system:authenticated group, so you’ll bind\n",
      "the ClusterRole to the group:\n",
      "$ kubectl create clusterrolebinding psp-all-users \n",
      "➥ --clusterrole=psp-default --group=system:authenticated\n",
      "clusterrolebinding \"psp-all-users\" created\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 430, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "398\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "You’ll bind the psp-privileged ClusterRole only to Bob:\n",
      "$ kubectl create clusterrolebinding psp-bob \n",
      "➥ --clusterrole=psp-privileged --user=bob\n",
      "clusterrolebinding \"psp-bob\" created\n",
      "As an authenticated user, Alice should now have access to the default PodSecurity-\n",
      "Policy, whereas Bob should have access to both the default and the privileged Pod-\n",
      "SecurityPolicies. Alice shouldn’t be able to create privileged pods, whereas Bob\n",
      "should. Let’s see if that’s true.\n",
      "CREATING ADDITIONAL USERS FOR KUBECTL\n",
      "But how do you authenticate as Alice or Bob instead of whatever you’re authenticated\n",
      "as currently? The book’s appendix A explains how kubectl can be used with multiple\n",
      "clusters, but also with multiple contexts. A context includes the user credentials used\n",
      "for talking to a cluster. Turn to appendix A to find out more. Here we’ll show the bare\n",
      "commands enabling you to use kubectl as Alice or Bob. \n",
      " First, you’ll create two new users in kubectl’s config with the following two\n",
      "commands:\n",
      "$ kubectl config set-credentials alice --username=alice --password=password\n",
      "User \"alice\" set.\n",
      "$ kubectl config set-credentials bob --username=bob --password=password\n",
      "User \"bob\" set.\n",
      "It should be obvious what the commands do. Because you’re setting username and\n",
      "password credentials, kubectl will use basic HTTP authentication for these two users\n",
      "(other authentication methods include tokens, client certificates, and so on).\n",
      "CREATING PODS AS A DIFFERENT USER\n",
      "You can now try creating a privileged pod while authenticating as Alice. You can tell\n",
      "kubectl which user credentials to use by using the --user option:\n",
      "$ kubectl --user alice create -f pod-privileged.yaml\n",
      "Error from server (Forbidden): error when creating \"pod-privileged.yaml\": \n",
      "pods \"pod-privileged\" is forbidden: unable to validate against any pod \n",
      "security policy: [spec.containers[0].securityContext.privileged: Invalid \n",
      "value: true: Privileged containers are not allowed]\n",
      "As expected, the API server doesn’t allow Alice to create privileged pods. Now, let’s see\n",
      "if it allows Bob to do that:\n",
      "$ kubectl --user bob create -f pod-privileged.yaml\n",
      "pod \"pod-privileged\" created\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 431, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "399\n",
      "Isolating the pod network\n",
      "And there you go. You’ve successfully used RBAC to make the Admission Control\n",
      "plugin use different PodSecurityPolicy resources for different users.\n",
      "13.4\n",
      "Isolating the pod network\n",
      "Up to now in this chapter, we’ve explored many security-related configuration options\n",
      "that apply to individual pods and their containers. In the remainder of this chapter,\n",
      "we’ll look at how the network between pods can be secured by limiting which pods can\n",
      "talk to which pods.\n",
      " Whether this is configurable or not depends on which container networking\n",
      "plugin is used in the cluster. If the networking plugin supports it, you can configure\n",
      "network isolation by creating NetworkPolicy resources. \n",
      " A NetworkPolicy applies to pods that match its label selector and specifies either\n",
      "which sources can access the matched pods or which destinations can be accessed\n",
      "from the matched pods. This is configured through ingress and egress rules, respec-\n",
      "tively. Both types of rules can match only the pods that match a pod selector, all\n",
      "pods in a namespace whose labels match a namespace selector, or a network IP\n",
      "block specified using Classless Inter-Domain Routing (CIDR) notation (for example,\n",
      "192.168.1.0/24). \n",
      " We’ll look at both ingress and egress rules and all three matching options.\n",
      "NOTE\n",
      "Ingress rules in a NetworkPolicy have nothing to do with the Ingress\n",
      "resource discussed in chapter 5.\n",
      "13.4.1 Enabling network isolation in a namespace\n",
      "By default, pods in a given namespace can be accessed by anyone. First, you’ll need\n",
      "to change that. You’ll create a default-deny NetworkPolicy, which will prevent all\n",
      "clients from connecting to any pod in your namespace. The NetworkPolicy defini-\n",
      "tion is shown in the following listing.\n",
      "apiVersion: networking.k8s.io/v1\n",
      "kind: NetworkPolicy\n",
      "metadata:\n",
      "  name: default-deny\n",
      "spec:\n",
      "  podSelector:        \n",
      "When you create this NetworkPolicy in a certain namespace, no one can connect to\n",
      "any pod in that namespace. \n",
      " \n",
      " \n",
      " \n",
      "Listing 13.21\n",
      "A default-deny NetworkPolicy: network-policy-default-deny.yaml\n",
      "Empty pod selector \n",
      "matches all pods in the \n",
      "same namespace\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 432, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "400\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "NOTE\n",
      "The CNI plugin or other type of networking solution used in the clus-\n",
      "ter must support NetworkPolicy, or else there will be no effect on inter-pod\n",
      "connectivity.\n",
      "13.4.2 Allowing only some pods in the namespace to connect to \n",
      "a server pod\n",
      "To let clients connect to the pods in the namespace, you must now explicitly say who\n",
      "can connect to the pods. By who I mean which pods. Let’s explore how to do this\n",
      "through an example. \n",
      " Imagine having a PostgreSQL database pod running in namespace foo and a web-\n",
      "server pod that uses the database. Other pods are also in the namespace, and you\n",
      "don’t want to allow them to connect to the database. To secure the network, you need\n",
      "to create the NetworkPolicy resource shown in the following listing in the same name-\n",
      "space as the database pod.\n",
      "apiVersion: networking.k8s.io/v1\n",
      "kind: NetworkPolicy\n",
      "metadata:\n",
      "  name: postgres-netpolicy\n",
      "spec:\n",
      "  podSelector:                     \n",
      "    matchLabels:                   \n",
      "      app: database                \n",
      "  ingress:                           \n",
      "  - from:                            \n",
      "    - podSelector:                   \n",
      "        matchLabels:                 \n",
      "          app: webserver             \n",
      "    ports:                     \n",
      "    - port: 5432               \n",
      "The example NetworkPolicy allows pods with the app=webserver label to connect to\n",
      "pods with the app=database label, and only on port 5432. Other pods can’t connect to\n",
      "the database pods, and no one (not even the webserver pods) can connect to anything\n",
      "other than port 5432 of the database pods. This is shown in figure 13.4.\n",
      " Client pods usually connect to server pods through a Service instead of directly to\n",
      "the pod, but that doesn’t change anything. The NetworkPolicy is enforced when con-\n",
      "necting through a Service, as well.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Listing 13.22\n",
      "A NetworkPolicy for the Postgres pod: network-policy-postgres.yaml\n",
      "This policy secures \n",
      "access to pods with \n",
      "app=database label.\n",
      "It allows incoming connections \n",
      "only from pods with the \n",
      "app=webserver label.\n",
      "Connections to this \n",
      "port are allowed.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 433, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "401\n",
      "Isolating the pod network\n",
      "13.4.3 Isolating the network between Kubernetes namespaces\n",
      "Now let’s look at another example, where multiple tenants are using the same Kuber-\n",
      "netes cluster. Each tenant can use multiple namespaces, and each namespace has a\n",
      "label specifying the tenant it belongs to. For example, one of those tenants is Man-\n",
      "ning. All their namespaces have been labeled with tenant: manning. In one of their\n",
      "namespaces, they run a Shopping Cart microservice that needs to be available to all\n",
      "pods running in any of their namespaces. Obviously, they don’t want any other tenants\n",
      "to access their microservice.\n",
      " To secure their microservice, they create the NetworkPolicy resource shown in the\n",
      "following listing.\n",
      "apiVersion: networking.k8s.io/v1\n",
      "kind: NetworkPolicy\n",
      "metadata:\n",
      "  name: shoppingcart-netpolicy\n",
      "spec:\n",
      "  podSelector:                       \n",
      "    matchLabels:                     \n",
      "      app: shopping-cart             \n",
      "  ingress:\n",
      "  - from:\n",
      "    - namespaceSelector:            \n",
      "        matchLabels:                \n",
      "          tenant: manning           \n",
      "    ports:\n",
      "    - port: 80\n",
      "Listing 13.23\n",
      "NetworkPolicy for the shopping cart pod(s): network-policy-cart.yaml\n",
      "app: database\n",
      "Pod:\n",
      "database\n",
      "Port\n",
      "5432\n",
      "Port\n",
      "9876\n",
      "app: webserver\n",
      "Pod:\n",
      "webserver\n",
      "Pod selector:\n",
      "app=webserver\n",
      "Pod selector:\n",
      "app=database\n",
      "app: webserver\n",
      "Pod:\n",
      "webserver\n",
      "Other pods\n",
      "NetworkPolicy: postgres-netpolicy\n",
      "Figure 13.4\n",
      "A NetworkPolicy allowing only some pods to access other pods and only on a specific \n",
      "port\n",
      "This policy applies to pods \n",
      "labeled as microservice= \n",
      "shopping-cart.\n",
      "Only pods running in namespaces \n",
      "labeled as tenant=manning are \n",
      "allowed to access the microservice.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 434, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "402\n",
      "CHAPTER 13\n",
      "Securing cluster nodes and the network\n",
      "This NetworkPolicy ensures only pods running in namespaces labeled as tenant:\n",
      "manning can access their Shopping Cart microservice, as shown in figure 13.5.\n",
      "If the shopping cart provider also wants to give access to other tenants (perhaps to\n",
      "one of their partner companies), they can either create an additional NetworkPolicy\n",
      "resource or add an additional ingress rule to their existing NetworkPolicy.\n",
      "NOTE\n",
      "In a multi-tenant Kubernetes cluster, tenants usually can’t add labels\n",
      "(or annotations) to their namespaces themselves. If they could, they’d be able\n",
      "to circumvent the namespaceSelector-based ingress rules.\n",
      "13.4.4 Isolating using CIDR notation\n",
      "Instead of specifying a pod- or namespace selector to define who can access the pods\n",
      "targeted in the NetworkPolicy, you can also specify an IP block in CIDR notation. For\n",
      "example, to allow the shopping-cart pods from the previous section to only be acces-\n",
      "sible from IPs in the 192.168.1.1 to .255 range, you’d specify the ingress rule in the\n",
      "next listing.\n",
      "  ingress:\n",
      "  - from:\n",
      "    - ipBlock:                    \n",
      "        cidr: 192.168.1.0/24      \n",
      "Listing 13.24\n",
      "Specifying an IP block in an ingress rule: network-policy-cidr.yaml\n",
      "app: shopping-cart\n",
      "Pod:\n",
      "shopping-cart\n",
      "Port\n",
      "80\n",
      "Namespace selector:\n",
      "tenant=manning\n",
      "Pod selector:\n",
      "app=shopping-cart\n",
      "Other pods\n",
      "Pods\n",
      "NetworkPolicy:\n",
      "shoppingcart-netpolicy\n",
      "Namespace: manningA\n",
      "Namespace: ecommerce-ltd\n",
      "Other namespaces\n",
      "tenant: manning\n",
      "Pods\n",
      "Namespace: manningB\n",
      "tenant: manning\n",
      "Figure 13.5\n",
      "A NetworkPolicy only allowing pods in namespaces matching a namespaceSelector to access a \n",
      "specific pod.\n",
      "This ingress rule only allows traffic from \n",
      "clients in the 192.168.1.0/24 IP block. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 435, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "403\n",
      "Summary\n",
      "13.4.5 Limiting the outbound traffic of a set of pods\n",
      "In all previous examples, you’ve been limiting the inbound traffic to the pods that\n",
      "match the NetworkPolicy’s pod selector using ingress rules, but you can also limit\n",
      "their outbound traffic through egress rules. An example is shown in the next listing.\n",
      "spec:\n",
      "  podSelector:               \n",
      "    matchLabels:             \n",
      "      app: webserver         \n",
      "  egress:               \n",
      "  - to:                       \n",
      "    - podSelector:            \n",
      "        matchLabels:          \n",
      "          app: database       \n",
      "The NetworkPolicy in the previous listing allows pods that have the app=webserver\n",
      "label to only access pods that have the app=database label and nothing else (neither\n",
      "other pods, nor any other IP, regardless of whether it’s internal or external to the\n",
      "cluster).\n",
      "13.5\n",
      "Summary\n",
      "In this chapter, you learned about securing cluster nodes from pods and pods from\n",
      "other pods. You learned that\n",
      "Pods can use the node’s Linux namespaces instead of using their own.\n",
      "Containers can be configured to run as a different user and/or group than the\n",
      "one defined in the container image.\n",
      "Containers can also run in privileged mode, allowing them to access the node’s\n",
      "devices that are otherwise not exposed to pods.\n",
      "Containers can be run as read-only, preventing processes from writing to the\n",
      "container’s filesystem (and only allowing them to write to mounted volumes).\n",
      "Cluster-level PodSecurityPolicy resources can be created to prevent users from\n",
      "creating pods that could compromise a node.\n",
      "PodSecurityPolicy resources can be associated with specific users using RBAC’s\n",
      "ClusterRoles and ClusterRoleBindings.\n",
      "NetworkPolicy resources are used to limit a pod’s inbound and/or outbound\n",
      "traffic.\n",
      "In the next chapter, you’ll learn how computational resources available to pods can be\n",
      "constrained and how a pod’s quality of service is configured.\n",
      "Listing 13.25\n",
      "Using egress rules in a NetworkPolicy: network-policy-egress.yaml\n",
      "This policy applies to pods with \n",
      "the app=webserver label.\n",
      "It limits\n",
      "the pods’\n",
      "outbound\n",
      "traffic.\n",
      "Webserver pods may only \n",
      "connect to pods with the \n",
      "app=database label.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 436, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "404\n",
      "Managing pods’\n",
      " computational resources\n",
      "Up to now you’ve created pods without caring about how much CPU and memory\n",
      "they’re allowed to consume. But as you’ll see in this chapter, setting both how\n",
      "much a pod is expected to consume and the maximum amount it’s allowed to con-\n",
      "sume is a vital part of any pod definition. Setting these two sets of parameters\n",
      "makes sure that a pod takes only its fair share of the resources provided by the\n",
      "Kubernetes cluster and also affects how pods are scheduled across the cluster.\n",
      "This chapter covers\n",
      "Requesting CPU, memory, and other \n",
      "computational resources for containers\n",
      "Setting a hard limit for CPU and memory\n",
      "Understanding Quality of Service guarantees for \n",
      "pods\n",
      "Setting default, min, and max resources for pods \n",
      "in a namespace\n",
      "Limiting the total amount of resources available \n",
      "in a namespace\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 437, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "405\n",
      "Requesting resources for a pod’s containers\n",
      "14.1\n",
      "Requesting resources for a pod’s containers\n",
      "When creating a pod, you can specify the amount of CPU and memory that a con-\n",
      "tainer needs (these are called requests) and a hard limit on what it may consume\n",
      "(known as limits). They’re specified for each container individually, not for the pod as\n",
      "a whole. The pod’s resource requests and limits are the sum of the requests and lim-\n",
      "its of all its containers. \n",
      "14.1.1 Creating pods with resource requests\n",
      "Let’s look at an example pod manifest, which has the CPU and memory requests spec-\n",
      "ified for its single container, as shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: requests-pod\n",
      "spec:\n",
      "  containers:\n",
      "  - image: busybox\n",
      "    command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"]\n",
      "    name: main              \n",
      "    resources:              \n",
      "      requests:             \n",
      "        cpu: 200m          \n",
      "        memory: 10Mi    \n",
      "In the pod manifest, your single container requires one-fifth of a CPU core (200 mil-\n",
      "licores) to run properly. Five such pods/containers can run sufficiently fast on a single\n",
      "CPU core. \n",
      " When you don’t specify a request for CPU, you’re saying you don’t care how much\n",
      "CPU time the process running in your container is allotted. In the worst case, it may\n",
      "not get any CPU time at all (this happens when a heavy demand by other processes\n",
      "exists on the CPU). Although this may be fine for low-priority batch jobs, which aren’t\n",
      "time-critical, it obviously isn’t appropriate for containers handling user requests.\n",
      " In the pod spec, you’re also requesting 10 mebibytes of memory for the container.\n",
      "By doing that, you’re saying that you expect the processes running inside the con-\n",
      "tainer to use at most 10 mebibytes of RAM. They might use less, but you’re not expect-\n",
      "ing them to use more than that in normal circumstances. Later in this chapter you’ll\n",
      "see what happens if they do.\n",
      " Now you’ll run the pod. When the pod starts, you can take a quick look at the pro-\n",
      "cess’ CPU consumption by running the top command inside the container, as shown\n",
      "in the following listing.\n",
      "Listing 14.1\n",
      "A pod with resource requests: requests-pod.yaml\n",
      "You’re specifying resource \n",
      "requests for the main container.\n",
      "The container requests 200 \n",
      "millicores (that is, 1/5 of a \n",
      "single CPU core’s time).\n",
      "The container also\n",
      "requests 10 mebibytes\n",
      "of memory.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 438, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "406\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "$ kubectl exec -it requests-pod top\n",
      "Mem: 1288116K used, 760368K free, 9196K shrd, 25748K buff, 814840K cached\n",
      "CPU:  9.1% usr 42.1% sys  0.0% nic 48.4% idle  0.0% io  0.0% irq  0.2% sirq\n",
      "Load average: 0.79 0.52 0.29 2/481 10\n",
      "  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\n",
      "    1     0 root     R     1192  0.0   1 50.2 dd if /dev/zero of /dev/null\n",
      "    7     0 root     R     1200  0.0   0  0.0 top\n",
      "The dd command you’re running in the container consumes as much CPU as it can,\n",
      "but it only runs a single thread so it can only use a single core. The Minikube VM,\n",
      "which is where this example is running, has two CPU cores allotted to it. That’s why\n",
      "the process is shown consuming 50% of the whole CPU. \n",
      " Fifty percent of two cores is obviously one whole core, which means the container\n",
      "is using more than the 200 millicores you requested in the pod specification. This is\n",
      "expected, because requests don’t limit the amount of CPU a container can use. You’d\n",
      "need to specify a CPU limit to do that. You’ll try that later, but first, let’s see how spec-\n",
      "ifying resource requests in a pod affects the scheduling of the pod.\n",
      "14.1.2 Understanding how resource requests affect scheduling\n",
      "By specifying resource requests, you’re specifying the minimum amount of resources\n",
      "your pod needs. This information is what the Scheduler uses when scheduling the pod\n",
      "to a node. Each node has a certain amount of CPU and memory it can allocate to\n",
      "pods. When scheduling a pod, the Scheduler will only consider nodes with enough\n",
      "unallocated resources to meet the pod’s resource requirements. If the amount of\n",
      "unallocated CPU or memory is less than what the pod requests, Kubernetes will not\n",
      "schedule the pod to that node, because the node can’t provide the minimum amount\n",
      "required by the pod.\n",
      "UNDERSTANDING HOW THE SCHEDULER DETERMINES IF A POD CAN FIT ON A NODE\n",
      "What’s important and somewhat surprising here is that the Scheduler doesn’t look at\n",
      "how much of each individual resource is being used at the exact time of scheduling\n",
      "but at the sum of resources requested by the existing pods deployed on the node.\n",
      "Even though existing pods may be using less than what they’ve requested, scheduling\n",
      "another pod based on actual resource consumption would break the guarantee given\n",
      "to the already deployed pods.\n",
      " This is visualized in figure 14.1. Three pods are deployed on the node. Together,\n",
      "they’ve requested 80% of the node’s CPU and 60% of the node’s memory. Pod D,\n",
      "shown at the bottom right of the figure, cannot be scheduled onto the node because it\n",
      "requests 25% of the CPU, which is more than the 20% of unallocated CPU. The fact\n",
      "that the three pods are currently using only 70% of the CPU makes no difference.\n",
      "Listing 14.2\n",
      "Examining CPU and memory usage from within a container\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 439, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "407\n",
      "Requesting resources for a pod’s containers\n",
      "UNDERSTANDING HOW THE SCHEDULER USES PODS’ REQUESTS WHEN SELECTING THE BEST NODE \n",
      "FOR A POD\n",
      "You may remember from chapter 11 that the Scheduler first filters the list of nodes to\n",
      "exclude those that the pod can’t fit on and then prioritizes the remaining nodes per the\n",
      "configured prioritization functions. Among others, two prioritization functions rank\n",
      "nodes based on the amount of resources requested: LeastRequestedPriority and\n",
      "MostRequestedPriority. The first one prefers nodes with fewer requested resources\n",
      "(with a greater amount of unallocated resources), whereas the second one is the exact\n",
      "opposite—it prefers nodes that have the most requested resources (a smaller amount of\n",
      "unallocated CPU and memory). But, as we’ve discussed, they both consider the amount\n",
      "of requested resources, not the amount of resources actually consumed.\n",
      " The Scheduler is configured to use only one of those functions. You may wonder\n",
      "why anyone would want to use the MostRequestedPriority function. After all, if you\n",
      "have a set of nodes, you usually want to spread CPU load evenly across them. However,\n",
      "that’s not the case when running on cloud infrastructure, where you can add and\n",
      "remove nodes whenever necessary. By configuring the Scheduler to use the Most-\n",
      "RequestedPriority function, you guarantee that Kubernetes will use the smallest pos-\n",
      "sible number of nodes while still providing each pod with the amount of CPU/memory\n",
      "it requests. By keeping pods tightly packed, certain nodes are left vacant and can be\n",
      "removed. Because you’re paying for individual nodes, this saves you money.\n",
      "INSPECTING A NODE’S CAPACITY\n",
      "Let’s see the Scheduler in action. You’ll deploy another pod with four times the\n",
      "amount of requested resources as before. But before you do that, let’s see your node’s\n",
      "capacity. Because the Scheduler needs to know how much CPU and memory each\n",
      "node has, the Kubelet reports this data to the API server, making it available through\n",
      "Pod C\n",
      "Node\n",
      "Pod A\n",
      "Unallocated\n",
      "CPU requests\n",
      "Pod B\n",
      "Pod A\n",
      "Currently unused\n",
      "CPU usage\n",
      "Pod B\n",
      "Pod C\n",
      "0%\n",
      "100%\n",
      "Pod A\n",
      "Memory requests\n",
      "Pod B\n",
      "Pod C\n",
      "Pod A\n",
      "Memory usage\n",
      "Pod B\n",
      "Pod C\n",
      "CPU requests\n",
      "Memory requests\n",
      "Unallocated\n",
      "Currently unused\n",
      "Pod D\n",
      "Pod D cannot be scheduled; its CPU\n",
      "requests exceed unallocated CPU\n",
      "Figure 14.1\n",
      "The Scheduler only cares about requests, not actual usage.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 440, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "408\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "the Node resource. You can see it by using the kubectl describe command as in the\n",
      "following listing.\n",
      "$ kubectl describe nodes\n",
      "Name:       minikube\n",
      "...\n",
      "Capacity:                       \n",
      "  cpu:           2               \n",
      "  memory:        2048484Ki       \n",
      "  pods:          110             \n",
      "Allocatable:                       \n",
      "  cpu:           2                  \n",
      "  memory:        1946084Ki          \n",
      "  pods:          110                \n",
      "...\n",
      "The output shows two sets of amounts related to the available resources on the node:\n",
      "the node’s capacity and allocatable resources. The capacity represents the total resources\n",
      "of a node, which may not all be available to pods. Certain resources may be reserved\n",
      "for Kubernetes and/or system components. The Scheduler bases its decisions only on\n",
      "the allocatable resource amounts.\n",
      " In the previous example, the node called minikube runs in a VM with two cores\n",
      "and has no CPU reserved, making the whole CPU allocatable to pods. Therefore,\n",
      "the Scheduler should have no problem scheduling another pod requesting 800\n",
      "millicores. \n",
      " Run the pod now. You can use the YAML file in the code archive, or run the pod\n",
      "with the kubectl run command like this:\n",
      "$ kubectl run requests-pod-2 --image=busybox --restart Never\n",
      "➥ --requests='cpu=800m,memory=20Mi' -- dd if=/dev/zero of=/dev/null\n",
      "pod \"requests-pod-2\" created\n",
      "Let’s see if it was scheduled:\n",
      "$ kubectl get po requests-pod-2\n",
      "NAME             READY     STATUS    RESTARTS   AGE\n",
      "requests-pod-2   1/1       Running   0          3m\n",
      "Okay, the pod has been scheduled and is running. \n",
      "CREATING A POD THAT DOESN’T FIT ON ANY NODE\n",
      "You now have two pods deployed, which together have requested a total of 1,000 mil-\n",
      "licores or exactly 1 core. You should therefore have another 1,000 millicores available\n",
      "for additional pods, right? You can deploy another pod with a resource request of\n",
      "1,000 millicores. Use a similar command as before:\n",
      "$ kubectl run requests-pod-3 --image=busybox --restart Never\n",
      "➥ --requests='cpu=1,memory=20Mi' -- dd if=/dev/zero of=/dev/null\n",
      "pod \"requests-pod-2\" created\n",
      "Listing 14.3\n",
      "A node’s capacity and allocatable resources\n",
      "The overall capacity \n",
      "of the node\n",
      "The resources \n",
      "allocatable to pods\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 441, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "409\n",
      "Requesting resources for a pod’s containers\n",
      "NOTE\n",
      "This time you’re specifying the CPU request in whole cores (cpu=1)\n",
      "instead of millicores (cpu=1000m).\n",
      "So far, so good. The pod has been accepted by the API server (you’ll remember from\n",
      "the previous chapter that the API server can reject pods if they’re invalid in any way).\n",
      "Now, check if the pod is running:\n",
      "$ kubectl get po requests-pod-3\n",
      "NAME             READY     STATUS    RESTARTS   AGE\n",
      "requests-pod-3   0/1       Pending   0          4m\n",
      "Even if you wait a while, the pod is still stuck at Pending. You can see more informa-\n",
      "tion on why that’s the case by using the kubectl describe command, as shown in\n",
      "the following listing.\n",
      "$ kubectl describe po requests-pod-3\n",
      "Name:       requests-pod-3\n",
      "Namespace:  default\n",
      "Node:       /                    \n",
      "...\n",
      "Conditions:\n",
      "  Type           Status\n",
      "  PodScheduled   False           \n",
      "...\n",
      "Events:\n",
      "... Warning  FailedScheduling    No nodes are available      \n",
      "                                 that match all of the       \n",
      "                                 following predicates::      \n",
      "                                 Insufficient cpu (1).       \n",
      "The output shows that the pod hasn’t been scheduled because it can’t fit on any node\n",
      "due to insufficient CPU on your single node. But why is that? The sum of the CPU\n",
      "requests of all three pods equals 2,000 millicores or exactly two cores, which is exactly\n",
      "what your node can provide. What’s wrong?\n",
      "DETERMINING WHY A POD ISN’T BEING SCHEDULED\n",
      "You can figure out why the pod isn’t being scheduled by inspecting the node resource.\n",
      "Use the kubectl describe node command again and examine the output more\n",
      "closely in the following listing.\n",
      "$ kubectl describe node\n",
      "Name:                   minikube\n",
      "...\n",
      "Non-terminated Pods:    (7 in total)\n",
      "  Namespace    Name            CPU Requ.   CPU Lim.  Mem Req.    Mem Lim.\n",
      "  ---------    ----            ----------  --------  ---------   --------\n",
      "  default      requests-pod    200m (10%)  0 (0%)    10Mi (0%)   0 (0%)\n",
      "Listing 14.4\n",
      "Examining why a pod is stuck at Pending with kubectl describe pod\n",
      "Listing 14.5\n",
      "Inspecting allocated resources on a node with kubectl describe node\n",
      "No node is \n",
      "associated \n",
      "with the pod.\n",
      "The pod hasn’t \n",
      "been scheduled.\n",
      "Scheduling has \n",
      "failed because of \n",
      "insufficient CPU.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 442, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "410\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "  default      requests-pod-2  800m (40%)  0 (0%)    20Mi (1%)   0 (0%)\n",
      "  kube-system  dflt-http-b...  10m (0%)    10m (0%)  20Mi (1%)   20Mi (1%)\n",
      "  kube-system  kube-addon-...  5m (0%)     0 (0%)    50Mi (2%)   0 (0%)\n",
      "  kube-system  kube-dns-26...  260m (13%)  0 (0%)    110Mi (5%)  170Mi (8%)\n",
      "  kube-system  kubernetes-...  0 (0%)      0 (0%)    0 (0%)      0 (0%)\n",
      "  kube-system  nginx-ingre...  0 (0%)      0 (0%)    0 (0%)      0 (0%)\n",
      "Allocated resources:\n",
      "  (Total limits may be over 100 percent, i.e., overcommitted.)\n",
      "  CPU Requests  CPU Limits      Memory Requests Memory Limits\n",
      "  ------------  ----------      --------------- -------------\n",
      "  1275m (63%)   10m (0%)        210Mi (11%)     190Mi (9%)\n",
      "If you look at the bottom left of the listing, you’ll see a total of 1,275 millicores have\n",
      "been requested by the running pods, which is 275 millicores more than what you\n",
      "requested for the first two pods you deployed. Something is eating up additional\n",
      "CPU resources. \n",
      " You can find the culprit in the list of pods in the previous listing. Three pods in the\n",
      "kube-system namespace have explicitly requested CPU resources. Those pods plus\n",
      "your two pods leave only 725 millicores available for additional pods. Because your\n",
      "third pod requested 1,000 millicores, the Scheduler won’t schedule it to this node, as\n",
      "that would make the node overcommitted. \n",
      "FREEING RESOURCES TO GET THE POD SCHEDULED\n",
      "The pod will only be scheduled when an adequate amount of CPU is freed (when one\n",
      "of the first two pods is deleted, for example). If you delete your second pod, the\n",
      "Scheduler will be notified of the deletion (through the watch mechanism described in\n",
      "chapter 11) and will schedule your third pod as soon as the second pod terminates.\n",
      "This is shown in the following listing.\n",
      "$ kubectl delete po requests-pod-2\n",
      "pod \"requests-pod-2\" deleted \n",
      "$ kubectl get po\n",
      "NAME             READY     STATUS        RESTARTS   AGE\n",
      "requests-pod     1/1       Running       0          2h\n",
      "requests-pod-2   1/1       Terminating   0          1h\n",
      "requests-pod-3   0/1       Pending       0          1h\n",
      "$ kubectl get po\n",
      "NAME             READY     STATUS    RESTARTS   AGE\n",
      "requests-pod     1/1       Running   0          2h\n",
      "requests-pod-3   1/1       Running   0          1h\n",
      "In all these examples, you’ve specified a request for memory, but it hasn’t played any\n",
      "role in the scheduling because your node has more than enough allocatable memory to\n",
      "accommodate all your pods’ requests. Both CPU and memory requests are treated the\n",
      "same way by the Scheduler, but in contrast to memory requests, a pod’s CPU requests\n",
      "also play a role elsewhere—while the pod is running. You’ll learn about this next.\n",
      "Listing 14.6\n",
      "Pod is scheduled after deleting another pod\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 443, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "411\n",
      "Requesting resources for a pod’s containers\n",
      "14.1.3 Understanding how CPU requests affect CPU time sharing\n",
      "You now have two pods running in your cluster (you can disregard the system pods\n",
      "right now, because they’re mostly idle). One has requested 200 millicores and the\n",
      "other one five times as much. At the beginning of the chapter, we said Kubernetes dis-\n",
      "tinguishes between resource requests and limits. You haven’t defined any limits yet, so\n",
      "the two pods are in no way limited when it comes to how much CPU they can each\n",
      "consume. If the process inside each pod consumes as much CPU time as it can, how\n",
      "much CPU time does each pod get? \n",
      " The CPU requests don’t only affect scheduling—they also determine how the\n",
      "remaining (unused) CPU time is distributed between pods. Because your first pod\n",
      "requested 200 millicores of CPU and the other one 1,000 millicores, any unused CPU\n",
      "will be split among the two pods in a 1 to 5 ratio, as shown in figure 14.2. If both pods\n",
      "consume as much CPU as they can, the first pod will get one sixth or 16.7% of the\n",
      "CPU time and the other one the remaining five sixths or 83.3%.\n",
      "But if one container wants to use up as much CPU as it can, while the other one is sit-\n",
      "ting idle at a given moment, the first container will be allowed to use the whole CPU\n",
      "time (minus the small amount of time used by the second container, if any). After all,\n",
      "it makes sense to use all the available CPU if no one else is using it, right? As soon as\n",
      "the second container needs CPU time, it will get it and the first container will be throt-\n",
      "tled back.\n",
      "14.1.4 Defining and requesting custom resources\n",
      "Kubernetes also allows you to add your own custom resources to a node and request\n",
      "them in the pod’s resource requests. Initially these were known as Opaque Integer\n",
      "Resources, but were replaced with Extended Resources in Kubernetes version 1.8.\n",
      "Pod A:\n",
      "200 m\n",
      "CPU\n",
      "requests\n",
      "Pod B: 1000 m\n",
      "800 m available\n",
      "CPU\n",
      "usage\n",
      "2000 m\n",
      "1000 m\n",
      "0 m\n",
      "Pod A and B requests\n",
      "are in 1:5 ratio.\n",
      "Available CPU time is\n",
      "distributed in same ratio.\n",
      "Pod B: 1667 m\n",
      "133 m\n",
      "(1/6)\n",
      "667 m\n",
      "(5/6)\n",
      "Pod A:\n",
      "333 m\n",
      "Figure 14.2\n",
      "Unused CPU time is distributed to containers based on their CPU requests.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 444, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "412\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      " First, you obviously need to make Kubernetes aware of your custom resource by\n",
      "adding it to the Node object’s capacity field. This can be done by performing a\n",
      "PATCH HTTP request. The resource name can be anything, such as example.org/my-\n",
      "resource, as long as it doesn’t start with the kubernetes.io domain. The quantity\n",
      "must be an integer (for example, you can’t set it to 100 millis, because 0.1 isn’t an inte-\n",
      "ger; but you can set it to 1000m or 2000m or, simply, 1 or 2). The value will be copied\n",
      "from the capacity to the allocatable field automatically.\n",
      " Then, when creating pods, you specify the same resource name and the requested\n",
      "quantity under the resources.requests field in the container spec or with --requests\n",
      "when using kubectl run like you did in previous examples. The Scheduler will make\n",
      "sure the pod is only deployed to a node that has the requested amount of the custom\n",
      "resource available. Every deployed pod obviously reduces the number of allocatable\n",
      "units of the resource.\n",
      " An example of a custom resource could be the number of GPU units available on the\n",
      "node. Pods requiring the use of a GPU specify that in their requests. The Scheduler then\n",
      "makes sure the pod is only scheduled to nodes with at least one GPU still unallocated.\n",
      "14.2\n",
      "Limiting resources available to a container\n",
      "Setting resource requests for containers in a pod ensures each container gets the min-\n",
      "imum amount of resources it needs. Now let’s see the other side of the coin—the\n",
      "maximum amount the container will be allowed to consume. \n",
      "14.2.1 Setting a hard limit for the amount of resources a container can use\n",
      "We’ve seen how containers are allowed to use up all the CPU if all the other processes\n",
      "are sitting idle. But you may want to prevent certain containers from using up more\n",
      "than a specific amount of CPU. And you’ll always want to limit the amount of memory\n",
      "a container can consume. \n",
      " CPU is a compressible resource, which means the amount used by a container can\n",
      "be throttled without affecting the process running in the container in an adverse way.\n",
      "Memory is obviously different—it’s incompressible. Once a process is given a chunk of\n",
      "memory, that memory can’t be taken away from it until it’s released by the process\n",
      "itself. That’s why you need to limit the maximum amount of memory a container can\n",
      "be given. \n",
      " Without limiting memory, a container (or a pod) running on a worker node may\n",
      "eat up all the available memory and affect all other pods on the node and any new\n",
      "pods scheduled to the node (remember that new pods are scheduled to the node\n",
      "based on the memory requests and not actual memory usage). A single malfunction-\n",
      "ing or malicious pod can practically make the whole node unusable.\n",
      "CREATING A POD WITH RESOURCE LIMITS\n",
      "To prevent this from happening, Kubernetes allows you to specify resource limits for\n",
      "every container (along with, and virtually in the same way as, resource requests). The\n",
      "following listing shows an example pod manifest with resource limits.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 445, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "413\n",
      "Limiting resources available to a container\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: limited-pod\n",
      "spec:\n",
      "  containers:\n",
      "  - image: busybox\n",
      "    command: [\"dd\", \"if=/dev/zero\", \"of=/dev/null\"]\n",
      "    name: main\n",
      "    resources:            \n",
      "      limits:             \n",
      "        cpu: 1             \n",
      "        memory: 20Mi       \n",
      "This pod’s container has resource limits configured for both CPU and memory. The\n",
      "process or processes running inside the container will not be allowed to consume\n",
      "more than 1 CPU core and 20 mebibytes of memory. \n",
      "NOTE\n",
      "Because you haven’t specified any resource requests, they’ll be set to\n",
      "the same values as the resource limits.\n",
      "OVERCOMMITTING LIMITS\n",
      "Unlike resource requests, resource limits aren’t constrained by the node’s allocatable\n",
      "resource amounts. The sum of all limits of all the pods on a node is allowed to exceed\n",
      "100% of the node’s capacity (figure 14.3). Restated, resource limits can be overcom-\n",
      "mitted. This has an important consequence—when 100% of the node’s resources are\n",
      "used up, certain containers will need to be killed.\n",
      "You’ll see how Kubernetes decides which containers to kill in section 14.3, but individ-\n",
      "ual containers can be killed even if they try to use more than their resource limits\n",
      "specify. You’ll learn more about this next.\n",
      "Listing 14.7\n",
      "A pod with a hard limit on CPU and memory: limited-pod.yaml\n",
      "Specifying resource \n",
      "limits for the container\n",
      "This container will be \n",
      "allowed to use at \n",
      "most 1 CPU core.\n",
      "The container will be\n",
      "allowed to use up to 20\n",
      "mebibytes of memory.\n",
      "Node\n",
      "0%\n",
      "136%\n",
      "100%\n",
      "Pod A\n",
      "Memory requests\n",
      "Pod B\n",
      "Pod C\n",
      "Pod A\n",
      "Memory limits\n",
      "Pod B\n",
      "Unallocated\n",
      "Pod C\n",
      "Figure 14.3\n",
      "The sum of resource limits of all pods on a node can exceed 100% of the node’s \n",
      "capacity.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 446, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "414\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "14.2.2 Exceeding the limits\n",
      "What happens when a process running in a container tries to use a greater amount of\n",
      "resources than it’s allowed to? \n",
      " You’ve already learned that CPU is a compressible resource, and it’s only natural\n",
      "for a process to want to consume all of the CPU time when not waiting for an I/O\n",
      "operation. As you’ve learned, a process’ CPU usage is throttled, so when a CPU\n",
      "limit is set for a container, the process isn’t given more CPU time than the config-\n",
      "ured limit. \n",
      " With memory, it’s different. When a process tries to allocate memory over its\n",
      "limit, the process is killed (it’s said the container is OOMKilled, where OOM stands\n",
      "for Out Of Memory). If the pod’s restart policy is set to Always or OnFailure, the\n",
      "process is restarted immediately, so you may not even notice it getting killed. But if it\n",
      "keeps going over the memory limit and getting killed, Kubernetes will begin restart-\n",
      "ing it with increasing delays between restarts. You’ll see a CrashLoopBackOff status\n",
      "in that case:\n",
      "$ kubectl get po\n",
      "NAME        READY     STATUS             RESTARTS   AGE\n",
      "memoryhog   0/1       CrashLoopBackOff   3          1m\n",
      "The CrashLoopBackOff status doesn’t mean the Kubelet has given up. It means that\n",
      "after each crash, the Kubelet is increasing the time period before restarting the con-\n",
      "tainer. After the first crash, it restarts the container immediately and then, if it crashes\n",
      "again, waits for 10 seconds before restarting it again. On subsequent crashes, this\n",
      "delay is then increased exponentially to 20, 40, 80, and 160 seconds, and finally lim-\n",
      "ited to 300 seconds. Once the interval hits the 300-second limit, the Kubelet keeps\n",
      "restarting the container indefinitely every five minutes until the pod either stops\n",
      "crashing or is deleted. \n",
      " To examine why the container crashed, you can check the pod’s log and/or use\n",
      "the kubectl describe pod command, as shown in the following listing.\n",
      "$ kubectl describe pod\n",
      "Name:       memoryhog\n",
      "...\n",
      "Containers:\n",
      "  main:\n",
      "    ...\n",
      "    State:          Terminated          \n",
      "      Reason:       OOMKilled           \n",
      "      Exit Code:    137\n",
      "      Started:      Tue, 27 Dec 2016 14:55:53 +0100\n",
      "      Finished:     Tue, 27 Dec 2016 14:55:58 +0100\n",
      "    Last State:     Terminated            \n",
      "      Reason:       OOMKilled             \n",
      "      Exit Code:    137\n",
      "Listing 14.8\n",
      "Inspecting why a container terminated with kubectl describe pod\n",
      "The current container was \n",
      "killed because it was out \n",
      "of memory (OOM).\n",
      "The previous container \n",
      "was also killed because \n",
      "it was  OOM\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 447, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "415\n",
      "Limiting resources available to a container\n",
      "      Started:      Tue, 27 Dec 2016 14:55:37 +0100\n",
      "      Finished:     Tue, 27 Dec 2016 14:55:50 +0100\n",
      "    Ready:          False\n",
      "...\n",
      "The OOMKilled status tells you that the container was killed because it was out of mem-\n",
      "ory. In the previous listing, the container went over its memory limit and was killed\n",
      "immediately. \n",
      " It’s important not to set memory limits too low if you don’t want your container to\n",
      "be killed. But containers can get OOMKilled even if they aren’t over their limit. You’ll\n",
      "see why in section 14.3.2, but first, let’s discuss something that catches most users off-\n",
      "guard the first time they start specifying limits for their containers.\n",
      "14.2.3 Understanding how apps in containers see limits\n",
      "If you haven’t deployed the pod from listing 14.7, deploy it now:\n",
      "$ kubectl create -f limited-pod.yaml\n",
      "pod \"limited-pod\" created\n",
      "Now, run the top command in the container, the way you did at the beginning of the\n",
      "chapter. The command’s output is shown in the following listing.\n",
      "$ kubectl exec -it limited-pod top\n",
      "Mem: 1450980K used, 597504K free, 22012K shrd, 65876K buff, 857552K cached\n",
      "CPU: 10.0% usr 40.0% sys  0.0% nic 50.0% idle  0.0% io  0.0% irq  0.0% sirq\n",
      "Load average: 0.17 1.19 2.47 4/503 10\n",
      "  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND\n",
      "    1     0 root     R     1192  0.0   1 49.9 dd if /dev/zero of /dev/null\n",
      "    5     0 root     R     1196  0.0   0  0.0 top\n",
      "First, let me remind you that the pod’s CPU limit is set to 1 core and its memory limit\n",
      "is set to 20 MiB. Now, examine the output of the top command closely. Is there any-\n",
      "thing that strikes you as odd?\n",
      " Look at the amount of used and free memory. Those numbers are nowhere near\n",
      "the 20 MiB you set as the limit for the container. Similarly, you set the CPU limit to\n",
      "one core and it seems like the main process is using only 50% of the available CPU\n",
      "time, even though the dd command, when used like you’re using it, usually uses all the\n",
      "CPU it has available. What’s going on?\n",
      "UNDERSTANDING THAT CONTAINERS ALWAYS SEE THE NODE’S MEMORY, NOT THE CONTAINER’S\n",
      "The top command shows the memory amounts of the whole node the container is\n",
      "running on. Even though you set a limit on how much memory is available to a con-\n",
      "tainer, the container will not be aware of this limit. \n",
      "Listing 14.9\n",
      "Running the top command in a CPU- and memory-limited container\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 448, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "416\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      " This has an unfortunate effect on any application that looks up the amount of\n",
      "memory available on the system and uses that information to decide how much mem-\n",
      "ory it wants to reserve. \n",
      " The problem is visible when running Java apps, especially if you don’t specify the\n",
      "maximum heap size for the Java Virtual Machine with the -Xmx option. In that case,\n",
      "the JVM will set the maximum heap size based on the host’s total memory instead of\n",
      "the memory available to the container. When you run your containerized Java apps in\n",
      "a Kubernetes cluster on your laptop, the problem doesn’t manifest itself, because the\n",
      "difference between the memory limits you set for the pod and the total memory avail-\n",
      "able on your laptop is not that great. \n",
      " But when you deploy your pod onto a production system, where nodes have much\n",
      "more physical memory, the JVM may go over the container’s memory limit you config-\n",
      "ured and will be OOMKilled. \n",
      " And if you think setting the -Xmx option properly solves the issue, you’re wrong,\n",
      "unfortunately. The -Xmx option only constrains the heap size, but does nothing about\n",
      "the JVM’s off-heap memory. Luckily, new versions of Java alleviate that problem by tak-\n",
      "ing the configured container limits into account.\n",
      "UNDERSTANDING THAT CONTAINERS ALSO SEE ALL THE NODE’S CPU CORES\n",
      "Exactly like with memory, containers will also see all the node’s CPUs, regardless of\n",
      "the CPU limits configured for the container. Setting a CPU limit to one core doesn’t\n",
      "magically only expose only one CPU core to the container. All the CPU limit does is\n",
      "constrain the amount of CPU time the container can use. \n",
      " A container with a one-core CPU limit running on a 64-core CPU will get 1/64th\n",
      "of the overall CPU time. And even though its limit is set to one core, the container’s\n",
      "processes will not run on only one core. At different points in time, its code may be\n",
      "executed on different cores.\n",
      " Nothing is wrong with this, right? While that’s generally the case, at least one sce-\n",
      "nario exists where this situation is catastrophic.\n",
      " Certain applications look up the number of CPUs on the system to decide how\n",
      "many worker threads they should run. Again, such an app will run fine on a develop-\n",
      "ment laptop, but when deployed on a node with a much bigger number of cores, it’s\n",
      "going to spin up too many threads, all competing for the (possibly) limited CPU time.\n",
      "Also, each thread requires additional memory, causing the apps memory usage to sky-\n",
      "rocket. \n",
      " You may want to use the Downward API to pass the CPU limit to the container and\n",
      "use it instead of relying on the number of CPUs your app can see on the system. You\n",
      "can also tap into the cgroups system directly to get the configured CPU limit by read-\n",
      "ing the following files:\n",
      "/sys/fs/cgroup/cpu/cpu.cfs_quota_us\n",
      "/sys/fs/cgroup/cpu/cpu.cfs_period_us\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 449, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "417\n",
      "Understanding pod QoS classes\n",
      "14.3\n",
      "Understanding pod QoS classes\n",
      "We’ve already mentioned that resource limits can be overcommitted and that a\n",
      "node can’t necessarily provide all its pods the amount of resources specified in their\n",
      "resource limits. \n",
      " Imagine having two pods, where pod A is using, let’s say, 90% of the node’s mem-\n",
      "ory and then pod B suddenly requires more memory than what it had been using up\n",
      "to that point and the node can’t provide the required amount of memory. Which\n",
      "container should be killed? Should it be pod B, because its request for memory can’t\n",
      "be satisfied, or should pod A be killed to free up memory, so it can be provided to\n",
      "pod B? \n",
      " Obviously, it depends. Kubernetes can’t make a proper decision on its own. You\n",
      "need a way to specify which pods have priority in such cases. Kubernetes does this by\n",
      "categorizing pods into three Quality of Service (QoS) classes:\n",
      "\n",
      "BestEffort (the lowest priority)\n",
      "\n",
      "Burstable\n",
      "\n",
      "Guaranteed (the highest)\n",
      "14.3.1 Defining the QoS class for a pod\n",
      "You might expect these classes to be assignable to pods through a separate field in the\n",
      "manifest, but they aren’t. The QoS class is derived from the combination of resource\n",
      "requests and limits for the pod’s containers. Here’s how.\n",
      "ASSIGNING A POD TO THE BESTEFFORT CLASS\n",
      "The lowest priority QoS class is the BestEffort class. It’s assigned to pods that don’t\n",
      "have any requests or limits set at all (in any of their containers). This is the QoS class\n",
      "that has been assigned to all the pods you created in previous chapters. Containers\n",
      "running in these pods have had no resource guarantees whatsoever. In the worst\n",
      "case, they may get almost no CPU time at all and will be the first ones killed when\n",
      "memory needs to be freed for other pods. But because a BestEffort pod has no\n",
      "memory limits set, its containers may use as much memory as they want, if enough\n",
      "memory is available.\n",
      "ASSIGNING A POD TO THE GUARANTEED CLASS\n",
      "On the other end of the spectrum is the Guaranteed QoS class. This class is given to\n",
      "pods whose containers’ requests are equal to the limits for all resources. For a pod’s\n",
      "class to be Guaranteed, three things need to be true:\n",
      "Requests and limits need to be set for both CPU and memory.\n",
      "They need to be set for each container.\n",
      "They need to be equal (the limit needs to match the request for each resource\n",
      "in each container).\n",
      "Because a container’s resource requests, if not set explicitly, default to the limits,\n",
      "specifying the limits for all resources (for each container in the pod) is enough for\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 450, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "418\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "the pod to be Guaranteed. Containers in those pods get the requested amount of\n",
      "resources, but cannot consume additional ones (because their limits are no higher\n",
      "than their requests). \n",
      "ASSIGNING THE BURSTABLE QOS CLASS TO A POD\n",
      "In between BestEffort and Guaranteed is the Burstable QoS class. All other pods\n",
      "fall into this class. This includes single-container pods where the container’s limits\n",
      "don’t match its requests and all pods where at least one container has a resource\n",
      "request specified, but not the limit. It also includes pods where one container’s\n",
      "requests match their limits, but another container has no requests or limits specified.\n",
      "Burstable pods get the amount of resources they request, but are allowed to use addi-\n",
      "tional resources (up to the limit) if needed.\n",
      "UNDERSTANDING HOW THE RELATIONSHIP BETWEEN REQUESTS AND LIMITS DEFINES THE QOS CLASS\n",
      "All three QoS classes and their relationships with requests and limits are shown in fig-\n",
      "ure 14.4.\n",
      "Thinking about what QoS class a pod has can make your head spin, because it involves\n",
      "multiple containers, multiple resources, and all the possible relationships between\n",
      "requests and limits. It’s easier if you start by thinking about QoS at the container level\n",
      "(although QoS classes are a property of pods, not containers) and then derive the\n",
      "pod’s QoS class from the QoS classes of containers. \n",
      "FIGURING OUT A CONTAINER’S QOS CLASS\n",
      "Table 14.1 shows the QoS class based on how resource requests and limits are\n",
      "defined on a single container. For single-container pods, the QoS class applies to\n",
      "the pod as well.\n",
      " \n",
      "BestEffort\n",
      "QoS\n",
      "Requests\n",
      "Limits\n",
      "Burstable\n",
      "QoS\n",
      "Requests\n",
      "Limits\n",
      "Guaranteed\n",
      "QoS\n",
      "Requests\n",
      "Limits\n",
      "Requests and\n",
      "limits are not set\n",
      "Requests are\n",
      "below limits\n",
      "Requests\n",
      "equal limits\n",
      "Figure 14.4\n",
      "Resource requests, limits and QoS classes\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 451, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "419\n",
      "Understanding pod QoS classes\n",
      "NOTE\n",
      "If only requests are set, but not limits, refer to the table rows where\n",
      "requests are less than the limits. If only limits are set, requests default to the\n",
      "limits, so refer to the rows where requests equal limits.\n",
      "FIGURING OUT THE QOS CLASS OF A POD WITH MULTIPLE CONTAINERS\n",
      "For multi-container pods, if all the containers have the same QoS class, that’s also the\n",
      "pod’s QoS class. If at least one container has a different class, the pod’s QoS class is\n",
      "Burstable, regardless of what the container classes are. Table 14.2 shows how a two-\n",
      "container pod’s QoS class relates to the classes of its two containers. You can easily\n",
      "extend this to pods with more than two containers.\n",
      "NOTE\n",
      "A pod’s QoS class is shown when running kubectl describe pod and\n",
      "in the pod’s YAML/JSON manifest in the status.qosClass field.\n",
      "We’ve explained how QoS classes are determined, but we still need to look at how they\n",
      "determine which container gets killed in an overcommitted system.\n",
      "Table 14.1\n",
      "The QoS class of a single-container pod based on resource requests and limits\n",
      "CPU requests vs. limits\n",
      "Memory requests vs. limits\n",
      "Container QoS class\n",
      "None set\n",
      "None set\n",
      "BestEffort\n",
      "None set\n",
      "Requests < Limits\n",
      "Burstable\n",
      "None set\n",
      "Requests = Limits\n",
      "Burstable\n",
      "Requests < Limits\n",
      "None set\n",
      "Burstable\n",
      "Requests < Limits\n",
      "Requests < Limits\n",
      "Burstable\n",
      "Requests < Limits\n",
      "Requests = Limits\n",
      "Burstable\n",
      "Requests = Limits\n",
      "Requests = Limits\n",
      "Guaranteed\n",
      "Table 14.2\n",
      "A Pod’s QoS class derived from the classes of its containers\n",
      "Container 1 QoS class\n",
      "Container 2 QoS class\n",
      "Pod’s QoS class\n",
      "BestEffort\n",
      "BestEffort\n",
      "BestEffort\n",
      "BestEffort\n",
      "Burstable\n",
      "Burstable\n",
      "BestEffort\n",
      "Guaranteed\n",
      "Burstable\n",
      "Burstable\n",
      "Burstable\n",
      "Burstable\n",
      "Burstable\n",
      "Guaranteed\n",
      "Burstable\n",
      "Guaranteed\n",
      "Guaranteed\n",
      "Guaranteed\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 452, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "420\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "14.3.2 Understanding which process gets killed when memory is low\n",
      "When the system is overcommitted, the QoS classes determine which container gets\n",
      "killed first so the freed resources can be given to higher priority pods. First in line to\n",
      "get killed are pods in the BestEffort class, followed by Burstable pods, and finally\n",
      "Guaranteed pods, which only get killed if system processes need memory.\n",
      "UNDERSTANDING HOW QOS CLASSES LINE UP\n",
      "Let’s look at the example shown in figure 14.5. Imagine having two single-container\n",
      "pods, where the first one has the BestEffort QoS class, and the second one’s is\n",
      "Burstable. When the node’s whole memory is already maxed out and one of the pro-\n",
      "cesses on the node tries to allocate more memory, the system will need to kill one of\n",
      "the processes (perhaps even the process trying to allocate additional memory) to\n",
      "honor the allocation request. In this case, the process running in the BestEffort pod\n",
      "will always be killed before the one in the Burstable pod.\n",
      "Obviously, a BestEffort pod’s process will also be killed before any Guaranteed pods’\n",
      "processes are killed. Likewise, a Burstable pod’s process will also be killed before that\n",
      "of a Guaranteed pod. But what happens if there are only two Burstable pods? Clearly,\n",
      "the selection process needs to prefer one over the other.\n",
      "UNDERSTANDING HOW CONTAINERS WITH THE SAME QOS CLASS ARE HANDLED\n",
      "Each running process has an OutOfMemory (OOM) score. The system selects the\n",
      "process to kill by comparing OOM scores of all the running processes. When memory\n",
      "needs to be freed, the process with the highest score gets killed.\n",
      " OOM scores are calculated from two things: the percentage of the available mem-\n",
      "ory the process is consuming and a fixed OOM score adjustment, which is based on the\n",
      "pod’s QoS class and the container’s requested memory. When two single-container pods\n",
      "exist, both in the Burstable class, the system will kill the one using more of its requested\n",
      "BestEffort\n",
      "QoS pod\n",
      "Pod A\n",
      "First in line\n",
      "to be killed\n",
      "Actual usage\n",
      "Requests\n",
      "Limits\n",
      "Burstable\n",
      "QoS pod\n",
      "Pod B\n",
      "Second in line\n",
      "to be killed\n",
      "90% used\n",
      "Requests\n",
      "Limits\n",
      "Burstable\n",
      "QoS pod\n",
      "Pod C\n",
      "Third in line\n",
      "to be killed\n",
      "70% used\n",
      "Requests\n",
      "Limits\n",
      "Guaranteed\n",
      "QoS pod\n",
      "Pod D\n",
      "Last to\n",
      "be killed\n",
      "99% used\n",
      "Requests\n",
      "Limits\n",
      "Figure 14.5\n",
      "Which pods get killed first\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 453, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "421\n",
      "Setting default requests and limits for pods per namespace\n",
      "memory than the other, percentage-wise. That’s why in figure 14.5, pod B, using 90%\n",
      "of its requested memory, gets killed before pod C, which is only using 70%, even\n",
      "though it’s using more megabytes of memory than pod B. \n",
      " This shows you need to be mindful of not only the relationship between requests\n",
      "and limits, but also of requests and the expected actual memory consumption. \n",
      "14.4\n",
      "Setting default requests and limits for pods per \n",
      "namespace\n",
      "We’ve looked at how resource requests and limits can be set for each individual con-\n",
      "tainer. If you don’t set them, the container is at the mercy of all other containers that\n",
      "do specify resource requests and limits. It’s a good idea to set requests and limits on\n",
      "every container.\n",
      "14.4.1 Introducing the LimitRange resource\n",
      "Instead of having to do this for every container, you can also do it by creating a Limit-\n",
      "Range resource. It allows you to specify (for each namespace) not only the minimum\n",
      "and maximum limit you can set on a container for each resource, but also the default\n",
      "resource requests for containers that don’t specify requests explicitly, as depicted in\n",
      "figure 14.6.\n",
      "API server\n",
      "Validation\n",
      "Pod A\n",
      "manifest\n",
      "- Requests\n",
      "- Limits\n",
      "Pod A\n",
      "manifest\n",
      "- Requests\n",
      "- Limits\n",
      "Pod B\n",
      "manifest\n",
      "- No\n",
      "requests\n",
      "or limits\n",
      "Pod B\n",
      "manifest\n",
      "- No\n",
      "requests\n",
      "or limits\n",
      "Defaulting\n",
      "Rejected because\n",
      "requests and limits are\n",
      "outside min/max values\n",
      "Defaults\n",
      "applied\n",
      "Namespace XYZ\n",
      "LimitRange\n",
      "Pod B\n",
      "manifest\n",
      "- Default\n",
      "requests\n",
      "- Default\n",
      "limits\n",
      "Pod B\n",
      "- Default requests\n",
      "- Default limits\n",
      "- Min/max CPU\n",
      "- Min/max memory\n",
      "- Default requests\n",
      "- Default limits\n",
      "Figure 14.6\n",
      "A LimitRange is used for validation and defaulting pods.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 454, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "422\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "LimitRange resources are used by the LimitRanger Admission Control plugin (we\n",
      "explained what those plugins are in chapter 11). When a pod manifest is posted to the\n",
      "API server, the LimitRanger plugin validates the pod spec. If validation fails, the mani-\n",
      "fest is rejected immediately. Because of this, a great use-case for LimitRange objects is\n",
      "to prevent users from creating pods that are bigger than any node in the cluster. With-\n",
      "out such a LimitRange, the API server will gladly accept the pod, but then never\n",
      "schedule it. \n",
      " The limits specified in a LimitRange resource apply to each individual pod/con-\n",
      "tainer or other kind of object created in the same namespace as the LimitRange\n",
      "object. They don’t limit the total amount of resources available across all the pods in\n",
      "the namespace. This is specified through ResourceQuota objects, which are explained\n",
      "in section 14.5. \n",
      "14.4.2 Creating a LimitRange object\n",
      "Let’s look at a full example of a LimitRange and see what the individual properties do.\n",
      "The following listing shows the full definition of a LimitRange resource.\n",
      "apiVersion: v1\n",
      "kind: LimitRange\n",
      "metadata:\n",
      "  name: example\n",
      "spec:\n",
      "  limits:\n",
      "  - type: Pod           \n",
      "    min:                         \n",
      "      cpu: 50m                   \n",
      "      memory: 5Mi                \n",
      "    max:                          \n",
      "      cpu: 1                      \n",
      "      memory: 1Gi                 \n",
      "  - type: Container             \n",
      "    defaultRequest:             \n",
      "      cpu: 100m                 \n",
      "      memory: 10Mi              \n",
      "    default:                      \n",
      "      cpu: 200m                   \n",
      "      memory: 100Mi               \n",
      "    min:                         \n",
      "      cpu: 50m                   \n",
      "      memory: 5Mi                \n",
      "    max:                         \n",
      "      cpu: 1                     \n",
      "      memory: 1Gi                \n",
      "    maxLimitRequestRatio:         \n",
      "      cpu: 4                      \n",
      "      memory: 10                  \n",
      "Listing 14.10\n",
      "A LimitRange resource: limits.yaml\n",
      "Specifies the \n",
      "limits for a pod \n",
      "as a whole\n",
      "Minimum CPU and memory all the \n",
      "pod’s containers can request in total\n",
      "Maximum CPU and memory all the pod’s \n",
      "containers can request (and limit)\n",
      "The\n",
      "container\n",
      "limits are\n",
      "specified\n",
      "below this\n",
      "line.\n",
      "Default requests for CPU and memory \n",
      "that will be applied to containers that \n",
      "don’t specify them explicitly\n",
      "Default limits for containers \n",
      "that don’t specify them\n",
      "Minimum and maximum \n",
      "requests/limits that a \n",
      "container can have\n",
      "Maximum ratio between \n",
      "the limit and request \n",
      "for each resource\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 455, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "423\n",
      "Setting default requests and limits for pods per namespace\n",
      "  - type: PersistentVolumeClaim      \n",
      "    min:                             \n",
      "      storage: 1Gi                   \n",
      "    max:                             \n",
      "      storage: 10Gi                  \n",
      "As you can see from the previous example, the minimum and maximum limits for a\n",
      "whole pod can be configured. They apply to the sum of all the pod’s containers’\n",
      "requests and limits. \n",
      " Lower down, at the container level, you can set not only the minimum and maxi-\n",
      "mum, but also default resource requests (defaultRequest) and default limits\n",
      "(default) that will be applied to each container that doesn’t specify them explicitly. \n",
      " Beside the min, max, and default values, you can even set the maximum ratio of\n",
      "limits vs. requests. The previous listing sets the CPU maxLimitRequestRatio to 4,\n",
      "which means a container’s CPU limits will not be allowed to be more than four times\n",
      "greater than its CPU requests. A container requesting 200 millicores will not be\n",
      "accepted if its CPU limit is set to 801 millicores or higher. For memory, the maximum\n",
      "ratio is set to 10.\n",
      " In chapter 6 we looked at PersistentVolumeClaims (PVC), which allow you to claim\n",
      "a certain amount of persistent storage similarly to how a pod’s containers claim CPU\n",
      "and memory. In the same way you’re limiting the minimum and maximum amount of\n",
      "CPU a container can request, you should also limit the amount of storage a single\n",
      "PVC can request. A LimitRange object allows you to do that as well, as you can see at\n",
      "the bottom of the example.\n",
      " The example shows a single LimitRange object containing limits for everything,\n",
      "but you could also split them into multiple objects if you prefer to have them orga-\n",
      "nized per type (one for pod limits, another for container limits, and yet another for\n",
      "PVCs, for example). Limits from multiple LimitRange objects are all consolidated\n",
      "when validating a pod or PVC.\n",
      " Because the validation (and defaults) configured in a LimitRange object is per-\n",
      "formed by the API server when it receives a new pod or PVC manifest, if you modify\n",
      "the limits afterwards, existing pods and PVCs will not be revalidated—the new limits\n",
      "will only apply to pods and PVCs created afterward. \n",
      "14.4.3 Enforcing the limits\n",
      "With your limits in place, you can now try creating a pod that requests more CPU than\n",
      "allowed by the LimitRange. You’ll find the YAML for the pod in the code archive. The\n",
      "next listing only shows the part relevant to the discussion.\n",
      "    resources:\n",
      "      requests:\n",
      "        cpu: 2\n",
      "Listing 14.11\n",
      "A pod with CPU requests greater than the limit: limits-pod-too-big.yaml\n",
      "A LimitRange can also set \n",
      "the minimum and maximum \n",
      "amount of storage a PVC \n",
      "can request.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 456, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "424\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "The pod’s single container is requesting two CPUs, which is more than the maximum\n",
      "you set in the LimitRange earlier. Creating the pod yields the following result:\n",
      "$ kubectl create -f limits-pod-too-big.yaml \n",
      "Error from server (Forbidden): error when creating \"limits-pod-too-big.yaml\": \n",
      "pods \"too-big\" is forbidden: [\n",
      "  maximum cpu usage per Pod is 1, but request is 2., \n",
      "  maximum cpu usage per Container is 1, but request is 2.]\n",
      "I’ve modified the output slightly to make it more legible. The nice thing about the\n",
      "error message from the server is that it lists all the reasons why the pod was rejected,\n",
      "not only the first one it encountered. As you can see, the pod was rejected for two rea-\n",
      "sons: you requested two CPUs for the container, but the maximum CPU limit for a\n",
      "container is one. Likewise, the pod as a whole requested two CPUs, but the maximum\n",
      "is one CPU (if this was a multi-container pod, even if each individual container\n",
      "requested less than the maximum amount of CPU, together they’d still need to\n",
      "request less than two CPUs to pass the maximum CPU for pods). \n",
      "14.4.4 Applying default resource requests and limits\n",
      "Now let’s also see how default resource requests and limits are set on containers that\n",
      "don’t specify them. Deploy the kubia-manual pod from chapter 3 again:\n",
      "$ kubectl create -f ../Chapter03/kubia-manual.yaml\n",
      "pod \"kubia-manual\" created\n",
      "Before you set up your LimitRange object, all your pods were created without any\n",
      "resource requests or limits, but now the defaults are applied automatically when creat-\n",
      "ing the pod. You can confirm this by describing the kubia-manual pod, as shown in\n",
      "the following listing.\n",
      "$ kubectl describe po kubia-manual\n",
      "Name:           kubia-manual\n",
      "...\n",
      "Containers:\n",
      "  kubia:\n",
      "    Limits:\n",
      "      cpu:      200m\n",
      "      memory:   100Mi\n",
      "    Requests:\n",
      "      cpu:      100m\n",
      "      memory:   10Mi\n",
      "The container’s requests and limits match the ones you specified in the LimitRange\n",
      "object. If you used a different LimitRange specification in another namespace, pods\n",
      "created in that namespace would obviously have different requests and limits. This\n",
      "allows admins to configure default, min, and max resources for pods per namespace.\n",
      "Listing 14.12\n",
      "Inspecting limits that were applied to a pod automatically\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 457, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "425\n",
      "Limiting the total resources available in a namespace\n",
      "If namespaces are used to separate different teams or to separate development, QA,\n",
      "staging, and production pods running in the same Kubernetes cluster, using a differ-\n",
      "ent LimitRange in each namespace ensures large pods can only be created in certain\n",
      "namespaces, whereas others are constrained to smaller pods.\n",
      " But remember, the limits configured in a LimitRange only apply to each individual\n",
      "pod/container. It’s still possible to create many pods and eat up all the resources avail-\n",
      "able in the cluster. LimitRanges don’t provide any protection from that. A Resource-\n",
      "Quota object, on the other hand, does. You’ll learn about them next.\n",
      "14.5\n",
      "Limiting the total resources available in a namespace\n",
      "As you’ve seen, LimitRanges only apply to individual pods, but cluster admins also\n",
      "need a way to limit the total amount of resources available in a namespace. This is\n",
      "achieved by creating a ResourceQuota object. \n",
      "14.5.1 Introducing the ResourceQuota object\n",
      "In chapter 10 we said that several Admission Control plugins running inside the API\n",
      "server verify whether the pod may be created or not. In the previous section, I said\n",
      "that the LimitRanger plugin enforces the policies configured in LimitRange resources.\n",
      "Similarly, the ResourceQuota Admission Control plugin checks whether the pod\n",
      "being created would cause the configured ResourceQuota to be exceeded. If that’s\n",
      "the case, the pod’s creation is rejected. Because resource quotas are enforced at pod\n",
      "creation time, a ResourceQuota object only affects pods created after the Resource-\n",
      "Quota object is created—creating it has no effect on existing pods.\n",
      " A ResourceQuota limits the amount of computational resources the pods and the\n",
      "amount of storage PersistentVolumeClaims in a namespace can consume. It can also\n",
      "limit the number of pods, claims, and other API objects users are allowed to create\n",
      "inside the namespace. Because you’ve mostly dealt with CPU and memory so far, let’s\n",
      "start by looking at how to specify quotas for them.\n",
      "CREATING A RESOURCEQUOTA FOR CPU AND MEMORY\n",
      "The overall CPU and memory all the pods in a namespace are allowed to consume is\n",
      "defined by creating a ResourceQuota object as shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: ResourceQuota\n",
      "metadata:\n",
      "  name: cpu-and-mem\n",
      "spec:\n",
      "  hard:\n",
      "    requests.cpu: 400m\n",
      "    requests.memory: 200Mi\n",
      "    limits.cpu: 600m\n",
      "    limits.memory: 500Mi\n",
      "Listing 14.13\n",
      "A ResourceQuota resource for CPU and memory: quota-cpu-memory.yaml\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 458, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "426\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "Instead of defining a single total for each resource, you define separate totals for\n",
      "requests and limits for both CPU and memory. You’ll notice the structure is a bit dif-\n",
      "ferent, compared to that of a LimitRange. Here, both the requests and the limits for\n",
      "all resources are defined in a single place. \n",
      " This ResourceQuota sets the maximum amount of CPU pods in the namespace\n",
      "can request to 400 millicores. The maximum total CPU limits in the namespace are\n",
      "set to 600 millicores. For memory, the maximum total requests are set to 200 MiB,\n",
      "whereas the limits are set to 500 MiB.\n",
      " A ResourceQuota object applies to the namespace it’s created in, like a Limit-\n",
      "Range, but it applies to all the pods’ resource requests and limits in total and not to\n",
      "each individual pod or container separately, as shown in figure 14.7.\n",
      "INSPECTING THE QUOTA AND QUOTA USAGE\n",
      "After you post the ResourceQuota object to the API server, you can use the kubectl\n",
      "describe command to see how much of the quota is already used up, as shown in\n",
      "the following listing.\n",
      "$ kubectl describe quota\n",
      "Name:           cpu-and-mem\n",
      "Namespace:      default\n",
      "Resource        Used   Hard\n",
      "--------        ----   ----\n",
      "limits.cpu      200m   600m\n",
      "limits.memory   100Mi  500Mi\n",
      "requests.cpu    100m   400m\n",
      "requests.memory 10Mi   200Mi\n",
      "I only have the kubia-manual pod running, so the Used column matches its resource\n",
      "requests and limits. When I run additional pods, their requests and limits are added to\n",
      "the used amounts.\n",
      "Listing 14.14\n",
      "Inspecting the ResourceQuota with kubectl describe quota\n",
      "LimitRange\n",
      "ResourceQuota\n",
      "Namespace: FOO\n",
      "Pod A\n",
      "Pod B\n",
      "Pod C\n",
      "LimitRange\n",
      "ResourceQuota\n",
      "Namespace: BAR\n",
      "Pod D\n",
      "Pod E\n",
      "Pod F\n",
      "Figure 14.7\n",
      "LimitRanges apply to individual pods; ResourceQuotas apply to all pods in the \n",
      "namespace.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 459, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "427\n",
      "Limiting the total resources available in a namespace\n",
      "CREATING A LIMITRANGE ALONG WITH A RESOURCEQUOTA\n",
      "One caveat when creating a ResourceQuota is that you will also want to create a Limit-\n",
      "Range object alongside it. In your case, you have a LimitRange configured from the\n",
      "previous section, but if you didn’t have one, you couldn’t run the kubia-manual pod,\n",
      "because it doesn’t specify any resource requests or limits. Here’s what would happen\n",
      "in that case:\n",
      "$ kubectl create -f ../Chapter03/kubia-manual.yaml\n",
      "Error from server (Forbidden): error when creating \"../Chapter03/kubia-\n",
      "manual.yaml\": pods \"kubia-manual\" is forbidden: failed quota: cpu-and-\n",
      "mem: must specify limits.cpu,limits.memory,requests.cpu,requests.memory\n",
      "When a quota for a specific resource (CPU or memory) is configured (request or\n",
      "limit), pods need to have the request or limit (respectively) set for that same resource;\n",
      "otherwise the API server will not accept the pod. That’s why having a LimitRange with\n",
      "defaults for those resources can make life a bit easier for people creating pods.\n",
      "14.5.2 Specifying a quota for persistent storage\n",
      "A ResourceQuota object can also limit the amount of persistent storage that can be\n",
      "claimed in the namespace, as shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: ResourceQuota\n",
      "metadata:\n",
      "  name: storage\n",
      "spec:\n",
      "  hard:\n",
      "    requests.storage: 500Gi                               \n",
      "    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi     \n",
      "    standard.storageclass.storage.k8s.io/requests.storage: 1Ti\n",
      "In this example, the amount of storage all PersistentVolumeClaims in a namespace\n",
      "can request is limited to 500 GiB (by the requests.storage entry in the Resource-\n",
      "Quota object). But as you’ll remember from chapter 6, PersistentVolumeClaims can\n",
      "request a dynamically provisioned PersistentVolume of a specific StorageClass. That’s\n",
      "why Kubernetes also makes it possible to define storage quotas for each StorageClass\n",
      "individually. The previous example limits the total amount of claimable SSD storage\n",
      "(designated by the ssd StorageClass) to 300 GiB. The less-performant HDD storage\n",
      "(StorageClass standard) is limited to 1 TiB.\n",
      "14.5.3 Limiting the number of objects that can be created\n",
      "A ResourceQuota can also be configured to limit the number of Pods, Replication-\n",
      "Controllers, Services, and other objects inside a single namespace. This allows the\n",
      "cluster admin to limit the number of objects users can create based on their payment\n",
      "Listing 14.15\n",
      "A ResourceQuota for storage: quota-storage.yaml\n",
      "The amount of \n",
      "storage claimable \n",
      "overall\n",
      "The amount \n",
      "of claimable \n",
      "storage in \n",
      "StorageClass ssd\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 460, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "428\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "plan, for example, and can also limit the number of public IPs or node ports Ser-\n",
      "vices can use. \n",
      " The following listing shows what a ResourceQuota object that limits the number of\n",
      "objects may look like.\n",
      "apiVersion: v1\n",
      "kind: ResourceQuota\n",
      "metadata:\n",
      "  name: objects\n",
      "spec:\n",
      "  hard:\n",
      "    pods: 10                        \n",
      "    replicationcontrollers: 5       \n",
      "    secrets: 10                     \n",
      "    configmaps: 10                  \n",
      "    persistentvolumeclaims: 4       \n",
      "    services: 5                      \n",
      "    services.loadbalancers: 1        \n",
      "    services.nodeports: 2            \n",
      "    ssd.storageclass.storage.k8s.io/persistentvolumeclaims: 2   \n",
      "The ResourceQuota in this listing allows users to create at most 10 Pods in the name-\n",
      "space, regardless if they’re created manually or by a ReplicationController, Replica-\n",
      "Set, DaemonSet, Job, and so on. It also limits the number of ReplicationControllers to\n",
      "five. A maximum of five Services can be created, of which only one can be a LoadBal-\n",
      "ancer-type Service, and only two can be NodePort Services. Similar to how the maxi-\n",
      "mum amount of requested storage can be specified per StorageClass, the number of\n",
      "PersistentVolumeClaims can also be limited per StorageClass.\n",
      " Object count quotas can currently be set for the following objects: \n",
      "Pods\n",
      "ReplicationControllers \n",
      "Secrets\n",
      "ConfigMaps\n",
      "PersistentVolumeClaims\n",
      "Services (in general), and for two specific types of Services, such as Load-\n",
      "Balancer Services (services.loadbalancers) and NodePort Services (ser-\n",
      "vices.nodeports) \n",
      "Finally, you can even set an object count quota for ResourceQuota objects themselves.\n",
      "The number of other objects, such as ReplicaSets, Jobs, Deployments, Ingresses, and\n",
      "so on, cannot be limited yet (but this may have changed since the book was published,\n",
      "so please check the documentation for up-to-date information).\n",
      "Listing 14.16\n",
      "A ResourceQuota for max number of resources: quota-object-count.yaml\n",
      "Only 10 Pods, 5 ReplicationControllers, \n",
      "10 Secrets, 10 ConfigMaps, and \n",
      "4 PersistentVolumeClaims can be \n",
      "created in the namespace.\n",
      "Five Services overall can be created, \n",
      "of which at most one can be a \n",
      "LoadBalancer Service and at most \n",
      "two can be NodePort Services.\n",
      "Only two PVCs can claim storage\n",
      "with the ssd StorageClass.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 461, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "429\n",
      "Limiting the total resources available in a namespace\n",
      "14.5.4 Specifying quotas for specific pod states and/or QoS classes\n",
      "The quotas you’ve created so far have applied to all pods, regardless of their current\n",
      "state and QoS class. But quotas can also be limited to a set of quota scopes. Four scopes are\n",
      "currently available: BestEffort, NotBestEffort, Terminating, and NotTerminating. \n",
      " The BestEffort and NotBestEffort scopes determine whether the quota applies\n",
      "to pods with the BestEffort QoS class or with one of the other two classes (that is,\n",
      "Burstable and Guaranteed). \n",
      " The other two scopes (Terminating and NotTerminating) don’t apply to pods\n",
      "that are (or aren’t) in the process of shutting down, as the name might lead you to\n",
      "believe. We haven’t talked about this, but you can specify how long each pod is\n",
      "allowed to run before it’s terminated and marked as Failed. This is done by setting\n",
      "the activeDeadlineSeconds field in the pod spec. This property defines the number\n",
      "of seconds a pod is allowed to be active on the node relative to its start time before it’s\n",
      "marked as Failed and then terminated. The Terminating quota scope applies to pods\n",
      "that have the activeDeadlineSeconds set, whereas the NotTerminating applies to\n",
      "those that don’t. \n",
      " When creating a ResourceQuota, you can specify the scopes that it applies to. A\n",
      "pod must match all the specified scopes for the quota to apply to it. Additionally, what\n",
      "a quota can limit depends on the quota’s scope. BestEffort scope can only limit the\n",
      "number of pods, whereas the other three scopes can limit the number of pods,\n",
      "CPU/memory requests, and CPU/memory limits. \n",
      " If, for example, you want the quota to apply only to BestEffort, NotTerminating\n",
      "pods, you can create the ResourceQuota object shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: ResourceQuota\n",
      "metadata:\n",
      "  name: besteffort-notterminating-pods\n",
      "spec:\n",
      "  scopes:                 \n",
      "  - BestEffort            \n",
      "  - NotTerminating        \n",
      "  hard: \n",
      "    pods: 4          \n",
      "This quota ensures that at most four pods exist with the BestEffort QoS class,\n",
      "which don’t have an active deadline. If the quota was targeting NotBestEffort pods\n",
      "instead, you could also specify requests.cpu, requests.memory, limits.cpu, and\n",
      "limits.memory.\n",
      "NOTE\n",
      "Before you move on to the next section of this chapter, please delete\n",
      "all the ResourceQuota and LimitRange resources you created. You won’t\n",
      "Listing 14.17\n",
      "ResourceQuota for BestEffort/NotTerminating pods: \n",
      "quota-scoped.yaml\n",
      "This quota only applies to pods \n",
      "that have the BestEffort QoS and \n",
      "don’t have an active deadline set.\n",
      "Only four such \n",
      "pods can exist.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 462, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "430\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "need them anymore and they may interfere with examples in the following\n",
      "chapters.\n",
      "14.6\n",
      "Monitoring pod resource usage\n",
      "Properly setting resource requests and limits is crucial for getting the most out of your\n",
      "Kubernetes cluster. If requests are set too high, your cluster nodes will be underuti-\n",
      "lized and you’ll be throwing money away. If you set them too low, your apps will be\n",
      "CPU-starved or even killed by the OOM Killer. How do you find the sweet spot for\n",
      "requests and limits?\n",
      " You find it by monitoring the actual resource usage of your containers under the\n",
      "expected load levels. Once the application is exposed to the public, you should keep\n",
      "monitoring it and adjust the resource requests and limits if required.\n",
      "14.6.1 Collecting and retrieving actual resource usages\n",
      "How does one monitor apps running in Kubernetes? Luckily, the Kubelet itself\n",
      "already contains an agent called cAdvisor, which performs the basic collection of\n",
      "resource consumption data for both individual containers running on the node and\n",
      "the node as a whole. Gathering those statistics centrally for the whole cluster requires\n",
      "you to run an additional component called Heapster. \n",
      " Heapster runs as a pod on one of the nodes and is exposed through a regular\n",
      "Kubernetes Service, making it accessible at a stable IP address. It collects the data\n",
      "from all cAdvisors in the cluster and exposes it in a single location. Figure 14.8\n",
      "shows the flow of the metrics data from the pods, through cAdvisor and finally into\n",
      "Heapster.\n",
      "Kubelet\n",
      "cAdvisor\n",
      "Node 1\n",
      "Pod\n",
      "Pod\n",
      "Kubelet\n",
      "cAdvisor\n",
      "Node 2\n",
      "Pod\n",
      "Kubelet\n",
      "cAdvisor\n",
      "Node X\n",
      "Pod\n",
      "Heapster\n",
      "Each cAdvisor collects metrics from\n",
      "containers running on its node.\n",
      "Heapster runs on one of the nodes as a\n",
      "pod and collects metrics from all nodes.\n",
      "Figure 14.8\n",
      "The flow of metrics data into Heapster\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 463, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "431\n",
      "Monitoring pod resource usage\n",
      "The arrows in the figure show how the metrics data flows. They don’t show which com-\n",
      "ponent connects to which to get the data. The pods (or the containers running\n",
      "therein) don’t know anything about cAdvisor, and cAdvisor doesn’t know anything\n",
      "about Heapster. It’s Heapster that connects to all the cAdvisors, and it’s the cAdvisors\n",
      "that collect the container and node usage data without having to talk to the processes\n",
      "running inside the pods’ containers.\n",
      "ENABLING HEAPSTER\n",
      "If you’re running a cluster in Google Kubernetes Engine, Heapster is enabled by\n",
      "default. If you’re using Minikube, it’s available as an add-on and can be enabled with\n",
      "the following command:\n",
      "$ minikube addons enable heapster\n",
      "heapster was successfully enabled\n",
      "To run Heapster manually in other types of Kubernetes clusters, you can refer to\n",
      "instructions located at https:/\n",
      "/github.com/kubernetes/heapster. \n",
      " After enabling Heapster, you’ll need to wait a few minutes for it to collect metrics\n",
      "before you can see resource usage statistics for your cluster, so be patient. \n",
      "DISPLAYING CPU AND MEMORY USAGE FOR CLUSTER NODES\n",
      "Running Heapster in your cluster makes it possible to obtain resource usages for\n",
      "nodes and individual pods through the kubectl top command. To see how much\n",
      "CPU and memory is being used on your nodes, you can run the command shown in\n",
      "the following listing.\n",
      "$ kubectl top node\n",
      "NAME       CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%\n",
      "minikube   170m         8%        556Mi           27%\n",
      "This shows the actual, current CPU and memory usage of all the pods running on the\n",
      "node, unlike the kubectl describe node command, which shows the amount of CPU\n",
      "and memory requests and limits instead of actual runtime usage data. \n",
      "DISPLAYING CPU AND MEMORY USAGE FOR INDIVIDUAL PODS\n",
      "To see how much each individual pod is using, you can use the kubectl top pod com-\n",
      "mand, as shown in the following listing.\n",
      "$ kubectl top pod --all-namespaces\n",
      "NAMESPACE      NAME                             CPU(cores)   MEMORY(bytes)\n",
      "kube-system    influxdb-grafana-2r2w9           1m           32Mi\n",
      "kube-system    heapster-40j6d                   0m           18Mi\n",
      "Listing 14.18\n",
      "Actual CPU and memory usage of nodes\n",
      "Listing 14.19\n",
      "Actual CPU and memory usages of pods\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 464, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "432\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "default        kubia-3773182134-63bmb           0m           9Mi\n",
      "kube-system    kube-dns-v20-z0hq6               1m           11Mi\n",
      "kube-system    kubernetes-dashboard-r53mc       0m           14Mi\n",
      "kube-system    kube-addon-manager-minikube      7m           33Mi\n",
      "The outputs of both these commands are fairly simple, so you probably don’t need me\n",
      "to explain them, but I do need to warn you about one thing. Sometimes the top pod\n",
      "command will refuse to show any metrics and instead print out an error like this:\n",
      "$ kubectl top pod\n",
      "W0312 22:12:58.021885   15126 top_pod.go:186] Metrics not available for pod \n",
      "default/kubia-3773182134-63bmb, age: 1h24m19.021873823s\n",
      "error: Metrics not available for pod default/kubia-3773182134-63bmb, age: \n",
      "1h24m19.021873823s\n",
      "If this happens, don’t start looking for the cause of the error yet. Relax, wait a while,\n",
      "and rerun the command—it may take a few minutes, but the metrics should appear\n",
      "eventually. The kubectl top command gets the metrics from Heapster, which aggre-\n",
      "gates the data over a few minutes and doesn’t expose it immediately. \n",
      "TIP\n",
      "To see resource usages across individual containers instead of pods, you\n",
      "can use the --containers option. \n",
      "14.6.2 Storing and analyzing historical resource consumption statistics\n",
      "The top command only shows current resource usages—it doesn’t show you how\n",
      "much CPU or memory your pods consumed throughout the last hour, yesterday, or a\n",
      "week ago, for example. In fact, both cAdvisor and Heapster only hold resource usage\n",
      "data for a short window of time. If you want to analyze your pods’ resource consump-\n",
      "tion over longer time periods, you’ll need to run additional tools.\n",
      " When using Google Kubernetes Engine, you can monitor your cluster with Google\n",
      "Cloud Monitoring, but when you’re running your own local Kubernetes cluster\n",
      "(either through Minikube or other means), people usually use InfluxDB for storing\n",
      "statistics data and Grafana for visualizing and analyzing them. \n",
      "INTRODUCING INFLUXDB AND GRAFANA\n",
      "InfluxDB is an open source time-series database ideal for storing application metrics\n",
      "and other monitoring data. Grafana, also open source, is an analytics and visualization\n",
      "suite with a nice-looking web console that allows you to visualize the data stored in\n",
      "InfluxDB and discover how your application’s resource usage behaves over time (an\n",
      "example showing three Grafana charts is shown in figure 14.9).\n",
      " \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "width\n",
      "1700\n",
      "height\n",
      "1213\n",
      "PIX BUFFER SIZE\n",
      "6186300\n",
      "Original IMG_BUFFER_SIZE\n",
      "6186300\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011540>\n",
      "page_image_dict\n",
      "{'page': 465, 'img_cnt': 1, 'img_npy_lst': []}\n",
      "433\n",
      "Monitoring pod resource usage\n",
      "RUNNING INFLUXDB AND GRAFANA IN YOUR CLUSTER\n",
      "Both InfluxDB and Grafana can run as pods. Deploying them is straightforward. All\n",
      "the necessary manifests are available in the Heapster Git repository at http:/\n",
      "/github\n",
      ".com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb.\n",
      " When using Minikube, you don’t even need to deploy them manually, because\n",
      "they’re deployed along with Heapster when you enable the Heapster add-on.\n",
      "ANALYZING RESOURCE USAGE WITH GRAFANA\n",
      "To discover how much of each resource your pod requires over time, open the\n",
      "Grafana web console and explore the predefined dashboards. Generally, you can find\n",
      "out the URL of Grafana’s web console with kubectl cluster-info:\n",
      "$ kubectl cluster-info\n",
      "...\n",
      "monitoring-grafana is running at \n",
      "https://192.168.99.100:8443/api/v1/proxy/namespaces/kube-\n",
      "system/services/monitoring-grafana\n",
      "Figure 14.9\n",
      "Grafana dashboard showing CPU usage across the cluster\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 466, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "434\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "When using Minikube, Grafana’s web console is exposed through a NodePort Service,\n",
      "so you can open it in your browser with the following command:\n",
      "$ minikube service monitoring-grafana -n kube-system\n",
      "Opening kubernetes service kube-system/monitoring-grafana in default \n",
      "browser...\n",
      "A new browser window or tab will open and show the Grafana Home screen. On the\n",
      "right-hand side, you’ll see a list of dashboards containing two entries:\n",
      "Cluster\n",
      "Pods\n",
      "To see the resource usage statistics of the nodes, open the Cluster dashboard. There\n",
      "you’ll see several charts showing the overall cluster usage, usage by node, and the\n",
      "individual usage for CPU, memory, network, and filesystem. The charts will not only\n",
      "show the actual usage, but also the requests and limits for those resources (where\n",
      "they apply).\n",
      " If you then switch over to the Pods dashboard, you can examine the resource\n",
      "usages for each individual pod, again with both requests and limits shown alongside\n",
      "the actual usage. \n",
      " Initially, the charts show the statistics for the last 30 minutes, but you can zoom out\n",
      "and see the data for much longer time periods: days, months, or even years.\n",
      "USING THE INFORMATION SHOWN IN THE CHARTS\n",
      "By looking at the charts, you can quickly see if the resource requests or limits you’ve\n",
      "set for your pods need to be raised or whether they can be lowered to allow more pods\n",
      "to fit on your nodes. Let’s look at an example. Figure 14.10 shows the CPU and mem-\n",
      "ory charts for a pod.\n",
      " At the far right of the top chart, you can see the pod is using more CPU than was\n",
      "requested in the pod’s manifest. Although this isn’t problematic when this is the only\n",
      "pod running on the node, you should keep in mind that a pod is only guaranteed as\n",
      "much of a resource as it requests through resource requests. Your pod may be running\n",
      "fine now, but when other pods are deployed to the same node and start using the\n",
      "CPU, your pod’s CPU time may be throttled. Because of this, to ensure the pod can\n",
      "use as much CPU as it needs to at any time, you should raise the CPU resource request\n",
      "for the pod’s container.\n",
      " The bottom chart shows the pod’s memory usage and request. Here the situation is\n",
      "the exact opposite. The amount of memory the pod is using is well below what was\n",
      "requested in the pod’s spec. The requested memory is reserved for the pod and won’t\n",
      "be available to other pods. The unused memory is therefore wasted. You should\n",
      "decrease the pod’s memory request to make the memory available to other pods run-\n",
      "ning on the node. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "1616\n",
      "height\n",
      "1773\n",
      "PIX BUFFER SIZE\n",
      "8595504\n",
      "Original IMG_BUFFER_SIZE\n",
      "8595504\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014010880>\n",
      "page_image_dict\n",
      "{'page': 467, 'img_cnt': 1, 'img_npy_lst': []}\n",
      "435\n",
      "Summary\n",
      "14.7\n",
      "Summary\n",
      "This chapter has shown you that you need to consider your pod’s resource usage and\n",
      "configure both the resource requests and the limits for your pod to keep everything\n",
      "running smoothly. The key takeaways from this chapter are\n",
      "Specifying resource requests helps Kubernetes schedule pods across the cluster.\n",
      "Specifying resource limits keeps pods from starving other pods of resources.\n",
      "Unused CPU time is allocated based on containers’ CPU requests.\n",
      "Containers never get killed if they try to use too much CPU, but they are killed\n",
      "if they try to use too much memory.\n",
      "In an overcommitted system, containers also get killed to free memory for more\n",
      "important pods, based on the pods’ QoS classes and actual memory usage.\n",
      "Actual CPU usage is higher\n",
      "than what was requested.\n",
      "The application’s CPU time\n",
      "will be throttled when other\n",
      "apps demand more CPU.\n",
      "You should increase the\n",
      "CPU request.\n",
      "Actual memory usage is well\n",
      "below requested memory.\n",
      "You’ve reserved too much\n",
      "memory for this app. You’re\n",
      "wasting memory, because it\n",
      "won’t ever be used by this\n",
      "app and also can’t be used\n",
      "by other apps. You should\n",
      "decrease the memory\n",
      "request.\n",
      "CPU request\n",
      "CPU usage\n",
      "Memory request\n",
      "Memory usage\n",
      "Figure 14.10\n",
      "CPU and memory usage chart for a pod\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 468, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "436\n",
      "CHAPTER 14\n",
      "Managing pods’ computational resources\n",
      "You can use LimitRange objects to define the minimum, maximum, and default\n",
      "resource requests and limits for individual pods.\n",
      "You can use ResourceQuota objects to limit the amount of resources available\n",
      "to all the pods in a namespace.\n",
      "To know how high to set a pod’s resource requests and limits, you need to mon-\n",
      "itor how the pod uses resources over a long-enough time period.\n",
      "In the next chapter, you’ll see how these metrics can be used by Kubernetes to auto-\n",
      "matically scale your pods.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 469, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "437\n",
      "Automatic scaling\n",
      "of pods and cluster nodes\n",
      "Applications running in pods can be scaled out manually by increasing the\n",
      "replicas field in the ReplicationController, ReplicaSet, Deployment, or other\n",
      "scalable resource. Pods can also be scaled vertically by increasing their container’s\n",
      "resource requests and limits (though this can currently only be done at pod cre-\n",
      "ation time, not while the pod is running). Although manual scaling is okay for\n",
      "times when you can anticipate load spikes in advance or when the load changes\n",
      "gradually over longer periods of time, requiring manual intervention to handle\n",
      "sudden, unpredictable traffic increases isn’t ideal. \n",
      "This chapter covers\n",
      "Configuring automatic horizontal scaling of pods \n",
      "based on CPU utilization\n",
      "Configuring automatic horizontal scaling of pods \n",
      "based on custom metrics\n",
      "Understanding why vertical scaling of pods isn’t \n",
      "possible yet\n",
      "Understanding automatic horizontal scaling of \n",
      "cluster nodes\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 470, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "438\n",
      "CHAPTER 15\n",
      "Automatic scaling of pods and cluster nodes\n",
      " Luckily, Kubernetes can monitor your pods and scale them up automatically as\n",
      "soon as it detects an increase in the CPU usage or some other metric. If running on a\n",
      "cloud infrastructure, it can even spin up additional nodes if the existing ones can’t\n",
      "accept any more pods. This chapter will explain how to get Kubernetes to do both pod\n",
      "and node autoscaling.\n",
      " The autoscaling feature in Kubernetes was completely rewritten between the 1.6\n",
      "and the 1.7 version, so be aware you may find outdated information on this subject\n",
      "online.\n",
      "15.1\n",
      "Horizontal pod autoscaling\n",
      "Horizontal pod autoscaling is the automatic scaling of the number of pod replicas man-\n",
      "aged by a controller. It’s performed by the Horizontal controller, which is enabled and\n",
      "configured by creating a HorizontalPodAutoscaler (HPA) resource. The controller\n",
      "periodically checks pod metrics, calculates the number of replicas required to meet\n",
      "the target metric value configured in the HorizontalPodAutoscaler resource, and\n",
      "adjusts the replicas field on the target resource (Deployment, ReplicaSet, Replication-\n",
      "Controller, or StatefulSet). \n",
      "15.1.1 Understanding the autoscaling process\n",
      "The autoscaling process can be split into three steps:\n",
      "Obtain metrics of all the pods managed by the scaled resource object.\n",
      "Calculate the number of pods required to bring the metrics to (or close to) the\n",
      "specified target value.\n",
      "Update the replicas field of the scaled resource.\n",
      "Let’s examine all three steps next.\n",
      "OBTAINING POD METRICS\n",
      "The Autoscaler doesn’t perform the gathering of the pod metrics itself. It gets the\n",
      "metrics from a different source. As we saw in the previous chapter, pod and node met-\n",
      "rics are collected by an agent called cAdvisor, which runs in the Kubelet on each node,\n",
      "and then aggregated by the cluster-wide component called Heapster. The horizontal\n",
      "pod autoscaler controller gets the metrics of all the pods by querying Heapster\n",
      "through REST calls. The flow of metrics data is shown in figure 15.1 (although all the\n",
      "connections are initiated in the opposite direction).\n",
      "This implies that Heapster must be running in the cluster for autoscaling to work. If\n",
      "you’re using Minikube and were following along in the previous chapter, Heapster\n",
      "Pod(s)\n",
      "cAdvisor(s)\n",
      "Horizontal Pod Autoscaler(s)\n",
      "Heapster\n",
      "Figure 15.1\n",
      "Flow of metrics from the pod(s) to the HorizontalPodAutoscaler(s)\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 471, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "439\n",
      "Horizontal pod autoscaling\n",
      "should already be enabled in your cluster. If not, make sure to enable the Heapster\n",
      "add-on before trying out any autoscaling examples.\n",
      " Although you don’t need to query Heapster directly, if you’re interested in doing\n",
      "so, you’ll find both the Heapster Pod and the Service it’s exposed through in the\n",
      "kube-system namespace. \n",
      "CALCULATING THE REQUIRED NUMBER OF PODS\n",
      "Once the Autoscaler has metrics for all the pods belonging to the resource the Auto-\n",
      "scaler is scaling (the Deployment, ReplicaSet, ReplicationController, or StatefulSet\n",
      "resource), it can use those metrics to figure out the required number of replicas. It\n",
      "needs to find the number that will bring the average value of the metric across all\n",
      "those replicas as close to the configured target value as possible. The input to this cal-\n",
      "culation is a set of pod metrics (possibly multiple metrics per pod) and the output is a\n",
      "single integer (the number of pod replicas). \n",
      " When the Autoscaler is configured to consider only a single metric, calculating the\n",
      "required replica count is simple. All it takes is summing up the metrics values of all\n",
      "the pods, dividing that by the target value set on the HorizontalPodAutoscaler\n",
      "resource, and then rounding it up to the next-larger integer. The actual calculation is\n",
      "a bit more involved than this, because it also makes sure the Autoscaler doesn’t thrash\n",
      "around when the metric value is unstable and changes rapidly. \n",
      " When autoscaling is based on multiple pod metrics (for example, both CPU usage\n",
      "and Queries-Per-Second [QPS]), the calculation isn’t that much more complicated.\n",
      "The Autoscaler calculates the replica count for each metric individually and then\n",
      "takes the highest value (for example, if four pods are required to achieve the target\n",
      "CPU usage, and three pods are required to achieve the target QPS, the Autoscaler will\n",
      "scale to four pods). Figure 15.2 shows this example.\n",
      "A look at changes related to how the Autoscaler obtains metrics\n",
      "Prior to Kubernetes version 1.6, the HorizontalPodAutoscaler obtained the metrics\n",
      "from Heapster directly. In version 1.8, the Autoscaler can get the metrics through an\n",
      "aggregated version of the resource metrics API by starting the Controller Manager\n",
      "with the --horizontal-pod-autoscaler-use-rest-clients=true flag. From ver-\n",
      "sion 1.9, this behavior will be enabled by default.\n",
      "The core API server will not expose the metrics itself. From version 1.7, Kubernetes\n",
      "allows registering multiple API servers and making them appear as a single API\n",
      "server. This allows it to expose metrics through one of those underlying API servers.\n",
      "We’ll explain API server aggregation in the last chapter. \n",
      "Selecting what metrics collector to use in their clusters will be up to cluster adminis-\n",
      "trators. A simple translation layer is usually required to expose the metrics in the\n",
      "appropriate API paths and in the appropriate format.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 472, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "440\n",
      "CHAPTER 15\n",
      "Automatic scaling of pods and cluster nodes\n",
      "UPDATING THE DESIRED REPLICA COUNT ON THE SCALED RESOURCE\n",
      "The final step of an autoscaling operation is updating the desired replica count field\n",
      "on the scaled resource object (a ReplicaSet, for example) and then letting the Replica-\n",
      "Set controller take care of spinning up additional pods or deleting excess ones.\n",
      " The Autoscaler controller modifies the replicas field of the scaled resource\n",
      "through the Scale sub-resource. It enables the Autoscaler to do its work without know-\n",
      "ing any details of the resource it’s scaling, except for what’s exposed through the Scale\n",
      "sub-resource (see figure 15.3).\n",
      "This allows the Autoscaler to operate on any scalable resource, as long as the API\n",
      "server exposes the Scale sub-resource for it. Currently, it’s exposed for\n",
      "Deployments\n",
      "ReplicaSets\n",
      "ReplicationControllers\n",
      "StatefulSets\n",
      "These are currently the only objects you can attach an Autoscaler to.\n",
      "Pod 1\n",
      "CPU\n",
      "utilization\n",
      "QPS\n",
      "Pod 2\n",
      "Pod 3\n",
      "Target\n",
      "CPU utilization\n",
      "Target QPS\n",
      "Replicas: 4\n",
      "Replicas: 3\n",
      "Replicas: 4\n",
      "30\n",
      "12\n",
      "15\n",
      "20\n",
      "(15 + 30 + 12) / 20 = 57 / 20\n",
      "(60 + 90 + 50) / 50 = 200 / 50\n",
      "Max(4, 3)\n",
      "50%\n",
      "60%\n",
      "90%\n",
      "50%\n",
      "Figure 15.2\n",
      "Calculating the number of replicas from two metrics\n",
      "Autoscaler adjusts replicas (++ or --)\n",
      "Horizontal Pod Autoscaler\n",
      "Deployment, ReplicaSet,\n",
      "StatefulSet, or\n",
      "ReplicationController\n",
      "Scale\n",
      "sub-resource\n",
      "Figure 15.3\n",
      "The Horizontal Pod Autoscaler modifies only on the Scale sub-resource.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 473, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "441\n",
      "Horizontal pod autoscaling\n",
      "UNDERSTANDING THE WHOLE AUTOSCALING PROCESS\n",
      "You now understand the three steps involved in autoscaling, so let’s visualize all the\n",
      "components involved in the autoscaling process. They’re shown in figure 15.4.\n",
      "The arrows leading from the pods to the cAdvisors, which continue on to Heapster\n",
      "and finally to the Horizontal Pod Autoscaler, indicate the direction of the flow of met-\n",
      "rics data. It’s important to be aware that each component gets the metrics from the\n",
      "other components periodically (that is, cAdvisor gets the metrics from the pods in a\n",
      "continuous loop; the same is also true for Heapster and for the HPA controller). The\n",
      "end effect is that it takes quite a while for the metrics data to be propagated and a res-\n",
      "caling action to be performed. It isn’t immediate. Keep this in mind when you observe\n",
      "the Autoscaler in action next.\n",
      "15.1.2 Scaling based on CPU utilization\n",
      "Perhaps the most important metric you’ll want to base autoscaling on is the amount of\n",
      "CPU consumed by the processes running inside your pods. Imagine having a few pods\n",
      "providing a service. When their CPU usage reaches 100% it’s obvious they can’t cope\n",
      "with the demand anymore and need to be scaled either up (vertical scaling—increas-\n",
      "ing the amount of CPU the pods can use) or out (horizontal scaling—increasing the\n",
      "number of pods). Because we’re talking about the horizontal pod autoscaler here,\n",
      "Autoscaler adjusts\n",
      "replicas (++ or --)\n",
      "Heapster collects\n",
      "metrics from all nodes\n",
      "cAdvisor collects metrics\n",
      "from all containers on a node\n",
      "Deployment\n",
      "ReplicaSet\n",
      "Autoscaler collects\n",
      "metrics from Heapster\n",
      "Kubelet\n",
      "cAdvisor\n",
      "Node 1\n",
      "Pod\n",
      "Pod\n",
      "Kubelet\n",
      "cAdvisor\n",
      "Node 2\n",
      "Pod\n",
      "Node X\n",
      "Heapster\n",
      "Horizontal Pod\n",
      "Autoscaler\n",
      "Figure 15.4\n",
      "How the autoscaler obtains metrics and rescales the target deployment \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 474, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "442\n",
      "CHAPTER 15\n",
      "Automatic scaling of pods and cluster nodes\n",
      "we’re only focusing on scaling out (increasing the number of pods). By doing that,\n",
      "the average CPU usage should come down. \n",
      " Because CPU usage is usually unstable, it makes sense to scale out even before the\n",
      "CPU is completely swamped—perhaps when the average CPU load across the pods\n",
      "reaches or exceeds 80%. But 80% of what, exactly?\n",
      "TIP\n",
      "Always set the target CPU usage well below 100% (and definitely never\n",
      "above 90%) to leave enough room for handling sudden load spikes.\n",
      "As you may remember from the previous chapter, the process running inside a con-\n",
      "tainer is guaranteed the amount of CPU requested through the resource requests\n",
      "specified for the container. But at times when no other processes need CPU, the pro-\n",
      "cess may use all the available CPU on the node. When someone says a pod is consum-\n",
      "ing 80% of the CPU, it’s not clear if they mean 80% of the node’s CPU, 80% of the\n",
      "pod’s guaranteed CPU (the resource request), or 80% of the hard limit configured\n",
      "for the pod through resource limits. \n",
      " As far as the Autoscaler is concerned, only the pod’s guaranteed CPU amount (the\n",
      "CPU requests) is important when determining the CPU utilization of a pod. The Auto-\n",
      "scaler compares the pod’s actual CPU consumption and its CPU requests, which\n",
      "means the pods you’re autoscaling need to have CPU requests set (either directly or\n",
      "indirectly through a LimitRange object) for the Autoscaler to determine the CPU uti-\n",
      "lization percentage.\n",
      "CREATING A HORIZONTALPODAUTOSCALER BASED ON CPU USAGE\n",
      "Let’s see how to create a HorizontalPodAutoscaler now and configure it to scale pods\n",
      "based on their CPU utilization. You’ll create a Deployment similar to the one in chap-\n",
      "ter 9, but as we’ve discussed, you’ll need to make sure the pods created by the Deploy-\n",
      "ment all have the CPU resource requests specified in order to make autoscaling\n",
      "possible. You’ll have to add a CPU resource request to the Deployment’s pod tem-\n",
      "plate, as shown in the following listing.\n",
      "apiVersion: extensions/v1beta1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: kubia\n",
      "spec:\n",
      "  replicas: 3                \n",
      "  template:\n",
      "    metadata:\n",
      "      name: kubia\n",
      "      labels:\n",
      "        app: kubia\n",
      "    spec:\n",
      "      containers:\n",
      "      - image: luksa/kubia:v1     \n",
      "        name: nodejs\n",
      "Listing 15.1\n",
      "Deployment with CPU requests set: deployment.yaml\n",
      "Manually setting the \n",
      "(initial) desired number \n",
      "of replicas to three\n",
      "Running the \n",
      "kubia:v1 image\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 475, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "443\n",
      "Horizontal pod autoscaling\n",
      "        resources:              \n",
      "          requests:             \n",
      "            cpu: 100m           \n",
      "This is a regular Deployment object—it doesn’t use autoscaling yet. It will run three\n",
      "instances of the kubia NodeJS app, with each instance requesting 100 millicores\n",
      "of CPU. \n",
      " After creating the Deployment, to enable horizontal autoscaling of its pods, you\n",
      "need to create a HorizontalPodAutoscaler (HPA) object and point it to the Deploy-\n",
      "ment. You could prepare and post the YAML manifest for the HPA, but an easier way\n",
      "exists—using the kubectl autoscale command:\n",
      "$ kubectl autoscale deployment kubia --cpu-percent=30 --min=1 --max=5\n",
      "deployment \"kubia\" autoscaled\n",
      "This creates the HPA object for you and sets the Deployment called kubia as the scal-\n",
      "ing target. You’re setting the target CPU utilization of the pods to 30% and specifying\n",
      "the minimum and maximum number of replicas. The Autoscaler will constantly keep\n",
      "adjusting the number of replicas to keep their CPU utilization around 30%, but it will\n",
      "never scale down to less than one or scale up to more than five replicas. \n",
      "TIP\n",
      "Always make sure to autoscale Deployments instead of the underlying\n",
      "ReplicaSets. This way, you ensure the desired replica count is preserved across\n",
      "application updates (remember that a Deployment creates a new ReplicaSet\n",
      "for each version). The same rule applies to manual scaling, as well.\n",
      "Let’s look at the definition of the HorizontalPodAutoscaler resource to gain a better\n",
      "understanding of it. It’s shown in the following listing.\n",
      "$ kubectl get hpa.v2beta1.autoscaling kubia -o yaml\n",
      "apiVersion: autoscaling/v2beta1            \n",
      "kind: HorizontalPodAutoscaler              \n",
      "metadata:\n",
      "  name: kubia               \n",
      "  ...\n",
      "spec:\n",
      "  maxReplicas: 5                   \n",
      "  metrics:                              \n",
      "  - resource:                           \n",
      "      name: cpu                         \n",
      "      targetAverageUtilization: 30      \n",
      "    type: Resource                      \n",
      "  minReplicas: 1                   \n",
      "  scaleTargetRef:                          \n",
      "    apiVersion: extensions/v1beta1         \n",
      "    kind: Deployment                       \n",
      "    name: kubia                            \n",
      "Listing 15.2\n",
      "A HorizontalPodAutoscaler YAML definition\n",
      "Requesting 100 millicores \n",
      "of CPU per pod\n",
      "HPA resources are in the \n",
      "autoscaling API group.\n",
      "Each HPA has a name (it doesn’t \n",
      "need to match the name of the \n",
      "Deployment as in this case).\n",
      "The\n",
      "minimum\n",
      "and\n",
      "maximum\n",
      "number of\n",
      "replicas\n",
      "you\n",
      "specified\n",
      "You’d like the Autoscaler to \n",
      "adjust the number of pods \n",
      "so they each utilize 30% of \n",
      "requested CPU.\n",
      "The target resource \n",
      "which this Autoscaler \n",
      "will act upon\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 476, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "444\n",
      "CHAPTER 15\n",
      "Automatic scaling of pods and cluster nodes\n",
      "status:\n",
      "  currentMetrics: []        \n",
      "  currentReplicas: 3        \n",
      "  desiredReplicas: 0        \n",
      "NOTE\n",
      "Multiple versions of HPA resources exist: the new autoscaling/v2beta1\n",
      "and the old autoscaling/v1. You’re requesting the new version here.\n",
      "SEEING THE FIRST AUTOMATIC RESCALE EVENT\n",
      "It takes a while for cAdvisor to get the CPU metrics and for Heapster to collect them\n",
      "before the Autoscaler can take action. During that time, if you display the HPA resource\n",
      "with kubectl get, the TARGETS column will show <unknown>:\n",
      "$ kubectl get hpa\n",
      "NAME      REFERENCE          TARGETS           MINPODS   MAXPODS   REPLICAS\n",
      "kubia     Deployment/kubia   <unknown> / 30%   1         5         0       \n",
      "Because you’re running three pods that are currently receiving no requests, which\n",
      "means their CPU usage should be close to zero, you should expect the Autoscaler to\n",
      "scale them down to a single pod, because even with a single pod, the CPU utilization\n",
      "will still be below the 30% target. \n",
      " And sure enough, the autoscaler does exactly that. It soon scales the Deployment\n",
      "down to a single replica:\n",
      "$ kubectl get deployment\n",
      "NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\n",
      "kubia     1         1         1            1           23m\n",
      "Remember, the autoscaler only adjusts the desired replica count on the Deployment.\n",
      "The Deployment controller then takes care of updating the desired replica count on\n",
      "the ReplicaSet object, which then causes the ReplicaSet controller to delete two excess\n",
      "pods, leaving one pod running.\n",
      " You can use kubectl describe to see more information on the HorizontalPod-\n",
      "Autoscaler and the operation of the underlying controller, as the following listing shows.\n",
      "$ kubectl describe hpa\n",
      "Name:                             kubia\n",
      "Namespace:                        default\n",
      "Labels:                           <none>\n",
      "Annotations:                      <none>\n",
      "CreationTimestamp:                Sat, 03 Jun 2017 12:59:57 +0200\n",
      "Reference:                        Deployment/kubia\n",
      "Metrics:                          ( current / target )\n",
      "  resource cpu on pods  \n",
      "  (as a percentage of request):   0% (0) / 30%\n",
      "Min replicas:                     1\n",
      "Max replicas:                     5\n",
      "Listing 15.3\n",
      "Inspecting a HorizontalPodAutoscaler with kubectl describe\n",
      "The current status \n",
      "of the Autoscaler\n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 477, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "445\n",
      "Horizontal pod autoscaling\n",
      "Events:\n",
      "From                        Reason              Message\n",
      "----                        ------              ---\n",
      "horizontal-pod-autoscaler   SuccessfulRescale   New size: 1; reason: All \n",
      "                                                metrics below target\n",
      "NOTE\n",
      "The output has been modified to make it more readable.\n",
      "Turn your focus to the table of events at the bottom of the listing. You see the horizon-\n",
      "tal pod autoscaler has successfully rescaled to one replica, because all metrics were\n",
      "below target. \n",
      "TRIGGERING A SCALE-UP\n",
      "You’ve already witnessed your first automatic rescale event (a scale-down). Now, you’ll\n",
      "start sending requests to your pod, thereby increasing its CPU usage, and you should\n",
      "see the autoscaler detect this and start up additional pods.\n",
      " You’ll need to expose the pods through a Service, so you can hit all of them through\n",
      "a single URL. You may remember that the easiest way to do that is with kubectl expose:\n",
      "$ kubectl expose deployment kubia --port=80 --target-port=8080\n",
      "service \"kubia\" exposed\n",
      "Before you start hitting your pod(s) with requests, you may want to run the follow-\n",
      "ing command in a separate terminal to keep an eye on what’s happening with the\n",
      "HorizontalPodAutoscaler and the Deployment, as shown in the following listing.\n",
      "$ watch -n 1 kubectl get hpa,deployment\n",
      "Every \n",
      "1.0s: \n",
      "kubectl \n",
      "get \n",
      "hpa,deployment \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "NAME        REFERENCE          TARGETS    MINPODS   MAXPODS   REPLICAS  AGE\n",
      "hpa/kubia   Deployment/kubia   0% / 30%   1         5         1         45m\n",
      "NAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\n",
      "deploy/kubia   1         1         1            1           56m\n",
      "TIP\n",
      "List multiple resource types with kubectl get by delimiting them with\n",
      "a comma. \n",
      "If you’re using OSX, you’ll have to replace the watch command with a loop, manually\n",
      "run kubectl get periodically, or use kubectl’s --watch option. But although a plain\n",
      "kubectl get can show multiple types of resources at once, that’s not the case when\n",
      "using the aforementioned --watch option, so you’ll need to use two terminals if you\n",
      "want to watch both the HPA and the Deployment objects. \n",
      " Keep an eye on the state of those two objects while you run a load-generating pod.\n",
      "You’ll run the following command in another terminal:\n",
      "$ kubectl run -it --rm --restart=Never loadgenerator --image=busybox \n",
      "➥ -- sh -c \"while true; do wget -O - -q http://kubia.default; done\"\n",
      "Listing 15.4\n",
      "Watching multiple resources in parallel\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 478, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "446\n",
      "CHAPTER 15\n",
      "Automatic scaling of pods and cluster nodes\n",
      "This will run a pod which repeatedly hits the kubia Service. You’ve seen the -it\n",
      "option a few times when running the kubectl exec command. As you can see, it can\n",
      "also be used with kubectl run. It allows you to attach the console to the process,\n",
      "which will not only show you the process’ output directly, but will also terminate the\n",
      "process as soon as you press CTRL+C. The --rm option causes the pod to be deleted\n",
      "afterward, and the --restart=Never option causes kubectl run to create an unman-\n",
      "aged pod directly instead of through a Deployment object, which you don’t need.\n",
      "This combination of options is useful for running commands inside the cluster with-\n",
      "out having to piggyback on an existing pod. It not only behaves the same as if you\n",
      "were running the command locally, it even cleans up everything when the command\n",
      "terminates. \n",
      "SEEING THE AUTOSCALER SCALE UP THE DEPLOYMENT\n",
      "As the load-generator pod runs, you’ll see it initially hitting the single pod. As before,\n",
      "it takes time for the metrics to be updated, but when they are, you’ll see the autoscaler\n",
      "increase the number of replicas. In my case, the pod’s CPU utilization initially jumped\n",
      "to 108%, which caused the autoscaler to increase the number of pods to four. The\n",
      "utilization on the individual pods then decreased to 74% and then stabilized at\n",
      "around 26%. \n",
      "NOTE\n",
      "If the CPU load in your case doesn’t exceed 30%, try running addi-\n",
      "tional load-generators.\n",
      "Again, you can inspect autoscaler events with kubectl describe to see what the\n",
      "autoscaler has done (only the most important information is shown in the following\n",
      "listing).\n",
      "From    Reason              Message\n",
      "----    ------              -------\n",
      "h-p-a   SuccessfulRescale   New size: 1; reason: All metrics below target\n",
      "h-p-a   SuccessfulRescale   New size: 4; reason: cpu resource utilization \n",
      "                            (percentage of request) above target\n",
      "Does it strike you as odd that the initial average CPU utilization in my case, when I\n",
      "only had one pod, was 108%, which is more than 100%? Remember, a container’s\n",
      "CPU utilization is the container’s actual CPU usage divided by its requested CPU. The\n",
      "requested CPU defines the minimum, not maximum amount of CPU available to the\n",
      "container, so a container may consume more than the requested CPU, bringing the\n",
      "percentage over 100. \n",
      " Before we go on, let’s do a little math and see how the autoscaler concluded that\n",
      "four replicas are needed. Initially, there was one replica handling requests and its\n",
      "CPU usage spiked to 108%. Dividing 108 by 30 (the target CPU utilization percent-\n",
      "age) gives 3.6, which the autoscaler then rounded up to 4. If you divide 108 by 4, you\n",
      "Listing 15.5\n",
      "Events of a HorizontalPodAutoscaler\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 479, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "447\n",
      "Horizontal pod autoscaling\n",
      "get 27%. If the autoscaler scales up to four pods, their average CPU utilization is\n",
      "expected to be somewhere in the neighborhood of 27%, which is close to the target\n",
      "value of 30% and almost exactly what the observed CPU utilization was.\n",
      "UNDERSTANDING THE MAXIMUM RATE OF SCALING\n",
      "In my case, the CPU usage shot up to 108%, but in general, the initial CPU usage\n",
      "could spike even higher. Even if the initial average CPU utilization was higher (say\n",
      "150%), requiring five replicas to achieve the 30% target, the autoscaler would still\n",
      "only scale up to four pods in the first step, because it has a limit on how many repli-\n",
      "cas can be added in a single scale-up operation. The autoscaler will at most double\n",
      "the number of replicas in a single operation, if more than two current replicas\n",
      "exist. If only one or two exist, it will scale up to a maximum of four replicas in a sin-\n",
      "gle step. \n",
      " Additionally, it has a limit on how soon a subsequent autoscale operation can\n",
      "occur after the previous one. Currently, a scale-up will occur only if no rescaling\n",
      "event occurred in the last three minutes. A scale-down event is performed even less\n",
      "frequently—every five minutes. Keep this in mind so you don’t wonder why the\n",
      "autoscaler refuses to perform a rescale operation even if the metrics clearly show\n",
      "that it should.\n",
      "MODIFYING THE TARGET METRIC VALUE ON AN EXISTING HPA OBJECT\n",
      "To wrap up this section, let’s do one last exercise. Maybe your initial CPU utilization\n",
      "target of 30% was a bit too low, so increase it to 60%. You do this by editing the HPA\n",
      "resource with the kubectl edit command. When the text editor opens, change the\n",
      "targetAverageUtilization field to 60, as shown in the following listing.\n",
      "...\n",
      "spec:\n",
      "  maxReplicas: 5\n",
      "  metrics:\n",
      "  - resource:\n",
      "      name: cpu\n",
      "      targetAverageUtilization: 60    \n",
      "    type: Resource\n",
      "...\n",
      "As with most other resources, after you modify the resource, your changes will be\n",
      "detected by the autoscaler controller and acted upon. You could also delete the\n",
      "resource and recreate it with different target values, because by deleting the HPA\n",
      "resource, you only disable autoscaling of the target resource (a Deployment in this\n",
      "case) and leave it at the scale it is at that time. The automatic scaling will resume after\n",
      "you create a new HPA resource for the Deployment.\n",
      "Listing 15.6\n",
      "Increasing the target CPU utilization by editing the HPA resource\n",
      "Change this \n",
      "from 30 to 60.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 480, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "448\n",
      "CHAPTER 15\n",
      "Automatic scaling of pods and cluster nodes\n",
      "15.1.3 Scaling based on memory consumption\n",
      "You’ve seen how easily the horizontal Autoscaler can be configured to keep CPU uti-\n",
      "lization at the target level. But what about autoscaling based on the pods’ memory\n",
      "usage? \n",
      " Memory-based autoscaling is much more problematic than CPU-based autoscal-\n",
      "ing. The main reason is because after scaling up, the old pods would somehow need to\n",
      "be forced to release memory. This needs to be done by the app itself—it can’t be done\n",
      "by the system. All the system could do is kill and restart the app, hoping it would use\n",
      "less memory than before. But if the app then uses the same amount as before, the\n",
      "Autoscaler would scale it up again. And again, and again, until it reaches the maxi-\n",
      "mum number of pods configured on the HPA resource. Obviously, this isn’t what any-\n",
      "one wants. Memory-based autoscaling was introduced in Kubernetes version 1.8, and\n",
      "is configured exactly like CPU-based autoscaling. Exploring it is left up to the reader.\n",
      "15.1.4 Scaling based on other and custom metrics\n",
      "You’ve seen how easy it is to scale pods based on their CPU usage. Initially, this was the\n",
      "only autoscaling option that was usable in practice. To have the autoscaler use custom,\n",
      "app-defined metrics to drive its autoscaling decisions was fairly complicated. The ini-\n",
      "tial design of the autoscaler didn’t make it easy to move beyond simple CPU-based\n",
      "scaling. This prompted the Kubernetes Autoscaling Special Interest Group (SIG) to\n",
      "redesign the autoscaler completely. \n",
      " If you’re interested in learning how complicated it was to use the initial autoscaler\n",
      "with custom metrics, I invite you to read my blog post entitled “Kubernetes autoscal-\n",
      "ing based on custom metrics without using a host port,” which you’ll find online at\n",
      "http:/\n",
      "/medium.com/@marko.luksa. You’ll learn about all the other problems I\n",
      "encountered when trying to set up autoscaling based on custom metrics. Luckily,\n",
      "newer versions of Kubernetes don’t have those problems. I’ll cover the subject in a\n",
      "new blog post. \n",
      " Instead of going through a complete example here, let’s quickly go over how to\n",
      "configure the autoscaler to use different metrics sources. We’ll start by examining how\n",
      "we defined what metric to use in our previous example. The following listing shows\n",
      "how your previous HPA object was configured to use the CPU usage metric.\n",
      "...\n",
      "spec:\n",
      "  maxReplicas: 5\n",
      "  metrics:\n",
      "  - type: Resource      \n",
      "    resource:\n",
      "      name: cpu                      \n",
      "      targetAverageUtilization: 30    \n",
      "...\n",
      "Listing 15.7\n",
      "HorizontalPodAutoscaler definition for CPU-based autoscaling\n",
      "Defines the type \n",
      "of metric\n",
      "The resource, whose \n",
      "utilization will be monitored\n",
      "The target utilization \n",
      "of this resource\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 481, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "449\n",
      "Horizontal pod autoscaling\n",
      "As you can see, the metrics field allows you to define more than one metric to use.\n",
      "In the listing, you’re using a single metric. Each entry defines the type of metric—\n",
      "in this case, a Resource metric. You have three types of metrics you can use in an\n",
      "HPA object:\n",
      "\n",
      "Resource\n",
      "\n",
      "Pods\n",
      "\n",
      "Object\n",
      "UNDERSTANDING THE RESOURCE METRIC TYPE\n",
      "The Resource type makes the autoscaler base its autoscaling decisions on a resource\n",
      "metric, like the ones specified in a container’s resource requests. We’ve already seen\n",
      "how to do that, so let’s focus on the other two types.\n",
      "UNDERSTANDING THE PODS METRIC TYPE\n",
      "The Pods type is used to refer to any other (including custom) metric related to the\n",
      "pod directly. An example of such a metric could be the already mentioned Queries-\n",
      "Per-Second (QPS) or the number of messages in a message broker’s queue (when the\n",
      "message broker is running as a pod). To configure the autoscaler to use the pod’s QPS\n",
      "metric, the HPA object would need to include the entry shown in the following listing\n",
      "under its metrics field.\n",
      "...\n",
      "spec:\n",
      "  metrics:\n",
      "  - type: Pods              \n",
      "    resource:\n",
      "      metricName: qps             \n",
      "      targetAverageValue: 100    \n",
      "...\n",
      "The example in the listing configures the autoscaler to keep the average QPS of all\n",
      "the pods managed by the ReplicaSet (or other) controller targeted by this HPA\n",
      "resource at 100. \n",
      "UNDERSTANDING THE OBJECT METRIC TYPE\n",
      "The Object metric type is used when you want to make the autoscaler scale pods\n",
      "based on a metric that doesn’t pertain directly to those pods. For example, you may\n",
      "want to scale pods according to a metric of another cluster object, such as an Ingress\n",
      "object. The metric could be QPS as in listing 15.8, the average request latency, or\n",
      "something else completely. \n",
      " Unlike in the previous case, where the autoscaler needed to obtain the metric for\n",
      "all targeted pods and then use the average of those values, when you use an Object\n",
      "metric type, the autoscaler obtains a single metric from the single object. In the HPA\n",
      "Listing 15.8\n",
      "Referring to a custom pod metric in the HPA\n",
      "Defines a pod metric\n",
      "The name of \n",
      "the metric\n",
      "The target average value \n",
      "across all targeted pods\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 482, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "450\n",
      "CHAPTER 15\n",
      "Automatic scaling of pods and cluster nodes\n",
      "definition, you need to specify the target object and the target value. The following\n",
      "listing shows an example.\n",
      "...\n",
      "spec:\n",
      "  metrics:\n",
      "  - type: Object                   \n",
      "    resource:\n",
      "      metricName: latencyMillis           \n",
      "      target: \n",
      "        apiVersion: extensions/v1beta1     \n",
      "        kind: Ingress                      \n",
      "        name: frontend                     \n",
      "      targetValue: 20                   \n",
      "  scaleTargetRef:                          \n",
      "    apiVersion: extensions/v1beta1         \n",
      "    kind: Deployment                       \n",
      "    name: kubia                            \n",
      "...\n",
      "In this example, the HPA is configured to use the latencyMillis metric of the\n",
      "frontend Ingress object. The target value for the metric is 20. The horizontal pod\n",
      "autoscaler will monitor the Ingress’ metric and if it rises too far above the target value,\n",
      "the autoscaler will scale the kubia Deployment resource. \n",
      "15.1.5 Determining which metrics are appropriate for autoscaling\n",
      "You need to understand that not all metrics are appropriate for use as the basis of\n",
      "autoscaling. As mentioned previously, the pods’ containers’ memory consumption isn’t\n",
      "a good metric for autoscaling. The autoscaler won’t function properly if increasing\n",
      "the number of replicas doesn’t result in a linear decrease of the average value of the\n",
      "observed metric (or at least close to linear). \n",
      " For example, if you have only a single pod instance and the value of the metric is X\n",
      "and the autoscaler scales up to two replicas, the metric needs to fall to somewhere\n",
      "close to X/2. An example of such a custom metric is Queries per Second (QPS),\n",
      "which in the case of web applications reports the number of requests the application\n",
      "is receiving per second. Increasing the number of replicas will always result in a pro-\n",
      "portionate decrease of QPS, because a greater number of pods will be handling the\n",
      "same total number of requests. \n",
      " Before you decide to base the autoscaler on your app’s own custom metric, be sure\n",
      "to think about how its value will behave when the number of pods increases or\n",
      "decreases.\n",
      "15.1.6 Scaling down to zero replicas\n",
      "The horizontal pod autoscaler currently doesn’t allow setting the minReplicas field\n",
      "to 0, so the autoscaler will never scale down to zero, even if the pods aren’t doing\n",
      "Listing 15.9\n",
      "Referring to a metric of a different object in the HPA\n",
      "Use metric of a \n",
      "specific object\n",
      "The name of \n",
      "the metric\n",
      "The specific object whose metric \n",
      "the autoscaler should obtain\n",
      "The\n",
      "Autoscaler\n",
      "should\n",
      "scale so\n",
      "the value\n",
      "of the\n",
      "metric\n",
      "stays close\n",
      "to this.\n",
      "The scalable resource the \n",
      "autoscaler will scale\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 483, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "451\n",
      "Vertical pod autoscaling\n",
      "anything. Allowing the number of pods to be scaled down to zero can dramatically\n",
      "increase the utilization of your hardware. When you run services that get requests only\n",
      "once every few hours or even days, it doesn’t make sense to have them running all the\n",
      "time, eating up resources that could be used by other pods. But you still want to have\n",
      "those services available immediately when a client request comes in. \n",
      " This is known as idling and un-idling. It allows pods that provide a certain service\n",
      "to be scaled down to zero. When a new request comes in, the request is blocked until\n",
      "the pod is brought up and then the request is finally forwarded to the pod. \n",
      " Kubernetes currently doesn’t provide this feature yet, but it will eventually. Check\n",
      "the documentation to see if idling has been implemented yet. \n",
      "15.2\n",
      "Vertical pod autoscaling\n",
      "Horizontal scaling is great, but not every application can be scaled horizontally. For\n",
      "such applications, the only option is to scale them vertically—give them more CPU\n",
      "and/or memory. Because a node usually has more resources than a single pod\n",
      "requests, it should almost always be possible to scale a pod vertically, right? \n",
      " Because a pod’s resource requests are configured through fields in the pod\n",
      "manifest, vertically scaling a pod would be performed by changing those fields. I\n",
      "say “would” because it’s currently not possible to change either resource requests\n",
      "or limits of existing pods. Before I started writing the book (well over a year ago), I\n",
      "was sure that by the time I wrote this chapter, Kubernetes would already support\n",
      "proper vertical pod autoscaling, so I included it in my proposal for the table of con-\n",
      "tents. Sadly, what seems like a lifetime later, vertical pod autoscaling is still not\n",
      "available yet. \n",
      "15.2.1 Automatically configuring resource requests\n",
      "An experimental feature sets the CPU and memory requests on newly created pods, if\n",
      "their containers don’t have them set explicitly. The feature is provided by an Admission\n",
      "Control plugin called InitialResources. When a new pod without resource requests is\n",
      "created, the plugin looks at historical resource usage data of the pod’s containers (per\n",
      "the underlying container image and tag) and sets the requests accordingly. \n",
      " You can deploy pods without specifying resource requests and rely on Kubernetes\n",
      "to eventually figure out what each container’s resource needs are. Effectively, Kuber-\n",
      "netes is vertically scaling the pod. For example, if a container keeps running out of\n",
      "memory, the next time a pod with that container image is created, its resource request\n",
      "for memory will be set higher automatically.\n",
      "15.2.2 Modifying resource requests while a pod is running\n",
      "Eventually, the same mechanism will be used to modify an existing pod’s resource\n",
      "requests, which means it will vertically scale the pod while it’s running. As I’m writing\n",
      "this, a new vertical pod autoscaling proposal is being finalized. Please refer to the\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 484, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "452\n",
      "CHAPTER 15\n",
      "Automatic scaling of pods and cluster nodes\n",
      "Kubernetes documentation to find out whether vertical pod autoscaling is already\n",
      "implemented or not.\n",
      "15.3\n",
      "Horizontal scaling of cluster nodes\n",
      "The Horizontal Pod Autoscaler creates additional pod instances when the need for\n",
      "them arises. But what about when all your nodes are at capacity and can’t run any\n",
      "more pods? Obviously, this problem isn’t limited only to when new pod instances are\n",
      "created by the Autoscaler. Even when creating pods manually, you may encounter the\n",
      "problem where none of the nodes can accept the new pods, because the node’s\n",
      "resources are used up by existing pods. \n",
      " In that case, you’d need to delete several of those existing pods, scale them down\n",
      "vertically, or add additional nodes to your cluster. If your Kubernetes cluster is run-\n",
      "ning on premises, you’d need to physically add a new machine and make it part of the\n",
      "Kubernetes cluster. But if your cluster is running on a cloud infrastructure, adding\n",
      "additional nodes is usually a matter of a few clicks or an API call to the cloud infra-\n",
      "structure. This can be done automatically, right?\n",
      " Kubernetes includes the feature to automatically request additional nodes from\n",
      "the cloud provider as soon as it detects additional nodes are needed. This is per-\n",
      "formed by the Cluster Autoscaler.\n",
      "15.3.1 Introducing the Cluster Autoscaler\n",
      "The Cluster Autoscaler takes care of automatically provisioning additional nodes\n",
      "when it notices a pod that can’t be scheduled to existing nodes because of a lack of\n",
      "resources on those nodes. It also de-provisions nodes when they’re underutilized for\n",
      "longer periods of time. \n",
      "REQUESTING ADDITIONAL NODES FROM THE CLOUD INFRASTRUCTURE\n",
      "A new node will be provisioned if, after a new pod is created, the Scheduler can’t\n",
      "schedule it to any of the existing nodes. The Cluster Autoscaler looks out for such\n",
      "pods and asks the cloud provider to start up an additional node. But before doing\n",
      "that, it checks whether the new node can even accommodate the pod. After all, if\n",
      "that’s not the case, it makes no sense to start up such a node.\n",
      " Cloud providers usually group nodes into groups (or pools) of same-sized nodes\n",
      "(or nodes having the same features). The Cluster Autoscaler thus can’t simply say\n",
      "“Give me an additional node.” It needs to also specify the node type.\n",
      " The Cluster Autoscaler does this by examining the available node groups to see if\n",
      "at least one node type would be able to fit the unscheduled pod. If exactly one such\n",
      "node group exists, the Autoscaler can increase the size of the node group to have the\n",
      "cloud provider add another node to the group. If more than one option is available,\n",
      "the Autoscaler must pick the best one. The exact meaning of “best” will obviously\n",
      "need to be configurable. In the worst case, it selects a random one. A simple overview\n",
      "of how the cluster Autoscaler reacts to an unschedulable pod is shown in figure 15.5.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 485, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "453\n",
      "Horizontal scaling of cluster nodes\n",
      "When the new node starts up, the Kubelet on that node contacts the API server and\n",
      "registers the node by creating a Node resource. From then on, the node is part of the\n",
      "Kubernetes cluster and pods can be scheduled to it.\n",
      " Simple, right? What about scaling down?\n",
      "RELINQUISHING NODES\n",
      "The Cluster Autoscaler also needs to scale down the number of nodes when they\n",
      "aren’t being utilized enough. The Autoscaler does this by monitoring the requested\n",
      "CPU and memory on all the nodes. If the CPU and memory requests of all the pods\n",
      "running on a given node are below 50%, the node is considered unnecessary. \n",
      " That’s not the only determining factor in deciding whether to bring a node down.\n",
      "The Autoscaler also checks to see if any system pods are running (only) on that node\n",
      "(apart from those that are run on every node, because they’re deployed by a Daemon-\n",
      "Set, for example). If a system pod is running on a node, the node won’t be relinquished.\n",
      "The same is also true if an unmanaged pod or a pod with local storage is running on the\n",
      "node, because that would cause disruption to the service the pod is providing. In other\n",
      "words, a node will only be returned to the cloud provider if the Cluster Autoscaler\n",
      "knows the pods running on the node will be rescheduled to other nodes.\n",
      " When a node is selected to be shut down, the node is first marked as unschedula-\n",
      "ble and then all the pods running on the node are evicted. Because all those pods\n",
      "belong to ReplicaSets or other controllers, their replacements are created and sched-\n",
      "uled to the remaining nodes (that’s why the node that’s being shut down is first\n",
      "marked as unschedulable).\n",
      "Node group X\n",
      "Node X1\n",
      "1. Autoscaler notices a\n",
      "Pod can’t be scheduled\n",
      "to existing nodes\n",
      "3. Autoscaler scales up the\n",
      "node group selected in\n",
      "previous step\n",
      "2. Autoscaler determines which node\n",
      "type (if any) would be able to ﬁt the\n",
      "pod. If multiple types could ﬁt the\n",
      "pod, it selects one of them.\n",
      "Cluster\n",
      "Autoscaler\n",
      "Pods\n",
      "Node X2\n",
      "Pods\n",
      "Node group Y\n",
      "Node Y1\n",
      "Pods\n",
      "Unschedulable\n",
      "pod\n",
      "Figure 15.5\n",
      "The Cluster Autoscaler scales up when it finds a pod that can’t be scheduled to \n",
      "existing nodes.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 486, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "454\n",
      "CHAPTER 15\n",
      "Automatic scaling of pods and cluster nodes\n",
      "15.3.2 Enabling the Cluster Autoscaler\n",
      "Cluster autoscaling is currently available on\n",
      "Google Kubernetes Engine (GKE)\n",
      "Google Compute Engine (GCE)\n",
      "Amazon Web Services (AWS)\n",
      "Microsoft Azure\n",
      "How you start the Autoscaler depends on where your Kubernetes cluster is running.\n",
      "For your kubia cluster running on GKE, you can enable the Cluster Autoscaler like\n",
      "this:\n",
      "$ gcloud container clusters update kubia --enable-autoscaling \\\n",
      "  --min-nodes=3 --max-nodes=5\n",
      "If your cluster is running on GCE, you need to set three environment variables before\n",
      "running kube-up.sh: \n",
      "\n",
      "KUBE_ENABLE_CLUSTER_AUTOSCALER=true\n",
      "\n",
      "KUBE_AUTOSCALER_MIN_NODES=3\n",
      "\n",
      "KUBE_AUTOSCALER_MAX_NODES=5\n",
      "Refer to the Cluster Autoscaler GitHub repo at https:/\n",
      "/github.com/kubernetes/auto-\n",
      "scaler/tree/master/cluster-autoscaler for information on how to enable it on other\n",
      "platforms. \n",
      "NOTE\n",
      "The Cluster Autoscaler publishes its status to the cluster-autoscaler-\n",
      "status ConfigMap in the kube-system namespace.\n",
      "15.3.3 Limiting service disruption during cluster scale-down\n",
      "When a node fails unexpectedly, nothing you can do will prevent its pods from becom-\n",
      "ing unavailable. But when a node is shut down voluntarily, either by the Cluster Auto-\n",
      "scaler or by a human operator, you can make sure the operation doesn’t disrupt the\n",
      "service provided by the pods running on that node through an additional feature.\n",
      "Manually cordoning and draining nodes\n",
      "A node can also be marked as unschedulable and drained manually. Without going\n",
      "into specifics, this is done with the following kubectl commands:\n",
      "\n",
      "kubectl cordon <node> marks the node as unschedulable (but doesn’t do\n",
      "anything with pods running on that node).\n",
      "\n",
      "kubectl drain <node> marks the node as unschedulable and then evicts all\n",
      "the pods from the node.\n",
      "In both cases, no new pods are scheduled to the node until you uncordon it again\n",
      "with kubectl uncordon <node>.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 487, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "455\n",
      "Horizontal scaling of cluster nodes\n",
      " Certain services require that a minimum number of pods always keeps running;\n",
      "this is especially true for quorum-based clustered applications. For this reason, Kuber-\n",
      "netes provides a way of specifying the minimum number of pods that need to keep\n",
      "running while performing these types of operations. This is done by creating a Pod-\n",
      "DisruptionBudget resource.\n",
      " Even though the name of the resource sounds complex, it’s one of the simplest\n",
      "Kubernetes resources available. It contains only a pod label selector and a number\n",
      "specifying the minimum number of pods that must always be available or, starting\n",
      "from Kubernetes version 1.7, the maximum number of pods that can be unavailable.\n",
      "We’ll look at what a PodDisruptionBudget (PDB) resource manifest looks like, but\n",
      "instead of creating it from a YAML file, you’ll create it with kubectl create pod-\n",
      "disruptionbudget and then obtain and examine the YAML later.\n",
      " If you want to ensure three instances of your kubia pod are always running (they\n",
      "have the label app=kubia), create the PodDisruptionBudget resource like this:\n",
      "$ kubectl create pdb kubia-pdb --selector=app=kubia --min-available=3\n",
      "poddisruptionbudget \"kubia-pdb\" created\n",
      "Simple, right? Now, retrieve the PDB’s YAML. It’s shown in the next listing.\n",
      "$ kubectl get pdb kubia-pdb -o yaml\n",
      "apiVersion: policy/v1beta1\n",
      "kind: PodDisruptionBudget\n",
      "metadata:\n",
      "  name: kubia-pdb\n",
      "spec:\n",
      "  minAvailable: 3         \n",
      "  selector:                \n",
      "    matchLabels:           \n",
      "      app: kubia           \n",
      "status:\n",
      "  ...\n",
      "You can also use a percentage instead of an absolute number in the minAvailable\n",
      "field. For example, you could state that 60% of all pods with the app=kubia label need\n",
      "to be running at all times.\n",
      "NOTE\n",
      "Starting with Kubernetes 1.7, the PodDisruptionBudget resource also\n",
      "supports the maxUnavailable field, which you can use instead of min-\n",
      "Available if you want to block evictions when more than that many pods are\n",
      "unavailable. \n",
      "We don’t have much more to say about this resource. As long as it exists, both the\n",
      "Cluster Autoscaler and the kubectl drain command will adhere to it and will never\n",
      "evict a pod with the app=kubia label if that would bring the number of such pods\n",
      "below three. \n",
      "Listing 15.10\n",
      "A PodDisruptionBudget definition\n",
      "How many pods should \n",
      "always be available\n",
      "The label selector that \n",
      "determines which pods \n",
      "this budget applies to\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 488, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "456\n",
      "CHAPTER 15\n",
      "Automatic scaling of pods and cluster nodes\n",
      " For example, if there were four pods altogether and minAvailable was set to three\n",
      "as in the example, the pod eviction process would evict pods one by one, waiting for\n",
      "the evicted pod to be replaced with a new one by the ReplicaSet controller, before\n",
      "evicting another pod. \n",
      "15.4\n",
      "Summary\n",
      "This chapter has shown you how Kubernetes can scale not only your pods, but also\n",
      "your nodes. You’ve learned that\n",
      "Configuring the automatic horizontal scaling of pods is as easy as creating a\n",
      "HorizontalPodAutoscaler object and pointing it to a Deployment, ReplicaSet,\n",
      "or ReplicationController and specifying the target CPU utilization for the pods.\n",
      "Besides having the Horizontal Pod Autoscaler perform scaling operations based\n",
      "on the pods’ CPU utilization, you can also configure it to scale based on your\n",
      "own application-provided custom metrics or metrics related to other objects\n",
      "deployed in the cluster.\n",
      "Vertical pod autoscaling isn’t possible yet.\n",
      "Even cluster nodes can be scaled automatically if your Kubernetes cluster runs\n",
      "on a supported cloud provider.\n",
      "You can run one-off processes in a pod and have the pod stopped and deleted\n",
      "automatically as soon you press CTRL+C by using kubectl run with the -it and\n",
      "--rm options.\n",
      "In the next chapter, you’ll explore advanced scheduling features, such as how to keep\n",
      "certain pods away from certain nodes and how to schedule pods either close together\n",
      "or apart.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 489, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "457\n",
      "Advanced scheduling\n",
      "Kubernetes allows you to affect where pods are scheduled. Initially, this was only\n",
      "done by specifying a node selector in the pod specification, but additional mech-\n",
      "anisms were later added that expanded this functionality. They’re covered in this\n",
      "chapter.\n",
      "16.1\n",
      "Using taints and tolerations to repel pods from \n",
      "certain nodes\n",
      "The first two features related to advanced scheduling that we’ll explore here are\n",
      "the node taints and pods’ tolerations of those taints. They’re used for restricting\n",
      "This chapter covers\n",
      "Using node taints and pod tolerations to keep \n",
      "pods away from certain nodes\n",
      "Defining node affinity rules as an alternative to \n",
      "node selectors\n",
      "Co-locating pods using pod affinity \n",
      "Keeping pods away from each other using pod \n",
      "anti-affinity\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 490, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "458\n",
      "CHAPTER 16\n",
      "Advanced scheduling\n",
      "which pods can use a certain node. A pod can only be scheduled to a node if it toler-\n",
      "ates the node’s taints.\n",
      " This is somewhat different from using node selectors and node affinity, which\n",
      "you’ll learn about later in this chapter. Node selectors and node affinity rules make\n",
      "it possible to select which nodes a pod can or can’t be scheduled to by specifically\n",
      "adding that information to the pod, whereas taints allow rejecting deployment of\n",
      "pods to certain nodes by only adding taints to the node without having to modify\n",
      "existing pods. Pods that you want deployed on a tainted node need to opt in to use\n",
      "the node, whereas with node selectors, pods explicitly specify which node(s) they\n",
      "want to be deployed to.\n",
      "16.1.1 Introducing taints and tolerations\n",
      "The best path to learn about node taints is to see an existing taint. Appendix B shows\n",
      "how to set up a multi-node cluster with the kubeadm tool. By default, the master node\n",
      "in such a cluster is tainted, so only Control Plane pods can be deployed on it. \n",
      "DISPLAYING A NODE’S TAINTS\n",
      "You can see the node’s taints using kubectl describe node, as shown in the follow-\n",
      "ing listing.\n",
      "$ kubectl describe node master.k8s\n",
      "Name:         master.k8s\n",
      "Role:\n",
      "Labels:       beta.kubernetes.io/arch=amd64\n",
      "              beta.kubernetes.io/os=linux\n",
      "              kubernetes.io/hostname=master.k8s\n",
      "              node-role.kubernetes.io/master=\n",
      "Annotations:  node.alpha.kubernetes.io/ttl=0\n",
      "              volumes.kubernetes.io/controller-managed-attach-detach=true\n",
      "Taints:       node-role.kubernetes.io/master:NoSchedule      \n",
      "...\n",
      "The master node has a single taint. Taints have a key, value, and an effect, and are repre-\n",
      "sented as <key>=<value>:<effect>. The master node’s taint shown in the previous\n",
      "listing has the key node-role.kubernetes.io/master, a null value (not shown in the\n",
      "taint), and the effect of NoSchedule. \n",
      " This taint prevents pods from being scheduled to the master node, unless those pods\n",
      "tolerate this taint. The pods that tolerate it are usually system pods (see figure 16.1).\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Listing 16.1\n",
      "Describing the master node in a cluster created with kubeadm\n",
      "The master node \n",
      "has one taint.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 491, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "459\n",
      "Using taints and tolerations to repel pods from certain nodes\n",
      "DISPLAYING A POD’S TOLERATIONS\n",
      "In a cluster installed with kubeadm, the kube-proxy cluster component runs as a pod\n",
      "on every node, including the master node, because master components that run as\n",
      "pods may also need to access Kubernetes Services. To make sure the kube-proxy pod\n",
      "also runs on the master node, it includes the appropriate toleration. In total, the pod\n",
      "has three tolerations, which are shown in the following listing.\n",
      "$ kubectl describe po kube-proxy-80wqm -n kube-system\n",
      "...\n",
      "Tolerations:    node-role.kubernetes.io/master=:NoSchedule\n",
      "                node.alpha.kubernetes.io/notReady=:Exists:NoExecute\n",
      "                node.alpha.kubernetes.io/unreachable=:Exists:NoExecute\n",
      "...\n",
      "As you can see, the first toleration matches the master node’s taint, allowing this kube-\n",
      "proxy pod to be scheduled to the master node. \n",
      "NOTE\n",
      "Disregard the equal sign, which is shown in the pod’s tolerations, but\n",
      "not in the node’s taints. Kubectl apparently displays taints and tolerations dif-\n",
      "ferently when the taint’s/toleration’s value is null.\n",
      "UNDERSTANDING TAINT EFFECTS\n",
      "The two other tolerations on the kube-proxy pod define how long the pod is allowed\n",
      "to run on nodes that aren’t ready or are unreachable (the time in seconds isn’t shown,\n",
      "Listing 16.2\n",
      "A pod’s tolerations\n",
      "System pod may be\n",
      "scheduled to master\n",
      "node because its\n",
      "toleration matches\n",
      "the node’s taint.\n",
      "System pod\n",
      "Master node\n",
      "Taint:\n",
      "node-role.kubernetes.io\n",
      "/master:NoSchedule\n",
      "Toleration:\n",
      "node-role.kubernetes.io\n",
      "/master:NoSchedule\n",
      "Regular pod\n",
      "Regular node\n",
      "No taints\n",
      "No tolerations\n",
      "Pods with no tolerations\n",
      "may only be scheduled\n",
      "to nodes without taints.\n",
      "Figure 16.1\n",
      "A pod is only scheduled to a node if it tolerates the node’s taints.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 492, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "460\n",
      "CHAPTER 16\n",
      "Advanced scheduling\n",
      "but can be seen in the pod’s YAML). Those two tolerations refer to the NoExecute\n",
      "instead of the NoSchedule effect. \n",
      " Each taint has an effect associated with it. Three possible effects exist:\n",
      "\n",
      "NoSchedule, which means pods won’t be scheduled to the node if they don’t tol-\n",
      "erate the taint.\n",
      "\n",
      "PreferNoSchedule is a soft version of NoSchedule, meaning the scheduler will\n",
      "try to avoid scheduling the pod to the node, but will schedule it to the node if it\n",
      "can’t schedule it somewhere else. \n",
      "\n",
      "NoExecute, unlike NoSchedule and PreferNoSchedule that only affect schedul-\n",
      "ing, also affects pods already running on the node. If you add a NoExecute taint\n",
      "to a node, pods that are already running on that node and don’t tolerate the\n",
      "NoExecute taint will be evicted from the node. \n",
      "16.1.2 Adding custom taints to a node\n",
      "Imagine having a single Kubernetes cluster where you run both production and non-\n",
      "production workloads. It’s of the utmost importance that non-production pods never\n",
      "run on the production nodes. This can be achieved by adding a taint to your produc-\n",
      "tion nodes. To add a taint, you use the kubectl taint command:\n",
      "$ kubectl taint node node1.k8s node-type=production:NoSchedule\n",
      "node \"node1.k8s\" tainted\n",
      "This adds a taint with key node-type, value production and the NoSchedule effect. If\n",
      "you now deploy multiple replicas of a regular pod, you’ll see none of them are sched-\n",
      "uled to the node you tainted, as shown in the following listing.\n",
      "$ kubectl run test --image busybox --replicas 5 -- sleep 99999\n",
      "deployment \"test\" created\n",
      "$ kubectl get po -o wide\n",
      "NAME                READY  STATUS    RESTARTS   AGE   IP          NODE\n",
      "test-196686-46ngl   1/1    Running   0          12s   10.47.0.1   node2.k8s\n",
      "test-196686-73p89   1/1    Running   0          12s   10.47.0.7   node2.k8s\n",
      "test-196686-77280   1/1    Running   0          12s   10.47.0.6   node2.k8s\n",
      "test-196686-h9m8f   1/1    Running   0          12s   10.47.0.5   node2.k8s\n",
      "test-196686-p85ll   1/1    Running   0          12s   10.47.0.4   node2.k8s\n",
      "Now, no one can inadvertently deploy pods onto the production nodes. \n",
      "16.1.3 Adding tolerations to pods\n",
      "To deploy production pods to the production nodes, they need to tolerate the taint\n",
      "you added to the nodes. The manifests of your production pods need to include the\n",
      "YAML snippet shown in the following listing.\n",
      " \n",
      "Listing 16.3\n",
      "Deploying pods without a toleration\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 493, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "461\n",
      "Using taints and tolerations to repel pods from certain nodes\n",
      "apiVersion: extensions/v1beta1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: prod\n",
      "spec:\n",
      "  replicas: 5\n",
      "  template:\n",
      "    spec:\n",
      "      ...\n",
      "      tolerations:\n",
      "      - key: node-type         \n",
      "        Operator: Equal        \n",
      "        value: production      \n",
      "        effect: NoSchedule     \n",
      "If you deploy this Deployment, you’ll see its pods get deployed to the production\n",
      "node, as shown in the next listing.\n",
      "$ kubectl get po -o wide\n",
      "NAME                READY  STATUS    RESTARTS   AGE   IP          NODE\n",
      "prod-350605-1ph5h   0/1    Running   0          16s   10.44.0.3   node1.k8s\n",
      "prod-350605-ctqcr   1/1    Running   0          16s   10.47.0.4   node2.k8s\n",
      "prod-350605-f7pcc   0/1    Running   0          17s   10.44.0.6   node1.k8s\n",
      "prod-350605-k7c8g   1/1    Running   0          17s   10.47.0.9   node2.k8s\n",
      "prod-350605-rp1nv   0/1    Running   0          17s   10.44.0.4   node1.k8s\n",
      "As you can see in the listing, production pods were also deployed to node2, which isn’t\n",
      "a production node. To prevent that from happening, you’d also need to taint the non-\n",
      "production nodes with a taint such as node-type=non-production:NoSchedule. Then\n",
      "you’d also need to add the matching toleration to all your non-production pods.\n",
      "16.1.4 Understanding what taints and tolerations can be used for\n",
      "Nodes can have more than one taint and pods can have more than one toleration. As\n",
      "you’ve seen, taints can only have a key and an effect and don’t require a value. Tolera-\n",
      "tions can tolerate a specific value by specifying the Equal operator (that’s also the\n",
      "default operator if you don’t specify one), or they can tolerate any value for a specific\n",
      "taint key if you use the Exists operator.\n",
      "USING TAINTS AND TOLERATIONS DURING SCHEDULING\n",
      "Taints can be used to prevent scheduling of new pods (NoSchedule effect) and to\n",
      "define unpreferred nodes (PreferNoSchedule effect) and even evict existing pods\n",
      "from a node (NoExecute).\n",
      " You can set up taints and tolerations any way you see fit. For example, you could\n",
      "partition your cluster into multiple partitions, allowing your development teams to\n",
      "schedule pods only to their respective nodes. You can also use taints and tolerations\n",
      "Listing 16.4\n",
      "A production Deployment with a toleration: production-deployment.yaml\n",
      "Listing 16.5\n",
      "Pods with the toleration are deployed on production node1\n",
      "This toleration allows the \n",
      "pod to be scheduled to \n",
      "production nodes.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 494, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "462\n",
      "CHAPTER 16\n",
      "Advanced scheduling\n",
      "when several of your nodes provide special hardware and only part of your pods need\n",
      "to use it.\n",
      "CONFIGURING HOW LONG AFTER A NODE FAILURE A POD IS RESCHEDULED\n",
      "You can also use a toleration to specify how long Kubernetes should wait before\n",
      "rescheduling a pod to another node if the node the pod is running on becomes\n",
      "unready or unreachable. If you look at the tolerations of one of your pods, you’ll see\n",
      "two tolerations, which are shown in the following listing.\n",
      "$ kubectl get po prod-350605-1ph5h -o yaml\n",
      "...\n",
      "  tolerations:\n",
      "  - effect: NoExecute                            \n",
      "    key: node.alpha.kubernetes.io/notReady       \n",
      "    operator: Exists                             \n",
      "    tolerationSeconds: 300                       \n",
      "  - effect: NoExecute                              \n",
      "    key: node.alpha.kubernetes.io/unreachable      \n",
      "    operator: Exists                               \n",
      "    tolerationSeconds: 300                         \n",
      "These two tolerations say that this pod tolerates a node being notReady or unreach-\n",
      "able for 300 seconds. The Kubernetes Control Plane, when it detects that a node is no\n",
      "longer ready or no longer reachable, will wait for 300 seconds before it deletes the\n",
      "pod and reschedules it to another node.\n",
      " These two tolerations are automatically added to pods that don’t define them. If\n",
      "that five-minute delay is too long for your pods, you can make the delay shorter by\n",
      "adding those two tolerations to the pod’s spec.\n",
      "NOTE\n",
      "This is currently an alpha feature, so it may change in future versions\n",
      "of Kubernetes. Taint-based evictions also aren’t enabled by default. You enable\n",
      "them by running the Controller Manager with the --feature-gates=Taint-\n",
      "BasedEvictions=true option.\n",
      "16.2\n",
      "Using node affinity to attract pods to certain nodes\n",
      "As you’ve learned, taints are used to keep pods away from certain nodes. Now you’ll\n",
      "learn about a newer mechanism called node affinity, which allows you to tell Kuberne-\n",
      "tes to schedule pods only to specific subsets of nodes.\n",
      "COMPARING NODE AFFINITY TO NODE SELECTORS\n",
      "The initial node affinity mechanism in early versions of Kubernetes was the node-\n",
      "Selector field in the pod specification. The node had to include all the labels speci-\n",
      "fied in that field to be eligible to become the target for the pod. \n",
      " Node selectors get the job done and are simple, but they don’t offer everything\n",
      "that you may need. Because of that, a more powerful mechanism was introduced.\n",
      "Listing 16.6\n",
      "Pod with default tolerations\n",
      "The pod tolerates the node being \n",
      "notReady for 300 seconds, before \n",
      "it needs to be rescheduled.\n",
      "The same applies to the \n",
      "node being unreachable.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 495, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "463\n",
      "Using node affinity to attract pods to certain nodes\n",
      "Node selectors will eventually be deprecated, so it’s important you understand the\n",
      "new node affinity rules.\n",
      " Similar to node selectors, each pod can define its own node affinity rules. These\n",
      "allow you to specify either hard requirements or preferences. By specifying a prefer-\n",
      "ence, you tell Kubernetes which nodes you prefer for a specific pod, and Kubernetes\n",
      "will try to schedule the pod to one of those nodes. If that’s not possible, it will choose\n",
      "one of the other nodes. \n",
      "EXAMINING THE DEFAULT NODE LABELS\n",
      "Node affinity selects nodes based on their labels, the same way node selectors do.\n",
      "Before you see how to use node affinity, let’s examine the labels of one of the nodes in\n",
      "a Google Kubernetes Engine cluster (GKE) to see what the default node labels are.\n",
      "They’re shown in the following listing.\n",
      "$ kubectl describe node gke-kubia-default-pool-db274c5a-mjnf\n",
      "Name:     gke-kubia-default-pool-db274c5a-mjnf\n",
      "Role:\n",
      "Labels:   beta.kubernetes.io/arch=amd64\n",
      "          beta.kubernetes.io/fluentd-ds-ready=true\n",
      "          beta.kubernetes.io/instance-type=f1-micro\n",
      "          beta.kubernetes.io/os=linux\n",
      "          cloud.google.com/gke-nodepool=default-pool\n",
      "          failure-domain.beta.kubernetes.io/region=europe-west1         \n",
      "          failure-domain.beta.kubernetes.io/zone=europe-west1-d         \n",
      "          kubernetes.io/hostname=gke-kubia-default-pool-db274c5a-mjnf   \n",
      "The node has many labels, but the last three are the most important when it comes to\n",
      "node affinity and pod affinity, which you’ll learn about later. The meaning of those\n",
      "three labels is as follows:\n",
      "\n",
      "failure-domain.beta.kubernetes.io/region specifies the geographical region\n",
      "the node is located in.\n",
      "\n",
      "failure-domain.beta.kubernetes.io/zone specifies the availability zone the\n",
      "node is in.\n",
      "\n",
      "kubernetes.io/hostname is obviously the node’s hostname.\n",
      "These and other labels can be used in pod affinity rules. In chapter 3, you already\n",
      "learned how you can add a custom label to nodes and use it in a pod’s node selector.\n",
      "You used the custom label to deploy pods only to nodes with that label by adding a node\n",
      "selector to the pods. Now, you’ll see how to do the same using node affinity rules.\n",
      "16.2.1 Specifying hard node affinity rules\n",
      "In the example in chapter 3, you used the node selector to deploy a pod that requires\n",
      "a GPU only to nodes that have a GPU. The pod spec included the nodeSelector field\n",
      "shown in the following listing.\n",
      "Listing 16.7\n",
      "Default labels of a node in GKE\n",
      "These three\n",
      "labels are the\n",
      "most important\n",
      "ones related to\n",
      "node affinity.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 496, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "464\n",
      "CHAPTER 16\n",
      "Advanced scheduling\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: kubia-gpu\n",
      "spec:\n",
      "  nodeSelector:          \n",
      "    gpu: \"true\"          \n",
      "  ...\n",
      "The nodeSelector field specifies that the pod should only be deployed on nodes that\n",
      "include the gpu=true label. If you replace the node selector with a node affinity rule,\n",
      "the pod definition will look like the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: kubia-gpu\n",
      "spec:\n",
      "  affinity:\n",
      "    nodeAffinity:\n",
      "      requiredDuringSchedulingIgnoredDuringExecution:\n",
      "        nodeSelectorTerms:\n",
      "        - matchExpressions:\n",
      "          - key: gpu\n",
      "            operator: In\n",
      "            values:\n",
      "            - \"true\"\n",
      "The first thing you’ll notice is that this is much more complicated than a simple node\n",
      "selector. But that’s because it’s much more expressive. Let’s examine the rule in detail. \n",
      "MAKING SENSE OF THE LONG NODEAFFINITY ATTRIBUTE NAME\n",
      "As you can see, the pod’s spec section contains an affinity field that contains a node-\n",
      "Affinity field, which contains a field with an extremely long name, so let’s focus on\n",
      "that first.\n",
      " Let’s break it down into two parts and examine what they mean:\n",
      "\n",
      "requiredDuringScheduling... means the rules defined under this field spec-\n",
      "ify the labels the node must have for the pod to be scheduled to the node.\n",
      "\n",
      "...IgnoredDuringExecution means the rules defined under the field don’t\n",
      "affect pods already executing on the node. \n",
      "At this point, let me make things easier for you by letting you know that affinity cur-\n",
      "rently only affects pod scheduling and never causes a pod to be evicted from a node.\n",
      "That’s why all the rules right now always end with IgnoredDuringExecution. Eventu-\n",
      "ally, Kubernetes will also support RequiredDuringExecution, which means that if you\n",
      "Listing 16.8\n",
      "A pod using a node selector: kubia-gpu-nodeselector.yaml\n",
      "Listing 16.9\n",
      "A pod using a nodeAffinity rule: kubia-gpu-nodeaffinity.yaml\n",
      "This pod is only scheduled \n",
      "to nodes that have the \n",
      "gpu=true label.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 497, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "465\n",
      "Using node affinity to attract pods to certain nodes\n",
      "remove a label from a node, pods that require the node to have that label will be\n",
      "evicted from such a node. As I’ve said, that’s not yet supported in Kubernetes, so let’s\n",
      "not concern ourselves with the second part of that long field any longer.\n",
      "UNDERSTANDING NODESELECTORTERMS\n",
      "By keeping what was explained in the previous section in mind, it’s easy to understand\n",
      "that the nodeSelectorTerms field and the matchExpressions field define which\n",
      "expressions the node’s labels must match for the pod to be scheduled to the node.\n",
      "The single expression in the example is simple to understand. The node must have a\n",
      "gpu label whose value is set to true. \n",
      " This pod will therefore only be scheduled to nodes that have the gpu=true label, as\n",
      "shown in figure 16.2.\n",
      "Now comes the more interesting part. Node also affinity allows you to prioritize nodes\n",
      "during scheduling. We’ll look at that next.\n",
      "16.2.2 Prioritizing nodes when scheduling a pod\n",
      "The biggest benefit of the newly introduced node affinity feature is the ability to spec-\n",
      "ify which nodes the Scheduler should prefer when scheduling a specific pod. This is\n",
      "done through the preferredDuringSchedulingIgnoredDuringExecution field.\n",
      " Imagine having multiple datacenters across different countries. Each datacenter\n",
      "represents a separate availability zone. In each zone, you have certain machines meant\n",
      "only for your own use and others that your partner companies can use. You now want\n",
      "to deploy a few pods and you’d prefer them to be scheduled to zone1 and to the\n",
      "Node with a GPU\n",
      "Pod\n",
      "Node afﬁnity\n",
      "Required label:\n",
      "gpu=true\n",
      "Pod\n",
      "No node afﬁnity\n",
      "gpu: true\n",
      "Node with a GPU\n",
      "Node without a GPU\n",
      "Node without a GPU\n",
      "gpu: true\n",
      "This pod may be scheduled only\n",
      "to nodes with gpu=true label\n",
      "This pod may be\n",
      "scheduled to any node\n",
      "Figure 16.2\n",
      "A pod’s node affinity specifies which labels a node must have for the pod to be \n",
      "scheduled to it.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 498, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "466\n",
      "CHAPTER 16\n",
      "Advanced scheduling\n",
      "machines reserved for your company’s deployments. If those machines don’t have\n",
      "enough room for the pods or if other important reasons exist that prevent them from\n",
      "being scheduled there, you’re okay with them being scheduled to the machines your\n",
      "partners use and to the other zones. Node affinity allows you to do that.\n",
      "LABELING NODES\n",
      "First, the nodes need to be labeled appropriately. Each node needs to have a label that\n",
      "designates the availability zone the node belongs to and a label marking it as either a\n",
      "dedicated or a shared node.\n",
      " Appendix B explains how to set up a three-node cluster (one master and two\n",
      "worker nodes) in VMs running locally. In the following examples, I’ll use the two worker\n",
      "nodes in that cluster, but you can also use Google Kubernetes Engine or any other\n",
      "multi-node cluster. \n",
      "NOTE\n",
      "Minikube isn’t the best choice for running these examples, because it\n",
      "runs only one node.\n",
      "First, label the nodes, as shown in the next listing.\n",
      "$ kubectl label node node1.k8s availability-zone=zone1\n",
      "node \"node1.k8s\" labeled\n",
      "$ kubectl label node node1.k8s share-type=dedicated\n",
      "node \"node1.k8s\" labeled\n",
      "$ kubectl label node node2.k8s availability-zone=zone2\n",
      "node \"node2.k8s\" labeled\n",
      "$ kubectl label node node2.k8s share-type=shared\n",
      "node \"node2.k8s\" labeled\n",
      "$ kubectl get node -L availability-zone -L share-type\n",
      "NAME         STATUS    AGE       VERSION   AVAILABILITY-ZONE   SHARE-TYPE\n",
      "master.k8s   Ready     4d        v1.6.4    <none>              <none>\n",
      "node1.k8s    Ready     4d        v1.6.4    zone1               dedicated\n",
      "node2.k8s    Ready     4d        v1.6.4    zone2               shared\n",
      "SPECIFYING PREFERENTIAL NODE AFFINITY RULES\n",
      "With the node labels set up, you can now create a Deployment that prefers dedicated\n",
      "nodes in zone1. The following listing shows the Deployment manifest.\n",
      "apiVersion: extensions/v1beta1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: pref\n",
      "spec:\n",
      "  template:\n",
      "    ...\n",
      "    spec:\n",
      "      affinity:\n",
      "        nodeAffinity:\n",
      "Listing 16.10\n",
      "Labeling nodes\n",
      "Listing 16.11\n",
      "Deployment with preferred node affinity: preferred-deployment.yaml\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "234\n",
      "height\n",
      "54\n",
      "PIX BUFFER SIZE\n",
      "37908\n",
      "Original IMG_BUFFER_SIZE\n",
      "37908\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "1\n",
      "height\n",
      "54\n",
      "PIX BUFFER SIZE\n",
      "162\n",
      "Original IMG_BUFFER_SIZE\n",
      "162\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "11\n",
      "height\n",
      "54\n",
      "PIX BUFFER SIZE\n",
      "1782\n",
      "Original IMG_BUFFER_SIZE\n",
      "1782\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "44\n",
      "height\n",
      "70\n",
      "PIX BUFFER SIZE\n",
      "9240\n",
      "Original IMG_BUFFER_SIZE\n",
      "9240\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "261\n",
      "height\n",
      "1\n",
      "PIX BUFFER SIZE\n",
      "783\n",
      "Original IMG_BUFFER_SIZE\n",
      "783\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "234\n",
      "height\n",
      "70\n",
      "PIX BUFFER SIZE\n",
      "49140\n",
      "Original IMG_BUFFER_SIZE\n",
      "49140\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "1\n",
      "height\n",
      "68\n",
      "PIX BUFFER SIZE\n",
      "204\n",
      "Original IMG_BUFFER_SIZE\n",
      "204\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "1\n",
      "height\n",
      "12\n",
      "PIX BUFFER SIZE\n",
      "36\n",
      "Original IMG_BUFFER_SIZE\n",
      "36\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "11\n",
      "height\n",
      "68\n",
      "PIX BUFFER SIZE\n",
      "2244\n",
      "Original IMG_BUFFER_SIZE\n",
      "2244\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "11\n",
      "height\n",
      "12\n",
      "PIX BUFFER SIZE\n",
      "396\n",
      "Original IMG_BUFFER_SIZE\n",
      "396\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "11\n",
      "height\n",
      "70\n",
      "PIX BUFFER SIZE\n",
      "2310\n",
      "Original IMG_BUFFER_SIZE\n",
      "2310\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "44\n",
      "height\n",
      "55\n",
      "PIX BUFFER SIZE\n",
      "7260\n",
      "Original IMG_BUFFER_SIZE\n",
      "7260\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "264\n",
      "height\n",
      "2\n",
      "PIX BUFFER SIZE\n",
      "1584\n",
      "Original IMG_BUFFER_SIZE\n",
      "1584\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "234\n",
      "height\n",
      "15\n",
      "PIX BUFFER SIZE\n",
      "10530\n",
      "Original IMG_BUFFER_SIZE\n",
      "10530\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "1\n",
      "height\n",
      "70\n",
      "PIX BUFFER SIZE\n",
      "210\n",
      "Original IMG_BUFFER_SIZE\n",
      "210\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "44\n",
      "height\n",
      "15\n",
      "PIX BUFFER SIZE\n",
      "1980\n",
      "Original IMG_BUFFER_SIZE\n",
      "1980\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "11\n",
      "height\n",
      "12\n",
      "PIX BUFFER SIZE\n",
      "396\n",
      "Original IMG_BUFFER_SIZE\n",
      "396\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "page_image_dict\n",
      "{'page': 499, 'img_cnt': 17, 'img_npy_lst': []}\n",
      "467\n",
      "Using node affinity to attract pods to certain nodes\n",
      "          preferredDuringSchedulingIgnoredDuringExecution:    \n",
      "          - weight: 80                               \n",
      "            preference:                              \n",
      "              matchExpressions:                      \n",
      "              - key: availability-zone               \n",
      "                operator: In                         \n",
      "                values:                              \n",
      "                - zone1                              \n",
      "          - weight: 20                     \n",
      "            preference:                    \n",
      "              matchExpressions:            \n",
      "              - key: share-type            \n",
      "                operator: In               \n",
      "                values:                    \n",
      "                - dedicated                \n",
      "      ...\n",
      "Let’s examine the listing closely. You’re defining a node affinity preference, instead of\n",
      "a hard requirement. You want the pods scheduled to nodes that include the labels\n",
      "availability-zone=zone1 and share-type=dedicated. You’re saying that the first\n",
      "preference rule is important by setting its weight to 80, whereas the second one is\n",
      "much less important (weight is set to 20).\n",
      "UNDERSTANDING HOW NODE PREFERENCES WORK\n",
      "If your cluster had many nodes, when scheduling the pods of the Deployment in the\n",
      "previous listing, the nodes would be split into four groups, as shown in figure 16.3.\n",
      "Nodes whose availability-zone and share-type labels match the pod’s node affin-\n",
      "ity are ranked the highest. Then, because of how the weights in the pod’s node affinity\n",
      "rules are configured, next come the shared nodes in zone1, then come the dedicated\n",
      "nodes in the other zones, and at the lowest priority are all the other nodes.\n",
      "You’re\n",
      "specifying\n",
      "preferences,\n",
      "not hard\n",
      "requirements.\n",
      "You prefer the pod to be \n",
      "scheduled to zone1. This \n",
      "is your most important \n",
      "preference.\n",
      "You also prefer that your \n",
      "pods be scheduled to \n",
      "dedicated nodes, but this is \n",
      "four times less important \n",
      "than your zone preference.\n",
      "Node\n",
      "Top priority\n",
      "Availability zone 1\n",
      "Pod\n",
      "Priority: 2\n",
      "Priority: 3\n",
      "Priority: 4\n",
      "Node afﬁnity\n",
      "Preferred labels:\n",
      "avail-zone: zone1 (weight 80)\n",
      "share: dedicated (weight 20)\n",
      "avail-zone: zone1\n",
      "share: dedicated\n",
      "Node\n",
      "avail-zone: zone1\n",
      "share: shared\n",
      "Node\n",
      "Availability zone 2\n",
      "avail-zone: zone2\n",
      "share: dedicated\n",
      "Node\n",
      "avail-zone: zone2\n",
      "share: shared\n",
      "This pod may be scheduled to\n",
      "any node, but certain nodes are\n",
      "preferred based on their labels.\n",
      "Figure 16.3\n",
      "Prioritizing nodes based on a pod’s node affinity preferences\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 500, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "468\n",
      "CHAPTER 16\n",
      "Advanced scheduling\n",
      "DEPLOYING THE PODS IN THE TWO-NODE CLUSTER\n",
      "If you create this Deployment in your two-node cluster, you should see most (if not\n",
      "all) of your pods deployed to node1. Examine the following listing to see if that’s true.\n",
      "$ kubectl get po -o wide\n",
      "NAME                READY   STATUS    RESTARTS  AGE   IP          NODE\n",
      "pref-607515-1rnwv   1/1     Running   0         4m    10.47.0.1   node2.k8s\n",
      "pref-607515-27wp0   1/1     Running   0         4m    10.44.0.8   node1.k8s\n",
      "pref-607515-5xd0z   1/1     Running   0         4m    10.44.0.5   node1.k8s\n",
      "pref-607515-jx9wt   1/1     Running   0         4m    10.44.0.4   node1.k8s\n",
      "pref-607515-mlgqm   1/1     Running   0         4m    10.44.0.6   node1.k8s\n",
      "Out of the five pods that were created, four of them landed on node1 and only one\n",
      "landed on node2. Why did one of them land on node2 instead of node1? The reason is\n",
      "that besides the node affinity prioritization function, the Scheduler also uses other pri-\n",
      "oritization functions to decide where to schedule a pod. One of those is the Selector-\n",
      "SpreadPriority function, which makes sure pods belonging to the same ReplicaSet or\n",
      "Service are spread around different nodes so a node failure won’t bring the whole ser-\n",
      "vice down. That’s most likely what caused one of the pods to be scheduled to node2.\n",
      " You can try scaling the Deployment up to 20 or more and you’ll see the majority of\n",
      "pods will be scheduled to node1. In my test, only two out of the 20 were scheduled to\n",
      "node2. If you hadn’t defined any node affinity preferences, the pods would have been\n",
      "spread around the two nodes evenly.\n",
      "16.3\n",
      "Co-locating pods with pod affinity and anti-affinity\n",
      "You’ve seen how node affinity rules are used to influence which node a pod is scheduled\n",
      "to. But these rules only affect the affinity between a pod and a node, whereas sometimes\n",
      "you’d like to have the ability to specify the affinity between pods themselves. \n",
      " For example, imagine having a frontend and a backend pod. Having those pods\n",
      "deployed near to each other reduces latency and improves the performance of the\n",
      "app. You could use node affinity rules to ensure both are deployed to the same node,\n",
      "rack, or datacenter, but then you’d have to specify exactly which node, rack, or data-\n",
      "center to schedule them to, which is not the best solution. It’s better to let Kubernetes\n",
      "deploy your pods anywhere it sees fit, while keeping the frontend and backend pods\n",
      "close together. This can be achieved using pod affinity. Let’s learn more about it with\n",
      "an example.\n",
      "16.3.1 Using inter-pod affinity to deploy pods on the same node\n",
      "You’ll deploy a backend pod and five frontend pod replicas with pod affinity config-\n",
      "ured so that they’re all deployed on the same node as the backend pod.\n",
      " First, deploy the backend pod:\n",
      "$ kubectl run backend -l app=backend --image busybox -- sleep 999999\n",
      "deployment \"backend\" created\n",
      "Listing 16.12\n",
      "Seeing where pods were scheduled\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 501, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "469\n",
      "Co-locating pods with pod affinity and anti-affinity\n",
      "This Deployment is not special in any way. The only thing you need to note is the\n",
      "app=backend label you added to the pod using the -l option. This label is what you’ll\n",
      "use in the frontend pod’s podAffinity configuration. \n",
      "SPECIFYING POD AFFINITY IN A POD DEFINITION\n",
      "The frontend pod’s definition is shown in the following listing.\n",
      "apiVersion: extensions/v1beta1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: frontend\n",
      "spec:\n",
      "  replicas: 5\n",
      "  template:\n",
      "    ...\n",
      "    spec:\n",
      "      affinity:\n",
      "        podAffinity:                                 \n",
      "          requiredDuringSchedulingIgnoredDuringExecution:   \n",
      "          - topologyKey: kubernetes.io/hostname           \n",
      "            labelSelector:                                \n",
      "              matchLabels:                                \n",
      "                app: backend                              \n",
      "      ...\n",
      "The listing shows that this Deployment will create pods that have a hard requirement\n",
      "to be deployed on the same node (specified by the topologyKey field) as pods that\n",
      "have the app=backend label (see figure 16.4).\n",
      "Listing 16.13\n",
      "Pod using podAffinity: frontend-podaffinity-host.yaml\n",
      "Defining \n",
      "podAffinity rules\n",
      "Defining a hard \n",
      "requirement, not \n",
      "a preference\n",
      "The pods of this Deployment \n",
      "must be deployed on the \n",
      "same node as the pods that \n",
      "match the selector.\n",
      "All frontend pods will\n",
      "be scheduled only to\n",
      "the node the backend\n",
      "pod was scheduled to.\n",
      "Some node\n",
      "Other nodes\n",
      "Frontend pods\n",
      "Backend\n",
      "pod\n",
      "Pod afﬁnity\n",
      "Label selector: app=backend\n",
      "Topology key: hostname\n",
      "app: backend\n",
      "Figure 16.4\n",
      "Pod affinity allows scheduling pods to the node where other pods \n",
      "with a specific label are.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 502, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "470\n",
      "CHAPTER 16\n",
      "Advanced scheduling\n",
      "NOTE\n",
      "Instead of the simpler matchLabels field, you could also use the more\n",
      "expressive matchExpressions field.\n",
      "DEPLOYING A POD WITH POD AFFINITY\n",
      "Before you create this Deployment, let’s see which node the backend pod was sched-\n",
      "uled to earlier:\n",
      "$ kubectl get po -o wide\n",
      "NAME                   READY  STATUS   RESTARTS  AGE  IP         NODE\n",
      "backend-257820-qhqj6   1/1    Running  0         8m   10.47.0.1  node2.k8s\n",
      "When you create the frontend pods, they should be deployed to node2 as well. You’re\n",
      "going to create the Deployment and see where the pods are deployed. This is shown\n",
      "in the next listing.\n",
      "$ kubectl create -f frontend-podaffinity-host.yaml\n",
      "deployment \"frontend\" created\n",
      "$ kubectl get po -o wide\n",
      "NAME                   READY  STATUS    RESTARTS  AGE  IP         NODE\n",
      "backend-257820-qhqj6   1/1    Running   0         8m   10.47.0.1  node2.k8s\n",
      "frontend-121895-2c1ts  1/1    Running   0         13s  10.47.0.6  node2.k8s\n",
      "frontend-121895-776m7  1/1    Running   0         13s  10.47.0.4  node2.k8s\n",
      "frontend-121895-7ffsm  1/1    Running   0         13s  10.47.0.8  node2.k8s\n",
      "frontend-121895-fpgm6  1/1    Running   0         13s  10.47.0.7  node2.k8s\n",
      "frontend-121895-vb9ll  1/1    Running   0         13s  10.47.0.5  node2.k8s\n",
      "All the frontend pods were indeed scheduled to the same node as the backend pod.\n",
      "When scheduling the frontend pod, the Scheduler first found all the pods that match\n",
      "the labelSelector defined in the frontend pod’s podAffinity configuration and\n",
      "then scheduled the frontend pod to the same node.\n",
      "UNDERSTANDING HOW THE SCHEDULER USES POD AFFINITY RULES\n",
      "What’s interesting is that if you now delete the backend pod, the Scheduler will sched-\n",
      "ule the pod to node2 even though it doesn’t define any pod affinity rules itself (the\n",
      "rules are only on the frontend pods). This makes sense, because otherwise if the back-\n",
      "end pod were to be deleted by accident and rescheduled to a different node, the fron-\n",
      "tend pods’ affinity rules would be broken. \n",
      " You can confirm the Scheduler takes other pods’ pod affinity rules into account, if\n",
      "you increase the Scheduler’s logging level and then check its log. The following listing\n",
      "shows the relevant log lines.\n",
      "... Attempting to schedule pod: default/backend-257820-qhqj6\n",
      "... ...\n",
      "... backend-qhqj6 -> node2.k8s: Taint Toleration Priority, Score: (10)\n",
      "Listing 16.14\n",
      "Deploying frontend pods and seeing which node they’re scheduled to\n",
      "Listing 16.15\n",
      "Scheduler log showing why the backend pod is scheduled to node2\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 503, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "471\n",
      "Co-locating pods with pod affinity and anti-affinity\n",
      "... backend-qhqj6 -> node1.k8s: Taint Toleration Priority, Score: (10)\n",
      "... backend-qhqj6 -> node2.k8s: InterPodAffinityPriority, Score: (10)\n",
      "... backend-qhqj6 -> node1.k8s: InterPodAffinityPriority, Score: (0)\n",
      "... backend-qhqj6 -> node2.k8s: SelectorSpreadPriority, Score: (10)\n",
      "... backend-qhqj6 -> node1.k8s: SelectorSpreadPriority, Score: (10)\n",
      "... backend-qhqj6 -> node2.k8s: NodeAffinityPriority, Score: (0)\n",
      "... backend-qhqj6 -> node1.k8s: NodeAffinityPriority, Score: (0)\n",
      "... Host node2.k8s => Score 100030\n",
      "... Host node1.k8s => Score 100022\n",
      "... Attempting to bind backend-257820-qhqj6 to node2.k8s\n",
      "If you focus on the two lines in bold, you’ll see that during the scheduling of the back-\n",
      "end pod, node2 received a higher score than node1 because of inter-pod affinity. \n",
      "16.3.2 Deploying pods in the same rack, availability zone, or \n",
      "geographic region\n",
      "In the previous example, you used podAffinity to deploy frontend pods onto the\n",
      "same node as the backend pods. You probably don’t want all your frontend pods to\n",
      "run on the same machine, but you’d still like to keep them close to the backend\n",
      "pod—for example, run them in the same availability zone. \n",
      "CO-LOCATING PODS IN THE SAME AVAILABILITY ZONE\n",
      "The cluster I’m using runs in three VMs on my local machine, so all the nodes are in\n",
      "the same availability zone, so to speak. But if the nodes were in different zones, all I’d\n",
      "need to do to run the frontend pods in the same zone as the backend pod would be to\n",
      "change the topologyKey property to failure-domain.beta.kubernetes.io/zone. \n",
      "CO-LOCATING PODS IN THE SAME GEOGRAPHICAL REGION\n",
      "To allow the pods to be deployed in the same region instead of the same zone (cloud\n",
      "providers usually have datacenters located in different geographical regions and split\n",
      "into multiple availability zones in each region), the topologyKey would be set to\n",
      "failure-domain.beta.kubernetes.io/region.\n",
      "UNDERSTANDING HOW TOPOLOGYKEY WORKS\n",
      "The way topologyKey works is simple. The three keys we’ve mentioned so far aren’t\n",
      "special. If you want, you can easily use your own topologyKey, such as rack, to have\n",
      "the pods scheduled to the same server rack. The only prerequisite is to add a rack\n",
      "label to your nodes. This scenario is shown in figure 16.5.\n",
      " For example, if you had 20 nodes, with 10 in each rack, you’d label the first ten as\n",
      "rack=rack1 and the others as rack=rack2. Then, when defining a pod’s podAffinity,\n",
      "you’d set the toplogyKey to rack. \n",
      " When the Scheduler is deciding where to deploy a pod, it checks the pod’s pod-\n",
      "Affinity config, finds the pods that match the label selector, and looks up the nodes\n",
      "they’re running on. Specifically, it looks up the nodes’ label whose key matches the\n",
      "topologyKey field specified in podAffinity. Then it selects all the nodes whose label\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 504, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "472\n",
      "CHAPTER 16\n",
      "Advanced scheduling\n",
      "matches the values of the pods it found earlier. In figure 16.5, the label selector\n",
      "matched the backend pod, which runs on Node 12. The value of the rack label on\n",
      "that node equals rack2, so when scheduling a frontend pod, the Scheduler will only\n",
      "select among the nodes that have the rack=rack2 label.\n",
      "NOTE\n",
      "By default, the label selector only matches pods in the same name-\n",
      "space as the pod that’s being scheduled. But you can also select pods from\n",
      "other namespaces by adding a namespaces field at the same level as label-\n",
      "Selector.\n",
      "16.3.3 Expressing pod affinity preferences instead of hard requirements\n",
      "Earlier, when we talked about node affinity, you saw that nodeAffinity can be used to\n",
      "express a hard requirement, which means a pod is only scheduled to nodes that match\n",
      "the node affinity rules. It can also be used to specify node preferences, to instruct the\n",
      "Scheduler to schedule the pod to certain nodes, while allowing it to schedule it any-\n",
      "where else if those nodes can’t fit the pod for any reason.\n",
      " The same also applies to podAffinity. You can tell the Scheduler you’d prefer to\n",
      "have your frontend pods scheduled onto the same node as your backend pod, but if\n",
      "that’s not possible, you’re okay with them being scheduled elsewhere. An example of\n",
      "a Deployment using the preferredDuringSchedulingIgnoredDuringExecution pod\n",
      "affinity rule is shown in the next listing.\n",
      "Frontend pods will be\n",
      "scheduled to nodes in\n",
      "the same rack as the\n",
      "backend pod.\n",
      "Node 1\n",
      "Rack 1\n",
      "rack: rack1\n",
      "Node 2\n",
      "rack: rack1\n",
      "Node 3\n",
      "...\n",
      "rack: rack1\n",
      "Node 10\n",
      "rack: rack1\n",
      "Node 11\n",
      "Rack 2\n",
      "rack: rack2\n",
      "Node 12\n",
      "rack: rack2\n",
      "...\n",
      "Node 20\n",
      "rack: rack2\n",
      "Backend\n",
      "pod\n",
      "app: backend\n",
      "Frontend pods\n",
      "Pod afﬁnity (required)\n",
      "Label selector: app=backend\n",
      "Topology key: rack\n",
      "Figure 16.5\n",
      "The topologyKey in podAffinity determines the scope of where the pod \n",
      "should be scheduled to.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 505, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "473\n",
      "Co-locating pods with pod affinity and anti-affinity\n",
      "apiVersion: extensions/v1beta1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: frontend\n",
      "spec:\n",
      "  replicas: 5\n",
      "  template:\n",
      "    ...\n",
      "    spec:\n",
      "      affinity:\n",
      "        podAffinity:\n",
      "          preferredDuringSchedulingIgnoredDuringExecution:  \n",
      "          - weight: 80                                        \n",
      "            podAffinityTerm:                                  \n",
      "              topologyKey: kubernetes.io/hostname             \n",
      "              labelSelector:                                  \n",
      "                matchLabels:                                  \n",
      "                  app: backend                                \n",
      "      containers: ...\n",
      "As in nodeAffinity preference rules, you need to define a weight for each rule. You\n",
      "also need to specify the topologyKey and labelSelector, as in the hard-requirement\n",
      "podAffinity rules. Figure 16.6 shows this scenario.\n",
      "Deploying this pod, as with your nodeAffinity example, deploys four pods on the same\n",
      "node as the backend pod, and one pod on the other node (see the following listing).\n",
      "Listing 16.16\n",
      "Pod affinity preference\n",
      "Preferred \n",
      "instead of \n",
      "Required\n",
      "A weight and a \n",
      "podAffinity term is \n",
      "specified as in the \n",
      "previous example\n",
      "The Scheduler will prefer\n",
      "Node 2 for frontend pods,\n",
      "but may schedule pods\n",
      "to Node 1 as well.\n",
      "Node 1\n",
      "Node 2\n",
      "Backend\n",
      "pod\n",
      "app: backend\n",
      "Frontend pod\n",
      "Pod afﬁnity (preferred)\n",
      "Label selector: app=backend\n",
      "Topology key: hostname\n",
      "hostname: node2\n",
      "hostname: node1\n",
      "Figure 16.6\n",
      "Pod affinity can be used to make the Scheduler prefer nodes where \n",
      "pods with a certain label are running. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 506, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "474\n",
      "CHAPTER 16\n",
      "Advanced scheduling\n",
      "$ kubectl get po -o wide\n",
      "NAME                   READY  STATUS   RESTARTS  AGE  IP          NODE\n",
      "backend-257820-ssrgj   1/1    Running  0         1h   10.47.0.9   node2.k8s\n",
      "frontend-941083-3mff9  1/1    Running  0         8m   10.44.0.4   node1.k8s\n",
      "frontend-941083-7fp7d  1/1    Running  0         8m   10.47.0.6   node2.k8s\n",
      "frontend-941083-cq23b  1/1    Running  0         8m   10.47.0.1   node2.k8s\n",
      "frontend-941083-m70sw  1/1    Running  0         8m   10.47.0.5   node2.k8s\n",
      "frontend-941083-wsjv8  1/1    Running  0         8m   10.47.0.4   node2.k8s\n",
      "16.3.4 Scheduling pods away from each other with pod anti-affinity\n",
      "You’ve seen how to tell the Scheduler to co-locate pods, but sometimes you may want\n",
      "the exact opposite. You may want to keep pods away from each other. This is called\n",
      "pod anti-affinity. It’s specified the same way as pod affinity, except that you use the\n",
      "podAntiAffinity property instead of podAffinity, which results in the Scheduler\n",
      "never choosing nodes where pods matching the podAntiAffinity’s label selector are\n",
      "running, as shown in figure 16.7.\n",
      "An example of why you’d want to use pod anti-affinity is when two sets of pods inter-\n",
      "fere with each other’s performance if they run on the same node. In that case, you\n",
      "want to tell the Scheduler to never schedule those pods on the same node. Another\n",
      "example would be to force the Scheduler to spread pods of the same group across dif-\n",
      "ferent availability zones or regions, so that a failure of a whole zone (or region) never\n",
      "brings the service down completely. \n",
      "Listing 16.17\n",
      "Pods deployed with podAffinity preferences\n",
      "These pods will NOT be scheduled\n",
      "to the same node(s) where pods\n",
      "with app=foo label are running.\n",
      "Some node\n",
      "Other nodes\n",
      "Pods\n",
      "Pod: foo\n",
      "Pod\n",
      "(required)\n",
      "anti-afﬁnity\n",
      "Label selector: app=foo\n",
      "Topology key: hostname\n",
      "app: foo\n",
      "Figure 16.7\n",
      "Using pod anti-affinity to keep pods away from nodes that run pods \n",
      "with a certain label.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 507, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "475\n",
      "Co-locating pods with pod affinity and anti-affinity\n",
      "USING ANTI-AFFINITY TO SPREAD APART PODS OF THE SAME DEPLOYMENT\n",
      "Let’s see how to force your frontend pods to be scheduled to different nodes. The fol-\n",
      "lowing listing shows how the pods’ anti-affinity is configured.\n",
      "apiVersion: extensions/v1beta1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: frontend\n",
      "spec:\n",
      "  replicas: 5\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:                  \n",
      "        app: frontend          \n",
      "    spec:\n",
      "      affinity:\n",
      "        podAntiAffinity:                                      \n",
      "          requiredDuringSchedulingIgnoredDuringExecution:     \n",
      "          - topologyKey: kubernetes.io/hostname            \n",
      "            labelSelector:                                 \n",
      "              matchLabels:                                 \n",
      "                app: frontend                              \n",
      "      containers: ...\n",
      "This time, you’re defining podAntiAffinity instead of podAffinity, and you’re mak-\n",
      "ing the labelSelector match the same pods that the Deployment creates. Let’s see\n",
      "what happens when you create this Deployment. The pods created by it are shown in\n",
      "the following listing.\n",
      "$ kubectl get po -l app=frontend -o wide\n",
      "NAME                    READY  STATUS   RESTARTS  AGE  IP         NODE\n",
      "frontend-286632-0lffz   0/1    Pending  0         1m   <none>\n",
      "frontend-286632-2rkcz   1/1    Running  0         1m   10.47.0.1  node2.k8s\n",
      "frontend-286632-4nwhp   0/1    Pending  0         1m   <none>\n",
      "frontend-286632-h4686   0/1    Pending  0         1m   <none>\n",
      "frontend-286632-st222   1/1    Running  0         1m   10.44.0.4  node1.k8s\n",
      "As you can see, only two pods were scheduled—one to node1, the other to node2. The\n",
      "three remaining pods are all Pending, because the Scheduler isn’t allowed to schedule\n",
      "them to the same nodes.\n",
      "USING PREFERENTIAL POD ANTI-AFFINITY\n",
      "In this case, you probably should have specified a soft requirement instead (using the\n",
      "preferredDuringSchedulingIgnoredDuringExecution property). After all, it’s not\n",
      "such a big problem if two frontend pods run on the same node. But in scenarios where\n",
      "that’s a problem, using requiredDuringScheduling is appropriate. \n",
      "Listing 16.18\n",
      "Pods with anti-affinity: frontend-podantiaffinity-host.yaml\n",
      "Listing 16.19\n",
      "Pods created by the Deployment\n",
      "The frontend pods have \n",
      "the app=frontend label.\n",
      "Defining hard-\n",
      "requirements for \n",
      "pod anti-affinity\n",
      "A frontend pod must not \n",
      "be scheduled to the same \n",
      "machine as a pod with \n",
      "app=frontend label.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 508, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "476\n",
      "CHAPTER 16\n",
      "Advanced scheduling\n",
      " As with pod affinity, the topologyKey property determines the scope of where the\n",
      "pod shouldn’t be deployed to. You can use it to ensure pods aren’t deployed to the\n",
      "same rack, availability zone, region, or any custom scope you create using custom\n",
      "node labels.\n",
      "16.4\n",
      "Summary\n",
      "In this chapter, we looked at how to ensure pods aren’t scheduled to certain nodes or\n",
      "are only scheduled to specific nodes, either because of the node’s labels or because of\n",
      "the pods running on them.\n",
      " You learned that\n",
      "If you add a taint to a node, pods won’t be scheduled to that node unless they\n",
      "tolerate that taint.\n",
      "Three types of taints exist: NoSchedule completely prevents scheduling, Prefer-\n",
      "NoSchedule isn’t as strict, and NoExecute even evicts existing pods from a node.\n",
      "The NoExecute taint is also used to specify how long the Control Plane should\n",
      "wait before rescheduling the pod when the node it runs on becomes unreach-\n",
      "able or unready.\n",
      "Node affinity allows you to specify which nodes a pod should be scheduled to. It\n",
      "can be used to specify a hard requirement or to only express a node preference.\n",
      "Pod affinity is used to make the Scheduler deploy pods to the same node where\n",
      "another pod is running (based on the pod’s labels). \n",
      "Pod affinity’s topologyKey specifies how close the pod should be deployed to\n",
      "the other pod (onto the same node or onto a node in the same rack, availability\n",
      "zone, or availability region).\n",
      "Pod anti-affinity can be used to keep certain pods away from each other. \n",
      "Both pod affinity and anti-affinity, like node affinity, can either specify hard\n",
      "requirements or preferences.\n",
      "In the next chapter, you’ll learn about best practices for developing apps and how to\n",
      "make them run smoothly in a Kubernetes environment.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 509, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "477\n",
      "Best practices\n",
      "for developing apps\n",
      "We’ve now covered most of what you need to know to run your apps in Kubernetes.\n",
      "We’ve explored what each individual resource does and how it’s used. Now we’ll see\n",
      "how to combine them in a typical application running on Kubernetes. We’ll also\n",
      "look at how to make an application run smoothly. After all, that’s the whole point\n",
      "of using Kubernetes, isn’t it? \n",
      " Hopefully, this chapter will help to clear up any misunderstandings and explain\n",
      "things that weren’t explained clearly yet. Along the way, we’ll also introduce a few\n",
      "additional concepts that haven’t been mentioned up to this point.\n",
      "This chapter covers\n",
      "Understanding which Kubernetes resources \n",
      "appear in a typical application\n",
      "Adding post-start and pre-stop pod lifecycle hooks\n",
      "Properly terminating an app without breaking \n",
      "client requests\n",
      "Making apps easy to manage in Kubernetes\n",
      "Using init containers in a pod\n",
      "Developing locally with Minikube\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 510, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "478\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "17.1\n",
      "Bringing everything together\n",
      "Let’s start by looking at what an actual application consists of. This will also give you a\n",
      "chance to see if you remember everything you’ve learned so far and look at the big\n",
      "picture. Figure 17.1 shows the Kubernetes components used in a typical application.\n",
      "A typical application manifest contains one or more Deployment and/or StatefulSet\n",
      "objects. Those include a pod template containing one or more containers, with a live-\n",
      "ness probe for each of them and a readiness probe for the service(s) the container\n",
      "provides (if any). Pods that provide services to others are exposed through one or\n",
      "more Services. When they need to be reachable from outside the cluster, the Services\n",
      "are either configured to be LoadBalancer or NodePort-type Services, or exposed\n",
      "through an Ingress resource. \n",
      " The pod templates (and the pods created from them) usually reference two types\n",
      "of Secrets—those for pulling container images from private image registries and those\n",
      "used directly by the process running inside the pods. The Secrets themselves are\n",
      "usually not part of the application manifest, because they aren’t configured by the\n",
      "application developers but by the operations team. Secrets are usually assigned to\n",
      "ServiceAccounts, which are assigned to individual pods. \n",
      "Deﬁned in the app manifest by the developer\n",
      "Pod template\n",
      "Deployment\n",
      "labels\n",
      "Pod(s)\n",
      "Label selector\n",
      "labels\n",
      "Created automatically at runtime\n",
      "Created by a cluster admin beforehand\n",
      "Container(s)\n",
      "Volume(s)\n",
      "ReplicaSet(s)\n",
      "Endpoints\n",
      "• Health probes\n",
      "• Environment variables\n",
      "• Volume mounts\n",
      "• Resource reqs/limits\n",
      "Horizontal\n",
      "PodAutoscaler\n",
      "StatefulSet\n",
      "DaemonSet\n",
      "Job\n",
      "CronJob\n",
      "Persistent\n",
      "Volume\n",
      "ConﬁgMap\n",
      "Service\n",
      "Persistent\n",
      "Volume\n",
      "Claim\n",
      "Secret(s)\n",
      "Service\n",
      "account\n",
      "Storage\n",
      "Class\n",
      "LimitRange\n",
      "ResourceQuota\n",
      "Ingress\n",
      "imagePullSecret\n",
      "Figure 17.1\n",
      "Resources in a typical application\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 511, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "479\n",
      "Understanding the pod’s lifecycle\n",
      " The application also contains one or more ConfigMaps, which are either used to\n",
      "initialize environment variables or mounted as a configMap volume in the pod. Cer-\n",
      "tain pods use additional volumes, such as an emptyDir or a gitRepo volume, whereas\n",
      "pods requiring persistent storage use persistentVolumeClaim volumes. The Persistent-\n",
      "VolumeClaims are also part of the application manifest, whereas StorageClasses refer-\n",
      "enced by them are created by system administrators upfront. \n",
      " In certain cases, an application also requires the use of Jobs or CronJobs. Daemon-\n",
      "Sets aren’t normally part of application deployments, but are usually created by sysad-\n",
      "mins to run system services on all or a subset of nodes. HorizontalPodAutoscalers\n",
      "are either included in the manifest by the developers or added to the system later by\n",
      "the ops team. The cluster administrator also creates LimitRange and ResourceQuota\n",
      "objects to keep compute resource usage of individual pods and all the pods (as a\n",
      "whole) under control.\n",
      " After the application is deployed, additional objects are created automatically by\n",
      "the various Kubernetes controllers. These include service Endpoints objects created\n",
      "by the Endpoints controller, ReplicaSets created by the Deployment controller, and\n",
      "the actual pods created by the ReplicaSet (or Job, CronJob, StatefulSet, or DaemonSet)\n",
      "controllers.\n",
      " Resources are often labeled with one or more labels to keep them organized. This\n",
      "doesn’t apply only to pods but to all other resources as well. In addition to labels, most\n",
      "resources also contain annotations that describe each resource, list the contact infor-\n",
      "mation of the person or team responsible for it, or provide additional metadata for\n",
      "management and other tools. \n",
      " At the center of all this is the Pod, which arguably is the most important Kuberne-\n",
      "tes resource. After all, each of your applications runs inside it. To make sure you know\n",
      "how to develop apps that make the most out of their environment, let’s take one last\n",
      "close look at pods—this time from the application’s perspective. \n",
      "17.2\n",
      "Understanding the pod’s lifecycle\n",
      "We’ve said that pods can be compared to VMs dedicated to running only a single\n",
      "application. Although an application running inside a pod is not unlike an application\n",
      "running in a VM, significant differences do exist. One example is that apps running in\n",
      "a pod can be killed any time, because Kubernetes needs to relocate the pod to\n",
      "another node for a reason or because of a scale-down request. We’ll explore this\n",
      "aspect next.\n",
      "17.2.1 Applications must expect to be killed and relocated\n",
      "Outside Kubernetes, apps running in VMs are seldom moved from one machine to\n",
      "another. When an operator moves the app, they can also reconfigure the app and\n",
      "manually check that the app is running fine in the new location. With Kubernetes,\n",
      "apps are relocated much more frequently and automatically—no human operator\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 512, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "480\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "reconfigures them and makes sure they still run properly after the move. This means\n",
      "application developers need to make sure their apps allow being moved relatively\n",
      "often. \n",
      "EXPECTING THE LOCAL IP AND HOSTNAME TO CHANGE\n",
      "When a pod is killed and run elsewhere (technically, it’s a new pod instance replac-\n",
      "ing the old one; the pod isn’t relocated), it not only has a new IP address but also a\n",
      "new name and hostname. Most stateless apps can usually handle this without any\n",
      "adverse effects, but stateful apps usually can’t. We’ve learned that stateful apps can\n",
      "be run through a StatefulSet, which ensures that when the app starts up on a new\n",
      "node after being rescheduled, it will still see the same host name and persistent state\n",
      "as before. The pod’s IP will change nevertheless. Apps need to be prepared for that\n",
      "to happen. The application developer therefore should never base membership in a\n",
      "clustered app on the member’s IP address, and if basing it on the hostname, should\n",
      "always use a StatefulSet.\n",
      "EXPECTING THE DATA WRITTEN TO DISK TO DISAPPEAR\n",
      "Another thing to keep in mind is that if the app writes data to disk, that data may not be\n",
      "available after the app is started inside a new pod, unless you mount persistent storage at\n",
      "the location the app is writing to. It should be clear this happens when the pod is\n",
      "rescheduled, but files written to disk will disappear even in scenarios that don’t involve\n",
      "any rescheduling. Even during the lifetime of a single pod, the files written to disk by\n",
      "the app running in the pod may disappear. Let me explain this with an example.\n",
      " Imagine an app that has a long and computationally intensive initial startup proce-\n",
      "dure. To help the app come up faster on subsequent startups, the developers make\n",
      "the app cache the results of the initial startup on disk (an example of this would be\n",
      "the scanning of all Java classes for annotations at startup and then writing the results\n",
      "to an index file). Because apps in Kubernetes run in containers by default, these files\n",
      "are written to the container’s filesystem. If the container is then restarted, they’re all\n",
      "lost, because the new container starts off with a completely new writable layer (see fig-\n",
      "ure 17.2).\n",
      " Don’t forget that individual containers may be restarted for several reasons, such\n",
      "as because the process crashes, because the liveness probe returned a failure, or\n",
      "because the node started running out of memory and the process was killed by the\n",
      "OOMKiller. When this happens, the pod is still the same, but the container itself is\n",
      "completely new. The Kubelet doesn’t run the same container again; it always creates a\n",
      "new container. \n",
      "USING VOLUMES TO PRESERVE DATA ACROSS CONTAINER RESTARTS\n",
      "When its container is restarted, the app in the example will need to perform the\n",
      "intensive startup procedure again. This may or may not be desired. To make sure data\n",
      "like this isn’t lost, you need to use at least a pod-scoped volume. Because volumes live\n",
      "and die together with the pod, the new container will be able to reuse the data written\n",
      "to the volume by the previous container (figure 17.3).\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 513, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "481\n",
      "Understanding the pod’s lifecycle\n",
      "Container\n",
      "Process\n",
      "Writes to\n",
      "Filesystem\n",
      "Writable layer\n",
      "Read-only layer\n",
      "Read-only layer\n",
      "Image layers\n",
      "Container crashes\n",
      "or is killed\n",
      "Pod\n",
      "New container\n",
      "New process\n",
      "Filesystem\n",
      "New writable layer\n",
      "Read-only layer\n",
      "Read-only layer\n",
      "Image layers\n",
      "New container started\n",
      "(part of the same pod)\n",
      "New container\n",
      "starts with new\n",
      "writeable layer:\n",
      "all ﬁles are lost\n",
      "Figure 17.2\n",
      "Files written to the container’s filesystem are lost when the container is restarted.\n",
      "Container\n",
      "Process\n",
      "Writes to\n",
      "Can read\n",
      "the same ﬁles\n",
      "Filesystem\n",
      "volumeMount\n",
      "Container crashes\n",
      "or is killed\n",
      "Pod\n",
      "New container\n",
      "New process\n",
      "Filesystem\n",
      "volumeMount\n",
      "New container started\n",
      "(part of the same pod)\n",
      "New process can\n",
      "use data preserved\n",
      "in the volume\n",
      "Volume\n",
      "Figure 17.3\n",
      "Using a volume to persist data across container restarts\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 514, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "482\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "Using a volume to preserve files across container restarts is a great idea sometimes,\n",
      "but not always. What if the data gets corrupted and causes the newly created process\n",
      "to crash again? This will result in a continuous crash loop (the pod will show the\n",
      "CrashLoopBackOff status). If you hadn’t used a volume, the new container would start\n",
      "from scratch and most likely not crash. Using volumes to preserve files across con-\n",
      "tainer restarts like this is a double-edged sword. You need to think carefully about\n",
      "whether to use them or not.\n",
      "17.2.2 Rescheduling of dead or partially dead pods\n",
      "If a pod’s container keeps crashing, the Kubelet will keep restarting it indefinitely.\n",
      "The time between restarts will be increased exponentially until it reaches five minutes.\n",
      "During those five minute intervals, the pod is essentially dead, because its container’s\n",
      "process isn’t running. To be fair, if it’s a multi-container pod, certain containers may\n",
      "be running normally, so the pod is only partially dead. But if a pod contains only a sin-\n",
      "gle container, the pod is effectively dead and completely useless, because no process is\n",
      "running in it anymore.\n",
      " You may find it surprising to learn that such pods aren’t automatically removed\n",
      "and rescheduled, even if they’re part of a ReplicaSet or similar controller. If you cre-\n",
      "ate a ReplicaSet with a desired replica count of three, and then one of the containers\n",
      "in one of those pods starts crashing, Kubernetes will not delete and replace the pod.\n",
      "The end result is a ReplicaSet with only two properly running replicas instead of the\n",
      "desired three (figure 17.4).\n",
      "You’d probably expect the pod to be deleted and replaced with another pod instance\n",
      "that might run successfully on another node. After all, the container may be crashing\n",
      "because of a node-related problem that doesn’t manifest itself on other nodes. Sadly,\n",
      "that isn’t the case. The ReplicaSet controller doesn’t care if the pods are dead—all it\n",
      "ReplicaSet\n",
      "Desired replicas: 3\n",
      "Actual replicas: 3\n",
      "Only two pods are actually\n",
      "performing their jobs\n",
      "Third pod’s status is Running,\n",
      "but its container keeps crashing,\n",
      "with signiﬁcant delays between\n",
      "restarts (CrashLoopBackOff)\n",
      "We want\n",
      "three pods\n",
      "Pod\n",
      "Running\n",
      "container\n",
      "Pod\n",
      "Running\n",
      "container\n",
      "Pod\n",
      "Dead\n",
      "container\n",
      "Figure 17.4\n",
      "A ReplicaSet controller doesn’t reschedule dead pods.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 515, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "483\n",
      "Understanding the pod’s lifecycle\n",
      "cares about is that the number of pods matches the desired replica count, which in\n",
      "this case, it does.\n",
      " If you’d like to see for yourself, I’ve included a YAML manifest for a ReplicaSet\n",
      "whose pods will keep crashing (see file replicaset-crashingpods.yaml in the code\n",
      "archive). If you create the ReplicaSet and inspect the pods that are created, the follow-\n",
      "ing listing is what you’ll see.\n",
      "$ kubectl get po\n",
      "NAME                  READY     STATUS             RESTARTS   AGE\n",
      "crashing-pods-f1tcd   0/1       CrashLoopBackOff   5          6m     \n",
      "crashing-pods-k7l6k   0/1       CrashLoopBackOff   5          6m\n",
      "crashing-pods-z7l3v   0/1       CrashLoopBackOff   5          6m\n",
      "$ kubectl describe rs crashing-pods\n",
      "Name:           crashing-pods\n",
      "Replicas:       3 current / 3 desired                       \n",
      "Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed      \n",
      "$ kubectl describe po crashing-pods-f1tcd\n",
      "Name:           crashing-pods-f1tcd\n",
      "Namespace:      default\n",
      "Node:           minikube/192.168.99.102\n",
      "Start Time:     Thu, 02 Mar 2017 14:02:23 +0100\n",
      "Labels:         app=crashing-pods\n",
      "Status:         Running                      \n",
      "In a way, it’s understandable that Kubernetes behaves this way. The container will be\n",
      "restarted every five minutes in the hope that the underlying cause of the crash will be\n",
      "resolved. The rationale is that rescheduling the pod to another node most likely\n",
      "wouldn’t fix the problem anyway, because the app is running inside a container and\n",
      "all the nodes should be mostly equivalent. That’s not always the case, but it is most of\n",
      "the time. \n",
      "17.2.3 Starting pods in a specific order\n",
      "One other difference between apps running in pods and those managed manually is\n",
      "that the ops person deploying those apps knows about the dependencies between\n",
      "them. This allows them to start the apps in order. \n",
      "UNDERSTANDING HOW PODS ARE STARTED\n",
      "When you use Kubernetes to run your multi-pod applications, you don’t have a built-\n",
      "in way to tell Kubernetes to run certain pods first and the rest only when the first pods\n",
      "are already up and ready to serve. Sure, you could post the manifest for the first app\n",
      "and then wait for the pod(s) to be ready before you post the second manifest, but your\n",
      "Listing 17.1\n",
      "ReplicaSet and pods that keep crashing\n",
      "The pod’s status shows the Kubelet is\n",
      "delaying the restart because the\n",
      "container keeps crashing.\n",
      "No action taken \n",
      "by the controller, \n",
      "because current \n",
      "replicas match \n",
      "desired replicas\n",
      "Three \n",
      "replicas are \n",
      "shown as \n",
      "running.\n",
      "kubectl describe \n",
      "also shows pod’s \n",
      "status as running\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 516, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "484\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "whole system is usually defined in a single YAML or JSON containing multiple Pods,\n",
      "Services, and other objects. \n",
      " The Kubernetes API server does process the objects in the YAML/JSON in the\n",
      "order they’re listed, but this only means they’re written to etcd in that order. You have\n",
      "no guarantee that pods will also be started in that order. \n",
      " But you can prevent a pod’s main container from starting until a precondition is\n",
      "met. This is done by including an init containers in the pod. \n",
      "INTRODUCING INIT CONTAINERS\n",
      "In addition to regular containers, pods can also include init containers. As the name\n",
      "suggests, they can be used to initialize the pod—this often means writing data to the\n",
      "pod’s volumes, which are then mounted into the pod’s main container(s).\n",
      " A pod may have any number of init containers. They’re executed sequentially and\n",
      "only after the last one completes are the pod’s main containers started. This means\n",
      "init containers can also be used to delay the start of the pod’s main container(s)—for\n",
      "example, until a certain precondition is met. An init container could wait for a service\n",
      "required by the pod’s main container to be up and ready. When it is, the init container\n",
      "terminates and allows the main container(s) to be started. This way, the main con-\n",
      "tainer wouldn’t use the service before it’s ready.\n",
      " Let’s look at an example of a pod using an init container to delay the start of the\n",
      "main container. Remember the fortune pod you created in chapter 7? It’s a web\n",
      "server that returns a fortune quote as a response to client requests. Now, let’s imagine\n",
      "you have a fortune-client pod that requires the fortune Service to be up and run-\n",
      "ning before its main container starts. You can add an init container, which checks\n",
      "whether the Service is responding to requests. Until that’s the case, the init container\n",
      "keeps retrying. Once it gets a response, the init container terminates and lets the main\n",
      "container start.\n",
      "ADDING AN INIT CONTAINER TO A POD\n",
      "Init containers can be defined in the pod spec like main containers but through the\n",
      "spec.initContainers field. You’ll find the complete YAML for the fortune-client pod\n",
      "in the book’s code archive. The following listing shows the part where the init con-\n",
      "tainer is defined.\n",
      "spec:\n",
      "  initContainers:      \n",
      "  - name: init\n",
      "    image: busybox\n",
      "    command:\n",
      "    - sh\n",
      "    - -c\n",
      "    - 'while true; do echo \"Waiting for fortune service to come up...\";  \n",
      "    ➥ wget http://fortune -q -T 1 -O /dev/null >/dev/null 2>/dev/null   \n",
      "    ➥ && break; sleep 1; done; echo \"Service is up! Starting main       \n",
      "    ➥ container.\"'\n",
      "Listing 17.2\n",
      "An init container defined in a pod: fortune-client.yaml\n",
      "You’re defining \n",
      "an init container, \n",
      "not a regular \n",
      "container.\n",
      "The init container runs a\n",
      "loop that runs until the\n",
      "fortune Service is up.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 517, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "485\n",
      "Understanding the pod’s lifecycle\n",
      "When you deploy this pod, only its init container is started. This is shown in the pod’s\n",
      "status when you list pods with kubectl get:\n",
      "$ kubectl get po\n",
      "NAME             READY     STATUS     RESTARTS   AGE\n",
      "fortune-client   0/1       Init:0/1   0          1m\n",
      "The STATUS column shows that zero of one init containers have finished. You can see\n",
      "the log of the init container with kubectl logs:\n",
      "$ kubectl logs fortune-client -c init\n",
      "Waiting for fortune service to come up...\n",
      "When running the kubectl logs command, you need to specify the name of the init\n",
      "container with the -c switch (in the example, the name of the pod’s init container is\n",
      "init, as you can see in listing 17.2).\n",
      " The main container won’t run until you deploy the fortune Service and the\n",
      "fortune-server pod. You’ll find them in the fortune-server.yaml file. \n",
      "BEST PRACTICES FOR HANDLING INTER-POD DEPENDENCIES\n",
      "You’ve seen how an init container can be used to delay starting the pod’s main con-\n",
      "tainer(s) until a precondition is met (making sure the Service the pod depends on is\n",
      "ready, for example), but it’s much better to write apps that don’t require every service\n",
      "they rely on to be ready before the app starts up. After all, the service may also go\n",
      "offline later, while the app is already running.\n",
      " The application needs to handle internally the possibility that its dependencies\n",
      "aren’t ready. And don’t forget readiness probes. If an app can’t do its job because one\n",
      "of its dependencies is missing, it should signal that through its readiness probe, so\n",
      "Kubernetes knows it, too, isn’t ready. You’ll want to do this not only because it pre-\n",
      "vents the app from being added as a service endpoint, but also because the app’s read-\n",
      "iness is also used by the Deployment controller when performing a rolling update,\n",
      "thereby preventing a rollout of a bad version. \n",
      "17.2.4 Adding lifecycle hooks\n",
      "We’ve talked about how init containers can be used to hook into the startup of the\n",
      "pod, but pods also allow you to define two lifecycle hooks:\n",
      "Post-start hooks\n",
      "Pre-stop hooks\n",
      "These lifecycle hooks are specified per container, unlike init containers, which apply\n",
      "to the whole pod. As their names suggest, they’re executed when the container starts\n",
      "and before it stops. \n",
      " Lifecycle hooks are similar to liveness and readiness probes in that they can either\n",
      "Execute a command inside the container\n",
      "Perform an HTTP GET request against a URL\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 518, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "486\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "Let’s look at the two hooks individually to see what effect they have on the container\n",
      "lifecycle.\n",
      "USING A POST-START CONTAINER LIFECYCLE HOOK\n",
      "A post-start hook is executed immediately after the container’s main process is started.\n",
      "You use it to perform additional operations when the application starts. Sure, if you’re\n",
      "the author of the application running in the container, you can always perform those\n",
      "operations inside the application code itself. But when you’re running an application\n",
      "developed by someone else, you mostly don’t want to (or can’t) modify its source\n",
      "code. Post-start hooks allow you to run additional commands without having to touch\n",
      "the app. These may signal to an external listener that the app is starting, or they may\n",
      "initialize the application so it can start doing its job.\n",
      " The hook is run in parallel with the main process. The name might be somewhat\n",
      "misleading, because it doesn’t wait for the main process to start up fully (if the process\n",
      "has an initialization procedure, the Kubelet obviously can’t wait for the procedure to\n",
      "complete, because it has no way of knowing when that is). \n",
      " But even though the hook runs asynchronously, it does affect the container in two\n",
      "ways. Until the hook completes, the container will stay in the Waiting state with the\n",
      "reason ContainerCreating. Because of this, the pod’s status will be Pending instead of\n",
      "Running. If the hook fails to run or returns a non-zero exit code, the main container\n",
      "will be killed. \n",
      " A pod manifest containing a post-start hook looks like the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: pod-with-poststart-hook\n",
      "spec:\n",
      "  containers:\n",
      "  - image: luksa/kubia\n",
      "    name: kubia\n",
      "    lifecycle:          \n",
      "      postStart:        \n",
      "        exec:                                                               \n",
      "          command:                                                          \n",
      "          - sh                                                              \n",
      "          - -c                                                              \n",
      "          - \"echo 'hook will fail with exit code 15'; sleep 5; exit 15\"     \n",
      "In the example, the echo, sleep, and exit commands are executed along with the\n",
      "container’s main process as soon as the container is created. Rather than run a com-\n",
      "mand like this, you’d typically run a shell script or a binary executable file stored in\n",
      "the container image. \n",
      " Sadly, if the process started by the hook logs to the standard output, you can’t see\n",
      "the output anywhere. This makes debugging lifecycle hooks painful. If the hook fails,\n",
      "Listing 17.3\n",
      "A pod with a post-start lifecycle hook: post-start-hook.yaml\n",
      "The hook is executed as \n",
      "the container starts.\n",
      "It executes the\n",
      "postStart.sh\n",
      "script in the /bin\n",
      "directory inside\n",
      "the container.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 519, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "487\n",
      "Understanding the pod’s lifecycle\n",
      "you’ll only see a FailedPostStartHook warning among the pod’s events (you can see\n",
      "them using kubectl describe pod). A while later, you’ll see more information on why\n",
      "the hook failed, as shown in the following listing.\n",
      "FailedSync   Error syncing pod, skipping: failed to \"StartContainer\" for \n",
      "             \"kubia\" with PostStart handler: command 'sh -c echo 'hook \n",
      "             will fail with exit code 15'; sleep 5 ; exit 15' exited \n",
      "             with 15: : \"PostStart Hook Failed\" \n",
      "The number 15 in the last line is the exit code of the command. When using an HTTP\n",
      "GET hook handler, the reason may look like the following listing (you can try this by\n",
      "deploying the post-start-hook-httpget.yaml file from the book’s code archive).\n",
      "FailedSync   Error syncing pod, skipping: failed to \"StartContainer\" for \n",
      "             \"kubia\" with PostStart handler: Get \n",
      "             http://10.32.0.2:9090/postStart: dial tcp 10.32.0.2:9090: \n",
      "             getsockopt: connection refused: \"PostStart Hook Failed\" \n",
      "NOTE\n",
      "The post-start hook is intentionally misconfigured to use port 9090\n",
      "instead of the correct port 8080, to show what happens when the hook fails.\n",
      "The standard and error outputs of command-based post-start hooks aren’t logged any-\n",
      "where, so you may want to have the process the hook invokes log to a file in the con-\n",
      "tainer’s filesystem, which will allow you to examine the contents of the file with\n",
      "something like this:\n",
      "$ kubectl exec my-pod cat logfile.txt \n",
      "If the container gets restarted for whatever reason (including because the hook failed),\n",
      "the file may be gone before you can examine it. You can work around that by mount-\n",
      "ing an emptyDir volume into the container and having the hook write to it.\n",
      "USING A PRE-STOP CONTAINER LIFECYCLE HOOK\n",
      "A pre-stop hook is executed immediately before a container is terminated. When a\n",
      "container needs to be terminated, the Kubelet will run the pre-stop hook, if config-\n",
      "ured, and only then send a SIGTERM to the process (and later kill the process if it\n",
      "doesn’t terminate gracefully). \n",
      " A pre-stop hook can be used to initiate a graceful shutdown of the container, if it\n",
      "doesn’t shut down gracefully upon receipt of a SIGTERM signal. They can also be used\n",
      "to perform arbitrary operations before shutdown without having to implement those\n",
      "operations in the application itself (this is useful when you’re running a third-party\n",
      "app, whose source code you don’t have access to and/or can’t modify). \n",
      " Configuring a pre-stop hook in a pod manifest isn’t very different from adding a\n",
      "post-start hook. The previous example showed a post-start hook that executes a com-\n",
      "Listing 17.4\n",
      "Pod’s events showing the exit code of the failed command-based hook\n",
      "Listing 17.5\n",
      "Pod’s events showing the reason why an HTTP GET hook failed\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 520, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "488\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "mand, so we’ll look at a pre-stop hook that performs an HTTP GET request now. The\n",
      "following listing shows how to define a pre-stop HTTP GET hook in a pod.\n",
      "    lifecycle:\n",
      "      preStop:            \n",
      "        httpGet:          \n",
      "          port: 8080          \n",
      "          path: shutdown      \n",
      "The pre-stop hook defined in this listing performs an HTTP GET request to http:/\n",
      "/\n",
      "POD_IP:8080/shutdown as soon as the Kubelet starts terminating the container.\n",
      "Apart from the port and path shown in the listing, you can also set the fields scheme\n",
      "(HTTP or HTTPS) and host, as well as httpHeaders that should be sent in the\n",
      "request. The host field defaults to the pod IP. Be sure not to set it to localhost,\n",
      "because localhost would refer to the node, not the pod.\n",
      " In contrast to the post-start hook, the container will be terminated regardless of\n",
      "the result of the hook—an error HTTP response code or a non-zero exit code when\n",
      "using a command-based hook will not prevent the container from being terminated.\n",
      "If the pre-stop hook fails, you’ll see a FailedPreStopHook warning event among the\n",
      "pod’s events, but because the pod is deleted soon afterward (after all, the pod’s dele-\n",
      "tion is what triggered the pre-stop hook in the first place), you may not even notice\n",
      "that the pre-stop hook failed to run properly. \n",
      "TIP\n",
      "If the successful completion of the pre-stop hook is critical to the proper\n",
      "operation of your system, verify whether it’s being executed at all. I’ve wit-\n",
      "nessed situations where the pre-stop hook didn’t run and the developer\n",
      "wasn’t even aware of that.\n",
      "USING A PRE-STOP HOOK BECAUSE YOUR APP DOESN’T RECEIVE THE SIGTERM SIGNAL\n",
      "Many developers make the mistake of defining a pre-stop hook solely to send a SIGTERM\n",
      "signal to their apps in the pre-stop hook. They do this because they don’t see their appli-\n",
      "cation receive the SIGTERM signal sent by the Kubelet. The reason why the signal isn’t\n",
      "received by the application isn’t because Kubernetes isn’t sending it, but because the sig-\n",
      "nal isn’t being passed to the app process inside the container itself. If your container\n",
      "image is configured to run a shell, which in turn runs the app process, the signal may be\n",
      "eaten up by the shell itself, instead of being passed down to the child process.\n",
      " In such cases, instead of adding a pre-stop hook to send the signal directly to your\n",
      "app, the proper fix is to make sure the shell passes the signal to the app. This can be\n",
      "achieved by handling the signal in the shell script running as the main container pro-\n",
      "cess and then passing it on to the app. Or you could not configure the container image\n",
      "to run a shell at all and instead run the application binary directly. You do this by using\n",
      "the exec form of ENTRYPOINT or CMD in the Dockerfile: ENTRYPOINT [\"/mybinary\"]\n",
      "instead of ENTRYPOINT /mybinary.\n",
      "Listing 17.6\n",
      "A pre-stop hook YAML snippet: pre-stop-hook-httpget.yaml\n",
      "This is a pre-stop hook that \n",
      "performs an HTTP GET request.\n",
      "The request is sent to \n",
      "http://POD_IP:8080/shutdown.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 521, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "489\n",
      "Understanding the pod’s lifecycle\n",
      " A container using the first form runs the mybinary executable as its main process,\n",
      "whereas the second form runs a shell as the main process with the mybinary process\n",
      "executed as a child of the shell process.\n",
      "UNDERSTANDING THAT LIFECYCLE HOOKS TARGET CONTAINERS, NOT PODS\n",
      "As a final thought on post-start and pre-stop hooks, let me emphasize that these lifecy-\n",
      "cle hooks relate to containers, not pods. You shouldn’t use a pre-stop hook for run-\n",
      "ning actions that need to be performed when the pod is terminating. The reason is\n",
      "that the pre-stop hook gets called when the container is being terminated (most likely\n",
      "because of a failed liveness probe). This may happen multiple times in the pod’s life-\n",
      "time, not only when the pod is in the process of being shut down. \n",
      "17.2.5 Understanding pod shutdown\n",
      "We’ve touched on the subject of pod termination, so let’s explore this subject in more\n",
      "detail and go over exactly what happens during pod shutdown. This is important for\n",
      "understanding how to cleanly shut down an application running in a pod.\n",
      " Let’s start at the beginning. A pod’s shut-down is triggered by the deletion of the\n",
      "Pod object through the API server. Upon receiving an HTTP DELETE request, the\n",
      "API server doesn’t delete the object yet, but only sets a deletionTimestamp field in it.\n",
      "Pods that have the deletionTimestamp field set are terminating. \n",
      " Once the Kubelet notices the pod needs to be terminated, it starts terminating\n",
      "each of the pod’s containers. It gives each container time to shut down gracefully, but\n",
      "the time is limited. That time is called the termination grace period and is configu-\n",
      "rable per pod. The timer starts as soon as the termination process starts. Then the fol-\n",
      "lowing sequence of events is performed:\n",
      "1\n",
      "Run the pre-stop hook, if one is configured, and wait for it to finish.\n",
      "2\n",
      "Send the SIGTERM signal to the main process of the container.\n",
      "3\n",
      "Wait until the container shuts down cleanly or until the termination grace\n",
      "period runs out.\n",
      "4\n",
      "Forcibly kill the process with SIGKILL, if it hasn’t terminated gracefully yet.\n",
      "The sequence of events is illustrated in figure 17.5.\n",
      "Pre-stop hook process\n",
      "Termination grace period\n",
      "Main container process\n",
      "Container shutdown\n",
      "initiated\n",
      "Container killed\n",
      "if still running\n",
      "Time\n",
      "SIGTERM\n",
      "SIGKILL\n",
      "Figure 17.5\n",
      "The container termination sequence\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 522, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "490\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "SPECIFYING THE TERMINATION GRACE PERIOD\n",
      "The termination grace period can be configured in the pod spec by setting the spec.\n",
      "terminationGracePeriodSeconds field. It defaults to 30, which means the pod’s con-\n",
      "tainers will be given 30 seconds to terminate gracefully before they’re killed forcibly. \n",
      "TIP\n",
      "You should set the grace period to long enough so your process can fin-\n",
      "ish cleaning up in that time. \n",
      "The grace period specified in the pod spec can also be overridden when deleting the\n",
      "pod like this:\n",
      "$ kubectl delete po mypod --grace-period=5\n",
      "This will make the Kubelet wait five seconds for the pod to shut down cleanly. When\n",
      "all the pod’s containers stop, the Kubelet notifies the API server and the Pod resource\n",
      "is finally deleted. You can force the API server to delete the resource immediately,\n",
      "without waiting for confirmation, by setting the grace period to zero and adding the\n",
      "--force option like this:\n",
      "$ kubectl delete po mypod --grace-period=0 --force\n",
      "Be careful when using this option, especially with pods of a StatefulSet. The Stateful-\n",
      "Set controller takes great care to never run two instances of the same pod at the same\n",
      "time (two pods with the same ordinal index and name and attached to the same\n",
      "PersistentVolume). By force-deleting a pod, you’ll cause the controller to create a\n",
      "replacement pod without waiting for the containers of the deleted pod to shut\n",
      "down. In other words, two instances of the same pod might be running at the same\n",
      "time, which may cause your stateful cluster to malfunction. Only delete stateful pods\n",
      "forcibly when you’re absolutely sure the pod isn’t running anymore or can’t talk to\n",
      "the other members of the cluster (you can be sure of this when you confirm that the\n",
      "node that hosted the pod has failed or has been disconnected from the network and\n",
      "can’t reconnect). \n",
      " Now that you understand how containers are shut down, let’s look at it from the\n",
      "application’s perspective and go over how applications should handle the shutdown\n",
      "procedure.\n",
      "IMPLEMENTING THE PROPER SHUTDOWN HANDLER IN YOUR APPLICATION\n",
      "Applications should react to a SIGTERM signal by starting their shut-down procedure\n",
      "and terminating when it finishes. Instead of handling the SIGTERM signal, the applica-\n",
      "tion can be notified to shut down through a pre-stop hook. In both cases, the app\n",
      "then only has a fixed amount of time to terminate cleanly. \n",
      " But what if you can’t predict how long the app will take to shut down cleanly? For\n",
      "example, imagine your app is a distributed data store. On scale-down, one of the pod\n",
      "instances will be deleted and therefore shut down. In the shut-down procedure, the\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 523, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "491\n",
      "Understanding the pod’s lifecycle\n",
      "pod needs to migrate all its data to the remaining pods to make sure it’s not lost.\n",
      "Should the pod start migrating the data upon receiving a termination signal (through\n",
      "either the SIGTERM signal or through a pre-stop hook)? \n",
      " Absolutely not! This is not recommended for at least the following two reasons:\n",
      "A container terminating doesn’t necessarily mean the whole pod is being\n",
      "terminated.\n",
      "You have no guarantee the shut-down procedure will finish before the process\n",
      "is killed.\n",
      "This second scenario doesn’t happen only when the grace period runs out before the\n",
      "application has finished shutting down gracefully, but also when the node running\n",
      "the pod fails in the middle of the container shut-down sequence. Even if the node\n",
      "then starts up again, the Kubelet will not restart the shut-down procedure (it won’t\n",
      "even start up the container again). There are absolutely no guarantees that the pod\n",
      "will be allowed to complete its whole shut-down procedure.\n",
      "REPLACING CRITICAL SHUT-DOWN PROCEDURES WITH DEDICATED SHUT-DOWN PROCEDURE PODS\n",
      "How do you ensure that a critical shut-down procedure that absolutely must run to\n",
      "completion does run to completion (for example, to ensure that a pod’s data is\n",
      "migrated to other pods)?\n",
      " One solution is for the app (upon receipt of a termination signal) to create a new\n",
      "Job resource that would run a new pod, whose sole job is to migrate the deleted pod’s\n",
      "data to the remaining pods. But if you’ve been paying attention, you’ll know that you\n",
      "have no guarantee the app will indeed manage to create the Job object every single\n",
      "time. What if the node fails exactly when the app tries to do that? \n",
      " The proper way to handle this problem is by having a dedicated, constantly run-\n",
      "ning pod that keeps checking for the existence of orphaned data. When this pod finds\n",
      "the orphaned data, it can migrate it to the remaining pods. Rather than a constantly\n",
      "running pod, you can also use a CronJob resource and run the pod periodically. \n",
      " You may think StatefulSets could help here, but they don’t. As you’ll remember,\n",
      "scaling down a StatefulSet leaves PersistentVolumeClaims orphaned, leaving the data\n",
      "stored on the PersistentVolume stranded. Yes, upon a subsequent scale-up, the Persistent-\n",
      "Volume will be reattached to the new pod instance, but what if that scale-up never\n",
      "happens (or happens after a long time)? For this reason, you may want to run a\n",
      "data-migrating pod also when using StatefulSets (this scenario is shown in figure 17.6).\n",
      "To prevent the migration from occurring during an application upgrade, the data-\n",
      "migrating pod could be configured to wait a while to give the stateful pod time to\n",
      "come up again before performing the migration.\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 524, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "492\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "17.3\n",
      "Ensuring all client requests are handled properly\n",
      "You now have a good sense of how to make pods shut down cleanly. Now, we’ll look at\n",
      "the pod’s lifecycle from the perspective of the pod’s clients (clients consuming the ser-\n",
      "vice the pod is providing). This is important to understand if you don’t want clients to\n",
      "run into problems when you scale pods up or down.\n",
      " It goes without saying that you want all client requests to be handled properly. You\n",
      "obviously don’t want to see broken connections when pods are starting up or shutting\n",
      "down. By itself, Kubernetes doesn’t prevent this from happening. Your app needs to\n",
      "follow a few rules to prevent broken connections. First, let’s focus on making sure all\n",
      "connections are handled properly when the pod starts up.\n",
      "17.3.1 Preventing broken client connections when a pod is starting up\n",
      "Ensuring each connection is handled properly at pod startup is simple if you under-\n",
      "stand how Services and service Endpoints work. When a pod is started, it’s added as an\n",
      "endpoint to all the Services, whose label selector matches the pod’s labels. As you may\n",
      "remember from chapter 5, the pod also needs to signal to Kubernetes that it’s ready.\n",
      "Until it is, it won’t become a service endpoint and therefore won’t receive any requests\n",
      "from clients. \n",
      " If you don’t specify a readiness probe in your pod spec, the pod is always considered\n",
      "ready. It will start receiving requests almost immediately—as soon as the first kube-proxy\n",
      "updates the iptables rules on its node and the first client pod tries to connect to the\n",
      "service. If your app isn’t ready to accept connections by then, clients will see “connec-\n",
      "tion refused” types of errors.\n",
      " All you need to do is make sure that your readiness probe returns success only\n",
      "when your app is ready to properly handle incoming requests. A good first step is to\n",
      "add an HTTP GET readiness probe and point it to the base URL of your app. In many\n",
      "Pod\n",
      "A-0\n",
      "Pod\n",
      "A-1\n",
      "StatefulSet A\n",
      "Replicas: 2\n",
      "Scale\n",
      "down\n",
      "PVC\n",
      "A-0\n",
      "PV\n",
      "PVC\n",
      "A-1\n",
      "PV\n",
      "Pod\n",
      "A-0\n",
      "StatefulSet A\n",
      "Replicas: 1\n",
      "Transfers data to\n",
      "remaining pod(s)\n",
      "Connects to\n",
      "orphaned PVC\n",
      "Data-migrating\n",
      "Pod\n",
      "Job\n",
      "PVC\n",
      "A-0\n",
      "PV\n",
      "PVC\n",
      "A-1\n",
      "PV\n",
      "Figure 17.6\n",
      "Using a dedicated pod to migrate data \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 525, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "493\n",
      "Ensuring all client requests are handled properly\n",
      "cases that gets you far enough and saves you from having to implement a special read-\n",
      "iness endpoint in your app. \n",
      "17.3.2 Preventing broken connections during pod shut-down\n",
      "Now let’s see what happens at the other end of a pod’s life—when the pod is deleted and\n",
      "its containers are terminated. We’ve already talked about how the pod’s containers\n",
      "should start shutting down cleanly as soon they receive the SIGTERM signal (or when its\n",
      "pre-stop hook is executed). But does that ensure all client requests are handled properly? \n",
      " How should the app behave when it receives a termination signal? Should it con-\n",
      "tinue to accept requests? What about requests that have already been received but\n",
      "haven’t completed yet? What about persistent HTTP connections, which may be in\n",
      "between requests, but are open (when no active request exists on the connection)?\n",
      "Before we can answer those questions, we need to take a detailed look at the chain of\n",
      "events that unfolds across the cluster when a Pod is deleted. \n",
      "UNDERSTANDING THE SEQUENCE OF EVENTS OCCURRING AT POD DELETION\n",
      "In chapter 11 we took an in-depth look at what components make up a Kubernetes clus-\n",
      "ter. You need to always keep in mind that those components run as separate processes on\n",
      "multiple machines. They aren’t all part of a single big monolithic process. It takes time\n",
      "for all the components to be on the same page regarding the state of the cluster. Let’s\n",
      "explore this fact by looking at what happens across the cluster when a Pod is deleted.\n",
      " When a request for a pod deletion is received by the API server, it first modifies the\n",
      "state in etcd and then notifies its watchers of the deletion. Among those watchers are\n",
      "the Kubelet and the Endpoints controller. The two sequences of events, which happen\n",
      "in parallel (marked with either A or B), are shown in figure 17.7.\n",
      "A2. Stop\n",
      "containers\n",
      "API server\n",
      "kube-proxy\n",
      "Kubelet\n",
      "Worker node\n",
      "Endpoints\n",
      "controller\n",
      "kube-proxy\n",
      "Pod\n",
      "(containers)\n",
      "Client\n",
      "Delete\n",
      "pod\n",
      "B1. Pod deletion\n",
      "notiﬁcation\n",
      "B2. Remove pod\n",
      "as endpoint\n",
      "A1. Pod deletion\n",
      "notiﬁcation\n",
      "B3. Endpoint\n",
      "modiﬁcation\n",
      "notiﬁcation\n",
      "B4. Remove pod\n",
      "from iptables\n",
      "B4. Remove pod\n",
      "from iptables\n",
      "iptables\n",
      "iptables\n",
      "Worker node\n",
      "Figure 17.7\n",
      "Sequence of events that occurs when a Pod is deleted\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 526, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "494\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "In the A sequence of events, you’ll see that as soon as the Kubelet receives the notifica-\n",
      "tion that the pod should be terminated, it initiates the shutdown sequence as explained\n",
      "in section 17.2.5 (run the pre-stop hook, send SIGTERM, wait for a period of time, and\n",
      "then forcibly kill the container if it hasn’t yet terminated on its own). If the app\n",
      "responds to the SIGTERM by immediately ceasing to receive client requests, any client\n",
      "trying to connect to it will receive a Connection Refused error. The time it takes for\n",
      "this to happen from the time the pod is deleted is relatively short because of the direct\n",
      "path from the API server to the Kubelet.\n",
      " Now, let’s look at what happens in the other sequence of events—the one leading\n",
      "up to the pod being removed from the iptables rules (sequence B in the figure).\n",
      "When the Endpoints controller (which runs in the Controller Manager in the Kuber-\n",
      "netes Control Plane) receives the notification of the Pod being deleted, it removes\n",
      "the pod as an endpoint in all services that the pod is a part of. It does this by modify-\n",
      "ing the Endpoints API object by sending a REST request to the API server. The API\n",
      "server then notifies all clients watching the Endpoints object. Among those watchers\n",
      "are all the kube-proxies running on the worker nodes. Each of these proxies then\n",
      "updates the iptables rules on its node, which is what prevents new connections\n",
      "from being forwarded to the terminating pod. An important detail here is that\n",
      "removing the iptables rules has no effect on existing connections—clients who are\n",
      "already connected to the pod will still send additional requests to the pod through\n",
      "those existing connections.\n",
      " Both of these sequences of events happen in parallel. Most likely, the time it takes\n",
      "to shut down the app’s process in the pod is slightly shorter than the time required for\n",
      "the iptables rules to be updated. The chain of events that leads to iptables rules\n",
      "being updated is considerably longer (see figure 17.8), because the event must first\n",
      "reach the Endpoints controller, which then sends a new request to the API server, and\n",
      "A2. Send\n",
      "SIGTERM\n",
      "API server\n",
      "API server\n",
      "Kubelet\n",
      "Endpoints\n",
      "controller\n",
      "Container(s)\n",
      "A1. Watch\n",
      "notiﬁcation\n",
      "(pod modiﬁed)\n",
      "B1. Watch\n",
      "notiﬁcation\n",
      "(pod modiﬁed)\n",
      "B2. Remove pod’s IP\n",
      "from endpoints\n",
      "kube-proxy\n",
      "B4. Update\n",
      "iptables\n",
      "rules\n",
      "iptables\n",
      "kube-proxy\n",
      "iptables\n",
      "Time\n",
      "B3. Watch notiﬁcation\n",
      "(endpoints changed)\n",
      "Figure 17.8\n",
      "Timeline of events when pod is deleted\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 527, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "495\n",
      "Ensuring all client requests are handled properly\n",
      "then the API server must notify the kube-proxy before the proxy finally modifies the\n",
      "iptables rules. A high probability exists that the SIGTERM signal will be sent well\n",
      "before the iptables rules are updated on all nodes.\n",
      " The end result is that the pod may still receive client requests after it was sent the\n",
      "termination signal. If the app closes the server socket and stops accepting connections\n",
      "immediately, this will cause clients to receive “Connection Refused” types of errors\n",
      "(similar to what happens at pod startup if your app isn’t capable of accepting connec-\n",
      "tions immediately and you don’t define a readiness probe for it). \n",
      "SOLVING THE PROBLEM\n",
      "Googling solutions to this problem makes it seem as though adding a readiness probe\n",
      "to your pod will solve the problem. Supposedly, all you need to do is make the readi-\n",
      "ness probe start failing as soon as the pod receives the SIGTERM. This is supposed to\n",
      "cause the pod to be removed as the endpoint of the service. But the removal would\n",
      "happen only after the readiness probe fails for a few consecutive times (this is configu-\n",
      "rable in the readiness probe spec). And, obviously, the removal then still needs to\n",
      "reach the kube-proxy before the pod is removed from iptables rules. \n",
      " In reality, the readiness probe has absolutely no bearing on the whole process at\n",
      "all. The Endpoints controller removes the pod from the service Endpoints as soon as\n",
      "it receives notice of the pod being deleted (when the deletionTimestamp field in the\n",
      "pod’s spec is no longer null). From that point on, the result of the readiness probe\n",
      "is irrelevant.\n",
      " What’s the proper solution to the problem? How can you make sure all requests\n",
      "are handled fully?\n",
      " It’s clear the pod needs to keep accepting connections even after it receives the ter-\n",
      "mination signal up until all the kube-proxies have finished updating the iptables\n",
      "rules. Well, it’s not only the kube-proxies. There may also be Ingress controllers or\n",
      "load balancers forwarding connections to the pod directly, without going through the\n",
      "Service (iptables). This also includes clients using client-side load-balancing. To\n",
      "ensure none of the clients experience broken connections, you’d have to wait until all\n",
      "of them somehow notify you they’ll no longer forward connections to the pod. \n",
      " That’s impossible, because all those components are distributed across many dif-\n",
      "ferent computers. Even if you knew the location of every one of them and could wait\n",
      "until all of them say it’s okay to shut down the pod, what do you do if one of them\n",
      "doesn’t respond? How long do you wait for the response? Remember, during that\n",
      "time, you’re holding up the shut-down process. \n",
      " The only reasonable thing you can do is wait for a long-enough time to ensure all\n",
      "the proxies have done their job. But how long is long enough? A few seconds should\n",
      "be enough in most situations, but there’s no guarantee it will suffice every time. When\n",
      "the API server or the Endpoints controller is overloaded, it may take longer for the\n",
      "notification to reach the kube-proxy. It’s important to understand that you can’t solve\n",
      "the problem perfectly, but even adding a 5- or 10-second delay should improve the\n",
      "user experience considerably. You can use a longer delay, but don’t go overboard,\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 528, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "496\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "because the delay will prevent the container from shutting down promptly and will\n",
      "cause the pod to be shown in lists long after it has been deleted, which is always frus-\n",
      "trating to the user deleting the pod.\n",
      "WRAPPING UP THIS SECTION\n",
      "To recap—properly shutting down an application includes these steps:\n",
      "Wait for a few seconds, then stop accepting new connections. \n",
      "Close all keep-alive connections not in the middle of a request.\n",
      "Wait for all active requests to finish.\n",
      "Then shut down completely.\n",
      "To understand what’s happening with the connections and requests during this pro-\n",
      "cess, examine figure 17.9 carefully.\n",
      "Not as simple as exiting the process immediately upon receiving the termination sig-\n",
      "nal, right? Is it worth going through all this? That’s for you to decide. But the least you\n",
      "can do is add a pre-stop hook that waits a few seconds, like the one in the following\n",
      "listing, perhaps.\n",
      "    lifecycle:                    \n",
      "      preStop:                    \n",
      "        exec:                     \n",
      "          command:                \n",
      "          - sh\n",
      "          - -c\n",
      "          - \"sleep 5\"\n",
      "Listing 17.7\n",
      "A pre-stop hook for preventing broken connections\n",
      "Delay (few seconds)\n",
      "Key:\n",
      "Connection\n",
      "Request\n",
      "iptables rules\n",
      "updated on all nodes\n",
      "(no new connections\n",
      "after this point)\n",
      "Stop\n",
      "accepting new\n",
      "connections\n",
      "Close inactive\n",
      "keep-alive\n",
      "connections\n",
      "and wait for\n",
      "active requests\n",
      "to ﬁnish\n",
      "When last\n",
      "active request\n",
      "completes,\n",
      "shut down\n",
      "completely\n",
      "Time\n",
      "SIGTERM\n",
      "Figure 17.9\n",
      "Properly handling existing and new connections after receiving a termination signal\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 529, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "497\n",
      "Making your apps easy to run and manage in Kubernetes\n",
      "This way, you don’t need to modify the code of your app at all. If your app already\n",
      "ensures all in-flight requests are processed completely, this pre-stop delay may be all\n",
      "you need.\n",
      "17.4\n",
      "Making your apps easy to run and manage in Kubernetes\n",
      "I hope you now have a better sense of how to make your apps handle clients nicely.\n",
      "Now we’ll look at other aspects of how an app should be built to make it easier to man-\n",
      "age in Kubernetes.\n",
      "17.4.1 Making manageable container images\n",
      "When you package your app into an image, you can choose to include the app’s\n",
      "binary executable and any additional libraries it needs, or you can package up a whole\n",
      "OS filesystem along with the app. Way too many people do this, even though it’s usu-\n",
      "ally unnecessary.\n",
      " Do you need every single file from an OS distribution in your image? Probably not.\n",
      "Most of the files will never be used and will make your image larger than it needs to\n",
      "be. Sure, the layering of images makes sure each individual layer is downloaded only\n",
      "once, but even having to wait longer than necessary the first time a pod is scheduled\n",
      "to a node is undesirable.\n",
      " Deploying new pods and scaling them should be fast. This demands having small\n",
      "images without unnecessary cruft. If you’re building apps using the Go language, your\n",
      "images don’t need to include anything else apart from the app’s single binary execut-\n",
      "able file. This makes Go-based container images extremely small and perfect for\n",
      "Kubernetes.\n",
      "TIP\n",
      "Use the FROM scratch directive in the Dockerfile for these images.\n",
      "But in practice, you’ll soon see these minimal images are extremely difficult to debug.\n",
      "The first time you need to run a tool such as ping, dig, curl, or something similar\n",
      "inside the container, you’ll realize how important it is for container images to also\n",
      "include at least a limited set of these tools. I can’t tell you what to include and what\n",
      "not to include in your images, because it depends on how you do things, so you’ll\n",
      "need to find the sweet spot yourself.\n",
      "17.4.2 Properly tagging your images and using imagePullPolicy wisely\n",
      "You’ll also soon learn that referring to the latest image tag in your pod manifests will\n",
      "cause problems, because you can’t tell which version of the image each individual pod\n",
      "replica is running. Even if initially all your pod replicas run the same image version, if\n",
      "you push a new version of the image under the latest tag, and then pods are resched-\n",
      "uled (or you scale up your Deployment), the new pods will run the new version,\n",
      "whereas the old ones will still be running the old one. Also, using the latest tag\n",
      "makes it impossible to roll back to a previous version (unless you push the old version\n",
      "of the image again).\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 530, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "498\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      " It’s almost mandatory to use tags containing a proper version designator instead\n",
      "of latest, except maybe in development. Keep in mind that if you use mutable tags\n",
      "(you push changes to the same tag), you’ll need to set the imagePullPolicy field in\n",
      "the pod spec to Always. But if you use that in production pods, be aware of the big\n",
      "caveat associated with it. If the image pull policy is set to Always, the container run-\n",
      "time will contact the image registry every time a new pod is deployed. This slows\n",
      "down pod startup a bit, because the node needs to check if the image has been mod-\n",
      "ified. Worse yet, this policy prevents the pod from starting up when the registry can-\n",
      "not be contacted.\n",
      "17.4.3 Using multi-dimensional instead of single-dimensional labels\n",
      "Don’t forget to label all your resources, not only Pods. Make sure you add multiple\n",
      "labels to each resource, so they can be selected across each individual dimension. You\n",
      "(or the ops team) will be grateful you did it when the number of resources increases.\n",
      " Labels may include things like\n",
      "The name of the application (or perhaps microservice) the resource belongs to\n",
      "Application tier (front-end, back-end, and so on)\n",
      "Environment (development, QA, staging, production, and so on)\n",
      "Version\n",
      "Type of release (stable, canary, green or blue for green/blue deployments, and\n",
      "so on)\n",
      "Tenant (if you’re running separate pods for each tenant instead of using name-\n",
      "spaces)\n",
      "Shard for sharded systems\n",
      "This will allow you to manage resources in groups instead of individually and make it\n",
      "easy to see where each resource belongs.\n",
      "17.4.4 Describing each resource through annotations\n",
      "To add additional information to your resources use annotations. At the least,\n",
      "resources should contain an annotation describing the resource and an annotation\n",
      "with contact information of the person responsible for it. \n",
      " In a microservices architecture, pods could contain an annotation that lists the\n",
      "names of the other services the pod is using. This makes it possible to show dependen-\n",
      "cies between pods. Other annotations could include build and version information\n",
      "and metadata used by tooling or graphical user interfaces (icon names, and so on).\n",
      " Both labels and annotations make managing running applications much easier, but\n",
      "nothing is worse than when an application starts crashing and you don’t know why.\n",
      "17.4.5 Providing information on why the process terminated\n",
      "Nothing is more frustrating than having to figure out why a container terminated\n",
      "(or is even terminating continuously), especially if it happens at the worst possible\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 531, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "499\n",
      "Making your apps easy to run and manage in Kubernetes\n",
      "moment. Be nice to the ops people and make their lives easier by including all the\n",
      "necessary debug information in your log files. \n",
      " But to make triage even easier, you can use one other Kubernetes feature that\n",
      "makes it possible to show the reason why a container terminated in the pod’s status.\n",
      "You do this by having the process write a termination message to a specific file in the\n",
      "container’s filesystem. The contents of this file are read by the Kubelet when the con-\n",
      "tainer terminates and are shown in the output of kubectl describe pod. If an applica-\n",
      "tion uses this mechanism, an operator can quickly see why the app terminated without\n",
      "even having to look at the container logs. \n",
      " The default file the process needs to write the message to is /dev/termination-log,\n",
      "but it can be changed by setting the terminationMessagePath field in the container\n",
      "definition in the pod spec. \n",
      " You can see this in action by running a pod whose container dies immediately, as\n",
      "shown in the following listing.\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: pod-with-termination-message\n",
      "spec:\n",
      "  containers:\n",
      "  - image: busybox\n",
      "    name: main\n",
      "    terminationMessagePath: /var/termination-reason         \n",
      "    command:\n",
      "    - sh\n",
      "    - -c\n",
      "    - 'echo \"I''ve had enough\" > /var/termination-reason ; exit 1'   \n",
      "When running this pod, you’ll soon see the pod’s status shown as CrashLoopBackOff.\n",
      "If you then use kubectl describe, you can see why the container died, without having\n",
      "to dig down into its logs, as shown in the following listing.\n",
      "$ kubectl describe po\n",
      "Name:           pod-with-termination-message\n",
      "...\n",
      "Containers:\n",
      "...\n",
      "    State:      Waiting\n",
      "      Reason:   CrashLoopBackOff\n",
      "    Last State: Terminated\n",
      "      Reason:   Error\n",
      "      Message:  I've had enough          \n",
      "      Exit Code:        1\n",
      "      Started:          Tue, 21 Feb 2017 21:38:31 +0100\n",
      "      Finished:         Tue, 21 Feb 2017 21:38:31 +0100\n",
      "Listing 17.8\n",
      "Pod writing a termination message: termination-message.yaml\n",
      "Listing 17.9\n",
      "Seeing the container’s termination message with kubectl describe\n",
      "You’re overriding the \n",
      "default path of the \n",
      "termination message file.\n",
      "The container\n",
      "will write the\n",
      "message to\n",
      "the file just\n",
      "before exiting.\n",
      "You can see the reason \n",
      "why the container died \n",
      "without having to \n",
      "inspect its logs.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 532, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "500\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "    Ready:              False\n",
      "    Restart Count:      6\n",
      "As you can see, the “I’ve had enough” message the process wrote to the file /var/ter-\n",
      "mination-reason is shown in the container’s Last State section. Note that this mecha-\n",
      "nism isn’t limited only to containers that crash. It can also be used in pods that run a\n",
      "completable task and terminate successfully (you’ll find an example in the file termi-\n",
      "nation-message-success.yaml). \n",
      " This mechanism is great for terminated containers, but you’ll probably agree that\n",
      "a similar mechanism would also be useful for showing app-specific status messages of\n",
      "running, not only terminated, containers. Kubernetes currently doesn’t provide any\n",
      "such functionality and I’m not aware of any plans to introduce it.\n",
      "NOTE\n",
      "If the container doesn’t write the message to any file, you can set the\n",
      "terminationMessagePolicy field to FallbackToLogsOnError. In that case,\n",
      "the last few lines of the container’s log are used as its termination message\n",
      "(but only when the container terminates unsuccessfully).\n",
      "17.4.6 Handling application logs\n",
      "While we’re on the subject of application logging, let’s reiterate that apps should write\n",
      "to the standard output instead of files. This makes it easy to view logs with the kubectl\n",
      "logs command. \n",
      "TIP\n",
      "If a container crashes and is replaced with a new one, you’ll see the new\n",
      "container’s log. To see the previous container’s logs, use the --previous\n",
      "option with kubectl logs.\n",
      "If the application logs to a file instead of the standard output, you can display the log\n",
      "file using an alternative approach: \n",
      "$ kubectl exec <pod> cat <logfile>\n",
      "This executes the cat command inside the container and streams the logs back to\n",
      "kubectl, which prints them out in your terminal. \n",
      "COPYING LOG AND OTHER FILES TO AND FROM A CONTAINER\n",
      "You can also copy the log file to your local machine using the kubectl cp command,\n",
      "which we haven’t looked at yet. It allows you to copy files from and into a container. For\n",
      "example, if a pod called foo-pod and its single container contains a file at /var/log/\n",
      "foo.log, you can transfer it to your local machine with the following command:\n",
      "$ kubectl cp foo-pod:/var/log/foo.log foo.log\n",
      "To copy a file from your local machine into the pod, specify the pod’s name in the sec-\n",
      "ond argument:\n",
      "$ kubectl cp localfile foo-pod:/etc/remotefile\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 533, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "501\n",
      "Making your apps easy to run and manage in Kubernetes\n",
      "This copies the file localfile to /etc/remotefile inside the pod’s container. If the pod has\n",
      "more than one container, you specify the container using the -c containerName option.\n",
      "USING CENTRALIZED LOGGING\n",
      "In a production system, you’ll want to use a centralized, cluster-wide logging solution,\n",
      "so all your logs are collected and (permanently) stored in a central location. This\n",
      "allows you to examine historical logs and analyze trends. Without such a system, a\n",
      "pod’s logs are only available while the pod exists. As soon as it’s deleted, its logs are\n",
      "deleted also. \n",
      " Kubernetes by itself doesn’t provide any kind of centralized logging. The compo-\n",
      "nents necessary for providing a centralized storage and analysis of all the container\n",
      "logs must be provided by additional components, which usually run as regular pods in\n",
      "the cluster. \n",
      " Deploying centralized logging solutions is easy. All you need to do is deploy a few\n",
      "YAML/JSON manifests and you’re good to go. On Google Kubernetes Engine, it’s\n",
      "even easier. Check the Enable Stackdriver Logging checkbox when setting up the clus-\n",
      "ter. Setting up centralized logging on an on-premises Kubernetes cluster is beyond the\n",
      "scope of this book, but I’ll give you a quick overview of how it’s usually done.\n",
      " You may have already heard of the ELK stack composed of ElasticSearch, Logstash,\n",
      "and Kibana. A slightly modified variation is the EFK stack, where Logstash is replaced\n",
      "with FluentD. \n",
      " When using the EFK stack for centralized logging, each Kubernetes cluster node\n",
      "runs a FluentD agent (usually as a pod deployed through a DaemonSet), which is\n",
      "responsible for gathering the logs from the containers, tagging them with pod-specific\n",
      "information, and delivering them to ElasticSearch, which stores them persistently.\n",
      "ElasticSearch is also deployed as a pod somewhere in the cluster. The logs can then be\n",
      "viewed and analyzed in a web browser through Kibana, which is a web tool for visualiz-\n",
      "ing ElasticSearch data. It also usually runs as a pod and is exposed through a Service.\n",
      "The three components of the EFK stack are shown in the following figure.\n",
      "NOTE\n",
      "In the next chapter, you’ll learn about Helm charts. You can use charts\n",
      "created by the Kubernetes community to deploy the EFK stack instead of cre-\n",
      "ating your own YAML manifests. \n",
      "Node 1\n",
      "Container logs\n",
      "Kibana\n",
      "Web\n",
      "browser\n",
      "FluentD\n",
      "Node 2\n",
      "Container logs\n",
      "FluentD\n",
      "Node 3\n",
      "Container logs\n",
      "FluentD\n",
      "ElasticSearch\n",
      "Figure 17.10\n",
      "Centralized logging with FluentD, ElasticSearch, and Kibana\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 534, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "502\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "HANDLING MULTI-LINE LOG STATEMENTS\n",
      "The FluentD agent stores each line of the log file as an entry in the ElasticSearch\n",
      "data store. There’s one problem with that. Log statements spanning multiple lines,\n",
      "such as exception stack traces in Java, appear as separate entries in the centralized\n",
      "logging system. \n",
      " To solve this problem, you can have the apps output JSON instead of plain text.\n",
      "This way, a multiline log statement can be stored and shown in Kibana as a single\n",
      "entry. But that makes viewing logs with kubectl logs much less human-friendly. \n",
      " The solution may be to keep outputting human-readable logs to standard output,\n",
      "while writing JSON logs to a file and having them processed by FluentD. This requires\n",
      "configuring the node-level FluentD agent appropriately or adding a logging sidecar\n",
      "container to every pod. \n",
      "17.5\n",
      "Best practices for development and testing\n",
      "We’ve talked about what to be mindful of when developing apps, but we haven’t\n",
      "talked about the development and testing workflows that will help you streamline\n",
      "those processes. I don’t want to go into too much detail here, because everyone needs\n",
      "to find what works best for them, but here are a few starting points.\n",
      "17.5.1 Running apps outside of Kubernetes during development\n",
      "When you’re developing an app that will run in a production Kubernetes cluster, does\n",
      "that mean you also need to run it in Kubernetes during development? Not really. Hav-\n",
      "ing to build the app after each minor change, then build the container image, push it\n",
      "to a registry, and then re-deploy the pods would make development slow and painful.\n",
      "Luckily, you don’t need to go through all that trouble.\n",
      " You can always develop and run apps on your local machine, the way you’re used\n",
      "to. After all, an app running in Kubernetes is a regular (although isolated) process\n",
      "running on one of the cluster nodes. If the app depends on certain features the\n",
      "Kubernetes environment provides, you can easily replicate that environment on your\n",
      "development machine.\n",
      " I’m not even talking about running the app in a container. Most of the time, you\n",
      "don’t need that—you can usually run the app directly from your IDE. \n",
      "CONNECTING TO BACKEND SERVICES\n",
      "In production, if the app connects to a backend Service and uses the BACKEND_SERVICE\n",
      "_HOST and BACKEND_SERVICE_PORT environment variables to find the Service’s coordi-\n",
      "nates, you can obviously set those environment variables on your local machine manu-\n",
      "ally and point them to the backend Service, regardless of if it’s running outside or\n",
      "inside a Kubernetes cluster. If it’s running inside Kubernetes, you can always (at least\n",
      "temporarily) make the Service accessible externally by changing it to a NodePort or a\n",
      "LoadBalancer-type Service. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 535, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "503\n",
      "Best practices for development and testing\n",
      "CONNECTING TO THE API SERVER\n",
      "Similarly, if your app requires access to the Kubernetes API server when running\n",
      "inside a Kubernetes cluster, it can easily talk to the API server from outside the cluster\n",
      "during development. If it uses the ServiceAccount’s token to authenticate itself, you\n",
      "can always copy the ServiceAccount’s Secret’s files to your local machine with kubectl\n",
      "cp. The API server doesn’t care if the client accessing it is inside or outside the cluster. \n",
      " If the app uses an ambassador container like the one described in chapter 8, you\n",
      "don’t even need those Secret files. Run kubectl proxy on your local machine, run\n",
      "your app locally, and it should be ready to talk to your local kubectl proxy (as long as\n",
      "it and the ambassador container bind the proxy to the same port).\n",
      " In this case, you’ll need to make sure the user account your local kubectl is using\n",
      "has the same privileges as the ServiceAccount the app will run under.\n",
      "RUNNING INSIDE A CONTAINER EVEN DURING DEVELOPMENT\n",
      "When during development you absolutely have to run the app in a container for what-\n",
      "ever reason, there is a way of avoiding having to build the container image every time.\n",
      "Instead of baking the binaries into the image, you can always mount your local filesys-\n",
      "tem into the container through Docker volumes, for example. This way, after you\n",
      "build a new version of the app’s binaries, all you need to do is restart the container (or\n",
      "not even that, if hot-redeploy is supported). No need to rebuild the image.\n",
      "17.5.2 Using Minikube in development\n",
      "As you can see, nothing forces you to run your app inside Kubernetes during develop-\n",
      "ment. But you may do that anyway to see how the app behaves in a true Kubernetes\n",
      "environment.\n",
      " You may have used Minikube to run examples in this book. Although a Minikube\n",
      "cluster runs only a single worker node, it’s nevertheless a valuable method of trying\n",
      "out your app in Kubernetes (and, of course, developing all the resource manifests that\n",
      "make up your complete application). Minikube doesn’t offer everything that a proper\n",
      "multi-node Kubernetes cluster usually provides, but in most cases, that doesn’t matter.\n",
      "MOUNTING LOCAL FILES INTO THE MINIKUBE VM AND THEN INTO YOUR CONTAINERS\n",
      "When you’re developing with Minikube and you’d like to try out every change to your\n",
      "app in your Kubernetes cluster, you can mount your local filesystem into the Minikube\n",
      "VM using the minikube mount command and then mount it into your containers\n",
      "through a hostPath volume. You’ll find additional instructions on how to do that\n",
      "in the Minikube documentation at https:/\n",
      "/github.com/kubernetes/minikube/tree/\n",
      "master/docs.\n",
      "USING THE DOCKER DAEMON INSIDE THE MINIKUBE VM TO BUILD YOUR IMAGES\n",
      "If you’re developing your app with Minikube and planning to build the container\n",
      "image after every change, you can use the Docker daemon inside the Minikube VM to\n",
      "do the building, instead of having to build the image through your local Docker dae-\n",
      "mon, push it to a registry, and then have it pulled by the daemon in the VM. To use\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 536, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "504\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "Minikube’s Docker daemon, all you need to do is point your DOCKER_HOST environ-\n",
      "ment variable to it. Luckily, this is much easier than it sounds. All you need to do is\n",
      "run the following command on your local machine:\n",
      "$ eval $(minikube docker-env)\n",
      "This will set all the required environment variables for you. You then build your\n",
      "images the same way as if the Docker daemon was running on your local machine.\n",
      "After you build the image, you don’t need to push it anywhere, because it’s already\n",
      "stored locally on the Minikube VM, which means new pods can use the image immedi-\n",
      "ately. If your pods are already running, you either need to delete them or kill their\n",
      "containers so they’re restarted.\n",
      "BUILDING IMAGES LOCALLY AND COPYING THEM OVER TO THE MINIKUBE VM DIRECTLY\n",
      "If you can’t use the daemon inside the VM to build the images, you still have a way to\n",
      "avoid having to push the image to a registry and have the Kubelet running in the\n",
      "Minikube VM pull it. If you build the image on your local machine, you can copy it\n",
      "over to the Minikube VM with the following command:\n",
      "$ docker save <image> | (eval $(minikube docker-env) && docker load)\n",
      "As before, the image is immediately ready to be used in a pod. But make sure the\n",
      "imagePullPolicy in your pod spec isn’t set to Always, because that would cause the\n",
      "image to be pulled from the external registry again and you’d lose the changes you’ve\n",
      "copied over.\n",
      "COMBINING MINIKUBE WITH A PROPER KUBERNETES CLUSTER\n",
      "You have virtually no limit when developing apps with Minikube. You can even com-\n",
      "bine a Minikube cluster with a proper Kubernetes cluster. I sometimes run my devel-\n",
      "opment workloads in my local Minikube cluster and have them talk to my other\n",
      "workloads that are deployed in a remote multi-node Kubernetes cluster thousands of\n",
      "miles away. \n",
      " Once I’m finished with development, I can move my local workloads to the remote\n",
      "cluster with no modifications and with absolutely no problems thanks to how Kuber-\n",
      "netes abstracts away the underlying infrastructure from the app.\n",
      "17.5.3 Versioning and auto-deploying resource manifests\n",
      "Because Kubernetes uses a declarative model, you never have to figure out the current\n",
      "state of your deployed resources and issue imperative commands to bring that state to\n",
      "what you desire. All you need to do is tell Kubernetes your desired state and it will take\n",
      "all the necessary actions to reconcile the cluster state with the desired state.\n",
      " You can store your collection of resource manifests in a Version Control System,\n",
      "enabling you to perform code reviews, keep an audit trail, and roll back changes\n",
      "whenever necessary. After each commit, you can run the kubectl apply command to\n",
      "have your changes reflected in your deployed resources. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 537, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "505\n",
      "Best practices for development and testing\n",
      " If you run an agent that periodically (or when it detects a new commit) checks out\n",
      "your manifests from the Version Control System (VCS), and then runs the apply com-\n",
      "mand, you can manage your running apps simply by committing changes to the VCS\n",
      "without having to manually talk to the Kubernetes API server. Luckily, the people at\n",
      "Box (which coincidently was used to host this book’s manuscript and other materials)\n",
      "developed and released a tool called kube-applier, which does exactly what I described.\n",
      "You’ll find the tool’s source code at https:/\n",
      "/github.com/box/kube-applier.\n",
      " You can use multiple branches to deploy the manifests to a development, QA, stag-\n",
      "ing, and production cluster (or in different namespaces in the same cluster).\n",
      "17.5.4 Introducing Ksonnet as an alternative to writing YAML/JSON \n",
      "manifests\n",
      "We’ve seen a number of YAML manifests throughout the book. I don’t see writing\n",
      "YAML as too big of a problem, especially once you learn how to use kubectl explain\n",
      "to see the available options, but some people do. \n",
      " Just as I was finalizing the manuscript for this book, a new tool called Ksonnet was\n",
      "announced. It’s a library built on top of Jsonnet, which is a data templating language\n",
      "for building JSON data structures. Instead of writing the complete JSON by hand, it\n",
      "lets you define parameterized JSON fragments, give them a name, and then build a\n",
      "full JSON manifest by referencing those fragments by name, instead of repeating the\n",
      "same JSON code in multiple locations—much like you use functions or methods in a\n",
      "programming language. \n",
      " Ksonnet defines the fragments you’d find in Kubernetes resource manifests, allow-\n",
      "ing you to quickly build a complete Kubernetes resource JSON manifest with much\n",
      "less code. The following listing shows an example.\n",
      "local k = import \"../ksonnet-lib/ksonnet.beta.1/k.libsonnet\";\n",
      "local container = k.core.v1.container;\n",
      "local deployment = k.apps.v1beta1.deployment;\n",
      "local kubiaContainer =                              \n",
      "  container.default(\"kubia\", \"luksa/kubia:v1\") +    \n",
      "  container.helpers.namedPort(\"http\", 8080);        \n",
      "deployment.default(\"kubia\", kubiaContainer) +    \n",
      "deployment.mixin.spec.replicas(3)                \n",
      "The kubia.ksonnet file shown in the listing is converted to a full JSON Deployment\n",
      "manifest when you run the following command:\n",
      "$ jsonnet kubia.ksonnet\n",
      "Listing 17.10\n",
      "The kubia Deployment written with Ksonnet: kubia.ksonnet\n",
      "This defines a container called kubia, \n",
      "which uses the luksa/kubia:v1 image \n",
      "and includes a port called http.\n",
      "This will be expanded into a full \n",
      "Deployment resource. The kubiaContainer \n",
      "defined here will be included in the \n",
      "Deployment’s pod template.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 538, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "506\n",
      "CHAPTER 17\n",
      "Best practices for developing apps\n",
      "The power of Ksonnet and Jsonnet becomes apparent when you realize you can define\n",
      "your own higher-level fragments and make all your manifests consistent and duplica-\n",
      "tion-free. You’ll find more information on using and installing Ksonnet and Jsonnet at\n",
      "https:/\n",
      "/github.com/ksonnet/ksonnet-lib.\n",
      "17.5.5 Employing Continuous Integration and Continuous Delivery \n",
      "(CI/CD)\n",
      "We’ve touched on automating the deployment of Kubernetes resources two sections\n",
      "back, but you may want to set up a complete CI/CD pipeline for building your appli-\n",
      "cation binaries, container images, and resource manifests and then deploying them in\n",
      "one or more Kubernetes clusters.\n",
      " You’ll find many online resources talking about this subject. Here, I’d like to point\n",
      "you specifically to the Fabric8 project (http:/\n",
      "/fabric8.io), which is an integrated\n",
      "development platform for Kubernetes. It includes Jenkins, the well-known, open-\n",
      "source automation system, and various other tools to deliver a full CI/CD pipeline\n",
      "for DevOps-style development, deployment, and management of microservices on\n",
      "Kubernetes.\n",
      " If you’d like to build your own solution, I also suggest looking at one of the Google\n",
      "Cloud Platform’s online labs that talks about this subject. It’s available at https:/\n",
      "/\n",
      "github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes.\n",
      "17.6\n",
      "Summary\n",
      "Hopefully, the information in this chapter has given you an even deeper insight into\n",
      "how Kubernetes works and will help you build apps that feel right at home when\n",
      "deployed to a Kubernetes cluster. The aim of this chapter was to\n",
      "Show you how all the resources covered in this book come together to repre-\n",
      "sent a typical application running in Kubernetes.\n",
      "Make you think about the difference between apps that are rarely moved\n",
      "between machines and apps running as pods, which are relocated much more\n",
      "frequently.\n",
      "Help you understand that your multi-component apps (or microservices, if you\n",
      "will) shouldn’t rely on a specific start-up order.\n",
      "Introduce init containers, which can be used to initialize a pod or delay the start\n",
      "of the pod’s main containers until a precondition is met.\n",
      "Teach you about container lifecycle hooks and when to use them.\n",
      "Gain a deeper insight into the consequences of the distributed nature of\n",
      "Kubernetes components and its eventual consistency model.\n",
      "Learn how to make your apps shut down properly without breaking client\n",
      "connections.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 539, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "507\n",
      "Summary\n",
      "Give you a few small tips on how to make your apps easier to manage by keep-\n",
      "ing image sizes small, adding annotations and multi-dimensional labels to all\n",
      "your resources, and making it easier to see why an application terminated.\n",
      "Teach you how to develop Kubernetes apps and run them locally or in Mini-\n",
      "kube before deploying them on a proper multi-node cluster.\n",
      "In the next and final chapter, we’ll learn how you can extend Kubernetes with your\n",
      "own custom API objects and controllers and how others have done it to create com-\n",
      "plete Platform-as-a-Service solutions on top of Kubernetes.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 540, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "508\n",
      "Extending Kubernetes\n",
      "You’re almost done. To wrap up, we’ll look at how you can define your own API\n",
      "objects and create controllers for those objects. We’ll also look at how others have\n",
      "extended Kubernetes and built Platform-as-a-Service solutions on top of it.\n",
      "18.1\n",
      "Defining custom API objects\n",
      "Throughout the book, you’ve learned about the API objects that Kubernetes pro-\n",
      "vides and how they’re used to build application systems. Currently, Kubernetes\n",
      "users mostly use only these objects even though they represent relatively low-level,\n",
      "generic concepts. \n",
      "This chapter covers\n",
      "Adding custom objects to Kubernetes\n",
      "Creating a controller for the custom object\n",
      "Adding custom API servers\n",
      "Self-provisioning of services with the Kubernetes \n",
      "Service Catalog\n",
      "Red Hat’s OpenShift Container Platform\n",
      "Deis Workflow and Helm\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 541, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "509\n",
      "Defining custom API objects\n",
      " As the Kubernetes ecosystem evolves, you’ll see more and more high-level objects,\n",
      "which will be much more specialized than the resources Kubernetes supports today.\n",
      "Instead of dealing with Deployments, Services, ConfigMaps, and the like, you’ll create\n",
      "and manage objects that represent whole applications or software services. A custom\n",
      "controller will observe those high-level objects and create low-level objects based on\n",
      "them. For example, to run a messaging broker inside a Kubernetes cluster, all you’ll\n",
      "need to do is create an instance of a Queue resource and all the necessary Secrets,\n",
      "Deployments, and Services will be created by a custom Queue controller. Kubernetes\n",
      "already provides ways of adding custom resources like this. \n",
      "18.1.1 Introducing CustomResourceDefinitions\n",
      "To define a new resource type, all you need to do is post a CustomResourceDefinition\n",
      "object (CRD) to the Kubernetes API server. The CustomResourceDefinition object is\n",
      "the description of the custom resource type. Once the CRD is posted, users can then\n",
      "create instances of the custom resource by posting JSON or YAML manifests to the\n",
      "API server, the same as with any other Kubernetes resource.\n",
      "NOTE\n",
      "Prior to Kubernetes 1.7, custom resources were defined through Third-\n",
      "PartyResource objects, which were similar to CustomResourceDefinitions, but\n",
      "were removed in version 1.8.\n",
      "Creating a CRD so that users can create objects of the new type isn’t a useful feature if\n",
      "those objects don’t make something tangible happen in the cluster. Each CRD will\n",
      "usually also have an associated controller (an active component doing something\n",
      "based on the custom objects), the same way that all the core Kubernetes resources\n",
      "have an associated controller, as was explained in chapter 11. For this reason, to prop-\n",
      "erly show what CustomResourceDefinitions allow you to do other than adding\n",
      "instances of a custom object, a controller must be deployed as well. You’ll do that in\n",
      "the next example.\n",
      "INTRODUCING THE EXAMPLE CUSTOMRESOURCEDEFINITION\n",
      "Let’s imagine you want to allow users of your Kubernetes cluster to run static websites\n",
      "as easily as possible, without having to deal with Pods, Services, and other Kubernetes\n",
      "resources. What you want to achieve is for users to create objects of type Website that\n",
      "contain nothing more than the website’s name and the source from which the web-\n",
      "site’s files (HTML, CSS, PNG, and others) should be obtained. You’ll use a Git reposi-\n",
      "tory as the source of those files. When a user creates an instance of the Website\n",
      "resource, you want Kubernetes to spin up a new web server pod and expose it through\n",
      "a Service, as shown in figure 18.1.\n",
      " To create the Website resource, you want users to post manifests along the lines of\n",
      "the one shown in the following listing.\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 542, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "510\n",
      "CHAPTER 18\n",
      "Extending Kubernetes\n",
      "kind: Website        \n",
      "metadata:\n",
      "  name: kubia             \n",
      "spec:\n",
      "  gitRepo: https://github.com/luksa/kubia-website-example.git   \n",
      "Like all other resources, your resource contains a kind and a metadata.name field,\n",
      "and like most resources, it also contains a spec section. It contains a single field called\n",
      "gitRepo (you can choose the name)—it specifies the Git repository containing the\n",
      "website’s files. You’ll also need to include an apiVersion field, but you don’t know yet\n",
      "what its value must be for custom resources.\n",
      " If you try posting this resource to Kubernetes, you’ll receive an error because\n",
      "Kubernetes doesn’t know what a Website object is yet:\n",
      "$ kubectl create -f imaginary-kubia-website.yaml\n",
      "error: unable to recognize \"imaginary-kubia-website.yaml\": no matches for \n",
      "➥ /, Kind=Website\n",
      "Before you can create instances of your custom object, you need to make Kubernetes\n",
      "recognize them.\n",
      "CREATING A CUSTOMRESOURCEDEFINITION OBJECT\n",
      "To make Kubernetes accept your custom Website resource instances, you need to post\n",
      "the CustomResourceDefinition shown in the following listing to the API server.\n",
      "apiVersion: apiextensions.k8s.io/v1beta1       \n",
      "kind: CustomResourceDefinition                 \n",
      "metadata:\n",
      "  name: websites.extensions.example.com      \n",
      "spec:\n",
      "  scope: Namespaced                          \n",
      "Listing 18.1\n",
      "An imaginary custom resource: imaginary-kubia-website.yaml\n",
      "Listing 18.2\n",
      "A CustomResourceDefinition manifest: website-crd.yaml\n",
      "Website\n",
      "kind: Website\n",
      "metadata:\n",
      "name: kubia\n",
      "spec:\n",
      "gitRepo:\n",
      "github.com/.../kubia.git\n",
      "Pod:\n",
      "kubia-website\n",
      "Service:\n",
      "kubia-website\n",
      "Figure 18.1\n",
      "Each Website object should result in the creation of a Service and an HTTP \n",
      "server Pod.\n",
      "A custom \n",
      "object kind\n",
      "The name of the website \n",
      "(used for naming the \n",
      "resulting Service and Pod)\n",
      "The Git \n",
      "repository \n",
      "holding the \n",
      "website’s files\n",
      "CustomResourceDefinitions belong \n",
      "to this API group and version.\n",
      "The full\n",
      "name of\n",
      "your\n",
      "custom\n",
      "object\n",
      "You want Website resources \n",
      "to be namespaced.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 543, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "511\n",
      "Defining custom API objects\n",
      "  group: extensions.example.com                \n",
      "  version: v1                                  \n",
      "  names:                                    \n",
      "    kind: Website                           \n",
      "    singular: website                       \n",
      "    plural: websites                        \n",
      "After you post the descriptor to Kubernetes, it will allow you to create any number of\n",
      "instances of the custom Website resource. \n",
      " You can create the CRD from the website-crd.yaml file available in the code archive:\n",
      "$ kubectl create -f website-crd-definition.yaml\n",
      "customresourcedefinition \"websites.extensions.example.com\" created\n",
      "I’m sure you’re wondering about the long name of the CRD. Why not call it Website?\n",
      "The reason is to prevent name clashes. By adding a suffix to the name of the CRD\n",
      "(which will usually include the name of the organization that created the CRD), you\n",
      "keep CRD names unique. Luckily, the long name doesn’t mean you’ll need to create\n",
      "your Website resources with kind: websites.extensions.example.com, but as kind:\n",
      "Website, as specified in the names.kind property of the CRD. The extensions.exam-\n",
      "ple.com part is the API group of your resource. \n",
      " You’ve seen how creating Deployment objects requires you to set apiVersion to\n",
      "apps/v1beta1 instead of v1. The part before the slash is the API group (Deployments\n",
      "belong to the apps API group), and the part after it is the version name (v1beta1 in\n",
      "the case of Deployments). When creating instances of the custom Website resource,\n",
      "the apiVersion property will need to be set to extensions.example.com/v1.\n",
      "CREATING AN INSTANCE OF A CUSTOM RESOURCE\n",
      "Considering what you learned, you’ll now create a proper YAML for your Website\n",
      "resource instance. The YAML manifest is shown in the following listing.\n",
      "apiVersion: extensions.example.com/v1       \n",
      "kind: Website                               \n",
      "metadata:\n",
      "  name: kubia                                \n",
      "spec:\n",
      "  gitRepo: https://github.com/luksa/kubia-website-example.git\n",
      "The kind of your resource is Website, and the apiVersion is composed of the API\n",
      "group and the version number you defined in the CustomResourceDefinition.\n",
      " Create your Website object now:\n",
      "$ kubectl create -f kubia-website.yaml\n",
      "website \"kubia\" created\n",
      "Listing 18.3\n",
      "A custom Website resource: kubia-website.yaml\n",
      "Define an API group and version \n",
      "of the Website resource.\n",
      "You need to specify the various \n",
      "forms of the custom object’s name.\n",
      "Your custom API\n",
      "group and version\n",
      "This manifest \n",
      "describes a Website \n",
      "resource instance.\n",
      "The name of the \n",
      "Website instance\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 544, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "512\n",
      "CHAPTER 18\n",
      "Extending Kubernetes\n",
      "The response tells you that the API server has accepted and stored your custom\n",
      "Website object. Let’s see if you can now retrieve it. \n",
      "RETRIEVING INSTANCES OF A CUSTOM RESOURCE\n",
      "List all the websites in your cluster:\n",
      "$ kubectl get websites\n",
      "NAME      KIND\n",
      "kubia     Website.v1.extensions.example.com\n",
      "As with existing Kubernetes resources, you can create and then list instances of cus-\n",
      "tom resources. You can also use kubectl describe to see the details of your custom\n",
      "object, or retrieve the whole YAML with kubectl get, as in the following listing.\n",
      "$ kubectl get website kubia -o yaml\n",
      "apiVersion: extensions.example.com/v1\n",
      "kind: Website\n",
      "metadata:\n",
      "  creationTimestamp: 2017-02-26T15:53:21Z\n",
      "  name: kubia\n",
      "  namespace: default\n",
      "  resourceVersion: \"57047\"\n",
      "  selfLink: /apis/extensions.example.com/v1/.../default/websites/kubia\n",
      "  uid: b2eb6d99-fc3b-11e6-bd71-0800270a1c50\n",
      "spec:\n",
      "  gitRepo: https://github.com/luksa/kubia-website-example.git\n",
      "Note that the resource includes everything that was in the original YAML definition,\n",
      "and that Kubernetes has initialized additional metadata fields the way it does with all\n",
      "other resources. \n",
      "DELETING AN INSTANCE OF A CUSTOM OBJECT\n",
      "Obviously, in addition to creating and retrieving custom object instances, you can also\n",
      "delete them:\n",
      "$ kubectl delete website kubia\n",
      "website \"kubia\" deleted\n",
      "NOTE\n",
      "You’re deleting an instance of a Website, not the Website CRD\n",
      "resource. You could also delete the CRD object itself, but let’s hold off on that\n",
      "for a while, because you’ll be creating additional Website instances in the\n",
      "next section. \n",
      "Let’s go over everything you’ve done. By creating a CustomResourceDefinition object,\n",
      "you can now store, retrieve, and delete custom objects through the Kubernetes API\n",
      "server. These objects don’t do anything yet. You’ll need to create a controller to make\n",
      "them do something. \n",
      "Listing 18.4\n",
      "Full Website resource definition retrieved from the API server\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 545, 'img_cnt': 0, 'img_npy_lst': []}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n",
      "Defining custom API objects\n",
      " In general, the point of creating custom objects like this isn’t always to make some-\n",
      "thing happen when the object is created. Certain custom objects are used to store data\n",
      "instead of using a more generic mechanism such as a ConfigMap. Applications run-\n",
      "ning inside pods can query the API server for those objects and read whatever is\n",
      "stored in them. \n",
      " But in this case, we said you wanted the existence of a Website object to result in\n",
      "the spinning up of a web server serving the contents of the Git repository referenced\n",
      "in the object. We’ll look at how to do that next.\n",
      "18.1.2 Automating custom resources with custom controllers\n",
      "To make your Website objects run a web server pod exposed through a Service, you’ll\n",
      "need to build and deploy a Website controller, which will watch the API server for the\n",
      "creation of Website objects and then create the Service and the web server Pod for\n",
      "each of them. \n",
      " To make sure the Pod is managed and survives node failures, the controller will\n",
      "create a Deployment resource instead of an unmanaged Pod directly. The controller’s\n",
      "operation is summarized in figure 18.2.\n",
      "I’ve written a simple initial version of the controller, which works well enough to\n",
      "show CRDs and the controller in action, but it’s far from being production-ready,\n",
      "because it’s overly simplified. The container image is available at docker.io/luksa/\n",
      "website-controller:latest, and the source code is at https:/\n",
      "/github.com/luksa/k8s-\n",
      "website-controller. Instead of going through its source code, I’ll explain what the con-\n",
      "troller does.\n",
      "API server\n",
      "Websites\n",
      "Website:\n",
      "kubia\n",
      "Deployments\n",
      "Deployment:\n",
      "kubia-website\n",
      "Services\n",
      "Service:\n",
      "kubia-website\n",
      "Website\n",
      "controller\n",
      "Watches\n",
      "Creates\n",
      "Figure 18.2\n",
      "The Website controller \n",
      "watches for Website objects and \n",
      "creates a Deployment and a Service.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 546, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "514\n",
      "CHAPTER 18\n",
      "Extending Kubernetes\n",
      "UNDERSTANDING WHAT THE WEBSITE CONTROLLER DOES\n",
      "Immediately upon startup, the controller starts to watch Website objects by requesting\n",
      "the following URL:\n",
      "http://localhost:8001/apis/extensions.example.com/v1/websites?watch=true\n",
      "You may recognize the hostname and port—the controller isn’t connecting to the\n",
      "API server directly, but is instead connecting to the kubectl proxy process, which\n",
      "runs in a sidecar container in the same pod and acts as the ambassador to the API\n",
      "server (we examined the ambassador pattern in chapter 8). The proxy forwards the\n",
      "request to the API server, taking care of both TLS encryption and authentication\n",
      "(see figure 18.3).\n",
      "Through the connection opened by this HTTP GET request, the API server will send\n",
      "watch events for every change to any Website object.\n",
      " The API server sends the ADDED watch event every time a new Website object is cre-\n",
      "ated. When the controller receives such an event, it extracts the Website’s name and\n",
      "the URL of the Git repository from the Website object it received in the watch event\n",
      "and creates a Deployment and a Service object by posting their JSON manifests to the\n",
      "API server. \n",
      " The Deployment resource contains a template for a pod with two containers\n",
      "(shown in figure 18.4): one running an nginx server and another one running a git-\n",
      "sync process, which keeps a local directory synced with the contents of a Git repo.\n",
      "The local directory is shared with the nginx container through an emptyDir volume\n",
      "(you did something similar to that in chapter 6, but instead of keeping the local\n",
      "directory synced with a Git repo, you used a gitRepo volume to download the Git\n",
      "repo’s contents at pod startup; the volume’s contents weren’t kept in sync with the\n",
      "Git repo afterward). The Service is a NodePort Service, which exposes your web\n",
      "server pod through a random port on each node (the same port is used on all\n",
      "nodes). When a pod is created by the Deployment object, clients can access the web-\n",
      "site through the node port.\n",
      "Pod: website-controller\n",
      "Container: main\n",
      "Website controller\n",
      "GET http://localhost:8001/apis/extensions.\n",
      "example.com/v1/websites?watch=true\n",
      "GET https://kubernetes:443/apis/extensions.\n",
      "example.com/v1/websites?watch=true\n",
      "Authorization: Bearer <token>\n",
      "Container: proxy\n",
      "kubectl proxy\n",
      "API server\n",
      "Figure 18.3\n",
      "The Website controller talks to the API server through a proxy (in the ambassador container).\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "129\n",
      "height\n",
      "54\n",
      "PIX BUFFER SIZE\n",
      "20898\n",
      "Original IMG_BUFFER_SIZE\n",
      "20898\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014010880>\n",
      "page_image_dict\n",
      "{'page': 547, 'img_cnt': 1, 'img_npy_lst': []}\n",
      "515\n",
      "Defining custom API objects\n",
      "The API server also sends a DELETED watch event when a Website resource instance is\n",
      "deleted. Upon receiving the event, the controller deletes the Deployment and the Ser-\n",
      "vice resources it created earlier. As soon as a user deletes the Website instance, the\n",
      "controller will shut down and remove the web server serving that website.\n",
      "NOTE\n",
      "My oversimplified controller isn’t implemented properly. The way it\n",
      "watches the API objects doesn’t guarantee it won’t miss individual watch\n",
      "events. The proper way to watch objects through the API server is to not only\n",
      "watch them, but also periodically re-list all objects in case any watch events\n",
      "were missed. \n",
      "RUNNING THE CONTROLLER AS A POD\n",
      "During development, I ran the controller on my local development laptop and used a\n",
      "locally running kubectl proxy process (not running as a pod) as the ambassador to\n",
      "the Kubernetes API server. This allowed me to develop quickly, because I didn’t need\n",
      "to build a container image after every change to the source code and then run it\n",
      "inside Kubernetes. \n",
      " When I’m ready to deploy the controller into production, the best way is to run the\n",
      "controller inside Kubernetes itself, the way you do with all the other core controllers.\n",
      "To run the controller in Kubernetes, you can deploy it through a Deployment resource.\n",
      "The following listing shows an example of such a Deployment.\n",
      "apiVersion: apps/v1beta1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: website-controller\n",
      "spec:\n",
      "  replicas: 1                      \n",
      "  template:\n",
      "Listing 18.5\n",
      "A Website controller Deployment: website-controller.yaml\n",
      "Pod\n",
      "Webserver\n",
      "container\n",
      "Web client\n",
      "git-sync\n",
      "container\n",
      "Serves website to\n",
      "web client through\n",
      "a random port\n",
      "Clones Git repo\n",
      "into volume and\n",
      "keeps it synced\n",
      "emptyDir\n",
      "volume\n",
      "Figure 18.4\n",
      "The pod serving \n",
      "the website specified in the \n",
      "Website object\n",
      "You’ll run a single \n",
      "replica of the \n",
      "controller.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 548, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "516\n",
      "CHAPTER 18\n",
      "Extending Kubernetes\n",
      "    metadata:\n",
      "      name: website-controller\n",
      "      labels:\n",
      "        app: website-controller\n",
      "    spec:\n",
      "      serviceAccountName: website-controller    \n",
      "      containers:                                    \n",
      "      - name: main                                   \n",
      "        image: luksa/website-controller              \n",
      "      - name: proxy                                  \n",
      "        image: luksa/kubectl-proxy:1.6.2             \n",
      "As you can see, the Deployment deploys a single replica of a two-container pod. One\n",
      "container runs your controller, whereas the other one is the ambassador container\n",
      "used for simpler communication with the API server. The pod runs under its own spe-\n",
      "cial ServiceAccount, so you’ll need to create it before you deploy the controller:\n",
      "$ kubectl create serviceaccount website-controller\n",
      "serviceaccount \"website-controller\" created\n",
      "If Role Based Access Control (RBAC) is enabled in your cluster, Kubernetes will not\n",
      "allow the controller to watch Website resources or create Deployments or Services. To\n",
      "allow it to do that, you’ll need to bind the website-controller ServiceAccount to the\n",
      "cluster-admin ClusterRole, by creating a ClusterRoleBinding like this:\n",
      "$ kubectl create clusterrolebinding website-controller \n",
      "➥ --clusterrole=cluster-admin \n",
      "➥ --serviceaccount=default:website-controller\n",
      "clusterrolebinding \"website-controller\" created\n",
      "Once you have the ServiceAccount and ClusterRoleBinding in place, you can deploy\n",
      "the controller’s Deployment. \n",
      "SEEING THE CONTROLLER IN ACTION\n",
      "With the controller now running, create the kubia Website resource again:\n",
      "$ kubectl create -f kubia-website.yaml\n",
      "website \"kubia\" created\n",
      "Now, let’s check the controller’s logs (shown in the following listing) to see if it has\n",
      "received the watch event.\n",
      "$ kubectl logs website-controller-2429717411-q43zs -c main\n",
      "2017/02/26 16:54:41 website-controller started.\n",
      "2017/02/26 16:54:47 Received watch event: ADDED: kubia: https://github.c...\n",
      "2017/02/26 16:54:47 Creating services with name kubia-website in namespa... \n",
      "2017/02/26 16:54:47 Response status: 201 Created\n",
      "2017/02/26 16:54:47 Creating deployments with name kubia-website in name... \n",
      "2017/02/26 16:54:47 Response status: 201 Created\n",
      "Listing 18.6\n",
      "Displaying logs of the Website controller\n",
      "It will run \n",
      "under a special \n",
      "ServiceAccount.\n",
      "Two containers: the \n",
      "main container and \n",
      "the proxy sidecar\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 549, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "517\n",
      "Defining custom API objects\n",
      "The logs show that the controller received the ADDED event and that it created a Service\n",
      "and a Deployment for the kubia-website Website. The API server responded with a\n",
      "201 Created response, which means the two resources should now exist. Let’s verify\n",
      "that the Deployment, Service and the resulting Pod were created. The following list-\n",
      "ing lists all Deployments, Services and Pods.\n",
      "$ kubectl get deploy,svc,po\n",
      "NAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE  AGE\n",
      "deploy/kubia-website        1         1         1            1          4s\n",
      "deploy/website-controller   1         1         1            1          5m\n",
      "NAME                CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\n",
      "svc/kubernetes      10.96.0.1      <none>        443/TCP        38d\n",
      "svc/kubia-website   10.101.48.23   <nodes>       80:32589/TCP   4s\n",
      "NAME                                     READY     STATUS    RESTARTS   AGE\n",
      "po/kubia-website-1029415133-rs715        2/2       Running   0          4s\n",
      "po/website-controller-1571685839-qzmg6   2/2       Running   1          5m\n",
      "There they are. The kubia-website Service, through which you can access your web-\n",
      "site, is available on port 32589 on all cluster nodes. You can access it with your browser.\n",
      "Awesome, right? \n",
      " Users of your Kubernetes cluster can now deploy static websites in seconds, with-\n",
      "out knowing anything about Pods, Services, or any other Kubernetes resources, except\n",
      "your custom Website resource. \n",
      " Obviously, you still have room for improvement. The controller could, for exam-\n",
      "ple, watch for Service objects and as soon as the node port is assigned, write the URL\n",
      "the website is accessible at into the status section of the Website resource instance\n",
      "itself. Or it could also create an Ingress object for each website. I’ll leave the imple-\n",
      "mentation of these additional features to you as an exercise.\n",
      "18.1.3 Validating custom objects\n",
      "You may have noticed that you didn’t specify any kind of validation schema in the Web-\n",
      "site CustomResourceDefinition. Users can include any field they want in the YAML of\n",
      "their Website object. The API server doesn’t validate the contents of the YAML (except\n",
      "the usual fields like apiVersion, kind, and metadata), so users can create invalid\n",
      "Website objects (without a gitRepo field, for example). \n",
      " Is it possible to add validation to the controller and prevent invalid objects from\n",
      "being accepted by the API server? It isn’t, because the API server first stores the object,\n",
      "then returns a success response to the client (kubectl), and only then notifies all the\n",
      "watchers (the controller is one of them). All the controller can really do is validate\n",
      "the object when it receives it in a watch event, and if the object is invalid, write the\n",
      "error message to the Website object (by updating the object through a new request to\n",
      "the API server). The user wouldn’t be notified of the error automatically. They’d have\n",
      "Listing 18.7\n",
      "The Deployment, Service, and Pod created for the kubia-website\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 550, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "518\n",
      "CHAPTER 18\n",
      "Extending Kubernetes\n",
      "to notice the error message by querying the API server for the Website object. Unless\n",
      "the user does this, they have no way of knowing whether the object is valid or not.\n",
      " This obviously isn’t ideal. You’d want the API server to validate the object and\n",
      "reject invalid objects immediately. Validation of custom objects was introduced in\n",
      "Kubernetes version 1.8 as an alpha feature. To have the API server validate your cus-\n",
      "tom objects, you need to enable the CustomResourceValidation feature gate in the\n",
      "API server and specify a JSON schema in the CRD.\n",
      "18.1.4 Providing a custom API server for your custom objects\n",
      "A better way of adding support for custom objects in Kubernetes is to implement your\n",
      "own API server and have the clients talk directly to it. \n",
      "INTRODUCING API SERVER AGGREGATION\n",
      "In Kubernetes version 1.7, you can integrate your custom API server with the main\n",
      "Kubernetes API server, through API server aggregation. Initially, the Kubernetes API\n",
      "server was a single monolithic component. From Kubernetes version 1.7, multiple\n",
      "aggregated API servers will be exposed at a single location. Clients can connect to the\n",
      "aggregated API and have their requests transparently forwarded to the appropriate\n",
      "API server. This way, the client wouldn’t even be aware that multiple API servers han-\n",
      "dle different objects behind the scenes. Even the core Kubernetes API server may\n",
      "eventually end up being split into multiple smaller API servers and exposed as a single\n",
      "server through the aggregator, as shown in figure 18.5.\n",
      "In your case, you could create an API server responsible for handling your Website\n",
      "objects. It could validate those objects the way the core Kubernetes API server validates\n",
      "them. You’d no longer need to create a CRD to represent those objects, because you’d\n",
      "implement the Website object type into the custom API server directly. \n",
      " Generally, each API server is responsible for storing their own resources. As shown\n",
      "in figure 18.5, it can either run its own instance of etcd (or a whole etcd cluster), or it\n",
      "Main\n",
      "API server\n",
      "Custom\n",
      "API server Y\n",
      "Custom\n",
      "API server X\n",
      "kubectl\n",
      "Uses its own etcd instance\n",
      "for storing its resources\n",
      "Uses CustomResourceDeﬁnitions\n",
      "in main API server as storage\n",
      "mechanism\n",
      "etcd\n",
      "etcd\n",
      "Figure 18.5\n",
      "API server aggregation\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 551, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "519\n",
      "Extending Kubernetes with the Kubernetes Service Catalog\n",
      "can store its resources in the core API server’s etcd store by creating CRD instances in\n",
      "the core API server. In that case, it needs to create a CRD object first, before creating\n",
      "instances of the CRD, the way you did in the example.\n",
      "REGISTERING A CUSTOM API SERVER\n",
      "To add a custom API server to your cluster, you’d deploy it as a pod and expose it\n",
      "through a Service. Then, to integrate it into the main API server, you’d deploy a YAML\n",
      "manifest describing an APIService resource like the one in the following listing.\n",
      "apiVersion: apiregistration.k8s.io/v1beta1   \n",
      "kind: APIService                             \n",
      "metadata:\n",
      "  name: v1alpha1.extensions.example.com\n",
      "spec:\n",
      "  group: extensions.example.com           \n",
      "  version: v1alpha1                      \n",
      "  priority: 150\n",
      "  service:                    \n",
      "    name: website-api         \n",
      "    namespace: default        \n",
      "After creating the APIService resource from the previous listing, client requests sent\n",
      "to the main API server that contain any resource from the extensions.example.com\n",
      "API group and version v1alpha1 would be forwarded to the custom API server pod(s)\n",
      "exposed through the website-api Service. \n",
      "CREATING CUSTOM CLIENTS\n",
      "While you can create custom resources from YAML files using the regular kubectl cli-\n",
      "ent, to make deployment of custom objects even easier, in addition to providing a cus-\n",
      "tom API server, you can also build a custom CLI tool. This will allow you to add\n",
      "dedicated commands for manipulating those objects, similar to how kubectl allows\n",
      "creating Secrets, Deployments, and other resources through resource-specific com-\n",
      "mands like kubectl create secret or kubectl create deployment.\n",
      " As I’ve already mentioned, custom API servers, API server aggregation, and other\n",
      "features related to extending Kubernetes are currently being worked on intensively, so\n",
      "they may change after the book is published. To get up-to-date information on the\n",
      "subject, refer to the Kubernetes GitHub repos at http:/\n",
      "/github.com/kubernetes.\n",
      "18.2\n",
      "Extending Kubernetes with the Kubernetes Service \n",
      "Catalog\n",
      "One of the first additional API servers that will be added to Kubernetes through API\n",
      "server aggregation is the Service Catalog API server. The Service Catalog is a hot topic\n",
      "in the Kubernetes community, so you may want to know about it. \n",
      " Currently, for a pod to consume a service (here I use the term generally, not in\n",
      "relation to Service resources; for example, a database service includes everything\n",
      "Listing 18.8\n",
      "An APIService YAML definition \n",
      "This is an APIService \n",
      "resource.\n",
      "The API group this API \n",
      "server is responsible for\n",
      "The supported API version\n",
      "The Service the custom API \n",
      "server is exposed through\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 552, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "520\n",
      "CHAPTER 18\n",
      "Extending Kubernetes\n",
      "required to allow users to use a database in their app), someone needs to deploy the\n",
      "pods providing the service, a Service resource, and possibly a Secret so the client pod\n",
      "can use it to authenticate with the service. That someone is usually the same user\n",
      "deploying the client pod or, if a team is dedicated to deploying these types of general\n",
      "services, the user needs to file a ticket and wait for the team to provision the service.\n",
      "This means the user needs to either create the manifests for all the components of the\n",
      "service, know where to find an existing set of manifests, know how to configure it\n",
      "properly, and deploy it manually, or wait for the other team to do it. \n",
      " But Kubernetes is supposed to be an easy-to-use, self-service system. Ideally, users\n",
      "whose apps require a certain service (for example, a web application requiring a back-\n",
      "end database), should be able to say to Kubernetes. “Hey, I need a PostgreSQL data-\n",
      "base. Please provision one and tell me where and how I can connect to it.” This will\n",
      "soon be possible through the Kubernetes Service Catalog. \n",
      "18.2.1 Introducing the Service Catalog\n",
      "As the name suggests, the Service Catalog is a catalog of services. Users can browse\n",
      "through the catalog and provision instances of the services listed in the catalog by\n",
      "themselves without having to deal with Pods, Services, ConfigMaps, and other resources\n",
      "required for the service to run. You’ll recognize that this is similar to what you did\n",
      "with the Website custom resource.\n",
      " Instead of adding custom resources to the API server for each type of service, the\n",
      "Service Catalog introduces the following four generic API resources:\n",
      "A ClusterServiceBroker, which describes an (external) system that can provision\n",
      "services\n",
      "A ClusterServiceClass, which describes a type of service that can be provisioned\n",
      "A ServiceInstance, which is one instance of a service that has been provisioned\n",
      "A ServiceBinding, which represents a binding between a set of clients (pods)\n",
      "and a ServiceInstance\n",
      "The relationships between those four resources are shown in the figure 18.6 and\n",
      "explained in the following paragraphs.\n",
      "In a nutshell, a cluster admin creates a ClusterServiceBroker resource for each service\n",
      "broker whose services they’d like to make available in the cluster. Kubernetes then asks\n",
      "the broker for a list of services that it can provide and creates a ClusterServiceClass\n",
      "resource for each of them. When a user requires a service to be provisioned, they create\n",
      "an ServiceInstance resource and then a ServiceBinding to bind that ServiceInstance to\n",
      "Client pods\n",
      "ServiceBinding\n",
      "ServiceInstance\n",
      "ClusterServiceClass(es)\n",
      "ClusterServiceBroker\n",
      "Figure 18.6\n",
      "The relationships between Service Catalog API resources. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 553, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "521\n",
      "Extending Kubernetes with the Kubernetes Service Catalog\n",
      "their pods. Those pods are then injected with a Secret that holds all the necessary cre-\n",
      "dentials and other data required to connect to the provisioned ServiceInstance.\n",
      " The Service Catalog system architecture is shown in figure 18.7.\n",
      "The components shown in the figure are explained in the following sections.\n",
      "18.2.2 Introducing the Service Catalog API server and Controller \n",
      "Manager\n",
      "Similar to core Kubernetes, the Service Catalog is a distributed system composed of\n",
      "three components:\n",
      "Service Catalog API Server\n",
      "etcd as the storage\n",
      "Controller Manager, where all the controllers run\n",
      "The four Service Catalog–related resources we introduced earlier are created by post-\n",
      "ing YAML/JSON manifests to the API server. It then stores them into its own etcd\n",
      "instance or uses CustomResourceDefinitions in the main API server as an alternative\n",
      "storage mechanism (in that case, no additional etcd instance is required). \n",
      " The controllers running in the Controller Manager are the ones doing some-\n",
      "thing with those resources. They obviously talk to the Service Catalog API server, the\n",
      "way other core Kubernetes controllers talk to the core API server. Those controllers\n",
      "don’t provision the requested services themselves. They leave that up to external\n",
      "service brokers, which are registered by creating ServiceBroker resources in the Ser-\n",
      "vice Catalog API.\n",
      "Kubernetes cluster\n",
      "External system(s)\n",
      "Kubernetes Service Catalog\n",
      "Client pods\n",
      "Provisioned\n",
      "services\n",
      "Broker A\n",
      "Broker B\n",
      "etcd\n",
      "Service\n",
      "Catalog\n",
      "API server\n",
      "Controller\n",
      "Manager\n",
      "kubectl\n",
      "Provisioned\n",
      "services\n",
      "Client pods use the\n",
      "provisioned services\n",
      "Figure 18.7\n",
      "The architecture of the Service Catalog\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 554, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "522\n",
      "CHAPTER 18\n",
      "Extending Kubernetes\n",
      "18.2.3 Introducing Service Brokers and the OpenServiceBroker API\n",
      "A cluster administrator can register one or more external ServiceBrokers in the Ser-\n",
      "vice Catalog. Every broker must implement the OpenServiceBroker API.\n",
      "INTRODUCING THE OPENSERVICEBROKER API\n",
      "The Service Catalog talks to the broker through that API. The API is relatively simple.\n",
      "It’s a REST API providing the following operations:\n",
      "Retrieving the list of services with GET /v2/catalog\n",
      "Provisioning a service instance (PUT /v2/service_instances/:id)\n",
      "Updating a service instance (PATCH /v2/service_instances/:id)\n",
      "Binding a service instance (PUT /v2/service_instances/:id/service_bind-\n",
      "ings/:binding_id)\n",
      "Unbinding an instance (DELETE /v2/service_instances/:id/service_bind-\n",
      "ings/:binding_id)\n",
      "Deprovisioning a service instance (DELETE /v2/service_instances/:id)\n",
      "You’ll find the OpenServiceBroker API spec at https:/\n",
      "/github.com/openservicebro-\n",
      "kerapi/servicebroker.\n",
      "REGISTERING BROKERS IN THE SERVICE CATALOG\n",
      "The cluster administrator registers a broker by posting a ServiceBroker resource man-\n",
      "ifest to the Service Catalog API, like the one shown in the following listing.\n",
      "apiVersion: servicecatalog.k8s.io/v1alpha1    \n",
      "kind: ClusterServiceBroker                                  \n",
      "metadata:\n",
      "  name: database-broker                          \n",
      "spec:\n",
      "  url: http://database-osbapi.myorganization.org  \n",
      "The listing describes an imaginary broker that can provision databases of different\n",
      "types. After the administrator creates the ClusterServiceBroker resource, a controller\n",
      "in the Service Catalog Controller Manager connects to the URL specified in the\n",
      "resource to retrieve the list of services this broker can provision.\n",
      " After the Service Catalog retrieves the list of services, it creates a ClusterService-\n",
      "Class resource for each of them. Each ClusterServiceClass resource describes a sin-\n",
      "gle type of service that can be provisioned (an example of a ClusterServiceClass is\n",
      "“PostgreSQL database”). Each ClusterServiceClass has one or more service plans asso-\n",
      "ciated with it. These allow the user to choose the level of service they need (for exam-\n",
      "ple, a database ClusterServiceClass could provide a “Free” plan, where the size of the\n",
      "Listing 18.9\n",
      "A ClusterServiceBroker manifest: database-broker.yaml\n",
      "The resource kind and \n",
      "the API group and version\n",
      "The name of this broker\n",
      "Where the Service Catalog\n",
      "can contact the broker\n",
      "(its OpenServiceBroker [OSB] API URL)\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 555, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "523\n",
      "Extending Kubernetes with the Kubernetes Service Catalog\n",
      "database is limited and the underlying storage is a spinning disk, and a “Premium”\n",
      "plan, with unlimited size and SSD storage). \n",
      "LISTING THE AVAILABLE SERVICES IN A CLUSTER\n",
      "Users of the Kubernetes cluster can retrieve a list of all services that can be provi-\n",
      "sioned in the cluster with kubectl get serviceclasses, as shown in the following\n",
      "listing.\n",
      "$ kubectl get clusterserviceclasses\n",
      "NAME                KIND\n",
      "postgres-database   ClusterServiceClass.v1alpha1.servicecatalog.k8s.io\n",
      "mysql-database      ServiceClass.v1alpha1.servicecatalog.k8s.io\n",
      "mongodb-database    ServiceClass.v1alpha1.servicecatalog.k8s.io\n",
      "The listing shows ClusterServiceClasses for services that your imaginary database bro-\n",
      "ker could provide. You can compare ClusterServiceClasses to StorageClasses, which we\n",
      "discussed in chapter 6. StorageClasses allow you to select the type of storage you’d like\n",
      "to use in your pods, while ClusterServiceClasses allow you to select the type of service.\n",
      " You can see details of one of the ClusterServiceClasses by retrieving its YAML. An\n",
      "example is shown in the following listing.\n",
      "$ kubectl get serviceclass postgres-database -o yaml\n",
      "apiVersion: servicecatalog.k8s.io/v1alpha1\n",
      "bindable: true\n",
      "brokerName: database-broker                     \n",
      "description: A PostgreSQL database\n",
      "kind: ClusterServiceClass\n",
      "metadata:\n",
      "  name: postgres-database\n",
      "  ...\n",
      "planUpdatable: false\n",
      "plans:\n",
      "- description: A free (but slow) PostgreSQL instance        \n",
      "  name: free                                                \n",
      "  osbFree: true                                             \n",
      "  ...\n",
      "- description: A paid (very fast) PostgreSQL instance      \n",
      "  name: premium                                            \n",
      "  osbFree: false                                           \n",
      "  ...\n",
      "The ClusterServiceClass in the listing contains two plans—a free plan, and a premium\n",
      "plan. You can see that this ClusterServiceClass is provided by the database-broker\n",
      "broker.\n",
      "Listing 18.10\n",
      "List of ClusterServiceClasses in a cluster\n",
      "Listing 18.11\n",
      "A ClusterServiceClass definition\n",
      "This ClusterServiceClass \n",
      "is provided by the \n",
      "database-broker.\n",
      "A free plan for \n",
      "this service\n",
      "A paid plan\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 556, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "524\n",
      "CHAPTER 18\n",
      "Extending Kubernetes\n",
      "18.2.4 Provisioning and using a service\n",
      "Let’s imagine the pods you’re deploying need to use a database. You’ve inspected the\n",
      "list of available ClusterServiceClasses and have chosen to use the free plan of the\n",
      "postgres-database ClusterServiceClass. \n",
      "PROVISIONING A SERVICEINSTANCE\n",
      "To have the database provisioned for you, all you need to do is create a Service-\n",
      "Instance resource, as shown in the following listing.\n",
      "apiVersion: servicecatalog.k8s.io/v1alpha1\n",
      "kind: ServiceInstance\n",
      "metadata:\n",
      "  name: my-postgres-db                     \n",
      "spec:\n",
      "  clusterServiceClassName: postgres-database        \n",
      "  clusterServicePlanName: free                             \n",
      "  parameters:\n",
      "    init-db-args: --data-checksums         \n",
      "You created a ServiceInstance called my-postgres-db (that will be the name of the\n",
      "resource you’re deploying) and specified the ClusterServiceClass and the chosen\n",
      "plan. You’re also specifying a parameter, which is specific for each broker and Cluster-\n",
      "ServiceClass. Let’s imagine you looked up the possible parameters in the broker’s doc-\n",
      "umentation.\n",
      " As soon as you create this resource, the Service Catalog will contact the broker the\n",
      "ClusterServiceClass belongs to and ask it to provision the service. It will pass on the\n",
      "chosen ClusterServiceClass and plan names, as well as all the parameters you specified.\n",
      " It’s then completely up to the broker to know what to do with this information. In\n",
      "your case, your database broker will probably spin up a new instance of a PostgreSQL\n",
      "database somewhere—not necessarily in the same Kubernetes cluster or even in\n",
      "Kubernetes at all. It could run a Virtual Machine and run the database in there. The\n",
      "Service Catalog doesn’t care, and neither does the user requesting the service. \n",
      " You can check if the service has been provisioned successfully by inspecting the\n",
      "status section of the my-postgres-db ServiceInstance you created, as shown in the\n",
      "following listing.\n",
      "$ kubectl get instance my-postgres-db -o yaml\n",
      "apiVersion: servicecatalog.k8s.io/v1alpha1\n",
      "kind: ServiceInstance\n",
      "...\n",
      "status:\n",
      "  asyncOpInProgress: false\n",
      "  conditions:\n",
      "Listing 18.12\n",
      "A ServiceInstance manifest: database-instance.yaml\n",
      "Listing 18.13\n",
      "Inspecting the status of a ServiceInstance\n",
      "You’re giving this \n",
      "Instance a name.\n",
      "The ServiceClass \n",
      "and Plan you want\n",
      "Additional parameters \n",
      "passed to the broker\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 557, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "525\n",
      "Extending Kubernetes with the Kubernetes Service Catalog\n",
      "  - lastTransitionTime: 2017-05-17T13:57:22Z\n",
      "    message: The instance was provisioned successfully    \n",
      "    reason: ProvisionedSuccessfully                       \n",
      "    status: \"True\"\n",
      "    type: Ready                   \n",
      "A database instance is now running somewhere, but how do you use it in your pods?\n",
      "To do that, you need to bind it.\n",
      "BINDING A SERVICEINSTANCE\n",
      "To use a provisioned ServiceInstance in your pods, you create a ServiceBinding\n",
      "resource, as shown in the following listing.\n",
      "apiVersion: servicecatalog.k8s.io/v1alpha1\n",
      "kind: ServiceBinding\n",
      "metadata:\n",
      "  name: my-postgres-db-binding\n",
      "spec:\n",
      "  instanceRef:                          \n",
      "    name: my-postgres-db                \n",
      "  secretName: postgres-secret           \n",
      "The listing shows that you’re defining a ServiceBinding resource called my-postgres-\n",
      "db-binding, in which you’re referencing the my-postgres-db service instance you\n",
      "created earlier. You’re also specifying a name of a Secret. You want the Service Catalog\n",
      "to put all the necessary credentials for accessing the service instance into a Secret\n",
      "called postgres-secret. But where are you binding the ServiceInstance to your pods?\n",
      "Nowhere, actually.\n",
      " Currently, the Service Catalog doesn’t yet make it possible to inject pods with the\n",
      "ServiceInstance’s credentials. This will be possible when a new Kubernetes feature\n",
      "called PodPresets is available. Until then, you can choose a name for the Secret\n",
      "where you want the credentials to be stored in and mount that Secret into your pods\n",
      "manually.\n",
      " When you submit the ServiceBinding resource from the previous listing to the Ser-\n",
      "vice Catalog API server, the controller will contact the Database broker once again\n",
      "and create a binding for the ServiceInstance you provisioned earlier. The broker\n",
      "responds with a list of credentials and other data necessary for connecting to the data-\n",
      "base. The Service Catalog creates a new Secret with the name you specified in the\n",
      "ServiceBinding resource and stores all that data in the Secret. \n",
      "USING THE NEWLY CREATED SECRET IN CLIENT PODS\n",
      "The Secret created by the Service Catalog system can be mounted into pods, so they\n",
      "can read its contents and use them to connect to the provisioned service instance (a\n",
      "PostgreSQL database in the example). The Secret could look like the one in the fol-\n",
      "lowing listing.\n",
      "Listing 18.14\n",
      "A ServiceBinding: my-postgres-db-binding.yaml\n",
      "The database was \n",
      "provisioned successfully.\n",
      "It’s ready to be used.\n",
      "You’re referencing the \n",
      "instance you created \n",
      "earlier.\n",
      "You’d like the credentials \n",
      "for accessing the service \n",
      "stored in this Secret.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 558, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "526\n",
      "CHAPTER 18\n",
      "Extending Kubernetes\n",
      "$ kubectl get secret postgres-secret -o yaml\n",
      "apiVersion: v1\n",
      "data:\n",
      "  host: <base64-encoded hostname of the database>     \n",
      "  username: <base64-encoded username>                 \n",
      "  password: <base64-encoded password>                 \n",
      "kind: Secret\n",
      "metadata:\n",
      "  name: postgres-secret\n",
      "  namespace: default\n",
      "  ...\n",
      "type: Opaque\n",
      "Because you can choose the name of the Secret yourself, you can deploy pods before\n",
      "provisioning or binding the service. As you learned in chapter 7, the pods won’t be\n",
      "started until such a Secret exists. \n",
      " If necessary, multiple bindings can be created for different pods. The service bro-\n",
      "ker can choose to use the same set of credentials in every binding, but it’s better to\n",
      "create a new set of credentials for every binding instance. This way, pods can be pre-\n",
      "vented from using the service by deleting the ServiceBinding resource.\n",
      "18.2.5 Unbinding and deprovisioning\n",
      "Once you no longer need a ServiceBinding, you can delete it the way you delete other\n",
      "resources:\n",
      "$ kubectl delete servicebinding my-postgres-db-binding\n",
      "servicebinding \"my-postgres-db-binding\" deleted\n",
      "When you do this, the Service Catalog controller will delete the Secret and call the bro-\n",
      "ker to perform an unbinding operation. The service instance (in your case a PostgreSQL\n",
      "database) is still running. You can therefore create a new ServiceBinding if you want.\n",
      " But if you don’t need the database instance anymore, you should delete the Service-\n",
      "Instance resource also:\n",
      "$ kubectl delete serviceinstance my-postgres-db\n",
      "serviceinstance \"my-postgres-db \" deleted\n",
      "Deleting the ServiceInstance resource causes the Service Catalog to perform a depro-\n",
      "visioning operation on the service broker. Again, exactly what that means is up to the\n",
      "service broker, but in your case, the broker should shut down the PostgreSQL data-\n",
      "base instance that it created when we provisioned the service instance.\n",
      "18.2.6 Understanding what the Service Catalog brings\n",
      "As you’ve learned, the Service Catalog enables service providers make it possible to\n",
      "expose those services in any Kubernetes cluster by registering the broker in that cluster.\n",
      "Listing 18.15\n",
      "A Secret holding the credentials for connecting to the service instance\n",
      "This is what the pod \n",
      "should use to connect to \n",
      "the database service.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 559, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "527\n",
      "Platforms built on top of Kubernetes\n",
      "For example, I’ve been involved with the Service Catalog since early on and have\n",
      "implemented a broker, which makes it trivial to provision messaging systems and\n",
      "expose them to pods in a Kubernetes cluster. Another team has implemented a broker\n",
      "that makes it easy to provision Amazon Web Services. \n",
      " In general, service brokers allow easy provisioning and exposing of services in\n",
      "Kubernetes and will make Kubernetes an even more awesome platform for deploying\n",
      "your applications. \n",
      "18.3\n",
      "Platforms built on top of Kubernetes\n",
      "I’m sure you’ll agree that Kubernetes is a great system by itself. Given that it’s easily\n",
      "extensible across all its components, it’s no wonder companies that had previously\n",
      "developed their own custom platforms are now re-implementing them on top of\n",
      "Kubernetes. Kubernetes is, in fact, becoming a widely accepted foundation for the\n",
      "new generation of Platform-as-a-Service offerings.\n",
      " Among the best-known PaaS systems built on Kubernetes are Deis Workflow and\n",
      "Red Hat’s OpenShift. We’ll do a quick overview of both systems to give you a sense of\n",
      "what they offer on top of all the awesome stuff Kubernetes already offers.\n",
      "18.3.1 Red Hat OpenShift Container Platform\n",
      "Red Hat OpenShift is a Platform-as-a-Service and as such, it has a strong focus on\n",
      "developer experience. Among its goals are enabling rapid development of applica-\n",
      "tions, as well as easy deployment, scaling, and long-term maintenance of those apps.\n",
      "OpenShift has been around much longer than Kubernetes. Versions 1 and 2 were\n",
      "built from the ground up and had nothing to do with Kubernetes, but when Kuberne-\n",
      "tes was announced, Red Hat decided to rebuild OpenShift version 3 from scratch—\n",
      "this time on top of Kubernetes. When a company such as Red Hat decides to throw\n",
      "away an old version of their software and build a new one on top of an existing tech-\n",
      "nology like Kubernetes, it should be clear to everyone how great Kubernetes is.\n",
      " Kubernetes automates rollouts and application scaling, whereas OpenShift also auto-\n",
      "mates the actual building of application images and their automatic deployment with-\n",
      "out requiring you to integrate a Continuous Integration solution into your cluster. \n",
      " OpenShift also provides user and group management, which allows you to run a\n",
      "properly secured multi-tenant Kubernetes cluster, where individual users are only\n",
      "allowed to access their own Kubernetes namespaces and the apps running in those\n",
      "namespaces are also fully network-isolated from each other by default. \n",
      "INTRODUCING ADDITIONAL RESOURCES AVAILABLE IN OPENSHIFT\n",
      "OpenShift provides some additional API objects in addition to all those available in\n",
      "Kubernetes. We’ll explain them in the next few paragraphs to give you a good over-\n",
      "view of what OpenShift does and what it provides.\n",
      " The additional resources include\n",
      "Users & Groups\n",
      "Projects\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 560, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "528\n",
      "CHAPTER 18\n",
      "Extending Kubernetes\n",
      "Templates\n",
      "BuildConfigs\n",
      "DeploymentConfigs\n",
      "ImageStreams\n",
      "Routes\n",
      "And others\n",
      "UNDERSTANDING USERS, GROUPS, AND PROJECTS\n",
      "We’ve said that OpenShift provides a proper multi-tenant environment to its users.\n",
      "Unlike Kubernetes, which doesn’t have an API object for representing an individual\n",
      "user of the cluster (but does have ServiceAccounts that represent services running in\n",
      "it), OpenShift provides powerful user management features, which make it possible to\n",
      "specify what each user can do and what they cannot. These features pre-date the Role-\n",
      "Based Access Control, which is now the standard in vanilla Kubernetes.\n",
      " Each user has access to certain Projects, which are nothing more than Kubernetes\n",
      "Namespaces with additional annotations. Users can only act on resources that reside\n",
      "in the projects the user has access to. Access to the project is granted by a cluster\n",
      "administrator. \n",
      "INTRODUCING APPLICATION TEMPLATES\n",
      "Kubernetes makes it possible to deploy a set of resources through a single JSON or\n",
      "YAML manifest. OpenShift takes this a step further by allowing that manifest to be\n",
      "parameterizable. A parameterizable list in OpenShift is called a Template; it’s a list of\n",
      "objects whose definitions can include placeholders that get replaced with parameter\n",
      "values when you process and then instantiate a template (see figure 18.8).\n",
      "The template itself is a JSON or YAML file containing a list of parameters that are ref-\n",
      "erenced in resources defined in that same JSON/YAML. The template can be stored\n",
      "in the API server like any other object. Before a template can be instantiated, it needs\n",
      "Template\n",
      "Parameters\n",
      "APP_NAME=\"kubia\"\n",
      "VOL_CAPACITY=\"5 Gi\"\n",
      "...\n",
      "Pod\n",
      "name: $(APP_NAME)\n",
      "Service\n",
      "name: $(APP_NAME)\n",
      "Template\n",
      "Pod\n",
      "name: kubia\n",
      "Service\n",
      "name: kubia\n",
      "Pod\n",
      "name: kubia\n",
      "Service\n",
      "name: kubia\n",
      "Process\n",
      "Create\n",
      "Figure 18.8\n",
      "OpenShift templates\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 561, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "529\n",
      "Platforms built on top of Kubernetes\n",
      "to be processed. To process a template, you supply the values for the template’s\n",
      "parameters and then OpenShift replaces the references to the parameters with those\n",
      "values. The result is a processed template, which is exactly like a Kubernetes resource\n",
      "list that can then be created with a single POST request.\n",
      " OpenShift provides a long list of pre-fabricated templates that allow users to\n",
      "quickly run complex applications by specifying a few arguments (or none at all, if the\n",
      "template provides good defaults for those arguments). For example, a template can\n",
      "enable the creation of all the Kubernetes resources necessary to run a Java EE appli-\n",
      "cation inside an Application Server, which connects to a back-end database, also\n",
      "deployed as part of that same template. All those components can be deployed with a\n",
      "single command.\n",
      "BUILDING IMAGES FROM SOURCE USING BUILDCONFIGS\n",
      "One of the best features of OpenShift is the ability to have OpenShift build and imme-\n",
      "diately deploy an application in the OpenShift cluster by pointing it to a Git repository\n",
      "holding the application’s source code. You don’t need to build the container image at\n",
      "all—OpenShift does that for you. This is done by creating a resource called Build-\n",
      "Config, which can be configured to trigger builds of container images immediately\n",
      "after a change is committed to the source Git repository. \n",
      " Although OpenShift doesn’t monitor the Git repository itself, a hook in the repos-\n",
      "itory can notify OpenShift of the new commit. OpenShift will then pull the changes\n",
      "from the Git repository and start the build process. A build mechanism called Source\n",
      "To Image can detect what type of application is in the Git repository and run the\n",
      "proper build procedure for it. For example, if it detects a pom.xml file, which is used\n",
      "in Java Maven-formatted projects, it runs a Maven build. The resulting artifacts are\n",
      "packaged into an appropriate container image, and are then pushed to an internal\n",
      "container registry (provided by OpenShift). From there, they can be pulled and run\n",
      "in the cluster immediately. \n",
      " By creating a BuildConfig object, developers can thus point to a Git repo and not\n",
      "worry about building container images. Developers have almost no need to know\n",
      "anything about containers. Once the ops team deploys an OpenShift cluster and\n",
      "gives developers access to it, those developers can develop their code, commit, and\n",
      "push it to a Git repo, the same way they used to before we started packaging apps into\n",
      "containers. Then OpenShift takes care of building, deploying, and managing apps\n",
      "from that code.\n",
      "AUTOMATICALLY DEPLOYING NEWLY BUILT IMAGES WITH DEPLOYMENTCONFIGS\n",
      "Once a new container image is built, it can also automatically be deployed in the clus-\n",
      "ter. This is enabled by creating a DeploymentConfig object and pointing it to an\n",
      "ImageStream. As the name suggests, an ImageStream is a stream of images. When an\n",
      "image is built, it’s added to the ImageStream. This enables the DeploymentConfig to\n",
      "notice the newly built image and allows it to take action and initiate a rollout of the\n",
      "new image (see figure 18.9).\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 562, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "530\n",
      "CHAPTER 18\n",
      "Extending Kubernetes\n",
      "A DeploymentConfig is almost identical to the Deployment object in Kubernetes, but\n",
      "it pre-dates it. Like a Deployment object, it has a configurable strategy for transition-\n",
      "ing between Deployments. It contains a pod template used to create the actual pods,\n",
      "but it also allows you to configure pre- and post-deployment hooks. In contrast to a\n",
      "Kubernetes Deployment, it creates ReplicationControllers instead of ReplicaSets and\n",
      "provides a few additional features.\n",
      "EXPOSING SERVICES EXTERNALLY USING ROUTES\n",
      "Early on, Kubernetes didn’t provide Ingress objects. To expose Services to the outside\n",
      "world, you needed to use NodePort or LoadBalancer-type Services. But at that time,\n",
      "OpenShift already provided a better option through a Route resource. A Route is sim-\n",
      "ilar to an Ingress, but it provides additional configuration related to TLS termination\n",
      "and traffic splitting. \n",
      " Similar to an Ingress controller, a Route needs a Router, which is a controller that\n",
      "provides the load balancer or proxy. In contrast to Kubernetes, the Router is available\n",
      "out of the box in OpenShift. \n",
      "TRYING OUT OPENSHIFT\n",
      "If you’re interested in trying out OpenShift, you can start by using Minishift, which is\n",
      "the OpenShift equivalent of Minikube, or you can try OpenShift Online Starter at\n",
      "https:/\n",
      "/manage.openshift.com, which is a free multi-tenant, hosted solution provided\n",
      "to get you started with OpenShift. \n",
      "18.3.2 Deis Workflow and Helm\n",
      "A company called Deis, which has recently been acquired by Microsoft, also provides a\n",
      "PaaS called Workflow, which is also built on top of Kubernetes. Besides Workflow,\n",
      "Pods\n",
      "Builder pod\n",
      "Replication\n",
      "Controller\n",
      "BuildConﬁg\n",
      "Git repo\n",
      "DeploymentConﬁg\n",
      "ImageStream\n",
      "Build trigger\n",
      "Clones Git repo, builds new\n",
      "image from source, and adds\n",
      "it to the ImageStream\n",
      "Watches for new images in ImageStream\n",
      "and rolls out new version (similarly to a\n",
      "Deployment)\n",
      "Figure 18.9\n",
      "BuildConfigs and DeploymentConfigs in OpenShift\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 563, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "531\n",
      "Platforms built on top of Kubernetes\n",
      "they’ve also developed a tool called Helm, which is gaining traction in the Kubernetes\n",
      "community as a standard way of deploying existing apps in Kubernetes. We’ll take a\n",
      "brief look at both.\n",
      "INTRODUCING DEIS WORKFLOW\n",
      "You can deploy Deis Workflow to any existing Kubernetes cluster (unlike OpenShift,\n",
      "which is a complete cluster with a modified API server and other Kubernetes compo-\n",
      "nents). When you run Workflow, it creates a set of Services and ReplicationControllers,\n",
      "which then provide developers with a simple, developer-friendly environment. \n",
      " Deploying new versions of your app is triggered by pushing your changes with git\n",
      "push deis master and letting Workflow take care of the rest. Similar to OpenShift,\n",
      "Workflow also provides a source to image mechanism, application rollouts and roll-\n",
      "backs, edge routing, and also log aggregation, metrics, and alerting, which aren’t\n",
      "available in core Kubernetes. \n",
      " To run Workflow in your Kubernetes cluster, you first need to install the Deis Work-\n",
      "flow and Helm CLI tools and then install Workflow into your cluster. We won’t go into\n",
      "how to do that here, but if you’d like to learn more, visit the website at https:/\n",
      "/deis\n",
      ".com/workflow. What we’ll explore here is the Helm tool, which can be used without\n",
      "Workflow and has gained popularity in the community.\n",
      "DEPLOYING RESOURCES THROUGH HELM\n",
      "Helm is a package manager for Kubernetes (similar to OS package managers like yum\n",
      "or apt in Linux or homebrew in MacOS). \n",
      " Helm is comprised of two things:\n",
      "A helm CLI tool (the client).\n",
      "Tiller, a server component running as a Pod inside the Kubernetes cluster.\n",
      "Those two components are used to deploy and manage application packages in a\n",
      "Kubernetes cluster. Helm application packages are called Charts. They’re combined\n",
      "with a Config, which contains configuration information and is merged into a Chart\n",
      "to create a Release, which is a running instance of an application (a combined Chart\n",
      "and Config). You deploy and manage Releases using the helm CLI tool, which talks to\n",
      "the Tiller server, which is the component that creates all the necessary Kubernetes\n",
      "resources defined in the Chart, as shown in figure 18.10.\n",
      " You can create charts yourself and keep them on your local disk, or you can use\n",
      "any existing chart, which is available in the growing list of helm charts maintained by\n",
      "the community at https:/\n",
      "/github.com/kubernetes/charts. The list includes charts for\n",
      "applications such as PostgreSQL, MySQL, MariaDB, Magento, Memcached, MongoDB,\n",
      "OpenVPN, PHPBB, RabbitMQ, Redis, WordPress, and others.\n",
      " Similar to how you don’t build and install apps developed by other people to your\n",
      "Linux system manually, you probably don’t want to build and manage your own\n",
      "Kubernetes manifests for such applications, right? That’s why you’ll want to use Helm\n",
      "and the charts available in the GitHub repository I mentioned. \n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 564, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "532\n",
      "CHAPTER 18\n",
      "Extending Kubernetes\n",
      "When you want to run a PostgreSQL or a MySQL database in your Kubernetes cluster,\n",
      "don’t start writing manifests for them. Instead, check if someone else has already gone\n",
      "through the trouble and prepared a Helm chart for it. \n",
      " Once someone prepares a Helm chart for a specific application and adds it to the\n",
      "Helm chart GitHub repo, installing the whole application takes a single one-line com-\n",
      "mand. For example, to run MySQL in your Kubernetes cluster, all you need to do is\n",
      "clone the charts Git repo to your local machine and run the following command (pro-\n",
      "vided you have Helm’s CLI tool and Tiller running in your cluster):\n",
      "$ helm install --name my-database stable/mysql\n",
      "This will create all the necessary Deployments, Services, Secrets, and PersistentVolu-\n",
      "meClaims needed to run MySQL in your cluster. You don’t need to concern yourself\n",
      "with what components you need and how to configure them to run MySQL properly.\n",
      "I’m sure you’ll agree this is awesome.\n",
      "TIP\n",
      "One of the most interesting charts available in the repo is an OpenVPN\n",
      "chart, which runs an OpenVPN server inside your Kubernetes cluster and\n",
      "allows you to enter the pod network through VPN and access Services as if\n",
      "your local machine was a pod in the cluster. This is useful when you’re devel-\n",
      "oping apps and running them locally.\n",
      "These were several examples of how Kubernetes can be extended and how companies\n",
      "like Red Hat and Deis (now Microsoft) have extended it. Now go and start riding the\n",
      "Kubernetes wave yourself!\n",
      "Kubernetes cluster\n",
      "Chart\n",
      "and\n",
      "Conﬁg\n",
      "Helm\n",
      "Charts\n",
      "(ﬁles on\n",
      "local disk)\n",
      "Tiller\n",
      "(pod)\n",
      "Deployments,\n",
      "Services, and\n",
      "other objects\n",
      "helm\n",
      "CLI tool\n",
      "Manages\n",
      "charts\n",
      "Combines Chart and\n",
      "Conﬁg into a Release\n",
      "Creates Kubernetes objects\n",
      "deﬁned in the Release\n",
      "Figure 18.10\n",
      "Overview of Helm\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 565, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "533\n",
      "Summary\n",
      "18.4\n",
      "Summary\n",
      "This final chapter has shown you how you can go beyond the existing functionalities\n",
      "Kubernetes provides and how companies like Dies and Red Hat have done it. You’ve\n",
      "learned how\n",
      "Custom resources can be registered in the API server by creating a Custom-\n",
      "ResourceDefinition object.\n",
      "Instances of custom objects can be stored, retrieved, updated, and deleted with-\n",
      "out having to change the API server code.\n",
      "A custom controller can be implemented to bring those objects to life.\n",
      "Kubernetes can be extended with custom API servers through API aggregation.\n",
      "Kubernetes Service Catalog makes it possible to self-provision external services\n",
      "and expose them to pods running in the Kubernetes cluster.\n",
      "Platforms-as-a-Service built on top of Kubernetes make it easy to build contain-\n",
      "erized applications inside the same Kubernetes cluster that then runs them. \n",
      "A package manager called Helm makes deploying existing apps without requir-\n",
      "ing you to build resource manifests for them.\n",
      "Thank you for taking the time to read through this long book. I hope you’ve learned\n",
      "as much from reading it as I have from writing it.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 566, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "534\n",
      "appendix A\n",
      "Using kubectl\n",
      "with multiple clusters\n",
      "A.1\n",
      "Switching between Minikube and Google Kubernetes \n",
      "Engine\n",
      "The examples in this book can either be run in a cluster created with Minikube, or\n",
      "one created with Google Kubernetes Engine (GKE). If you plan on using both, you\n",
      "need to know how to switch between them. A detailed explanation of how to use\n",
      "kubectl with multiple clusters is described in the next section. Here we look at how\n",
      "to switch between Minikube and GKE.\n",
      "SWITCHING TO MINIKUBE\n",
      "Luckily, every time you start up a Minikube cluster with minikube start, it also\n",
      "reconfigures kubectl to use it:\n",
      "$ minikube start\n",
      "Starting local Kubernetes cluster...\n",
      "...\n",
      "Setting up kubeconfig...                            \n",
      "Kubectl is now configured to use the cluster.       \n",
      "After switching from Minikube to GKE, you can switch back by stopping Minikube\n",
      "and starting it up again. kubectl will then be re-configured to use the Minikube clus-\n",
      "ter again.\n",
      "SWITCHING TO GKE\n",
      "To switch to using the GKE cluster, you can use the following command:\n",
      "$ gcloud container clusters get-credentials my-gke-cluster\n",
      "This will configure kubectl to use the GKE cluster called my-gke-cluster.\n",
      "Minikube sets up kubectl every \n",
      "time you start the cluster.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 567, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "535\n",
      "Using kubectl with multiple clusters or namespaces\n",
      "GOING FURTHER\n",
      "These two methods should be enough to get you started quickly, but to understand\n",
      "the complete picture of using kubectl with multiple clusters, study the next section. \n",
      "A.2\n",
      "Using kubectl with multiple clusters or namespaces\n",
      "If you need to switch between different Kubernetes clusters, or if you want to work in a\n",
      "different namespace than the default and don’t want to specify the --namespace\n",
      "option every time you run kubectl, here’s how to do it.\n",
      "A.2.1\n",
      "Configuring the location of the kubeconfig file\n",
      "The config used by kubectl is usually stored in the ~/.kube/config file. If it’s stored\n",
      "somewhere else, the KUBECONFIG environment variable needs to point to its location. \n",
      "NOTE\n",
      "You can use multiple config files and have kubectl use them all at\n",
      "once by specifying all of them in the KUBECONFIG environment variable (sepa-\n",
      "rate them with a colon).\n",
      "A.2.2\n",
      "Understanding the contents of the kubeconfig file\n",
      "An example config file is shown in the following listing.\n",
      "apiVersion: v1\n",
      "clusters:\n",
      "- cluster:                                                 \n",
      "    certificate-authority: /home/luksa/.minikube/ca.crt    \n",
      "    server: https://192.168.99.100:8443                    \n",
      "  name: minikube                                           \n",
      "contexts:\n",
      "- context:                          \n",
      "    cluster: minikube               \n",
      "    user: minikube                  \n",
      "    namespace: default              \n",
      "  name: minikube                    \n",
      "current-context: minikube             \n",
      "kind: Config\n",
      "preferences: {}\n",
      "users:\n",
      "- name: minikube                                             \n",
      "  user:                                                      \n",
      "    client-certificate: /home/luksa/.minikube/apiserver.crt  \n",
      "    client-key: /home/luksa/.minikube/apiserver.key          \n",
      "The kubeconfig file consists of four sections:\n",
      "■\n",
      "A list of clusters\n",
      "■\n",
      "A list of users\n",
      "■\n",
      "A list of contexts\n",
      "■\n",
      "The name of the current context\n",
      "Listing A.1\n",
      "Example kubeconfig file\n",
      "Contains \n",
      "information about a \n",
      "Kubernetes cluster\n",
      "Defines a \n",
      "kubectl \n",
      "context\n",
      "The current context \n",
      "kubectl uses\n",
      "Contains \n",
      "a user’s \n",
      "credentials\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 568, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "536\n",
      "APPENDIX A\n",
      "Using kubectl with multiple clusters\n",
      "Each cluster, user, and context has a name. The name is used to refer to the context,\n",
      "user, or cluster. \n",
      "CLUSTERS\n",
      "A cluster entry represents a Kubernetes cluster and contains the URL of the API\n",
      "server, the certificate authority (CA) file, and possibly a few other configuration\n",
      "options related to communication with the API server. The CA certificate can be\n",
      "stored in a separate file and referenced in the kubeconfig file, or it can be included in\n",
      "it directly in the certificate-authority-data field.\n",
      "USERS\n",
      "Each user defines the credentials to use when talking to an API server. This can be a\n",
      "username and password pair, an authentication token, or a client key and certificate.\n",
      "The certificate and key can be included in the kubeconfig file (through the client-\n",
      "certificate-data and client-key-data properties) or stored in separate files and\n",
      "referenced in the config file, as shown in listing A.1.\n",
      "CONTEXTS\n",
      "A context ties together a cluster, a user, and the default namespace kubectl should use\n",
      "when performing commands. Multiple contexts can point to the same user or cluster. \n",
      "THE CURRENT CONTEXT\n",
      "While there can be multiple contexts defined in the kubeconfig file, at any given time\n",
      "only one of them is the current context. Later we’ll see how the current context can\n",
      "be changed.\n",
      "A.2.3\n",
      "Listing, adding, and modifying kube config entries\n",
      "You can edit the file manually to add, modify, and remove clusters, users, or contexts,\n",
      "but you can also do it through one of the kubectl config commands.\n",
      "ADDING OR MODIFYING A CLUSTER\n",
      "To add another cluster, use the kubectl config set-cluster command:\n",
      "$ kubectl config set-cluster my-other-cluster \n",
      "➥ --server=https://k8s.example.com:6443 \n",
      "➥ --certificate-authority=path/to/the/cafile\n",
      "This will add a cluster called my-other-cluster with the API server located at https:/\n",
      "/\n",
      "k8s.example.com:6443. To see additional options you can pass to the command, run\n",
      "kubectl config set-cluster to have it print out usage examples.\n",
      " If a cluster by that name already exists, the set-cluster command will overwrite\n",
      "its configuration options. \n",
      "ADDING OR MODIFYING USER CREDENTIALS\n",
      "Adding and modifying users is similar to adding or modifying a cluster. To add a user\n",
      "that authenticates with the API server using a username and password, run the follow-\n",
      "ing command:\n",
      "$ kubectl config set-credentials foo --username=foo --password=pass\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 569, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "537\n",
      "Using kubectl with multiple clusters or namespaces\n",
      "To use token-based authentication, run the following instead:\n",
      "$ kubectl config set-credentials foo --token=mysecrettokenXFDJIQ1234\n",
      "Both these examples store user credentials under the name foo. If you use the same\n",
      "credentials for authenticating against different clusters, you can define a single user\n",
      "and use it with both clusters. \n",
      "TYING CLUSTERS AND USER CREDENTIALS TOGETHER\n",
      "A context defines which user to use with which cluster, but can also define the name-\n",
      "space that kubectl should use, when you don’t specify the namespace explicitly with\n",
      "the --namespace or -n option.\n",
      " The following command is used to create a new context that ties together the clus-\n",
      "ter and the user you created:\n",
      "$ kubectl config set-context some-context --cluster=my-other-cluster \n",
      "➥ --user=foo --namespace=bar\n",
      "This creates a context called some-context that uses the my-other-cluster cluster\n",
      "and the foo user credentials. The default namespace in this context is set to bar. \n",
      " You can also use the same command to change the namespace of your current\n",
      "context, for example. You can get the name of the current context like so:\n",
      "$ kubectl config current-context\n",
      "minikube\n",
      "You then change the namespace like this:\n",
      "$ kubectl config set-context minikube --namespace=another-namespace\n",
      "Running this simple command once is much more user-friendly compared to having\n",
      "to include the --namespace option every time you run kubectl.\n",
      "TIP\n",
      "To easily switch between namespaces, define an alias like this: alias\n",
      "kcd='kubectl config set-context $(kubectl config current-context)\n",
      "--namespace '. You can then switch between namespaces with kcd some-\n",
      "namespace.\n",
      "A.2.4\n",
      "Using kubectl with different clusters, users, and contexts\n",
      "When you run kubectl commands, the cluster, user, and namespace defined in the\n",
      "kubeconfig’s current context are used, but you can override them using the following\n",
      "command-line options:\n",
      "■\n",
      "--user to use a different user from the kubeconfig file.\n",
      "■\n",
      "--username and --password to use a different username and/or password (they\n",
      "don’t need to be specified in the config file). If using other types of authentica-\n",
      "tion, you can use --client-key and --client-certificate or --token.\n",
      "■\n",
      "--cluster to use a different cluster (must be defined in the config file).\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 570, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "538\n",
      "APPENDIX A\n",
      "Using kubectl with multiple clusters\n",
      "■\n",
      "--server to specify the URL of a different server (which isn’t in the config file).\n",
      "■\n",
      "--namespace to use a different namespace.\n",
      "A.2.5\n",
      "Switching between contexts \n",
      "Instead of modifying the current context as in one of the previous examples, you can\n",
      "also use the set-context command to create an additional context and then switch\n",
      "between contexts. This is handy when working with multiple clusters (use set-clus-\n",
      "ter to create cluster entries for them). \n",
      " Once you have multiple contexts set up, switching between them is trivial:\n",
      "$ kubectl config use-context my-other-context\n",
      "This switches the current context to my-other-context. \n",
      "A.2.6\n",
      "Listing contexts and clusters\n",
      "To list all the contexts defined in your kubeconfig file, run the following command:\n",
      "$ kubectl config get-contexts\n",
      "CURRENT   NAME          CLUSTER       AUTHINFO            NAMESPACE\n",
      "*         minikube      minikube      minikube            default\n",
      "          rpi-cluster   rpi-cluster   admin/rpi-cluster\n",
      "          rpi-foo       rpi-cluster   admin/rpi-cluster   foo\n",
      "As you can see, I’m using three different contexts. The rpi-cluster and the rpi-foo\n",
      "contexts use the same cluster and credentials, but default to different namespaces.\n",
      " Listing clusters is similar:\n",
      "$ kubectl config get-clusters\n",
      "NAME\n",
      "rpi-cluster\n",
      "minikube\n",
      "Credentials can’t be listed for security reasons.\n",
      "A.2.7\n",
      "Deleting contexts and clusters\n",
      "To clean up the list of contexts or clusters, you can either delete the entries from the\n",
      "kubeconfig file manually or use the following two commands:\n",
      "$ kubectl config delete-context my-unused-context\n",
      "and\n",
      "$ kubectl config delete-cluster my-old-cluster\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 571, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "539\n",
      "appendix B\n",
      "Setting up a multi-node\n",
      "cluster with kubeadm\n",
      "This appendix shows how to install a Kubernetes cluster with multiple nodes. You’ll\n",
      "run the nodes inside virtual machines through VirtualBox, but you can also use a\n",
      "different virtualization tool or bare-metal machines. To set up both the master and\n",
      "the worker nodes, you’ll use the kubeadm tool.\n",
      "B.1\n",
      "Setting up the OS and required packages\n",
      "First, you need to download and install VirtualBox, if you don’t have it installed\n",
      "already. You can download it from https:/\n",
      "/www.virtualbox.org/wiki/Downloads.\n",
      "Once you have it running, download the CentOS 7 minimal ISO image from\n",
      "www.centos.org/download. You can also use a different Linux distribution, but\n",
      "make sure it’s supported by checking the http:/\n",
      "/kubernetes.io website.\n",
      "B.1.1\n",
      "Creating the virtual machine\n",
      "Next, you’ll create the VM for your Kubernetes master. Start by clicking the New\n",
      "icon in the upper-left corner. Then enter “k8s-master” as the name, and select\n",
      "Linux as the Type and Red Hat (64-bit) as the version, as shown in figure B.1.\n",
      " After clicking the Next button, you can set the VM’s memory size and set up the\n",
      "hard disk. If you have enough memory, select at least 2GB (keep in mind you’ll run\n",
      "three such VMs). When creating the hard disk, leave the default options selected.\n",
      "Here’s what they were in my case:\n",
      "■\n",
      "Hard disk file type: VDI (VirtualBox Disk Image)\n",
      "■\n",
      "Storage on physical hard disk: Dynamically allocated\n",
      "■\n",
      "File location and size: k8s-master, size 8GB\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "1700\n",
      "height\n",
      "1361\n",
      "PIX BUFFER SIZE\n",
      "6941100\n",
      "Original IMG_BUFFER_SIZE\n",
      "6941100\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011b40>\n",
      "page_image_dict\n",
      "{'page': 572, 'img_cnt': 1, 'img_npy_lst': []}\n",
      "540\n",
      "APPENDIX B\n",
      "Setting up a multi-node cluster with kubeadm\n",
      "B.1.2\n",
      "Configuring the network adapter for the VM\n",
      "Once you’re done creating the VM, you need to configure its network adapter,\n",
      "because the default won’t allow you to run multiple nodes properly. You’ll configure\n",
      "the adapter so it uses the Bridged Adapter mode. This will connect your VMs to the\n",
      "same network your host computer is in. Each VM will get its own IP address, the same\n",
      "way as if it were a physical machine connected to the same switch your host computer\n",
      "is connected to. Other options are much more complicated, because they usually\n",
      "require two network adapters to be set up.\n",
      " To configure the network adapter, make sure the VM is selected in the main Virtual-\n",
      "Box window and then click the Settings icon (next to the New icon you clicked before). \n",
      " A window like the one shown in figure B.2 will appear. On the left-hand side, select\n",
      "Network and then, in the main panel on the right, select Attached to: Bridged Adapter,\n",
      "as shown in the figure. In the Name drop-down menu, select your host machine’s\n",
      "adapter, which you use to connect your machine to the network.\n",
      "Figure B.1\n",
      "Creating a Virtual Machine in VirtualBox\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "1375\n",
      "height\n",
      "854\n",
      "PIX BUFFER SIZE\n",
      "3522750\n",
      "Original IMG_BUFFER_SIZE\n",
      "3522750\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011b40>\n",
      "width\n",
      "1000\n",
      "height\n",
      "774\n",
      "PIX BUFFER SIZE\n",
      "2322000\n",
      "Original IMG_BUFFER_SIZE\n",
      "2322000\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011b40>\n",
      "page_image_dict\n",
      "{'page': 573, 'img_cnt': 2, 'img_npy_lst': []}\n",
      "541\n",
      "Setting up the OS and required packages\n",
      "B.1.3\n",
      "Installing the operating system\n",
      "You’re now ready to run the VM and install the operating system. Ensure the VM is still\n",
      "selected in the list and click on the Start icon at the top of the VirtualBox main window.\n",
      "SELECTING THE START-UP DISK\n",
      "Before the VM starts up, VirtualBox will ask you what start-up disk to use. Click the icon\n",
      "next to the drop-down list (shown in figure B.3) and then find and select the CentOS\n",
      "ISO image you downloaded earlier. Then click Start to boot up the VM.\n",
      "Figure B.2\n",
      "Configuring the network adapter for the VM\n",
      "Figure B.3\n",
      "Selecting \n",
      "the installation ISO image\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "1550\n",
      "height\n",
      "1163\n",
      "PIX BUFFER SIZE\n",
      "5407950\n",
      "Original IMG_BUFFER_SIZE\n",
      "5407950\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011b40>\n",
      "page_image_dict\n",
      "{'page': 574, 'img_cnt': 1, 'img_npy_lst': []}\n",
      "542\n",
      "APPENDIX B\n",
      "Setting up a multi-node cluster with kubeadm\n",
      "INITIATING THE INSTALL\n",
      "When the VM starts up, a textual menu screen will appear. Use the cursor up key to\n",
      "select the Install CentOS Linux 7 option and press the Enter button. \n",
      "SETTING INSTALLATION OPTIONS\n",
      "After a few moments, a graphical Welcome to CentOS Linux 7 screen will appear,\n",
      "allowing you to select the language you wish to use. I suggest you keep the language\n",
      "set to English. Click the Continue button to get to the main setup screen as shown in\n",
      "figure B.4.\n",
      "TIP\n",
      "When you click into the VM’s window, your keyboard and mouse will be\n",
      "captured by the VM. To release them, press the key shown at the bottom-right\n",
      "corner of the VirtualBox window the VM is running in. This is usually the Right\n",
      "Control key on Windows and Linux or the left Command key on MacOS.\n",
      "First, click Installation Destination and then immediately click the Done button on\n",
      "the screen that appears (you don’t need to click anywhere else). \n",
      " Then click on Network & Host Name. On the next screen, first enable the network\n",
      "adapter by clicking the ON/OFF switch in the top right corner. Then enter the host\n",
      "Figure B.4\n",
      "The main setup screen\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "1700\n",
      "height\n",
      "629\n",
      "PIX BUFFER SIZE\n",
      "3207900\n",
      "Original IMG_BUFFER_SIZE\n",
      "3207900\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011b40>\n",
      "width\n",
      "1700\n",
      "height\n",
      "783\n",
      "PIX BUFFER SIZE\n",
      "3993300\n",
      "Original IMG_BUFFER_SIZE\n",
      "3993300\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014011b40>\n",
      "page_image_dict\n",
      "{'page': 575, 'img_cnt': 2, 'img_npy_lst': []}\n",
      "543\n",
      "Setting up the OS and required packages\n",
      "name into the field at the bottom left, as shown in figure B.5. You’re currently setting\n",
      "up the master, so set the host name to master.k8s. Click the Apply button next to the\n",
      "text field to confirm the new host name.\n",
      "To return to the main setup screen, click the Done button in the top-left corner.\n",
      " You also need to set the correct time zone. Click Date & Time and then, on the\n",
      "screen that opens, select the Region and City or click your location on the map. Return\n",
      "to the main screen by clicking the Done button in the top-left corner.\n",
      "RUNNING THE INSTALL\n",
      "To start the installation, click the Begin Installation button in the bottom-right corner.\n",
      "A screen like the one in figure B.6 will appear. While the OS is being installed, set the\n",
      "Figure B.5\n",
      "Setting the hostname and configuring the network adapter\n",
      "Figure B.6\n",
      "Setting the root password while the OS is being installed and rebooting afterward\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 576, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "544\n",
      "APPENDIX B\n",
      "Setting up a multi-node cluster with kubeadm\n",
      "root password and create a user account, if you want. When the installation completes,\n",
      "click the Reboot button at the bottom right.\n",
      "B.1.4\n",
      "Installing Docker and Kubernetes\n",
      "Log into the machine as root. First, you need to disable two security features: SELinux\n",
      "and the firewall.\n",
      "DISABLING SELINUX\n",
      "To disable SELinux, run the following command:\n",
      "# setenforce 0\n",
      "But this only disables it temporarily (until the next reboot). To disable it perma-\n",
      "nently, edit the /etc/selinux/config file and change the SELINUX=enforcing line to\n",
      "SELINUX=permissive.\n",
      "DISABLING THE FIREWALL\n",
      "You’ll also disable the firewall, so you don’t run into any firewall-related problems.\n",
      "Run the following command:\n",
      "# systemctl disable firewalld && systemctl stop firewalld\n",
      "Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1...\n",
      "Removed symlink /etc/systemd/system/basic.target.wants/firewalld.service.\n",
      "ADDING THE KUBERNETES YUM REPO\n",
      "To make the Kubernetes RPM packages available to the yum package manager, you’ll\n",
      "add a kubernetes.repo file to the /etc/yum.repos.d/ directory as shown in the follow-\n",
      "ing listing.\n",
      "# cat <<EOF > /etc/yum.repos.d/kubernetes.repo\n",
      "[kubernetes]\n",
      "name=Kubernetes\n",
      "baseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64\n",
      "enabled=1\n",
      "gpgcheck=1\n",
      "repo_gpgcheck=1\n",
      "gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg\n",
      "        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\n",
      "EOF\n",
      "NOTE\n",
      "Make sure no whitespace exists after EOF if you’re copying and pasting. \n",
      "INSTALLING DOCKER, KUBELET, KUBEADM, KUBECTL, AND KUBERNETES-CNI\n",
      "Now you’re ready to install all the packages you need:\n",
      "# yum install -y docker kubelet kubeadm kubectl kubernetes-cni\n",
      "Listing B.1\n",
      "Adding the Kubernetes RPM repo\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 577, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "545\n",
      "Setting up the OS and required packages\n",
      "As you can see, you’re installing quite a few packages. Here’s what they are:\n",
      "■\n",
      "docker—The container runtime\n",
      "■\n",
      "kubelet—The Kubernetes node agent, which will run everything for you\n",
      "■\n",
      "kubeadm—A tool for deploying multi-node Kubernetes clusters\n",
      "■\n",
      "kubectl—The command line tool for interacting with Kubernetes\n",
      "■\n",
      "kubernetes-cni—The Kubernetes Container Networking Interface\n",
      "Once they’re all installed, you need to manually enable the docker and the kubelet\n",
      "services:\n",
      "# systemctl enable docker && systemctl start docker\n",
      "# systemctl enable kubelet && systemctl start kubelet\n",
      "ENABLING THE NET.BRIDGE.BRIDGE-NF-CALL-IPTABLES KERNEL OPTION\n",
      "I’ve noticed that something disables the bridge-nf-call-iptables kernel parameter,\n",
      "which is required for Kubernetes services to operate properly. To rectify the problem,\n",
      "you need to run the following two commands:\n",
      "# sysctl -w net.bridge.bridge-nf-call-iptables=1\n",
      "# echo \"net.bridge.bridge-nf-call-iptables=1\" > /etc/sysctl.d/k8s.conf\n",
      "DISABLING SWAP\n",
      "The Kubelet won’t run if swap is enabled, so you’ll disable it with the following\n",
      "command:\n",
      "# swapoff -a &&  sed -i '/ swap / s/^/#/' /etc/fstab\n",
      "B.1.5\n",
      "Cloning the VM\n",
      "Everything you’ve done up to this point must be done on every machine you plan to\n",
      "use in your cluster. If you’re doing this on bare metal, you need to repeat the process\n",
      "described in the previous section at least two more times—for each worker node. If\n",
      "you’re building the cluster using virtual machines, now’s the time to clone the VM, so\n",
      "you end up with three different VMs.\n",
      "SHUTTING DOWN THE VM\n",
      "To clone the machine in VirtualBox, first shut down the VM by running the shutdown\n",
      "command:\n",
      "# shutdown now\n",
      "CLONING THE VM\n",
      "Now, right-click on the VM in the VirtualBox UI and select Clone. Enter the name for\n",
      "the new machine as shown in figure B.7 (for example, k8s-node1 for the first clone or\n",
      "k8s-node2 for the second one). Make sure you check the Reinitialize the MAC address\n",
      "of all network cards option, so each VM uses different MAC addresses (because\n",
      "they’re going to be located in the same network).\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "width\n",
      "1175\n",
      "height\n",
      "839\n",
      "PIX BUFFER SIZE\n",
      "2957475\n",
      "Original IMG_BUFFER_SIZE\n",
      "2957475\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014010880>\n",
      "page_image_dict\n",
      "{'page': 578, 'img_cnt': 1, 'img_npy_lst': []}\n",
      "546\n",
      "APPENDIX B\n",
      "Setting up a multi-node cluster with kubeadm\n",
      "Click the Next button and then make sure the Full clone option is selected before\n",
      "clicking Next again. Then, on the next screen, click Clone (leave the Current machine\n",
      "state option selected).\n",
      " Repeat the process for the VM for the second node and then start all three VMs by\n",
      "selecting all three and clicking the Start icon. \n",
      "CHANGING THE HOSTNAME ON THE CLONED VMS\n",
      "Because you created two clones from your master VM, all three VMs have the same host-\n",
      "name configured. Therefore, you need to change the hostnames of the two clones. To\n",
      "do that, log into each of the two nodes (as root) and run the following command:\n",
      "# hostnamectl --static set-hostname node1.k8s\n",
      "NOTE\n",
      "Be sure to set the hostname to node2.k8s on the second node.\n",
      "CONFIGURING NAME RESOLUTION FOR ALL THREE HOSTS\n",
      "You need to ensure that all three nodes are resolvable either by adding records to a\n",
      "DNS server or by editing the /etc/hosts file on all of them. For example, you need to\n",
      "add the following three lines to the hosts file (replace the IPs with those of your VMs),\n",
      "as shown in the following listing.\n",
      "192.168.64.138 master.k8s\n",
      "192.168.64.139 node1.k8s\n",
      "192.168.64.140 node2.k8s\n",
      "Listing B.2\n",
      "Entries to add to /etc/hosts on each cluster node\n",
      "Figure B.7\n",
      "Cloning the master VM\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 579, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "547\n",
      "Configuring the master with kubeadm\n",
      "You can get each node’s IP by logging into the node as root, running ip addr and\n",
      "finding the IP address associated with the enp0s3 network adapter, as shown in the fol-\n",
      "lowing listing.\n",
      "# ip addr\n",
      "1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1\n",
      "    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n",
      "    inet 127.0.0.1/8 scope host lo\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 ::1/128 scope host\n",
      "       valid_lft forever preferred_lft forever\n",
      "2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state \n",
      "UP qlen 1000\n",
      "    link/ether 08:00:27:db:c3:a4 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 192.168.64.138/24 brd 192.168.64.255 scope global dynamic enp0s3\n",
      "       valid_lft 59414sec preferred_lft 59414sec\n",
      "    inet6 fe80::77a9:5ad6:2597:2e1b/64 scope link\n",
      "       valid_lft forever preferred_lft forever\n",
      "The command’s output in the previous listing shows that the machine’s IP address is\n",
      "192.168.64.138. You’ll need to run this command on each of your nodes to get all\n",
      "their IPs.\n",
      "B.2\n",
      "Configuring the master with kubeadm\n",
      "You’re now ready to finally set up the Kubernetes Control Plane on your master node. \n",
      "RUNNING KUBEADM INIT TO INITIALIZE THE MASTER\n",
      "Thanks to the awesome kubeadm tool, all you need to do to initialize the master is run\n",
      "a single command, as shown in the following listing.\n",
      "# kubeadm init\n",
      "[kubeadm] WARNING: kubeadm is in beta, please do not use it for production \n",
      "clusters.\n",
      "[init] Using Kubernetes version: v.1.8.4\n",
      "...\n",
      "You should now deploy a pod network to the cluster.\n",
      "Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n",
      "  http://kubernetes.io/docs/admin/addons/\n",
      "You can now join any number of machines by running the following on each node \n",
      "as root:\n",
      "kubeadm join --token eb3877.3585d0423978c549 192.168.64.138:6443 \n",
      "--discovery-token-ca-cert-hash \n",
      "sha256:037d2c5505294af196048a17f184a79411c7b1eac48aaa0ad137075be3d7a847\n",
      "NOTE\n",
      "Write down the command shown in the last line of kubeadm init’s out-\n",
      "put. You’ll need it later.\n",
      "Listing B.3\n",
      "Looking up each node’s IP address\n",
      "Listing B.4\n",
      "Initializing the master node with kubeadm init\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 580, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "548\n",
      "APPENDIX B\n",
      "Setting up a multi-node cluster with kubeadm\n",
      "Kubeadm has deployed all the necessary Control Plane components, including etcd,\n",
      "the API server, Scheduler, and Controller Manager. It has also deployed the kube-\n",
      "proxy, making Kubernetes services available from the master node. \n",
      "B.2.1\n",
      "Understanding how kubeadm runs the components\n",
      "All these components are running as containers. You can use the docker ps command\n",
      "to confirm this. But kubeadm doesn’t use Docker directly to run them. It deploys their\n",
      "YAML descriptors to the /etc/kubernetes/manifests directory. This directory is moni-\n",
      "tored by the Kubelet, which then runs these components through Docker. The com-\n",
      "ponents run as Pods. You can see them with the kubectl get command. But first, you\n",
      "need to configure kubectl.\n",
      "RUNNING KUBECTL ON THE MASTER\n",
      "You installed kubectl along with docker, kubeadm, and other packages in one of the\n",
      "initial steps. But you can’t use kubectl to talk to your cluster without first configuring\n",
      "it through a kubeconfig file.\n",
      " Luckily, the necessary configuration is stored in the /etc/kubernetes/admin.conf\n",
      "file. All you need to do is make kubectl use it by setting the KUBECONFIG environment\n",
      "variable, as explained in appendix A:\n",
      "# export KUBECONFIG=/etc/kubernetes/admin.conf\n",
      "LISTING THE PODS\n",
      "To test kubectl, you can list the pods of the Control Plane (they’re in the kube-system\n",
      "namespace), as shown in the following listing.\n",
      "# kubectl get po -n kube-system\n",
      "NAME                                 READY     STATUS    RESTARTS   AGE\n",
      "etcd-master.k8s                      1/1       Running   0          21m\n",
      "kube-apiserver-master.k8s            1/1       Running   0          22m\n",
      "kube-controller-manager-master.k8s   1/1       Running   0          21m\n",
      "kube-dns-3913472980-cn6kz            0/3       Pending   0          22m\n",
      "kube-proxy-qb709                     1/1       Running   0          22m\n",
      "kube-scheduler-master.k8s            1/1       Running   0          21m\n",
      "LISTING NODES\n",
      "You’re finished with setting up the master, but you still need to set up the nodes.\n",
      "Although you already installed the Kubelet on both of your two worker nodes (you\n",
      "either installed each node separately or cloned the initial VM after you installed all\n",
      "the required packages), they aren’t part of your Kubernetes cluster yet. You can see\n",
      "that by listing nodes with kubectl:\n",
      "# kubectl get node\n",
      "NAME         STATUS     ROLES     AGE       VERSION\n",
      "master.k8s   NotReady   master    2m        v1.8.4\n",
      "Listing B.5\n",
      "System pods in the kube-system namespace\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 581, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "549\n",
      "Configuring worker nodes with kubeadm\n",
      "See, only the master is listed as a node. And even the master is shown as being Not-\n",
      "Ready. You’ll see why later. Now, you’ll set up your two nodes.\n",
      "B.3\n",
      "Configuring worker nodes with kubeadm\n",
      "When using kubeadm, configuring worker nodes is even easier than configuring the\n",
      "master. In fact, when you ran the kubeadm init command to set up your master, it\n",
      "already told you how to configure your worker nodes (repeated in the next listing).\n",
      "You can now join any number of machines by running the following on each node \n",
      "as root:\n",
      "kubeadm join --token eb3877.3585d0423978c549 192.168.64.138:6443 \n",
      "--discovery-token-ca-cert-hash \n",
      "sha256:037d2c5505294af196048a17f184a79411c7b1eac48aaa0ad137075be3d7a847\n",
      "All you need to do is run the kubeadm join command with the specified token and the\n",
      "master’s IP address/port on both of your nodes. It then takes less than a minute for\n",
      "the nodes to register themselves with the master. You can confirm they’re registered\n",
      "by running the kubectl get node command on the master again:\n",
      "# kubectl get nodes\n",
      "NAME         STATUS     ROLES     AGE       VERSION\n",
      "master.k8s   NotReady   master    3m        v1.8.4\n",
      "node1.k8s    NotReady   <none>    3s        v1.8.4\n",
      "node2.k8s    NotReady   <none>    5s        v1.8.4\n",
      "Okay, you’ve made progress. Your Kubernetes cluster now consists of three nodes, but\n",
      "none of them are ready. Let’s investigate.\n",
      " Let’s use the kubectl describe command in the following listing to see more\n",
      "information. Somewhere at the top, you’ll see a list of Conditions, showing the\n",
      "current conditions on the node. One of them will show the following Reason and\n",
      "Message.\n",
      "# kubectl describe node node1.k8s\n",
      "...\n",
      "KubeletNotReady    runtime network not ready: NetworkReady=false \n",
      "                   reason:NetworkPluginNotReady message:docker: \n",
      "                   network plugin is not ready: cni config uninitialized\n",
      "According to this, the Kubelet isn’t fully ready, because the container network (CNI)\n",
      "plugin isn’t ready, which is expected, because you haven’t deployed the CNI plugin\n",
      "yet. You’ll deploy one now.\n",
      "Listing B.6\n",
      "Last part of the output of the kubeadm init command\n",
      "Listing B.7\n",
      "Kubectl describe shows why the node isn’t ready\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 582, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "550\n",
      "APPENDIX B\n",
      "Setting up a multi-node cluster with kubeadm\n",
      "B.3.1\n",
      "Setting up the container network\n",
      "You’ll install the Weave Net container networking plugin, but several alternatives are\n",
      "also available. They’re listed among the available Kubernetes add-ons at http:/\n",
      "/kuber-\n",
      "netes.io/docs/admin/addons/.\n",
      " Deploying the Weave Net plugin (like most other add-ons) is as simple as this:\n",
      "$ kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl \n",
      "version | base64 | tr -d '\\n')\n",
      "This will deploy a DaemonSet and a few security-related resources (refer to chapter 12\n",
      "for an explanation of the ClusterRole and ClusterRoleBinding, which are deployed\n",
      "alongside the DaemonSet).\n",
      " Once the DaemonSet controller creates the pods and they’re started on all your\n",
      "nodes, the nodes should become ready:\n",
      "# k get node\n",
      "NAME         STATUS    ROLES     AGE       VERSION\n",
      "master.k8s   Ready     master    9m        v1.8.4\n",
      "node1.k8s    Ready     <none>    5m        v1.8.4\n",
      "node2.k8s    Ready     <none>    5m        v1.8.4\n",
      "And that’s it. You now have a fully functioning three-node Kubernetes cluster with an\n",
      "overlay network provided by Weave Net. All the required components, except for the\n",
      "Kubelet itself, are running as pods, managed by the Kubelet, as shown in the follow-\n",
      "ing listing.\n",
      "# kubectl get po --all-namespaces\n",
      "NAMESPACE     NAME                                 READY     STATUS    AGE\n",
      "kube-system   etcd-master.k8s                      1/1       Running   1h\n",
      "kube-system   kube-apiserver-master.k8s            1/1       Running   1h\n",
      "kube-system   kube-controller-manager-master.k8s   1/1       Running   1h\n",
      "kube-system   kube-dns-3913472980-cn6kz            3/3       Running   1h\n",
      "kube-system   kube-proxy-hcqnx                     1/1       Running   24m\n",
      "kube-system   kube-proxy-jvdlr                     1/1       Running   24m\n",
      "kube-system   kube-proxy-qb709                     1/1       Running   1h\n",
      "kube-system   kube-scheduler-master.k8s            1/1       Running   1h\n",
      "kube-system   weave-net-58zbk                      2/2       Running   7m\n",
      "kube-system   weave-net-91kjd                      2/2       Running   7m\n",
      "kube-system   weave-net-vt279                      2/2       Running   7m\n",
      "B.4\n",
      "Using the cluster from your local machine\n",
      "Up to this point, you’ve used kubectl on the master node to talk to the cluster. You’ll\n",
      "probably want to configure the kubectl instance on your local machine, too. \n",
      " To do that, you need to copy the /etc/kubernetes/admin.conf file from the mas-\n",
      "ter to your local machine with the following command:\n",
      "$ scp root@192.168.64.138:/etc/kubernetes/admin.conf ~/.kube/config2\n",
      "Listing B.8\n",
      "System pods in the kube-system namespace after deploying Weave Net\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 583, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "551\n",
      "Using the cluster from your local machine\n",
      "Replace the IP with that of your master. Then you point the KUBECONFIG environment\n",
      "variable to the ~/.kube/config2 file like this:\n",
      "$ export KUBECONFIG=~/.kube/config2\n",
      "Kubectl will now use this config file. To switch back to using the previous one, unset\n",
      "the environment variable. \n",
      " You’re now all set to use the cluster from your local machine.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 584, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "552\n",
      "appendix C\n",
      "Using other container\n",
      "runtimes\n",
      "C.1\n",
      "Replacing Docker with rkt\n",
      "We’ve mentioned rkt (pronounced rock-it) a few times in this book. Like Docker, it\n",
      "runs applications in isolated containers, using the same Linux technologies as\n",
      "those used by Docker. Let’s look at how rkt differs from Docker and how to try it in\n",
      "Minikube.\n",
      " The first great thing about rkt is that it directly supports the notion of a Pod\n",
      "(running multiple related containers), unlike Docker, which only runs individual\n",
      "containers. Rkt is based on open standards and was built with security in mind from\n",
      "the start (for example, images are signed, so you can be sure they haven’t been tam-\n",
      "pered with). Unlike Docker, which initially had a client-server based architecture\n",
      "that didn’t play well with init systems such as systemd, rkt is a CLI tool that runs\n",
      "your container directly, instead of telling a daemon to run it. A nice thing about rkt\n",
      "is that it can run existing Docker-formatted container images, so you don’t need to\n",
      "repackage your applications to get started with rkt.\n",
      "C.1.1\n",
      "Configuring Kubernetes to use rkt\n",
      "As you may remember from chapter 11, the Kubelet is the only Kubernetes\n",
      "component that talks to the Container Runtime. To get Kubernetes to use rkt\n",
      "instead of Docker, you need to configure the Kubelet to use it by running it with\n",
      "the --container-runtime=rkt command-line option. But be aware that support\n",
      "for rkt isn’t as mature as support for Docker. \n",
      " Please refer to the Kubernetes documentation for more information on how to\n",
      "use rkt and what is or isn’t supported. Here, we’ll go over a quick example to get\n",
      "you started.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 585, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "553\n",
      "Replacing Docker with rkt\n",
      "C.1.2\n",
      "Trying out rkt with Minikube\n",
      "Luckily, to get started with rkt on Kubernetes, all you need is the same Minikube exe-\n",
      "cutable you’re already using. To use rkt as the container runtime in Minikube, all you\n",
      "need to do is start Minikube with the following two options:\n",
      "$ minikube start --container-runtime=rkt --network-plugin=cni \n",
      "NOTE\n",
      "You may need to run minikube delete to delete the existing Minikube\n",
      "VM first. \n",
      "The --container-runtime=rkt option obviously configures the Kubelet to use rkt as\n",
      "the Container Runtime, whereas the --network-plugin=cni makes it use the Con-\n",
      "tainer Network Interface as the network plugin. Without this option, pods won’t run,\n",
      "so it’s imperative you use it.\n",
      "RUNNING A POD\n",
      "Once the Minikube VM is up, you can interact with Kubernetes exactly like before.\n",
      "You can deploy the kubia app with the kubectl run command, for example:\n",
      "$ kubectl run kubia --image=luksa/kubia --port 8080\n",
      "deployment \"kubia\" created\n",
      "When the pod starts up, you can see it’s running through rkt by inspecting its contain-\n",
      "ers with kubectl describe, as shown in the following listing.\n",
      "$ kubectl describe pods\n",
      "Name:           kubia-3604679414-l1nn3\n",
      "...\n",
      "Status:         Running\n",
      "IP:             10.1.0.2\n",
      "Controllers:    ReplicaSet/kubia-3604679414\n",
      "Containers:\n",
      "  kubia:\n",
      "    Container ID:       rkt://87a138ce-...-96e375852997:kubia    \n",
      "    Image:              luksa/kubia\n",
      "    Image ID:           rkt://sha512-5bbc5c7df6148d30d74e0...    \n",
      "...\n",
      "You can also try hitting the pod’s HTTP port to see if it’s responding properly to\n",
      "HTTP requests. You can do this by creating a NodePort Service or by using kubectl\n",
      "port-forward, for example. \n",
      "INSPECTING THE RUNNING CONTAINERS IN THE MINIKUBE VM\n",
      "To get more familiar with rkt, you can try logging into the Minikube VM with the fol-\n",
      "lowing command:\n",
      "$ minikube ssh\n",
      "Listing C.1\n",
      "Pod running with rkt\n",
      "The container \n",
      "and image IDs \n",
      "mention rkt \n",
      "instead of \n",
      "Docker.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 586, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "554\n",
      "APPENDIX C\n",
      "Using other container runtimes\n",
      "Then, you can use rkt list to see the running pods and containers, as shown in the\n",
      "following listing.\n",
      "$ rkt list\n",
      "UUID      APP                 IMAGE NAME                       STATE   ...\n",
      "4900e0a5  k8s-dashboard       gcr.io/google_containers/kun...  running ...\n",
      "564a6234  nginx-ingr-ctrlr    gcr.io/google_containers/ngi...  running ...\n",
      "5dcafffd  dflt-http-backend   gcr.io/google_containers/def...  running ...\n",
      "707a306c  kube-addon-manager  gcr.io/google-containers/kub...  running ...\n",
      "87a138ce  kubia               registry-1.docker.io/luksa/k...  running ...\n",
      "d97f5c29  kubedns             gcr.io/google_containers/k8s...  running ...\n",
      "          dnsmasq             gcr.io/google_containers/k8...            \n",
      "          sidecar             gcr.io/google_containers/k8...\n",
      "You can see the kubia container, as well as other system containers running (the ones\n",
      "deployed in pods in the kube-system namespace). Notice how the bottom two con-\n",
      "tainers don’t have anything listed in the UUID or STATE columns? That’s because they\n",
      "belong to the same pod as the kubedns container listed above them. \n",
      " Rkt prints containers belonging to the same pod grouped together. Each pod\n",
      "(instead of each container) has its own UUID and state. If you tried doing this when\n",
      "you were using Docker as the Container Runtime, you’ll appreciate how much easier\n",
      "it is to see all the pods and their containers with rkt. You’ll notice no infrastructure\n",
      "container exists for each pod (we explained them in chapter 11). That’s because of\n",
      "rkt’s native support for pods.\n",
      "LISTING CONTAINER IMAGES\n",
      "If you’ve played around with Docker CLI commands, you’ll get familiar quickly with\n",
      "rkt’s commands. Run rkt without any arguments and you’ll see all the commands you\n",
      "can run. For example, to list container images, you run the command in the follow-\n",
      "ing listing.\n",
      "$ rkt image list\n",
      "ID           NAME                          SIZE    IMPORT TIME  LAST USED\n",
      "sha512-a9c3  ...addon-manager:v6.4-beta.1  245MiB  24 min ago   24 min ago\n",
      "sha512-a078  .../rkt/stage1-coreos:1.24.0  224MiB  24 min ago   24 min ago\n",
      "sha512-5bbc  ...ker.io/luksa/kubia:latest  1.3GiB  23 min ago   23 min ago\n",
      "sha512-3931  ...es-dashboard-amd64:v1.6.1  257MiB  22 min ago   22 min ago\n",
      "sha512-2826  ...ainers/defaultbackend:1.0  15MiB   22 min ago   22 min ago\n",
      "sha512-8b59  ...s-controller:0.9.0-beta.4  233MiB  22 min ago   22 min ago\n",
      "sha512-7b59  ...dns-kube-dns-amd64:1.14.2  100MiB  21 min ago   21 min ago\n",
      "sha512-39c6  ...nsmasq-nanny-amd64:1.14.2  86MiB   21 min ago   21 min ago\n",
      "sha512-89fe  ...-dns-sidecar-amd64:1.14.2  85MiB   21 min ago   21 min ago\n",
      "These are all Docker-formatted container images. You can also try building images in\n",
      "the OCI image format (OCI stands for Open Container Initiative) with the acbuild\n",
      "Listing C.2\n",
      "Listing running containers with rkt list\n",
      "Listing C.3\n",
      "Listing images with rkt image list\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 587, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "555\n",
      "Using other container runtimes through the CRI\n",
      "tool (available at https:/\n",
      "/github.com/containers/build) and running them with rkt.\n",
      "Doing that is outside the scope of this book, so I’ll let you try it on your own.\n",
      " The information explained in this appendix so far should be enough to get you\n",
      "started using rkt with Kubernetes. Refer to the rkt documentation at https:/\n",
      "/coreos\n",
      ".com/rkt and Kubernetes documentation at https:/\n",
      "/kubernetes.io/docs for addi-\n",
      "tional information.\n",
      "C.2\n",
      "Using other container runtimes through the CRI\n",
      "Kubernetes’ support for other container runtimes doesn’t stop with Docker and rkt.\n",
      "Both of those runtimes were initially integrated directly into Kubernetes, but in\n",
      "Kubernetes version 1.5, the Container Runtime Interface (CRI) was introduced. CRI\n",
      "is a plugin API enabling easy integration of other container runtimes with Kuberne-\n",
      "tes. People are now free to plug other container runtimes into Kubernetes without\n",
      "having to dig deep into Kubernetes code. All they need to do is implement a few inter-\n",
      "face methods. \n",
      " From Kubernetes version 1.6 onward, CRI is the default interface the Kubelet uses.\n",
      "Both Docker and rkt are now used through the CRI (no longer directly).\n",
      "C.2.1\n",
      "Introducing the CRI-O Container Runtime\n",
      "Beside Docker and rkt, a new CRI implementation called CRI-O allows Kubernetes to\n",
      "directly launch and manage OCI-compliant containers, without requiring you to\n",
      "deploy any additional Container Runtime. \n",
      " You can try CRI-O with Minikube by starting it with --container-runtime=crio.\n",
      "C.2.2\n",
      "Running apps in virtual machines instead of containers \n",
      "Kubernetes is a container orchestration system, right? Throughout the book, we\n",
      "explored many features that show that it’s much more than an orchestration system,\n",
      "but the bottom line is that when you run an app with Kubernetes, the app always runs\n",
      "inside a container, right? You may find it surprising that’s no longer the case.\n",
      " New CRI implementations are being developed that allow Kubernetes to run apps\n",
      "in virtual machines instead of in containers. One such implementation, called Frakti,\n",
      "allows you to run regular Docker-based container images directly through a hypervi-\n",
      "sor, which means each container runs its own kernel. This allows much better isola-\n",
      "tion between containers compared to when they use the same kernel. \n",
      " And there’s more. Another CRI implementation is the Mirantis Virtlet, which\n",
      "makes it possible to run actual VM images (in the QCOW2 image file format, which is\n",
      "one of the formats used by the QEMU virtual machine tool) instead of container\n",
      "images. When you use the Virtlet as the CRI plugin, Kubernetes spins up a VM for\n",
      "each pod. How awesome is that?\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 588, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "556\n",
      "appendix D\n",
      "Cluster Federation\n",
      "In the section about high availability in chapter 11 we explored how Kubernetes\n",
      "can deal with failures of individual machines and even failures of whole server racks\n",
      "or the supporting infrastructure. But what if the whole datacenter goes dark?\n",
      " To make sure you’re not susceptible to datacenter-wide outages, apps should be\n",
      "deployed in multiple datacenters or cloud availability zones. When one of those\n",
      "datacenters or availability zones becomes unavailable, client requests can be routed\n",
      "to the apps running in the remaining healthy datacenters or zones.\n",
      " While Kubernetes doesn’t require you to run the Control Plane and the nodes\n",
      "in the same datacenter, you’ll almost always want to do that to keep network latency\n",
      "between them low and to reduce the possibility of them becoming disconnected\n",
      "from each other. Instead of having a single cluster spread across multiple locations,\n",
      "a better alternative is to have an individual Kubernetes cluster in every location.\n",
      "We’ll explore this approach in this appendix.\n",
      "D.1\n",
      "Introducing Kubernetes Cluster Federation\n",
      "Kubernetes allows you to combine multiple clusters into a cluster of clusters\n",
      "through Cluster Federation. It allows users to deploy and manage apps across mul-\n",
      "tiple clusters running in different locations in the world, but also across different\n",
      "cloud providers combined with on-premises clusters (hybrid cloud). The motiva-\n",
      "tion for Cluster Federation isn’t only to ensure high availability, but also to com-\n",
      "bine multiple heterogeneous clusters into a single super-cluster managed through\n",
      "a single management interface. \n",
      " For example, by combining an on-premises cluster with one running on a cloud\n",
      "provider’s infrastructure, you can run privacy-sensitive components of your applica-\n",
      "tion system on-premises, while the non-sensitive parts can run in the cloud.\n",
      "Another example is initially running your application only in a small on-premises\n",
      "cluster, but when the application’s compute requirements exceed the cluster’s\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 589, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "557\n",
      "Understanding the architecture\n",
      "capacity, letting the application spill over to a cloud-based cluster, which is automati-\n",
      "cally provisioned on the cloud provider’s infrastructure.\n",
      "D.2\n",
      "Understanding the architecture\n",
      "Let’s take a quick look at what Kubernetes Cluster Federation is. A cluster of clusters\n",
      "can be compared to a regular cluster where instead of nodes, you have complete clus-\n",
      "ters. Just as a Kubernetes cluster consists of a Control Plane and multiple worker\n",
      "nodes, a federated cluster consists of a Federated Control Plane and multiple Kuber-\n",
      "netes clusters. Similar to how the Kubernetes Control Plane manages applications\n",
      "across a set of worker nodes, the Federated Control Plane does the same thing, but\n",
      "across a set of clusters instead of nodes. \n",
      " The Federated Control Plane consists of three things:\n",
      "■\n",
      "etcd for storing the federated API objects\n",
      "■\n",
      "Federation API server\n",
      "■\n",
      "Federation Controller Manager\n",
      "This isn’t much different from the regular Kubernetes Control Plane. etcd stores the\n",
      "federated API objects, the API server is the REST endpoint all other components talk\n",
      "to, and the Federation Controller Manager runs the various federation controllers\n",
      "that perform operations based on the API objects you create through the API server. \n",
      " Users talk to the Federation API server to create federated API objects (or feder-\n",
      "ated resources). The federation controllers watch these objects and then talk to the\n",
      "underlying clusters’ API servers to create regular Kubernetes resources. The architec-\n",
      "ture of a federated cluster is shown in figure D.1.\n",
      "San Francisco\n",
      "Control Plane\n",
      "etcd\n",
      "Federation Controller Manager\n",
      "Controller\n",
      "Manager\n",
      "API server\n",
      "etcd\n",
      "Federation\n",
      "API server\n",
      "Scheduler\n",
      "Worker node\n",
      "Kubelet\n",
      "Worker node\n",
      "Kubelet\n",
      "Worker node\n",
      "Kubelet\n",
      "Control Plane\n",
      "etcd\n",
      "Controller\n",
      "Manager\n",
      "API server\n",
      "Scheduler\n",
      "Worker node\n",
      "Kubelet\n",
      "Worker node\n",
      "Kubelet\n",
      "Worker node\n",
      "Kubelet\n",
      "London\n",
      "Other\n",
      "locations\n",
      "Figure D.1\n",
      "Cluster Federation with clusters in different geographical locations\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 590, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "558\n",
      "APPENDIX A\n",
      "Cluster Federation\n",
      "D.3\n",
      "Understanding federated API objects\n",
      "The federated API server allows you to create federated variants of the objects you\n",
      "learned about throughout the book. \n",
      "D.3.1\n",
      "Introducing federated versions of Kubernetes resources\n",
      "At the time of writing this, the following federated resources are supported:\n",
      "■\n",
      "Namespaces\n",
      "■\n",
      "ConfigMaps and Secrets\n",
      "■\n",
      "Services and Ingresses\n",
      "■\n",
      "Deployments, ReplicaSets, Jobs, and Daemonsets\n",
      "■\n",
      "HorizontalPodAutoscalers\n",
      "NOTE\n",
      "Check the Kubernetes Cluster Federation documentation for an up-to-\n",
      "date list of supported federated resources.\n",
      "In addition to these resources, the Federated API server also supports the Cluster\n",
      "object, which represents an underlying Kubernetes cluster, the same way a Node object\n",
      "represents a worker node in a regular Kubernetes cluster. To help you visualize how fed-\n",
      "erated objects relate to the objects created in the underlying clusters, see figure D.2.\n",
      "San Francisco\n",
      "Namespace: foo\n",
      "ReplicaSet X-432\n",
      "Replicas: 3\n",
      "Pod X-432-5\n",
      "Deployment X\n",
      "Replicas: 3\n",
      "Federated resources\n",
      "Secret W\n",
      "Secret Y\n",
      "Namespace: foo\n",
      "Deployment X\n",
      "Replicas: 5\n",
      "Secret Y\n",
      "London\n",
      "Namespace: foo\n",
      "ReplicaSet X-432\n",
      "Replicas: 2\n",
      "Pod X-432-8\n",
      "Deployment X\n",
      "Replicas: 2\n",
      "Secret W\n",
      "Secret Y\n",
      "Cluster:\n",
      "London\n",
      "Cluster:\n",
      "San Francisco\n",
      "Secret W\n",
      "Ingress Z\n",
      "Figure D.2\n",
      "The relationship between federated resources and regular resources in underlying clusters\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 591, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "559\n",
      "Understanding federated API objects\n",
      "D.3.2\n",
      "Understanding what federated resources do\n",
      "For part of the federated objects, when you create the object in the Federation API\n",
      "server, the controllers running in the Federation Controller Manager will create regu-\n",
      "lar cluster-scoped resources in all underlying Kubernetes clusters and manage them\n",
      "until the federated object is deleted. \n",
      " For certain federated resource types, the resources created in the underlying clus-\n",
      "ters are exact replicas of the federated resource; for others, they’re slightly modified\n",
      "versions, whereas certain federated resources don’t cause anything to be created in\n",
      "the underlying clusters at all. The replicas are kept in sync with the original federated\n",
      "versions. But the synchronization is one-directional only—from the federation server\n",
      "down to the underlying clusters. If you modify the resource in an underlying cluster,\n",
      "the changes will not be synced up to the Federation API server.\n",
      " For example, if you create a namespace in the federated API server, a namespace\n",
      "with the same name will be created in all underlying clusters. If you then create a fed-\n",
      "erated ConfigMap inside that namespace, a ConfigMap with that exact name and con-\n",
      "tents will be created in all underlying clusters, in the same namespace. This also applies\n",
      "to Secrets, Services, and DaemonSets.\n",
      " ReplicaSets and Deployments are different. They aren’t blindly copied to the\n",
      "underlying clusters, because that’s not what the user usually wants. After all, if you cre-\n",
      "ate a Deployment with a desired replica count of 10, you probably don’t want 10 pod\n",
      "replicas running in each underlying cluster. You want 10 replicas in total. Because of\n",
      "this, when you specify a desired replica count in a Deployment or ReplicaSet, the Fed-\n",
      "eration controllers create underlying Deployments/ReplicaSets so that the sum of\n",
      "their desired replica counts equals the desired replica count specified in the federated\n",
      "Deployment or ReplicaSet. By default, the replicas are spread evenly across the clus-\n",
      "ters, but this can be overridden.\n",
      "NOTE\n",
      "Currently, you need to connect to each cluster’s API server individu-\n",
      "ally to get the list of pods running in that cluster. You can’t list all the clusters’\n",
      "pods through the Federated API server.\n",
      "A federated Ingress resource, on the other hand, doesn’t result in the creation of any\n",
      "Ingress objects in the underlying clusters. You may remember from chapter 5 that an\n",
      "Ingress represents a single point of entry for external clients to a Service. Because of\n",
      "this, a federated Ingress resource creates a global, multi-cluster-wide entry point to the\n",
      "Services across all underlying clusters. \n",
      "NOTE\n",
      "As for regular Ingresses, a federated Ingress controller is required\n",
      "for this. \n",
      "Setting up federated Kubernetes clusters is outside the scope of this book, so you can\n",
      "learn more about the subject by referring to the Cluster Federation sections in the\n",
      "user and administration guides in the Kubernetes online documentation at http:/\n",
      "/\n",
      "kubernetes.io/docs/.\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 592, 'img_cnt': 0, 'img_npy_lst': []}\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 593, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "561\n",
      "index\n",
      "Symbols\n",
      "$(ENV_VAR) syntax 199\n",
      "$(ENV_VARIABLE_NAME) \n",
      "syntax 205\n",
      "$(VAR) syntax 198\n",
      "* (asterisk) character 117\n",
      "- - (double dash) 125\n",
      "Numerics\n",
      "137 exit code 88\n",
      "143 exit code 89\n",
      "8080 port 48, 67\n",
      "8888 port 67\n",
      "A\n",
      "-a option 35\n",
      "ABAC (Attribute-based access \n",
      "control) 353\n",
      "access controls. See RBAC\n",
      "accounts. See service accounts\n",
      "actions 353–354\n",
      "activeDeadlineSeconds \n",
      "property 116\n",
      "ad hoc tasks 112\n",
      "adapters\n",
      "network, configuring for \n",
      "VMs 540\n",
      "node networks, shutting \n",
      "down 304–305\n",
      "ADDED watch event 514\n",
      "add-ons 328–330\n",
      "components 310\n",
      "deploying 328–329\n",
      "DNS servers 329\n",
      "Ingress controllers 329\n",
      "using 329–330\n",
      "addresses attribute 138\n",
      "addresses, of API servers 239–240\n",
      "admin ClusterRole, granting full \n",
      "control of namespaces \n",
      "with 372\n",
      "algorithms, default \n",
      "scheduling 319\n",
      "aliases\n",
      "creating for external \n",
      "services 134\n",
      "for kubectl 41–42\n",
      "all keyword 82\n",
      "--all option 81\n",
      "--all-namespaces option 144\n",
      "allocatable resources 408\n",
      "allowed capabilities, \n",
      "configuring 394–395\n",
      "adding capabilities to all \n",
      "containers 395\n",
      "dropping capabilities from \n",
      "containers 395\n",
      "specifying which capabilities \n",
      "can be added to \n",
      "containers 394\n",
      "allowedCapabilities 394\n",
      "AlwaysPullImages 317\n",
      "Amazon Web Services. See AWS\n",
      "ambassador containers\n",
      "communicating with API servers \n",
      "through 245\n",
      "patterns 244\n",
      "running curl pods with 244–245\n",
      "simplifying API server commu-\n",
      "nication with 243–245\n",
      "annotations\n",
      "adding 76\n",
      "describing resources through\n",
      "498\n",
      "modifying 76\n",
      "of objects, looking up 75–76\n",
      "updating 232\n",
      "API (application program inter-\n",
      "face)\n",
      "aggregation 518–519\n",
      "custom, providing for custom \n",
      "objects 518–519\n",
      "defining custom objects 508–519\n",
      "automating custom resources \n",
      "with custom \n",
      "controllers 513–517\n",
      "CRD 509–513\n",
      "providing custom API \n",
      "servers for custom \n",
      "objects 518–519\n",
      "validating custom \n",
      "objects 517–518\n",
      "federated objects 558–559\n",
      "Service Catalog 521\n",
      "See also OpenServiceBroker API\n",
      "API servers 233–248, 316–318, \n",
      "346–374\n",
      "accessing through kubectl \n",
      "proxy 234–235\n",
      "authentication of 346–353\n",
      "groups 346–348\n",
      "service accounts 348–353\n",
      "users 347–348\n",
      "authorizing clients 317\n",
      "communicating from within \n",
      "pods 238–243\n",
      "communicating with pods \n",
      "through 295–297\n",
      "communicating with, through \n",
      "ambassador containers 245\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 594, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "562\n",
      "API servers (continued)\n",
      "connecting to 503\n",
      "connecting to cluster-internal \n",
      "services through 299\n",
      "exploring 235–236, 248\n",
      "finding addresses 239–240\n",
      "modifying resources in \n",
      "requests 317\n",
      "notifying clients of resource \n",
      "changes 318–319\n",
      "persistently storing \n",
      "resources 318\n",
      "running multiple instances \n",
      "of 343\n",
      "running static pods \n",
      "without 326–327\n",
      "securing clusters with \n",
      "RBAC 353–373\n",
      "binding roles to service \n",
      "accounts 359–360\n",
      "default ClusterRoleBindings\n",
      "371–373\n",
      "default ClusterRoles 371–373\n",
      "granting authorization \n",
      "permissions 373\n",
      "including service accounts \n",
      "from other namespaces \n",
      "in RoleBinding 361\n",
      "RBAC authorization \n",
      "plugins 353–354\n",
      "RBAC resources 355–357\n",
      "using ClusterRoleBindings\n",
      "362–371\n",
      "using ClusterRoles 362–371\n",
      "using RoleBindings 358–359\n",
      "using Roles 358–359\n",
      "simplifying communication\n",
      "243–245\n",
      "ambassador container \n",
      "patterns 244\n",
      "running curl pod 244–245\n",
      "using client libraries to commu-\n",
      "nicate with 246–248\n",
      "building libraries with \n",
      "OpenAPI 248\n",
      "building libraries with \n",
      "Swagger 248\n",
      "interacting with FABRIC8 \n",
      "Java client 247–248\n",
      "using existing client \n",
      "libraries 246–247\n",
      "using custom service account \n",
      "tokens to communicate \n",
      "with 352–353\n",
      "validating resources 318\n",
      "verifying identity of 240–241\n",
      "See also REST API\n",
      "apiVersion property 106, 358, 510\n",
      "app=kubia label 98, 123\n",
      "app=pc label selector 72\n",
      "application logs 65–66\n",
      "application program interface. See \n",
      "API\n",
      "application templates, in Red Hat \n",
      "OpenShift Container \n",
      "platform 528–529\n",
      "applications\n",
      "accessing 32\n",
      "best practices for \n",
      "developing 477, 502, \n",
      "506–507\n",
      "auto-deploying resource \n",
      "manifests 504–505\n",
      "employing CI/CD 506\n",
      "ensuring client requests are \n",
      "handled properly 492–\n",
      "497\n",
      "Ksonnet as alternative to \n",
      "writing JSON/YAML \n",
      "manifests 505–506\n",
      "pod lifecycles 479–491\n",
      "using Minikube 503–504\n",
      "versioning resource \n",
      "manifests 504–505\n",
      "compromised 373\n",
      "containerized, \n",
      "configuring 191–192\n",
      "creating 290–291\n",
      "creating versions of 268\n",
      "deploying through \n",
      "StatefulSets 291–295\n",
      "creating governing \n",
      "services 292–294\n",
      "creating persistent \n",
      "volumes 291–292\n",
      "creating StatefulSets 294\n",
      "examining PersistentVolume-\n",
      "Claims 295\n",
      "examining stateful pods\n",
      "294–295\n",
      "describing resources through \n",
      "annotations 498\n",
      "descriptions, effect on running \n",
      "containers 19–20\n",
      "examining nodes 51–52\n",
      "displaying pod IP when list-\n",
      "ing pods 51\n",
      "displaying pod node when \n",
      "listing pods 51\n",
      "inspecting details of pod with \n",
      "kubectl describe 52\n",
      "exposing through services using \n",
      "single YAML file 255\n",
      "handling logs 500–502\n",
      "copying files to and from \n",
      "containers 500–501\n",
      "copying logs to and from \n",
      "containers 500–501\n",
      "handling multi-line log \n",
      "statements 502\n",
      "using centralized \n",
      "logging 501\n",
      "highly available 341\n",
      "horizontally scaling 48–50\n",
      "increasing replica count 49\n",
      "requests hitting three pods \n",
      "when hitting services 50\n",
      "results of scale-out 49–50\n",
      "visualizing new states 50\n",
      "implementing shutdown han-\n",
      "dlers in 490–491\n",
      "in containers, limits as seen \n",
      "by 415–416\n",
      "killing 479–482\n",
      "expecting data written to disk \n",
      "to disappear 480\n",
      "expecting hostnames to \n",
      "change 480\n",
      "expecting local IP to \n",
      "change 480\n",
      "using volumes to preserve \n",
      "data across container \n",
      "restarts 480–482\n",
      "making container images 497\n",
      "managing 497–502\n",
      "monolithic, vs. microservices\n",
      "3–6\n",
      "multi-tier, splitting into multiple \n",
      "pods 59\n",
      "Node.js\n",
      "creating 28–29\n",
      "deploying 42–44\n",
      "non-horizontally scalable, using \n",
      "leader-election for 341\n",
      "overview 478–479\n",
      "providing consistent environ-\n",
      "ment to 6\n",
      "providing information about pro-\n",
      "cess termination 498–500\n",
      "relocating 479–482\n",
      "expecting data written to disk \n",
      "to disappear 480\n",
      "expecting hostnames to \n",
      "change 480\n",
      "expecting local IP to \n",
      "change 480\n",
      "using volumes to preserve \n",
      "data across container \n",
      "restarts 480–482\n",
      "running 19–21, 497–502\n",
      "in VMs instead of \n",
      "containers 555\n",
      "keeping containers \n",
      "running 20–21\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 595, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "563\n",
      "applications (continued)\n",
      "locating containers 21\n",
      "outside of Kubernetes during \n",
      "development 502–503\n",
      "scaling number of copies 21\n",
      "through services using single \n",
      "YAML file 255\n",
      "shut-down procedures 496–497\n",
      "splitting into microservices 3–4\n",
      "tagging images 497–498\n",
      "updating declaratively using \n",
      "Deployment 261–278\n",
      "blocking rollouts of bad \n",
      "versions 274–278\n",
      "controlling rate of \n",
      "rollout 271–273\n",
      "creating Deployments\n",
      "262–264\n",
      "pausing rollout process\n",
      "273–274\n",
      "rolling back \n",
      "deployments 268–270\n",
      "updating Deployments\n",
      "264–268\n",
      "using imagePullPolicy 497–498\n",
      "using multi-dimensional labels \n",
      "vs. single-dimensional \n",
      "labels 498\n",
      "using pre-stop hooks when not \n",
      "receiving SIGTERM \n",
      "signal 488–489\n",
      "args array 196\n",
      "arguments\n",
      "defining in Docker 193–195\n",
      "CMD instruction 193\n",
      "ENTRYPOINT instruction\n",
      "193\n",
      "making INTERVAL configu-\n",
      "rable in fortune images\n",
      "194–195\n",
      "shell forms vs. exec \n",
      "forms 193–194\n",
      "overriding in Kubernetes\n",
      "195–196\n",
      "See also command-line argu-\n",
      "ments\n",
      "asterisks 117\n",
      "at-most-one semantics 290\n",
      "Attribute-based access control. See \n",
      "ABAC\n",
      "authenticating\n",
      "API servers 346–353\n",
      "groups 347–348\n",
      "service accounts 348, \n",
      "351–353\n",
      "users 347–348\n",
      "clients with authentication \n",
      "plugins 317\n",
      "creating Secrets for, with \n",
      "Docker registries 223\n",
      "with API servers 241–242\n",
      "authorization plugins, RBAC\n",
      "353–354\n",
      "authorizations\n",
      "clients with plugins for 317\n",
      "granting permissions 373\n",
      "service accounts tying into 349\n",
      "auto-deploying, resource \n",
      "manifests 504–505\n",
      "automatic scaling 23\n",
      "automating custom resources \n",
      "with custom controllers\n",
      "513–517\n",
      "running controllers as \n",
      "pods 515–516\n",
      "Website controllers 514–515\n",
      "Autoscaler, using to scale up \n",
      "Deployments 446–447\n",
      "autoscaling\n",
      "metrics appropriate for 450\n",
      "process of 438–441\n",
      "calculating required number \n",
      "of pods 439\n",
      "obtaining pod metrics\n",
      "438–439\n",
      "updating replica count on \n",
      "scaled resources 440\n",
      "See also horizontal autoscaling\n",
      "availability zones\n",
      "co-locating pods in same 471\n",
      "deploying pods in same\n",
      "471–472\n",
      "availability-zone label 467\n",
      "AWS (Amazon Web Services) 37, \n",
      "174, 454\n",
      "awsElasticBlockStore volume 162, \n",
      "174\n",
      "azureDisk volume 162, 174\n",
      "azureFile volume 174\n",
      "B\n",
      "backend services, connecting \n",
      "to 502\n",
      "backend-database 129–130\n",
      "base images 29\n",
      "bash process 33\n",
      "batch API groups, REST \n",
      "endpoints 236–237\n",
      "BestEffort class, assigning pods \n",
      "to 417\n",
      "binary data, using Secrets for 217\n",
      "binding\n",
      "Roles to service accounts\n",
      "359–360\n",
      "service instances 525\n",
      "to host ports without using \n",
      "host network namespaces\n",
      "377–379\n",
      "blocking rollouts 274–278\n",
      "configuring deadlines for \n",
      "rollouts 278\n",
      "defining readiness probes to \n",
      "prevent rollouts 275\n",
      "minReadySeconds 274–275\n",
      "preventing rollouts with readi-\n",
      "ness probes 277–278\n",
      "updating deployments with \n",
      "kubectl apply 276–277\n",
      "blue-green deployment 253\n",
      "Borg 16\n",
      "brokers. See service brokers\n",
      "BuildConfigs, building images \n",
      "from source using 529\n",
      "Burstable QoS class, assigning to \n",
      "pods 418\n",
      "busybox image 26, 112\n",
      "C\n",
      "-c option 66, 245\n",
      "CA (certificate authority) 170, \n",
      "240\n",
      "--cacert option 240\n",
      "cAdvisor 431\n",
      "canary release 69, 273\n",
      "capabilities\n",
      "adding to all containers 395\n",
      "dropping from containers\n",
      "385–386, 395\n",
      "kernel, adding to \n",
      "containers 384–385\n",
      "specifying which can be added \n",
      "to containers 394\n",
      "See also allowed capabilities\n",
      "capacity, of nodes 407–408\n",
      "CAP_CHOWN capability 385, 395\n",
      "CAP_SYS_TIME capability 384\n",
      "categorizing worker nodes with \n",
      "labels 74\n",
      "CentOS ISO image 541\n",
      "central processing units. See CPUs\n",
      "cephfs volume 163\n",
      "cert files, from Secrets 221\n",
      "certificate-authority-data field 536\n",
      "certificates, TLS 147–149\n",
      "CI/CD (Continuous Integration \n",
      "and Continuous \n",
      "Delivery) 506\n",
      "cinder volume 163\n",
      "claiming\n",
      "benefits of 182\n",
      "PersistentVolumes 179–181\n",
      "client binaries, downloading 39\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 596, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "564\n",
      "client libraries, communicating \n",
      "with API servers through\n",
      "246–248\n",
      "building libraries with \n",
      "OpenAPI 248\n",
      "building libraries with \n",
      "Swagger 248\n",
      "exploring API servers with \n",
      "Swagger UI 248\n",
      "interacting with FABRIC8 Java \n",
      "client 247–248\n",
      "using existing client \n",
      "libraries 246–247\n",
      "client-certificate-data \n",
      "property 536\n",
      "client-key-data property 536\n",
      "clients\n",
      "authenticating 317\n",
      "authorizing 317\n",
      "custom, creating 519\n",
      "handling requests 492–497\n",
      "non-preservation of IP 142\n",
      "notifying of resource \n",
      "changes 318–319\n",
      "pods, using newly created \n",
      "Secrets in 525–526\n",
      "preventing broken \n",
      "connections 492–497\n",
      "sequence of events at pod \n",
      "deletion 493–495\n",
      "troubleshooting 495–496\n",
      "See also external clients\n",
      "cloning VMs 545–547\n",
      "changing hostname on 546\n",
      "configuring name resolution for \n",
      "hosts 546–547\n",
      "shutting down 545\n",
      "Cloud infrastructure, requesting \n",
      "nodes from 452–453\n",
      "Cluster Autoscaler 452–453\n",
      "enabling 454\n",
      "relinquishing nodes 453\n",
      "requesting additional nodes \n",
      "from Cloud infrastructure\n",
      "452–453\n",
      "cluster events, observing 332–333\n",
      "Cluster Federation 556–559\n",
      "architecture of 557\n",
      "federated API objects 558–559\n",
      "overview 556–557\n",
      "cluster nodes\n",
      "displaying CPU usage for 431\n",
      "displaying memory usage \n",
      "for 431\n",
      "horizontal scaling of 452–456\n",
      "securing 375–403\n",
      "configuring container secu-\n",
      "rity contexts 380–389\n",
      "isolating pod networks\n",
      "399–402\n",
      "restricting use of security-\n",
      "related features 389–399\n",
      "using host node namespaces \n",
      "in pods 376–380\n",
      "clustered data stores 303–304\n",
      "cluster-internal services, connect-\n",
      "ing through API servers 299\n",
      "clusterIP field 154, 293\n",
      "ClusterIP service 45\n",
      "cluster-level resources, allowing \n",
      "access to 362–365\n",
      "ClusterRoleBindings\n",
      "combing with \n",
      "ClusterRoles 370–371\n",
      "combining with Role \n",
      "resources 370–371\n",
      "combining with \n",
      "RoleBindings 370–371\n",
      "default 371–373\n",
      "using 362–371\n",
      "allowing access to cluster-level \n",
      "resources 362–365\n",
      "allowing access to non-\n",
      "resource URLs 365–367\n",
      "ClusterRoles 362–371\n",
      "allowing access to cluster-level \n",
      "resources 362–365\n",
      "allowing access to non-resource \n",
      "URLs 365–367\n",
      "combining with ClusterRole-\n",
      "Bindings 370–371\n",
      "combining with \n",
      "RoleBindings 370–371\n",
      "combining with Roles 370–371\n",
      "default 371–373\n",
      "allowing modifying \n",
      "resources 372\n",
      "allowing read-only access to \n",
      "resources 372\n",
      "granting full control of \n",
      "namespaces 372\n",
      "using to grant access to \n",
      "resources in specific \n",
      "namespaces 367–370\n",
      "clusters\n",
      "adding 536\n",
      "combining Minikube with 504\n",
      "confirming communication \n",
      "with kubectl 38\n",
      "connecting to services living \n",
      "outside of 131–134\n",
      "creating alias for external \n",
      "services 134\n",
      "service endpoints 131–134\n",
      "creating with three nodes 39\n",
      "deleting 538\n",
      "enabling RBAC resources \n",
      "in 356\n",
      "etcd, running 342–343\n",
      "highly available, running\n",
      "341–345\n",
      "making applications highly \n",
      "available 341\n",
      "making Control Plane com-\n",
      "ponents highly \n",
      "available 342–345\n",
      "hosted, using with GKE 38–41\n",
      "in Docker 36–42\n",
      "running local single-node \n",
      "clusters with Minikube\n",
      "37–38\n",
      "setting up aliases for \n",
      "kubectl 41–42\n",
      "setting up command-line \n",
      "completion for \n",
      "kubectl 41–42\n",
      "in kubeconfig files 536\n",
      "Kubernetes, architecture of\n",
      "18–19\n",
      "limiting service disruption during \n",
      "scale-down of 454–456\n",
      "listing 538\n",
      "listing job instances in 237–238\n",
      "listing nodes 40\n",
      "listing services available in 523\n",
      "local single-node, running with \n",
      "Minikube 37–38\n",
      "modifying 536\n",
      "multi-node with kubeadm\n",
      "539–551\n",
      "configuring masters 547–549\n",
      "configuring worker \n",
      "nodes 549–550\n",
      "setting up operating \n",
      "systems 539\n",
      "setting up required \n",
      "packages 539\n",
      "overview of 39\n",
      "running Grafana in 433\n",
      "running InfluxDB in 433\n",
      "securing with RBAC 353–373\n",
      "binding roles to service \n",
      "accounts 359–360\n",
      "default ClusterRoleBindings\n",
      "371–373\n",
      "default ClusterRoles 371–373\n",
      "granting authorization \n",
      "permissions 373\n",
      "including service accounts \n",
      "from other \n",
      "namespaces 361\n",
      "RBAC authorization \n",
      "plugins 353–354\n",
      "RBAC resources 355–357\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 597, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "565\n",
      "clusters (continued)\n",
      "starting with Minikube 37–38\n",
      "testing services from within 124\n",
      "two-node, deploying pods \n",
      "in 468\n",
      "tying with user credentials 537\n",
      "using from local machines\n",
      "550–551\n",
      "using kubectl with \n",
      "multiple 534–538\n",
      "adding kube config \n",
      "entries 536–537\n",
      "configuring location of \n",
      "kubeconfig files 535\n",
      "contents of kubeconfig \n",
      "files 535–536\n",
      "deleting contexts 538\n",
      "listing contexts 538\n",
      "listing kube config \n",
      "entries 536–537\n",
      "modifying kube config \n",
      "entries 536–537\n",
      "switching between \n",
      "contexts 538\n",
      "switching between Minikube \n",
      "and GKE 534–535\n",
      "See also ClusterRoleBindings; \n",
      "ClusterRoles; RoleBind-\n",
      "ings; Roles; cluster nodes\n",
      "CMD instruction 193\n",
      "CNI (Container Network \n",
      "Interface) 335, 338, 549\n",
      "collecting resource usages\n",
      "430–432\n",
      "COMMAND column 334\n",
      "command-line arguments\n",
      "passing ConfigMap entries \n",
      "as 204–205\n",
      "passing to containers 192–196\n",
      "command-line completion, for \n",
      "kubectl 41–42\n",
      "commands\n",
      "defining in Docker 193–195\n",
      "CMD instruction 193\n",
      "ENTRYPOINT instruction\n",
      "193\n",
      "making INTERVAL configu-\n",
      "rable in fortune images\n",
      "194–195\n",
      "shell forms vs. exec forms\n",
      "193–194\n",
      "overriding in Kubernetes\n",
      "195–196\n",
      "remotely executing, in running \n",
      "containers 124–126\n",
      "communication\n",
      "between pods on different \n",
      "nodes 337–338\n",
      "between pods on same \n",
      "node 336–337\n",
      "of components 311\n",
      "with API servers 352–353\n",
      "with pods through API \n",
      "servers 295–297\n",
      "completions property 114\n",
      "components\n",
      "add-ons 310\n",
      "communicating 311\n",
      "involved in controllers 330\n",
      "isolating with Linux container \n",
      "technologies 8\n",
      "of Control Plane 310\n",
      "making highly available\n",
      "342–345\n",
      "using leader-election in 344\n",
      "of Kubernetes 310–312\n",
      "running 311–312\n",
      "running with kubeadm 548–549\n",
      "ComponentStatus 311\n",
      "computational resources 404–436\n",
      "limiting resources available to \n",
      "containers 412–416\n",
      "exceeding limits 414–415\n",
      "limits as seen by applications \n",
      "in containers 415–416\n",
      "setting hard limits for \n",
      "resources containers can \n",
      "use 412–413\n",
      "limiting total resources available \n",
      "in namespaces 425–429\n",
      "limiting objects that can be \n",
      "created 427–428\n",
      "ResourceQuota \n",
      "resources 425–427\n",
      "specifying quotas for per-\n",
      "sistent storage 427\n",
      "specifying quotas for specific \n",
      "pod states 429\n",
      "specifying quotas for specific \n",
      "QoS classes 429\n",
      "monitoring pod resource \n",
      "usage 430–434\n",
      "analyzing historical resource \n",
      "consumption \n",
      "statistics 432–434\n",
      "collecting resource \n",
      "usages 430–432\n",
      "retrieving resource \n",
      "usages 430–432\n",
      "storing historical resource \n",
      "consumption \n",
      "statistics 432–434\n",
      "pod QoS classes 417–421\n",
      "defining QoS classes for \n",
      "pods 417–419\n",
      "killing processes 420–421\n",
      "requesting resources for pod \n",
      "containers 405–412\n",
      "creating pods with resource \n",
      "requests 405–406\n",
      "defining custom resources\n",
      "411–412\n",
      "effect of CPU requests on \n",
      "CPU time sharing 411\n",
      "effect of resource requests on \n",
      "scheduling 406–410\n",
      "requesting custom \n",
      "resources 411–412\n",
      "setting default limits for pods \n",
      "per namespace 421–425\n",
      "applying default resource \n",
      "limits 424–425\n",
      "creating LimitRange \n",
      "objects 422–423\n",
      "enforcing limits 423–424\n",
      "LimitRange resources\n",
      "421–422\n",
      "conditions, using multiple in label \n",
      "selectors 72\n",
      "config, updating without restart-\n",
      "ing application 211–213\n",
      "editing ConfigMap 212\n",
      "signaling Nginx to reload \n",
      "config 212\n",
      "updating ConfigMap 213\n",
      "updating files automatically\n",
      "212–213\n",
      "config-file.conf file 201\n",
      "CONFIG_FOO-BAR variable 204\n",
      "configMap volume\n",
      "examining mounted \n",
      "contents 209\n",
      "exposing ConfigMap entries as \n",
      "files 205–211\n",
      "setting file permissions for files \n",
      "in 211\n",
      "ConfigMaps\n",
      "creating 199–201, 206–207\n",
      "combining options 201\n",
      "entries from contents of \n",
      "files 201\n",
      "from files in directories 201\n",
      "using kubectl create \n",
      "ConfigMap command\n",
      "200–201\n",
      "decoupling configurations \n",
      "with 198–213\n",
      "editing 212\n",
      "exposing entries as files 205–211\n",
      "examining mounted config-\n",
      "Map volume contents\n",
      "209\n",
      "mounting directory hiding \n",
      "existing files 210\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 598, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "566\n",
      "ConfigMaps (continued)\n",
      "setting file permissions for \n",
      "files 211\n",
      "verifying Nginx uses mounted \n",
      "config file 208\n",
      "exposing entries in volume\n",
      "209–210\n",
      "mounting entries as files\n",
      "210–211\n",
      "non-existing, referencing in \n",
      "pods 203\n",
      "overview 198\n",
      "passing entries 202–205\n",
      "updating 213\n",
      "using entries in volume 207–208\n",
      "versus Secrets 217–218\n",
      "configurations, decoupling with \n",
      "ConfigMap\n",
      "creating ConfigMaps 200–201\n",
      "passing ConfigMap \n",
      "entries 202–205\n",
      "updating application config \n",
      "without restarting \n",
      "application 211–213\n",
      "using configMap volume to \n",
      "expose ConfigMap entries \n",
      "as files 205–211\n",
      "configuring\n",
      "allowed capabilities 394–395\n",
      "adding capabilities to all \n",
      "containers 395\n",
      "dropping capabilities from \n",
      "containers 395\n",
      "specifying which capabilities \n",
      "can be added to \n",
      "containers 394\n",
      "container security \n",
      "contexts 380–389\n",
      "adding individual kernel \n",
      "capabilities to \n",
      "containers 384–385\n",
      "dropping capabilities from \n",
      "containers 385–386\n",
      "preventing containers from \n",
      "running as root 382\n",
      "preventing processes from \n",
      "writing to container \n",
      "filesystems 386–387\n",
      "running containers as specific \n",
      "user 381–382\n",
      "running pods in privileged \n",
      "mode 382–384\n",
      "running pods without specify-\n",
      "ing security contexts\n",
      "381\n",
      "sharing volumes when con-\n",
      "tainers run as different \n",
      "users 387–389\n",
      "containerized applications\n",
      "191–192\n",
      "deadlines for rollouts 278\n",
      "default capabilities 394–395\n",
      "adding capabilities to all \n",
      "containers 395\n",
      "dropping capabilities from \n",
      "containers 395\n",
      "specifying which capabilities \n",
      "can be added to \n",
      "containers 394\n",
      "disallowed capabilities 394–395\n",
      "adding capabilities to all \n",
      "containers 395\n",
      "dropping capabilities from \n",
      "containers 395\n",
      "specifying which capabilities \n",
      "can be added to \n",
      "containers 394\n",
      "host in Ingress pointing to \n",
      "Ingress IP address 145\n",
      "Ingress, to handle TLS \n",
      "traffic 147–149\n",
      "INTERVAL in fortune \n",
      "images 194–195\n",
      "Job templates 117\n",
      "Kubernetes to use rkt 552\n",
      "location of kubeconfig files 535\n",
      "masters with kubeadm 547–549\n",
      "running components with \n",
      "kubeadm 548–549\n",
      "running kubeadm init to ini-\n",
      "tialize masters 547–548\n",
      "name resolution for hosts\n",
      "546–547\n",
      "network adapters for VMs 540\n",
      "pod rescheduling after node \n",
      "failures 462\n",
      "properties of liveness \n",
      "probes 88–89\n",
      "resource requests \n",
      "automatically 451\n",
      "schedule 117\n",
      "service endpoints \n",
      "manually 132–134\n",
      "creating endpoints resource \n",
      "for services without \n",
      "selectors 133–134\n",
      "creating services without \n",
      "selectors 132–133\n",
      "session affinity on services 126\n",
      "tab completion for kubectl 41–42\n",
      "worker nodes with kubeadm\n",
      "549–550\n",
      "connections\n",
      "external 141–142\n",
      "preventing breakage when pods \n",
      "shut down 493–497\n",
      "sequence of events at pod \n",
      "deletion 493–495\n",
      "troubleshooting 495–496\n",
      "preventing breakage when pods \n",
      "start up 492–493\n",
      "signaling when pods ready to \n",
      "accept 149–153\n",
      "to API servers 503\n",
      "to backend services 502\n",
      "to services living outside \n",
      "cluster 131–134\n",
      "creating alias for external \n",
      "services 134\n",
      "service endpoints 131–134\n",
      "to services through FQDN 130\n",
      "to services through load \n",
      "balancers 139–141\n",
      "container images\n",
      "building 29–32\n",
      "image layers 30–31\n",
      "overview 30\n",
      "with Dockerfile vs. \n",
      "manually 31–32\n",
      "creating Dockerfile for 29\n",
      "creating Node.js \n",
      "applications 28–29\n",
      "in Docker container \n",
      "platform 15\n",
      "pushing image to image \n",
      "registry 35–36\n",
      "pushing images to Docker \n",
      "Hub 36\n",
      "running images on different \n",
      "machines 36\n",
      "tagging images under addi-\n",
      "tional tags 35\n",
      "removing containers 34–35\n",
      "running 32–33\n",
      "accessing applications 32\n",
      "listing all running \n",
      "containers 32\n",
      "obtaining information about \n",
      "containers 33\n",
      "stopping containers 34–35\n",
      "versioning 28\n",
      "viewing environment of run-\n",
      "ning containers 33–34\n",
      "isolating filesystems 34\n",
      "processes running in host \n",
      "operating system 34\n",
      "running shells inside \n",
      "existing 33\n",
      "with out-of-range user IDs 393\n",
      "Container Network Interface. See \n",
      "CNI\n",
      "container networking interface. \n",
      "See Kubernetes-CNI\n",
      "container ports, specifying 63–65\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 599, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "567\n",
      "container runtimes 552–555\n",
      "replacing Docker with rkt\n",
      "552–555\n",
      "configuring Kubernetes to \n",
      "use rkt 552\n",
      "using rkt with Minikube\n",
      "553–555\n",
      "using through CRI 555\n",
      "CRI-O Container Runtime\n",
      "555\n",
      "running applications in VMs \n",
      "instead of \n",
      "containers 555\n",
      "CONTAINER_CPU_REQUEST\n",
      "_MILLICORES variable 228\n",
      "ContainerCreating 486\n",
      "CONTAINER_MEMORY_LIMIT\n",
      "_KIBIBYTES variable 228\n",
      "--container-runtime=rkt option\n",
      "552–553\n",
      "containers 7–16\n",
      "adding capabilities to all 395\n",
      "adding individual kernel capa-\n",
      "bilities to 384–385\n",
      "comparing VMs to 8–10\n",
      "configuring security \n",
      "contexts 380–389\n",
      "preventing containers from \n",
      "running as root 382\n",
      "running containers as specific \n",
      "user 381–382\n",
      "running pods in privileged \n",
      "mode 382–384\n",
      "running pods without \n",
      "specifying security \n",
      "contexts 381\n",
      "copying files to and from\n",
      "500–501\n",
      "copying logs to and from\n",
      "500–501\n",
      "determining QoS class of 418\n",
      "Docker container platform\n",
      "12–15\n",
      "building images 13\n",
      "comparing VMs to 14–15\n",
      "concepts 12–13\n",
      "distributing images 13\n",
      "image layers 15\n",
      "portability limitations of con-\n",
      "tainer images 15\n",
      "running images 13\n",
      "dropping capabilities \n",
      "from 385–386, 395\n",
      "existing, running shells inside 33\n",
      "exploring 33\n",
      "images\n",
      "creating 290–291\n",
      "listing 554–555\n",
      "Init containers, adding to \n",
      "pods 484–485\n",
      "instead of VMs, running appli-\n",
      "cations in 555\n",
      "isolating filesystems 34\n",
      "limiting resource available \n",
      "to 412–416\n",
      "limits as seen by applications \n",
      "in 415–416\n",
      "Linux, isolating components \n",
      "with 8\n",
      "listing all 32\n",
      "locating 21\n",
      "making images 497\n",
      "mechanisms for isolation 11\n",
      "mounting local files into 503\n",
      "multiple vs. one with multiple \n",
      "processes 56–57\n",
      "obtaining information about 33\n",
      "of pods\n",
      "requesting resources \n",
      "for 405–412\n",
      "resources requests for\n",
      "411–412\n",
      "running with kubelet 332\n",
      "organizing across pods 58–60\n",
      "splitting into multiple pods \n",
      "for scaling 59\n",
      "splitting multi-tier applica-\n",
      "tions into multiple \n",
      "pods 59\n",
      "overview 8–12\n",
      "isolating processes with Linux \n",
      "Namespaces 11\n",
      "limiting resources available to \n",
      "process 11–12\n",
      "partial isolation between 57\n",
      "passing command-line argu-\n",
      "ments to 192–196\n",
      "defining arguments in \n",
      "Docker 193–195\n",
      "defining command in \n",
      "Docker 193–195\n",
      "overriding arguments in \n",
      "Kubernetes 195–196\n",
      "overriding command in \n",
      "Kubernetes 195–196\n",
      "running fortune pods with \n",
      "custom interval 195–196\n",
      "passing ConfigMap entries \n",
      "to 202–203\n",
      "pods with multiple, determin-\n",
      "ing QoS classes of 419\n",
      "post-start, using lifecycle \n",
      "hooks 486–487\n",
      "pre-stop\n",
      "using lifecycle hooks\n",
      "487–488\n",
      "using lifecycle hooks when \n",
      "application not receiving \n",
      "SIGTERM signal\n",
      "488–489\n",
      "preventing processes from writ-\n",
      "ing to filesystems 386–387\n",
      "processes running in host oper-\n",
      "ating system 34\n",
      "remotely executing commands \n",
      "in 124–126\n",
      "removing 34–35\n",
      "rkt container platform, as alter-\n",
      "native to Docker 15–16\n",
      "running 20–21\n",
      "effect of application descrip-\n",
      "tions on 19–20\n",
      "inspecting in Minikube \n",
      "VM 553–554\n",
      "viewing environment of 33–34\n",
      "running applications inside \n",
      "during development 503\n",
      "running shells in 130–131\n",
      "seeing all node CPU cores 416\n",
      "seeing node memory 415–416\n",
      "setting environment variables \n",
      "for 196–198\n",
      "configuring INTERVAL in \n",
      "fortune images through \n",
      "environment variables\n",
      "197\n",
      "disadvantages of hardcoding \n",
      "environment \n",
      "variables 198\n",
      "referring to environment \n",
      "variables in variable \n",
      "values 198\n",
      "setting hard limits for resources \n",
      "used by 412–413\n",
      "creating pods with resource \n",
      "limits 412–413\n",
      "overcommitting limits 413\n",
      "setting up networks 550\n",
      "sharing data between with \n",
      "volumes 163–169\n",
      "using emptyDir volume\n",
      "163–166\n",
      "using Git repository as \n",
      "starting point for \n",
      "volume 166–169\n",
      "sharing IP 57\n",
      "sharing port space 57\n",
      "sharing volumes when running \n",
      "as different users 387–389\n",
      "specifying environment vari-\n",
      "ables in definitions 197\n",
      "specifying name when retriev-\n",
      "ing logs of multi-container \n",
      "pods 66\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 600, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "568\n",
      "containers (continued)\n",
      "specifying which capabilities \n",
      "can be added to 394\n",
      "stopping 34–35\n",
      "targeting 489\n",
      "using Secrets to pass sensitive \n",
      "data to 213–223\n",
      "ConfigMaps versus Secrets\n",
      "217–218\n",
      "creating Secrets 216\n",
      "default token Secrets 214–215\n",
      "image pull Secrets 222–223\n",
      "overview of Secrets 214\n",
      "using Secrets in pods\n",
      "218–222\n",
      "using volumes to preserve data \n",
      "across restarts 480–482\n",
      "when to use multiple in \n",
      "pod 59–60\n",
      "with same QoS classes, \n",
      "handling 420–421\n",
      "See also container images; side-\n",
      "car containers\n",
      "--containers option 432\n",
      "containers, Docker-based 13\n",
      "ContentAgent container 162\n",
      "contexts\n",
      "current, in kubeconfig files 536\n",
      "deleting 538\n",
      "in kubeconfig files 536\n",
      "listing 538\n",
      "switching between 538\n",
      "using kubectl with 537–538\n",
      "See also security contexts\n",
      "continuous delivery 6–7\n",
      "benefits of 7\n",
      "role of developers in 7\n",
      "role of sysadmins in 7\n",
      "Continuous Integration and Con-\n",
      "tinuous Delivery. See CI/CD\n",
      "Control Plane 18–19\n",
      "components of 310, 344\n",
      "making components highly \n",
      "available 342–345\n",
      "ensuring high availability of \n",
      "controllers 343–344\n",
      "ensuring high availability of \n",
      "Scheduler 343–344\n",
      "running etcd clusters 342–343\n",
      "running multiple instances of \n",
      "API servers 343\n",
      "using leader-election in Con-\n",
      "trol Plane components\n",
      "344\n",
      "Controller Manager, Control \n",
      "Plane component 19\n",
      "controllers 145, 321–326, 330–333\n",
      "chain of events 331–332\n",
      "Deployment controller cre-\n",
      "ates ReplicaSet 331\n",
      "kubelet runs pod \n",
      "containers 332\n",
      "ReplicaSet controller creates \n",
      "pods 332\n",
      "Scheduler assigns node to \n",
      "newly created pods 332\n",
      "components involved in 330\n",
      "custom, automating custom \n",
      "resources with 513–517\n",
      "DaemonSet 324\n",
      "Deployment controllers 324\n",
      "Endpoints controllers 325\n",
      "ensuring high availability \n",
      "of 343–344\n",
      "Job controllers 324\n",
      "Namespace controllers 325\n",
      "Node controllers 324\n",
      "observing cluster events 332–333\n",
      "overview 322, 326\n",
      "PersistentVolume \n",
      "controllers 325–326\n",
      "removing pods from 100\n",
      "ReplicaSet 324\n",
      "replication managers 323–324\n",
      "running as pods 515–516\n",
      "Service controllers 324\n",
      "StatefulSet controllers 324\n",
      "Website 514–515\n",
      "copies, scaling number of 21\n",
      "copying\n",
      "files to and from containers\n",
      "500–501\n",
      "images to Minikube VM \n",
      "directly 504\n",
      "logs to and from \n",
      "containers 500–501\n",
      "CPUs (central processing units)\n",
      "creating Horizontal pod Auto-\n",
      "scaler based on 442–443\n",
      "creating ResourceQuota \n",
      "resources for 425–426\n",
      "displaying usage\n",
      "for cluster nodes 431\n",
      "for pods 431–432\n",
      "nodes, seen by containers 416\n",
      "scaling based on 441–447\n",
      "automatic rescale \n",
      "events 444–445\n",
      "creating Horizontal pod \n",
      "Autoscaler based on \n",
      "CPU usage 442–443\n",
      "maximum rate of scaling 447\n",
      "modifying target metric val-\n",
      "ues on existing HPA \n",
      "objects 447\n",
      "triggering scale-ups 445–446\n",
      "using Autoscaler to scale up \n",
      "Deployments 446–447\n",
      "time sharing 411\n",
      "CrashLoopBackOff 414, 499\n",
      "CRD (CustomResource-\n",
      "Definitions) 509–513\n",
      "creating CRD objects 510–511\n",
      "creating instances of custom \n",
      "resources 511–512\n",
      "deleting instances of custom \n",
      "objects 512–513\n",
      "examples of 509–510\n",
      "retrieving instances of custom \n",
      "resources 512\n",
      "creation_method label 72\n",
      "credentials. See user credentials\n",
      "CRI (Container Runtime Inter-\n",
      "face), using container run-\n",
      "times through 555\n",
      "CronJob resources, creating\n",
      "116–117\n",
      "CRUD (Create, Read, Update, \n",
      "Delete) 316\n",
      "CSR (CertificateSigning-\n",
      "Request) 148\n",
      "curl command 124–126, 130–131, \n",
      "167\n",
      "curl pods, running with additional \n",
      "ambassador containers\n",
      "244–245\n",
      "CURL_CA_BUNDLE variable 241\n",
      "CURRENT column 49\n",
      "custom intervals, running fortune \n",
      "pods with 195–196\n",
      "custom-namespace.yaml file 78\n",
      "D\n",
      "-d flag 32\n",
      "DaemonSets 324, 328, 550, 559\n",
      "creating 111\n",
      "creating YAML definition 110\n",
      "examples of 109\n",
      "running one pod on each node \n",
      "with 108–112\n",
      "running pods on certain nodes \n",
      "with 109–112\n",
      "adding required label to \n",
      "nodes 111\n",
      "removing required label from \n",
      "nodes 111–112\n",
      "running pods on every node \n",
      "with 109\n",
      "dashboard 52–53\n",
      "accessing when running in man-\n",
      "aged GKE 52\n",
      "accessing when using \n",
      "Minikube 53\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 601, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "569\n",
      "data\n",
      "binary, using Secrets for 217\n",
      "passing to containers using \n",
      "Secrets 213–223\n",
      "ConfigMaps versus \n",
      "Secrets 217–218\n",
      "creating Secrets 216\n",
      "default token Secrets\n",
      "214–215\n",
      "image pull Secrets 222–223\n",
      "overview of Secrets 214\n",
      "using Secrets in pods\n",
      "218–222\n",
      "persisted by previous pod\n",
      "173–174\n",
      "using volumes to preserve \n",
      "across container \n",
      "restarts 480–482\n",
      "writing to persistent storage by \n",
      "adding documents to Mon-\n",
      "goDB database 173\n",
      "written to disks, \n",
      "disappearing 480\n",
      "data stores, clustered 303–304\n",
      "deadlines, configuring for \n",
      "rollouts 278\n",
      "declarative scaling 103\n",
      "decoupling\n",
      "configurations with \n",
      "ConfigMap 198–213\n",
      "ConfigMaps 198–199\n",
      "creating ConfigMaps\n",
      "200–201\n",
      "passing all entries of Config-\n",
      "Map as environment \n",
      "variables at once 204\n",
      "passing ConfigMap entries as \n",
      "command-line \n",
      "arguments 204–205\n",
      "passing ConfigMap entries to \n",
      "containers as environ-\n",
      "ment variables 202–203\n",
      "updating application config \n",
      "without restarting \n",
      "application 211, 213\n",
      "using configMap volume to \n",
      "expose ConfigMap \n",
      "entries as files 205, 211\n",
      "pods from underlying storage \n",
      "technologies 176–184\n",
      "benefits of using claims 182\n",
      "PersistentVolumeClaims\n",
      "176–177, 179–181\n",
      "PersistentVolumes 176–184\n",
      "default capabilities, \n",
      "configuring 394–395, 403\n",
      "adding capabilities to all \n",
      "containers 395\n",
      "dropping capabilities from \n",
      "container 395\n",
      "specifying which capabilities \n",
      "can be added to \n",
      "container 394\n",
      "default policy 397\n",
      "default token Secret 214–215\n",
      "defaultAddCapabilities 394\n",
      "defaultMode property 232\n",
      "Deis Helm package manager\n",
      "530–533\n",
      "DELETED watch event 515\n",
      "deleting\n",
      "clusters 538\n",
      "contexts 538\n",
      "instances of custom \n",
      "objects 512–513\n",
      "PersistentVolumeClaims 288\n",
      "Pet pods 297–298\n",
      "pods 80–82, 252–253, 306\n",
      "by deleting whole \n",
      "namespaces 80–81\n",
      "by name 80\n",
      "forcibly 307\n",
      "in namespace while keeping \n",
      "namespace 81–82\n",
      "manually 306–307\n",
      "sequence of events 493–495\n",
      "using label selectors 80\n",
      "pods marked for 307\n",
      "ReplicationControllers 103–104\n",
      "resources in namespace 82\n",
      "deletionTimestamp field 489, 495\n",
      "delivery. See continuous delivery\n",
      "dependencies\n",
      "inter-pod 485\n",
      "overview 5\n",
      "deploying\n",
      "add-ons 328–329\n",
      "applications through \n",
      "StatefulSets 291–295\n",
      "creating governing \n",
      "services 292–294\n",
      "creating persistent \n",
      "volumes 291–292\n",
      "creating StatefulSets 294\n",
      "examining PersistentVolume-\n",
      "Claims 295\n",
      "examining stateful pods\n",
      "294–295\n",
      "applications, simplifying 21–22\n",
      "microservices 5\n",
      "newly built images automatically \n",
      "with DeploymentConfigs\n",
      "529–530\n",
      "Node.js applications 42–44\n",
      "behind the scenes 44\n",
      "listing pods 43–44\n",
      "pods\n",
      "in same availability \n",
      "zone 471–472\n",
      "co-locating pods in same \n",
      "availability zone 471\n",
      "topologyKey 471–472\n",
      "in same geographical \n",
      "regions 471–472\n",
      "co-locating pods in \n",
      "same geographical \n",
      "region 471\n",
      "topologyKey 471–472\n",
      "in same rack 471–472\n",
      "in two-node clusters 468\n",
      "on same nodes, with \n",
      "inter-pod affinity\n",
      "468–471\n",
      "with container images \n",
      "with out-of-range user \n",
      "IDs 393\n",
      "with pod affinity 470\n",
      "with runAsUser outside of \n",
      "policy ranges 393\n",
      "privileged containers, creating \n",
      "PodSecurityPolicy to \n",
      "allow 396–397\n",
      "resources through Helm\n",
      "531–533\n",
      "versions 269\n",
      "Deployment controllers\n",
      "creating ReplicaSet controller \n",
      "with 331\n",
      "overview 324\n",
      "Deployment resource\n",
      "benefits of 267–268\n",
      "creating 262–264\n",
      "manifest, creating 262\n",
      "rolling back to specific \n",
      "revision 270\n",
      "strategies 264–265\n",
      "updating 264–268\n",
      "slowing down rolling \n",
      "updates 265\n",
      "triggering rolling \n",
      "updates 265–267\n",
      "using for updating applications \n",
      "declaratively 261–278\n",
      "blocking rollouts of bad \n",
      "versions 274–278\n",
      "controlling rate of \n",
      "rollout 271–273\n",
      "pausing rollout process\n",
      "273–274\n",
      "rolling back \n",
      "deployments 268–270\n",
      "DeploymentConfigs, deploying \n",
      "newly built images automati-\n",
      "cally with 529–530\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 602, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "570\n",
      "deployments 250–279\n",
      "displaying rollout history of 270\n",
      "performing automatic rolling \n",
      "updates 254–261\n",
      "obsolescence of kubectl roll-\n",
      "ing-update 260–261\n",
      "performing rolling updates \n",
      "with kubectl 256–260\n",
      "running initial version of \n",
      "applications 254–255\n",
      "rolling back 268–270\n",
      "creating application \n",
      "versions 268\n",
      "deploying versions 269\n",
      "rolling back to specific Deploy-\n",
      "ment revision 270\n",
      "undoing rollouts 269\n",
      "rollouts, displaying status of 263\n",
      "updating applications running \n",
      "in pods 251–253\n",
      "deleting old pods 252–253\n",
      "replacing old pods 252\n",
      "spinning up new pods\n",
      "252–253\n",
      "updating with kubectl \n",
      "apply 276–277\n",
      "using anti-affinity with pods of \n",
      "same 475\n",
      "using Autoscaler to scale \n",
      "up 446–447\n",
      "deprovisioning, instances 526\n",
      "describe command 409\n",
      "descriptors\n",
      "JSON, creating pods from 61–67\n",
      "YAML\n",
      "creating for pods 63–65\n",
      "creating pods from 61–67\n",
      "of existing pods 61–63\n",
      "DESIRED column 49\n",
      "developers, role in continuous \n",
      "delivery 7\n",
      "DevOps 7\n",
      "directories\n",
      "mounting hides existing \n",
      "files 210\n",
      "using multiple in same \n",
      "volume 282\n",
      "directory traversal attack 353\n",
      "disabling, firewalls 544\n",
      "disallowed capabilities, \n",
      "configuring 394–395\n",
      "adding capabilities to all \n",
      "containers 395\n",
      "dropping capabilities from \n",
      "container 395\n",
      "specifying which capabilities \n",
      "can be added to \n",
      "container 394\n",
      "disk=ssd label 110\n",
      "disks, data written to \n",
      "disappearing 480\n",
      "disruptions, of services 454–456\n",
      "distributing Docker images 13\n",
      "DNS (Domain Name System) 300\n",
      "discovering pods through\n",
      "155–156\n",
      "discovering services through 129\n",
      "implementing peer discovery \n",
      "through 301–302\n",
      "records returned for headless \n",
      "service 155–156\n",
      "servers 329\n",
      "dnsPolicy property 129\n",
      "Docker container platform 25–54\n",
      "clusters in 36–42\n",
      "running local single-node clus-\n",
      "ters with Minikube 37–38\n",
      "setting up aliases for \n",
      "kubectl 41–42\n",
      "setting up command-line \n",
      "completion for \n",
      "kubectl 41–42\n",
      "using hosted clusters with \n",
      "GKE 38–41\n",
      "comparing VMs to 14–15\n",
      "concepts 12–13\n",
      "container images 26–36\n",
      "building 29–32\n",
      "creating Dockerfile for 29\n",
      "creating Node.js \n",
      "applications 28–29\n",
      "pushing images to image \n",
      "registry 35–36\n",
      "removing containers 34–35\n",
      "running 32–33\n",
      "stopping containers 34–35\n",
      "viewing environment of run-\n",
      "ning containers 33–34\n",
      "defining arguments in 193–195\n",
      "CMD instruction 193\n",
      "ENTRYPOINT \n",
      "instruction 193\n",
      "making INTERVAL configu-\n",
      "rable in fortune \n",
      "images 194–195\n",
      "shell forms vs. exec \n",
      "forms 193–194\n",
      "defining commands in 193–195\n",
      "CMD instruction 193\n",
      "ENTRYPOINT instruction\n",
      "193\n",
      "making INTERVAL configu-\n",
      "rable in fortune \n",
      "images 194–195\n",
      "shell forms vs. exec \n",
      "forms 193–194\n",
      "images\n",
      "building 13\n",
      "distributing 13\n",
      "layers 15\n",
      "portability limitations of 15\n",
      "running 13\n",
      "installing 26–28\n",
      "registries, creating Secrets for \n",
      "authenticating with 223\n",
      "rkt container platform as alter-\n",
      "native to 15–16\n",
      "running first application on \n",
      "Kubernetes\n",
      "accessing web applications\n",
      "45–47\n",
      "deploying Node.js \n",
      "applications 42–44\n",
      "examining which nodes \n",
      "application is running \n",
      "on 51–52\n",
      "horizontally scaling \n",
      "applications 48–50\n",
      "logical parts of systems 47–48\n",
      "using Kubernetes \n",
      "dashboard 52–53\n",
      "running Hello World \n",
      "container 26–28\n",
      "behind the scenes 27\n",
      "running other images 27\n",
      "versioning container \n",
      "images 28\n",
      "Docker Hub\n",
      "pushing images to 36\n",
      "using private image repositories \n",
      "on 222\n",
      "Docker Hub ID 35\n",
      "docker images command 35\n",
      "Docker platform\n",
      "replacing with rkt 552–555\n",
      "configuring Kubernetes to \n",
      "use rkt 552\n",
      "using rkt with Minikube\n",
      "553–555\n",
      "using daemon inside Minikube \n",
      "VM 503–504\n",
      "docker ps command 548\n",
      "docker pull command 44\n",
      "Docker Registry 13\n",
      "docker run command 26\n",
      "Docker tool, installing 544–545\n",
      "disabling firewalls 544\n",
      "disabling SELinux 544\n",
      "enabling net.bridge.bridge-nf-\n",
      "call-iptables Kernel \n",
      "options 545\n",
      "Dockerfile\n",
      "building images with 31–32\n",
      "creating for container images 29\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 603, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "571\n",
      "DOCKER_HOST variable 32, 504\n",
      "Docker-registry Secrets, using \n",
      "in pod definitions 223\n",
      "documents, writing data to \n",
      "persistent storage by \n",
      "adding to MongoDB \n",
      "database 173\n",
      "DoesNotExist operator 107\n",
      "Domain Name System. See DNS\n",
      "double dash character 125\n",
      "downloading client binaries 39\n",
      "downtime, reducing by \n",
      "running multiple \n",
      "instances 341\n",
      "Downward API\n",
      "benefits of using 233\n",
      "passing metadata through\n",
      "226–233\n",
      "available metadata 226–227\n",
      "exposing metadata through \n",
      "environment \n",
      "variables 227–230\n",
      "passing metadata through \n",
      "files in downwardAPI \n",
      "volume 230–233\n",
      "downwardAPI volume, passing \n",
      "metadata through files \n",
      "in 163, 230–233\n",
      "benefits of using Downward \n",
      "API 233\n",
      "referring to container-level \n",
      "metadata in volume \n",
      "specification 233\n",
      "updating annotations 232\n",
      "updating labels 232\n",
      "downwardAPI.items attribute 231\n",
      "DSL (Domain-Specific-\n",
      "Language) 248\n",
      "dynamic provisioning\n",
      "of PersistentVolumes\n",
      "184–189\n",
      "defining available storage \n",
      "types through Storage-\n",
      "Class resources 185\n",
      "requesting storage class in \n",
      "PersistentVolume-\n",
      "Claims 185–187\n",
      "without specifying storage \n",
      "class 187–189\n",
      "without specifying storage class\n",
      "creating PersistentVolume-\n",
      "Claims 188–189\n",
      "examining default storage \n",
      "classes 188\n",
      "listing storage classes 187–188\n",
      "PersistentVolumeClaims \n",
      "bound to pre-provisioned \n",
      "Persistent-Volumes 189\n",
      "E\n",
      "echo command 27\n",
      "edit ClusterRole, allowing modifi-\n",
      "cation of resources with 372\n",
      "editing\n",
      "ConfigMap 212\n",
      "definitions, scaling Replication-\n",
      "Controller by 102–103\n",
      "EDITOR variable 102\n",
      "EFK Stack 501\n",
      "elastic block store volume, AWS 174\n",
      "ELK Stack 501\n",
      "emptyDir volume 162–166\n",
      "creating pods 164–165\n",
      "seeing pods in action 165\n",
      "specifying medium for 166\n",
      "using in pods 163–164\n",
      "--enable-swagger-ui=true \n",
      "option 248\n",
      "Endpoints controllers 325\n",
      "endpoints resources, creating for \n",
      "services without selectors\n",
      "133–134\n",
      "ENTRYPOINT instruction 193\n",
      "env command 128\n",
      "env label 72\n",
      "env=debug label 70\n",
      "env=devel label 105\n",
      "env=prod label 70\n",
      "env=production label 105\n",
      "envFrom attribute 204\n",
      "environment variables\n",
      "configuring INTERVAL in \n",
      "fortune images through\n",
      "197–198\n",
      "disadvantages of \n",
      "hardcoding 198\n",
      "discovering services \n",
      "through 128–129\n",
      "exposing metadata \n",
      "through 227–230\n",
      "exposing Secrets entries \n",
      "through 221–222\n",
      "passing all ConfigMap entries \n",
      "as 204\n",
      "passing ConfigMap entries to \n",
      "containers as 202–203\n",
      "referring to in variable \n",
      "values 198\n",
      "setting for containers 196, 198\n",
      "specifying in container \n",
      "definitions 197\n",
      "ephemeral pods 121\n",
      "etcd cluster 518\n",
      "etcd stores 312–316\n",
      "ensuring consistency of stored \n",
      "objects 314–315\n",
      "ensuring consistency when \n",
      "clustered 315\n",
      "ensuring validity of stored \n",
      "objects 314–315\n",
      "number of instances 316\n",
      "running clusters 342–343\n",
      "storing resources in 313–314\n",
      "etcd, Control Plane component 19\n",
      "exec forms, versus shell \n",
      "forms 193–194\n",
      "Exec probe 86, 90, 150\n",
      "Exists operator 107, 461\n",
      "explain command 175\n",
      "exposing\n",
      "multiple ports in same \n",
      "service 126–127\n",
      "multiple services through same \n",
      "Ingress 146–147\n",
      "mapping different services to \n",
      "different hosts 147\n",
      "mapping different services to \n",
      "different paths of same \n",
      "host 146\n",
      "services externally through \n",
      "Ingress resources 142–149\n",
      "benefits of using 142–143\n",
      "configuring Ingress to handle \n",
      "TLS traffic 147–149\n",
      "creating Ingress \n",
      "resources 144\n",
      "using Ingress controllers\n",
      "143–144\n",
      "services externally using \n",
      "Routes 530\n",
      "services to external clients\n",
      "134–142\n",
      "external connections 141–142\n",
      "through external load \n",
      "balancers 138–141\n",
      "using NodePort \n",
      "services 135–138\n",
      "external clients\n",
      "allowing NodePort services \n",
      "access to 137–138\n",
      "exposing services to 134–142\n",
      "external connections\n",
      "141–142\n",
      "through external load \n",
      "balancers 138–141\n",
      "using NodePort \n",
      "services 135–138\n",
      "external services, creating alias \n",
      "for 134\n",
      "ExternalIP 138\n",
      "EXTERNAL-IP column 136\n",
      "ExternalName services, \n",
      "creating 134\n",
      "external-traffic annotation 141\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 604, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "572\n",
      "F\n",
      "FABRIC8 Java client, interacting \n",
      "with 247–248\n",
      "FailedPostStartHook 487\n",
      "FailedPreStopHook 488\n",
      "FallbackToLogsOnError 500\n",
      "features. See security-related fea-\n",
      "tures\n",
      "federation. See Cluster Federation\n",
      "files\n",
      "copying to and from \n",
      "containers 500–501\n",
      "creating ConfigMap entries \n",
      "from contents of 201\n",
      "in configMap volumes, setting \n",
      "file permissions for 211\n",
      "in directories, creating Config-\n",
      "Map from 201\n",
      "in downwardAPI volume, \n",
      "passing metadata through\n",
      "230–233\n",
      "in sync with gitRepo \n",
      "volume 168\n",
      "mounted config, verifying \n",
      "Nginx using 208\n",
      "mounting 503\n",
      "mounting ConfigMap entries \n",
      "as 210–211\n",
      "mounting directory hiding \n",
      "existing 210\n",
      "on worker node filesystems, \n",
      "accessing 169–170\n",
      "updating automatically 212–213\n",
      "using configMap volume to \n",
      "expose ConfigMap entries \n",
      "as 205, 211\n",
      "filesystems\n",
      "of containers\n",
      "isolating 34\n",
      "preventing processes from \n",
      "writing to 386–387\n",
      "worker node, accessing files \n",
      "on 169–170\n",
      "firewalls\n",
      "changing rules to let external \n",
      "clients access NodePort \n",
      "services 137–138\n",
      "disabling 544\n",
      "Flannel 312\n",
      "flat networks, inter-pod 58\n",
      "flexVolume volume 163\n",
      "flocker volume 163\n",
      "foo namespace 358\n",
      "foo.default.svc.cluster.local \n",
      "domain 286\n",
      "FOO_SECRET variable 221\n",
      "--force option 490\n",
      "fortune\n",
      "images, configuring INTERVAL \n",
      "variables in 194–195\n",
      "pods, running with custom \n",
      "intervals 195–196\n",
      "fortune command 163, 165\n",
      "fortune images, INTERVAL vari-\n",
      "ables in 197\n",
      "fortune-config config map, \n",
      "modifying to enable HTTPS\n",
      "218–219\n",
      "fortune-https directory 216\n",
      "fortune-https Secret, mounting in \n",
      "pods 219–220\n",
      "fortuneloop container 209\n",
      "fortuneloop.sh script 164\n",
      "fortune-pod.yaml file 164\n",
      "forwarding, local network port to \n",
      "port in pod 67\n",
      "FQDN (fully qualified domain \n",
      "name) 129–130, 134\n",
      "FROM scratch directive 497\n",
      "--from-file argument 201\n",
      "fsGroup policies 392–394\n",
      "deploying pod with container \n",
      "image with an out-of-range \n",
      "user ID 393\n",
      "using MustRunAs rule 392–393\n",
      "fsGroup property 388\n",
      "G\n",
      "gateways 58\n",
      "GCE (Google Compute \n",
      "Engine) 185, 454\n",
      "creating persistent disks 171–172\n",
      "using in pod volumes 171–174\n",
      "creating GCE persistent \n",
      "disks 171–172\n",
      "creating pods using gcePer-\n",
      "sistentDisk volumes 172\n",
      "re-creating pods 173–174\n",
      "verifying pod can read data \n",
      "persisted by previous \n",
      "pod 173–174\n",
      "writing data to persistent stor-\n",
      "age by adding docu-\n",
      "ments to MongoDB \n",
      "database 173\n",
      "gcePersistentDisk volumes\n",
      "creating pods 172\n",
      "overview 162, 190\n",
      "gcloud command 39, 171\n",
      "gcloud compute ssh command 97\n",
      "--generator flag 42\n",
      "geographical regions\n",
      "co-locating pods in same 471\n",
      "deploying pods in same 471–472\n",
      "Git repositories\n",
      "as starting point for \n",
      "volumes 166–169\n",
      "files in sync with gitRepo \n",
      "volume 168\n",
      "gitRepo volume 169\n",
      "sidecar containers 168\n",
      "cloned, running web server pod \n",
      "serving files from 167\n",
      "private, using gitRepo volume \n",
      "with 168\n",
      "Git sync container 168\n",
      "gitRepo field 510, 517\n",
      "gitRepo volumes\n",
      "files in sync with 168\n",
      "overview 169\n",
      "using with private Git \n",
      "repositories 168\n",
      "GKE (Google Container \n",
      "Engine) 36, 454\n",
      "accessing dashboard when run-\n",
      "ning in 52\n",
      "and Minikube, switching \n",
      "between 534–535\n",
      "switching from Minikube \n",
      "to 534\n",
      "switching to Minikube \n",
      "from 534\n",
      "using hosted clusters with\n",
      "38–41\n",
      "creating clusters with three \n",
      "nodes 39\n",
      "downloading client \n",
      "binaries 39\n",
      "getting overview of \n",
      "clusters 39\n",
      "listing cluster nodes 40\n",
      "retrieving additional details \n",
      "of objects 41\n",
      "setting up Google Cloud \n",
      "projects 39\n",
      "glusterfs volume 163\n",
      "Google Cloud, setting up \n",
      "projects 39\n",
      "Google Compute Engine. See GCE\n",
      "Google Container Registry 35\n",
      "governing services\n",
      "creating 292–294\n",
      "overview 285–287\n",
      "gpu=true label 75, 464–465\n",
      "Grafana suite\n",
      "analyzing resource usage \n",
      "with 433–434\n",
      "overview 432\n",
      "running in clusters 433\n",
      "group ID (gid) 381\n",
      "grouping resources, with \n",
      "namespaces 76–80\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 605, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "573\n",
      "groups 347–348\n",
      "assigning different PodSecurity-\n",
      "Policies to 396–399\n",
      "in Red Hat OpenShift Con-\n",
      "tainer platform 528\n",
      "Guaranteed class, assigning pods \n",
      "to 417–418\n",
      "guarantees, StatefulSets 289–290\n",
      "at-most-one semantics 290\n",
      "implications of stable \n",
      "identity 289–290\n",
      "implications of stable \n",
      "storage 289–290\n",
      "H\n",
      "hardcoding environment vari-\n",
      "ables, disadvantages of 198\n",
      "hardware, improving utilization \n",
      "of 22\n",
      "headless services\n",
      "creating 154–155\n",
      "DNS returned for 155–156\n",
      "using for discovering individual \n",
      "pods 154–156\n",
      "discovering all pods 156\n",
      "discovering pods through \n",
      "DNS 155–156\n",
      "health checking 22–23\n",
      "health HTTP endpoint 89\n",
      "Heapster aggregator, \n",
      "enabling 431\n",
      "Hello World container 26–28\n",
      "behind the scenes 27\n",
      "running other images 27\n",
      "versioning container images 28\n",
      "Helm. See Deis Helm package \n",
      "manager\n",
      "hooks. See lifecycle hooks\n",
      "hops. See network hops\n",
      "horizontal autoscaling, of \n",
      "pods 438–451\n",
      "autoscaling process 438–441\n",
      "metrics appropriate for \n",
      "autoscaling 450\n",
      "scaling\n",
      "down to zero replicas\n",
      "450–451\n",
      "on CPU utilization 441–447\n",
      "on memory \n",
      "consumption 448\n",
      "on other and custom \n",
      "metrics 448–450\n",
      "Horizontal pod Autoscaler, creat-\n",
      "ing based on CPU \n",
      "usage 442–443\n",
      "horizontal scaling, of cluster \n",
      "nodes 452–456\n",
      "Cluster Autoscaler 452–453\n",
      "enabling Cluster \n",
      "Autoscaler 454\n",
      "limiting service disruption \n",
      "during cluster scale-\n",
      "down 454–456\n",
      "Host header 147\n",
      "host networks, namespaces\n",
      "377–379\n",
      "host nodes, using namespaces in \n",
      "pods 376–380\n",
      "host operating systems, container \n",
      "processes running in 34\n",
      "host ports, binding to 377–379\n",
      "hostIPC property 380\n",
      "hostnames\n",
      "changing on cloned VMs 546\n",
      "expecting changes to 480\n",
      "hostNetwork property 376\n",
      "hostPath volumes 162, 169–170\n",
      "hostPort property 377\n",
      "hosts\n",
      "configuring name resolution \n",
      "for 546–547\n",
      "mapping different services \n",
      "to 147\n",
      "mapping different services to \n",
      "paths of same 146\n",
      "HPA (HorizontalPodAutoscaler)\n",
      "modifying target metric values \n",
      "on existing objects 447\n",
      "overview 443\n",
      "html-generator container 165\n",
      "HTTP GET probe 85, 150\n",
      "HTTP-based liveness probes, \n",
      "creating 86–87\n",
      "httpGet liveness probe 87\n",
      "HTTPS (Hypertext Transfer \n",
      "Protocol Secure)\n",
      "218–219\n",
      "hybrid cloud 556\n",
      "hypervisors 9\n",
      "I\n",
      "identities\n",
      "network, providing 285–287\n",
      "governing service, \n",
      "overview 285–287\n",
      "scaling StatefulSets 287\n",
      "of API servers, verifying\n",
      "240–241\n",
      "providing stable for pods\n",
      "282–284\n",
      "stable, implications of\n",
      "289–290\n",
      "if statement 268\n",
      "IgnoredDuringExecution 464\n",
      "image layers 15, 30–31\n",
      "image pull Secrets 222–223, \n",
      "351\n",
      "creating Secrets for authenticat-\n",
      "ing with Docker \n",
      "registries 223\n",
      "specifying on every pod 223\n",
      "using Docker-registry Secrets in \n",
      "pod definitions 223\n",
      "using private image repositories \n",
      "on Docker Hub 222\n",
      "image registry\n",
      "pushing images to 35–36\n",
      "running images on different \n",
      "machines 36\n",
      "tagging images under addi-\n",
      "tional tags 35\n",
      "pushing images to Docker \n",
      "Hub 36\n",
      "imagePullPolicy\n",
      "overview 256, 317\n",
      "using 497–498\n",
      "imagePullSecrets field 222\n",
      "images\n",
      "building\n",
      "from source using \n",
      "BuildConfigs 529\n",
      "locally 504\n",
      "using Docker daemon inside \n",
      "Minikube VM 503–504\n",
      "copying to Minikube VM \n",
      "directly 504\n",
      "deploying automatically \n",
      "with DeploymentConfigs\n",
      "529–530\n",
      "Docker\n",
      "building 13\n",
      "distributing 13\n",
      "portability limitations of 15\n",
      "running 13\n",
      "fortune\n",
      "configuring INTERVAL vari-\n",
      "ables in 194–195\n",
      "configuring INTERVAL vari-\n",
      "ables through environ-\n",
      "ment variables 197\n",
      "of containers\n",
      "creating 290–291, 497\n",
      "listing 554–555\n",
      "pushing to Docker Hub 36\n",
      "pushing to image registry\n",
      "35–36\n",
      "running on different \n",
      "machines 36\n",
      "tagging 35, 497–498\n",
      "See also container images\n",
      "ImageStream 529\n",
      "In operator 107\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 606, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "574\n",
      "InfluxDB database\n",
      "overview 432\n",
      "running in clusters 433\n",
      "Ingress resource 135\n",
      "accessing pods through 145\n",
      "accessing services through 145\n",
      "benefits of using 142–143\n",
      "configuring to handle TLS \n",
      "traffic 147–149\n",
      "creating 144\n",
      "creating TLS certificate \n",
      "for 147–149\n",
      "exposing multiple services \n",
      "through 146–147\n",
      "mapping different services to \n",
      "different hosts 147\n",
      "mapping different services to \n",
      "different paths of same \n",
      "host 146\n",
      "exposing services externally \n",
      "through 142–149\n",
      "obtaining IP address of 145\n",
      "overview 145\n",
      "Ingresses 559\n",
      "Init containers, adding to \n",
      "pods 484–485\n",
      "initialDelaySeconds property 88\n",
      "initializing masters with kubeadm \n",
      "init 547–548\n",
      "initiating install of OS 542\n",
      "inspecting node capacity 407–408\n",
      "installing\n",
      "Docker 544–545\n",
      "disabling firewalls 544\n",
      "disabling SELinux 544\n",
      "enabling net.bridge.bridge-\n",
      "nf-call-iptables Kernel \n",
      "options 545\n",
      "kubeadm 544–545\n",
      "kubectl 38, 544–545\n",
      "Kubelet 544–545\n",
      "Kubernetes 544–545\n",
      "adding Kubernetes yum \n",
      "repo 544\n",
      "disabling firewalls 544\n",
      "disabling SELinux 544\n",
      "enabling net.bridge.bridge-\n",
      "nf-call-iptables Kernel \n",
      "option 545\n",
      "Kubernetes-CNI 544–545\n",
      "Minikube 37\n",
      "OS 541–544\n",
      "initiating 542\n",
      "running install 543–544\n",
      "selecting start-up disks 541\n",
      "setting installation \n",
      "options 542–543\n",
      "Internet protocol. See IP\n",
      "inter-pod affinity, to deploy pods \n",
      "on same nodes 468–471\n",
      "deploying pods with pod \n",
      "affinity 470\n",
      "specifying pod affinity in pod \n",
      "definitions 469\n",
      "using pod affinity rules with \n",
      "Scheduler 470–471\n",
      "INTERVAL variables, configuring \n",
      "in fortune images\n",
      "overview 194–195\n",
      "through environment \n",
      "variables 197\n",
      "intervals, running fortune pods \n",
      "with 195–196\n",
      "IP (Internet protocol)\n",
      "containers sharing 57\n",
      "external, accessing services \n",
      "through 46–47\n",
      "Ingress address 145\n",
      "local, expecting changes to 480\n",
      "of client, non-preservation of 142\n",
      "service, pinging 131\n",
      "IP addresses 121\n",
      "IPC (Inter-Process Communication)\n",
      "overview 11, 56, 376\n",
      "using namespaces 379–380\n",
      "iptables rules 327–328, 339–340, \n",
      "345, 492, 494–495\n",
      "iscsi volume 163\n",
      "isolating\n",
      "components with Linux con-\n",
      "tainer technologies 8\n",
      "container filesystems 34\n",
      "containers, mechanisms for 11\n",
      "networks between Kubernetes \n",
      "namespaces 401–402\n",
      "partially between containers of \n",
      "same pod 57\n",
      "pod networks 399–402\n",
      "processes with Linux \n",
      "Namespaces 11\n",
      "-it option 33\n",
      "items attribute 138\n",
      "J\n",
      "Job controllers 324\n",
      "Job resources 112\n",
      "configuring templates 117\n",
      "running multiple pod instances \n",
      "in 114–116\n",
      "running Job pods in \n",
      "parallel 115\n",
      "running Job pods \n",
      "sequentially 115\n",
      "scaling Job 116\n",
      "running on pods 114\n",
      "jobs\n",
      "defining 113–114\n",
      "listing instances in \n",
      "clusters 237–238\n",
      "retrieving instances by \n",
      "name 238\n",
      "scheduling 116–118\n",
      "creating CronJob 116–117\n",
      "overview 117\n",
      "JSON format\n",
      "creating pods from descriptors\n",
      "sending requests to pods\n",
      "66–67\n",
      "using kubectl create to create \n",
      "pods 65\n",
      "viewing application logs\n",
      "65–66\n",
      "manifests, writing 505–506\n",
      "JSONPath 138\n",
      "K\n",
      "keep-alive connections 140\n",
      "kernel capabilities, adding to \n",
      "containers 384–385\n",
      "Kernels, enabling \n",
      "net.bridge.bridge-nf-call-\n",
      "iptables option 545\n",
      "key files, from Secrets 221\n",
      "key=value format 232\n",
      "killing applications 479–482\n",
      "expecting data written to disk to \n",
      "disappear 480\n",
      "expecting hostnames to \n",
      "change 480\n",
      "expecting local IP to \n",
      "change 480\n",
      "using volumes to preserve data \n",
      "across container \n",
      "restarts 480–482\n",
      "Ksonnet, as alternative to writing \n",
      "JSON manifests 505–506\n",
      "kube config\n",
      "adding entries 536–537\n",
      "adding clusters 536\n",
      "adding user credentials\n",
      "536–537\n",
      "tying clusters and user cre-\n",
      "dentials together 537\n",
      "listing entries 536–537\n",
      "adding clusters 536\n",
      "adding user credentials\n",
      "536–537\n",
      "modifying clusters 536\n",
      "modifying user \n",
      "credentials 536–537\n",
      "tying clusters and user cre-\n",
      "dentials together 537\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 607, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "575\n",
      "kube config (continued)\n",
      "modifying entries 536–537\n",
      "modifying clusters 536\n",
      "modifying user credentials\n",
      "536–537\n",
      "tying clusters and user cre-\n",
      "dentials together 537\n",
      "kubeadm init command 549\n",
      "kubeadm join command 549\n",
      "kubeadm tool\n",
      "configuring masters with\n",
      "547–549\n",
      "configuring worker nodes \n",
      "with 549–550\n",
      "installing 544–545\n",
      "running components with\n",
      "548–549\n",
      "listing nodes 548–549\n",
      "listing pods 548\n",
      "running kubectl on \n",
      "masters 548\n",
      "running init to initialize \n",
      "masters 547–548\n",
      "setting up multi-node clusters \n",
      "with 539–551\n",
      "setting up operating \n",
      "systems 539\n",
      "setting up required \n",
      "packages 539\n",
      "using clusters from local \n",
      "machines 550–551\n",
      "kubeconfig files\n",
      "configuring location of 535\n",
      "contents of 535–536\n",
      "KUBECONFIG variable 535\n",
      "kubectl\n",
      "aliases for 41–42\n",
      "command-line completion\n",
      "41–42\n",
      "configuring tab completion \n",
      "for 41–42\n",
      "confirming cluster communicat-\n",
      "ing with 38\n",
      "creating additional users for 398\n",
      "installing 38\n",
      "logs, retrieving with pod logs 66\n",
      "performing rolling updates \n",
      "with 256–260\n",
      "replacing old pods with new \n",
      "pods by scaling two \n",
      "ReplicationControllers\n",
      "259–260\n",
      "steps performed before roll-\n",
      "ing update commences\n",
      "258–259\n",
      "proxy\n",
      "accessing API servers \n",
      "through 234–235\n",
      "exploring Kubernetes API \n",
      "through 235–236\n",
      "using with different \n",
      "clusters 537–538\n",
      "using with different \n",
      "contexts 537–538\n",
      "using with different users\n",
      "537–538\n",
      "using with multiple \n",
      "clusters 534–538\n",
      "adding kube config \n",
      "entries 536–537\n",
      "configuring location of \n",
      "kubeconfig files 535\n",
      "contents of kubeconfig \n",
      "files 535–536\n",
      "deleting clusters 538\n",
      "deleting contexts 538\n",
      "listing clusters 538\n",
      "listing contexts 538\n",
      "listing kube config \n",
      "entries 536–537\n",
      "modifying kube config \n",
      "entries 536–537\n",
      "switching between \n",
      "contexts 538\n",
      "switching between Minikube \n",
      "and GKE 534–535\n",
      "using with multiple \n",
      "Namespaces 535–538\n",
      "adding kube config \n",
      "entries 536–537\n",
      "configuring location of \n",
      "kubeconfig files 535\n",
      "contents of kubeconfig \n",
      "files 535–536\n",
      "deleting clusters 538\n",
      "deleting contexts 538\n",
      "listing clusters 538\n",
      "listing contexts 538\n",
      "listing kube config \n",
      "entries 536–537\n",
      "modifying kube config \n",
      "entries 536–537\n",
      "switching between \n",
      "contexts 538\n",
      "kubectl annotate command 76\n",
      "kubectl apply command 266, \n",
      "276–277, 504\n",
      "kubectl autoscale command 443\n",
      "kubectl cluster-info command 38, \n",
      "52\n",
      "kubectl command-line tool 39\n",
      "kubectl cp command 500\n",
      "kubectl create command 65, 94, \n",
      "114, 178, 255\n",
      "kubectl create configmap \n",
      "command 200–201\n",
      "kubectl create deployment \n",
      "command 519\n",
      "kubectl create -f command 65\n",
      "kubectl create namespace \n",
      "command 78–79\n",
      "kubectl create role command 359\n",
      "kubectl create secret \n",
      "command 519\n",
      "kubectl delete command 104\n",
      "kubectl describe command 41, 52, \n",
      "76, 95, 131, 408, 444, 512, \n",
      "549, 553\n",
      "kubectl describe node \n",
      "command 409\n",
      "kubectl describe pod \n",
      "command 215, 487, 499\n",
      "kubectl edit command 151\n",
      "kubectl edit method 266\n",
      "kubectl exec command 124, 128, \n",
      "130, 152, 446\n",
      "kubectl explain command 64, 505\n",
      "kubectl expose command 66, 123\n",
      "kubectl get command 48, 95\n",
      "kubectl get events 332\n",
      "kubectl get pods command 51, 124\n",
      "kubectl get serviceclasses \n",
      "command 523\n",
      "kubectl get services command 46\n",
      "kubectl get svc command 129\n",
      "kubectl logs command 87, 485, 502\n",
      "kubectl patch method 266\n",
      "kubectl port-forward \n",
      "command 67\n",
      "kubectl proxy command 244, 503\n",
      "kubectl proxy process 514–515\n",
      "kubectl replace method 266\n",
      "kubectl rolling-update \n",
      "command 257, 260–261, 264\n",
      "kubectl rollout status \n",
      "command 271\n",
      "kubectl run command 61, 82\n",
      "kubectl scale command 102–103, \n",
      "116\n",
      "kubectl set image command\n",
      "265–266\n",
      "kubectl tool\n",
      "installing 544–545\n",
      "running on masters 548\n",
      "kube-dns 129\n",
      "KUBE_EDITOR environment \n",
      "variable 102\n",
      "Kubelet node agent, \n",
      "installing 544–545\n",
      "kubelets 326–327\n",
      "overview 326\n",
      "running pod containers with 332\n",
      "running static pods without API \n",
      "servers 326–327\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 608, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "576\n",
      "kube-proxy 19, 312, 327, 330, 345\n",
      "overview 339\n",
      "using iptables 339–340\n",
      "kube-public namespace 77\n",
      "Kubernetes 16–24\n",
      "architecture of 310–330\n",
      "add-ons 328–330\n",
      "API servers 316–319\n",
      "clusters 18–19\n",
      "components 310–312\n",
      "components of Control \n",
      "Plane 310\n",
      "components running on \n",
      "worker nodes 310\n",
      "Controller 321–326\n",
      "etcd 312–316\n",
      "kubelet 326–327\n",
      "Scheduler 319–321\n",
      "Service Proxy 327–328\n",
      "benefits of using 21–24\n",
      "automatic scaling 23\n",
      "health checking 22–23\n",
      "improving hardware \n",
      "utilization 22\n",
      "self-healing 22–23\n",
      "simplifying application \n",
      "deployment 21–22\n",
      "simplifying application \n",
      "development 23–24\n",
      "dashboard 52–53\n",
      "accessing when running in \n",
      "managed GKE 52\n",
      "accessing when using \n",
      "Minikube 53\n",
      "installing 544–545\n",
      "adding Kubernetes yum \n",
      "repo 544\n",
      "disabling firewalls 544\n",
      "disabling SELinux 544\n",
      "enabling net.bridge.bridge-\n",
      "nf-call-iptables Kernel \n",
      "option 545\n",
      "master, checking node status as \n",
      "seen by 305\n",
      "origins of 16\n",
      "overriding arguments in\n",
      "195–196\n",
      "overriding commands in\n",
      "195–196\n",
      "overview 16–18\n",
      "focusing on core application \n",
      "features 17–18\n",
      "improving resource \n",
      "utilization 18\n",
      "running applications in 19–21\n",
      "effect of application descrip-\n",
      "tion on running \n",
      "containers 19–20\n",
      "keeping containers \n",
      "running 20–21\n",
      "locating containers 21\n",
      "scaling number of copies 21\n",
      "when indicated 2–7\n",
      "continuous delivery 6–7\n",
      "microservices vs. monolithic \n",
      "applications 3–6\n",
      "providing consistent applica-\n",
      "tion environment 6\n",
      "Kubernetes CNI (Container Net-\n",
      "working Interface) 545\n",
      "Kubernetes Control Plane 18\n",
      "KUBERNETES_SERVICE_HOST \n",
      "variable 239\n",
      "KUBERNETES_SERVICE_PORT \n",
      "variable 239\n",
      "kube-scheduler resource 344\n",
      "kube-system namespace 77, 312, \n",
      "314, 344, 454, 548\n",
      "kubia 42\n",
      "kubia-2qneh 100\n",
      "kubia-container 32–33\n",
      "kubia-dmdck 100\n",
      "kubia-gpu.yaml file 74\n",
      "kubia-manual 63\n",
      "kubia-rc.yaml file 93\n",
      "KUBIA_SERVICE_HOST \n",
      "variable 129\n",
      "KUBIA_SERVICE_PORT \n",
      "variable 129\n",
      "kubia-svc.yaml file 123\n",
      "kubia-website service 517\n",
      "L\n",
      "label selectors\n",
      "changing for Replication-\n",
      "Controller 100–101\n",
      "deleting pods using 80\n",
      "effect of changing 93\n",
      "listing pods using 71–72\n",
      "listing subsets of pods \n",
      "through 71–72\n",
      "ReplicaSets 107\n",
      "using multiple conditions in 72\n",
      "labels\n",
      "adding to nodes 111\n",
      "adding to pods managed by \n",
      "ReplicationControllers 99\n",
      "categorizing worker nodes \n",
      "with 74\n",
      "constraining pod scheduling \n",
      "with 73–75\n",
      "categorizing worker nodes \n",
      "with labels 74\n",
      "scheduling pods to specific \n",
      "nodes 74–75\n",
      "multi-dimensional vs. single-\n",
      "dimensional 498\n",
      "of existing pods, modifying\n",
      "70–71\n",
      "of managed pods 99–100\n",
      "organizing pods with 67, 71\n",
      "overview 68–69\n",
      "removing from nodes 111–112\n",
      "specifying when creating \n",
      "pods 69–70\n",
      "updating 232\n",
      "LAN (local area network) 58\n",
      "latest tag 28\n",
      "--leader-elect option 343\n",
      "leader-election\n",
      "for non-horizontally scalable \n",
      "applications 341\n",
      "using in Control Plane \n",
      "components 344\n",
      "LeastRequestedPriority 407\n",
      "libraries\n",
      "building with OpenAPI 248\n",
      "building with Swagger 248\n",
      "See also client libraries\n",
      "lifecycles, of pods 479–491\n",
      "adding lifecycle hooks 485–489\n",
      "hooks\n",
      "adding 485–489\n",
      "targeting containers with 489\n",
      "using post-start container\n",
      "486–487\n",
      "using pre-stop container\n",
      "487–488\n",
      "using pre-stop when applica-\n",
      "tion not receiving \n",
      "SIGTERM signal\n",
      "488–489\n",
      "killing applications 479–482\n",
      "pod shutdowns 489–491\n",
      "relocating applications 479–482\n",
      "rescheduling dead pods\n",
      "482–483\n",
      "rescheduling partially dead \n",
      "pods 482–483\n",
      "starting pods in specific \n",
      "order 483–485\n",
      "limiting resources\n",
      "available in namespaces 425–429\n",
      "limiting objects that can be \n",
      "created 427–428\n",
      "ResourceQuota \n",
      "resources 425–427\n",
      "specifying quotas for per-\n",
      "sistent storage 427\n",
      "specifying quotas for specific \n",
      "pod states 429\n",
      "specifying quotas for specific \n",
      "QoS classes 429\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 609, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "577\n",
      "limiting resources (continued)\n",
      "available to containers 412–416\n",
      "exceeding limits 414–415\n",
      "limits as seen by applications \n",
      "in containers 415–416\n",
      "setting hard limits for \n",
      "resources containers can \n",
      "use 412–413\n",
      "LimitRange objects, creating\n",
      "422–423\n",
      "limits\n",
      "as seen by applications in \n",
      "containers 415–416\n",
      "containers seeing all node \n",
      "CPU cores 416\n",
      "containers seeing node \n",
      "memory 415–416\n",
      "enforcing 423–424\n",
      "exceeding 414–415\n",
      "for pods per namespace, setting \n",
      "defaults 421–425\n",
      "overcommitting 413\n",
      "setting for resources used by \n",
      "containers 412–413\n",
      "See also resource limits\n",
      "Linux Control Groups \n",
      "(cgroups) 11\n",
      "Linux Namespaces 11\n",
      "Linux OS\n",
      "container technologies, isolat-\n",
      "ing components with 8\n",
      "Namespaces, isolating pro-\n",
      "cesses with 11\n",
      "listing\n",
      "all running containers 32\n",
      "cluster nodes 40\n",
      "clusters 538\n",
      "container images 554–555\n",
      "contexts 538\n",
      "job instances in clusters 237–238\n",
      "kube config entries 536–537\n",
      "adding clusters 536\n",
      "adding user credentials\n",
      "536–537\n",
      "modifying clusters 536\n",
      "modifying user credentials\n",
      "536–537\n",
      "tying clusters and user cre-\n",
      "dentials together 537\n",
      "nodes 548–549\n",
      "PersistentVolumeClaims 180\n",
      "PersistentVolumes 180–181\n",
      "pods 43–44, 51, 71–72, 548\n",
      "services 46\n",
      "services available in clusters 523\n",
      "storage classes 187–188\n",
      "subsets through label \n",
      "selectors 71–72\n",
      "liveness probes 85–86\n",
      "configuring properties of\n",
      "88–89\n",
      "creating 89–90\n",
      "HTTP-based 86–87\n",
      "implementing retry loops \n",
      "in 90\n",
      "keeping probes light 90\n",
      "what liveness probe should \n",
      "check 89–90\n",
      "in action 87–88\n",
      "load balancers\n",
      "connecting to services \n",
      "through 139–141\n",
      "external, exposing services \n",
      "through 138–141\n",
      "LoadBalancer service\n",
      "creating 139\n",
      "overview 135, 530\n",
      "local area network. See LAN\n",
      "local machines, using clusters \n",
      "from 550–551\n",
      "LogRotator container 162\n",
      "logs\n",
      "centralized, using 501\n",
      "copying to and from \n",
      "containers 500–501\n",
      "handling multi-line \n",
      "statements 502\n",
      "kubectl, retrieving with pod \n",
      "logs 66\n",
      "of applications 500–502\n",
      "of multi-container pods, specify-\n",
      "ing container name when \n",
      "retrieving 66\n",
      "pod, retrieving with kubectl \n",
      "logs 66\n",
      "See also application logs\n",
      "logVol 162\n",
      "ls command 151\n",
      "M\n",
      "manifests\n",
      "Deployment resource, \n",
      "creating 262\n",
      "JSON, writing 505–506\n",
      "resources 504–505\n",
      "YAML, writing 505–506\n",
      "mapping\n",
      "different services to different \n",
      "hosts 147\n",
      "different services to different \n",
      "paths of same host 146\n",
      "master node 18\n",
      "masters\n",
      "configuring with \n",
      "kubeadm 547–549\n",
      "running kubeadm init to \n",
      "initialize 547–548\n",
      "running kubectl on 548\n",
      "matchExpressions property 107, \n",
      "465, 470\n",
      "matchLabels field 105, 107, \n",
      "470\n",
      "maxSurge property 271–272\n",
      "maxUnavailable property\n",
      "271–274, 277, 455\n",
      "medium, specifying for emptyDir \n",
      "volume 166\n",
      "memory\n",
      "creating ResourceQuota \n",
      "resources for 425–426\n",
      "displaying usage\n",
      "for cluster nodes 431\n",
      "for pods 431–432\n",
      "killing processes when low\n",
      "420–421\n",
      "handling containers with \n",
      "same QoS class 420–421\n",
      "sequence of QoS classes 420\n",
      "nodes, as seen by containers\n",
      "415–416\n",
      "scaling based on consumption \n",
      "of 448\n",
      "Secret volumes stored in \n",
      "memory 221\n",
      "metadata\n",
      "container-level, in volume \n",
      "specifications 233\n",
      "exposing through environment \n",
      "variables 227–230\n",
      "passing through Downward \n",
      "API 226–233\n",
      "available metadata 226–227\n",
      "passing through files in \n",
      "downwardAPI volume\n",
      "230–233\n",
      "metadata section 201\n",
      "metadata.name field 510\n",
      "metadata.resourceVersion \n",
      "field 313\n",
      "metric types\n",
      "Object, scaling based on\n",
      "449–450\n",
      "pods\n",
      "obtaining 438–439\n",
      "scaling based on 449\n",
      "resource, scaling based on 449\n",
      "metrics\n",
      "appropriate for autoscaling\n",
      "450\n",
      "custom, scaling based on\n",
      "448–450\n",
      "modifying target values on exist-\n",
      "ing HPA objects 447\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 610, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "578\n",
      "microservices 3–6\n",
      "deploying 5\n",
      "divergence of environment \n",
      "requirements 5–6\n",
      "scaling 4\n",
      "splitting applications into 3–4\n",
      "minAvailable field 455\n",
      "minikube delete command 553\n",
      "minikube mount command 503\n",
      "minikube node 408\n",
      "Minikube tool\n",
      "accessing dashboard when \n",
      "using 53\n",
      "and GKE, switching \n",
      "between 534–535\n",
      "combining with Kubernetes \n",
      "clusters 504\n",
      "inspecting running containers \n",
      "in 553–554\n",
      "installing 37\n",
      "running local single-node \n",
      "Kubernetes clusters with\n",
      "37–38\n",
      "starting clusters with 37–38\n",
      "switching from GKE to 534\n",
      "switching to GKE from 534\n",
      "using in development 503–504\n",
      "building images locally 504\n",
      "copying images to Minikube \n",
      "VM directly 504\n",
      "mounting local files into \n",
      "containers 503\n",
      "mounting local files into \n",
      "Minikube VM 503\n",
      "using Docker daemon inside \n",
      "Minikube VM to build \n",
      "images 503–504\n",
      "using rkt with 553–555\n",
      "listing container images\n",
      "554–555\n",
      "running pods 553\n",
      "Minikube VM (virtual machine)\n",
      "directly copying images to 504\n",
      "mounting local files into 503\n",
      "using Docker daemon inside, to \n",
      "build images 503–504\n",
      "Minishift 530\n",
      "minReadySeconds attribute 265, \n",
      "274–275\n",
      "MongoDB database, adding docu-\n",
      "ments to 173\n",
      "monitoring pod resource \n",
      "usage 430–434\n",
      "analyzing historical resource \n",
      "consumption statistics\n",
      "432–434\n",
      "collecting resource usages\n",
      "430–432\n",
      "retrieving resource usages\n",
      "430–432\n",
      "storing historical resource \n",
      "consumption statistics\n",
      "432–434\n",
      "monolithic applications, vs. \n",
      "microservices 3–6\n",
      "MostRequestedPriority 407\n",
      "Mount (mnt) Namespace 11\n",
      "mountable Secrets 350–351\n",
      "mounted config files, verifying \n",
      "Nginx using 208\n",
      "mounting\n",
      "ConfigMap entries as files\n",
      "210–211\n",
      "directories hides existing \n",
      "files 210\n",
      "fortune-https Secret in \n",
      "pods 219–220\n",
      "local files into containers 503\n",
      "local files into Minikube \n",
      "VM 503\n",
      "multi-tier applications, splitting \n",
      "into multiple pods 59\n",
      "MustRunAs rules, using 392–393\n",
      "MustRunAsNonRoot rules, using \n",
      "in runAsUser fields 394\n",
      "my-nginx-config.conf entry 209\n",
      "my-postgres-db service 525\n",
      "MYSQL_ROOT_PASSWORD \n",
      "variable 192\n",
      "N\n",
      "-n flag 79\n",
      "-n option 537\n",
      "names\n",
      "configuring resolution for \n",
      "hosts 546–547\n",
      "deleting pods by 80\n",
      "of containers 66\n",
      "retrieving job instances by 238\n",
      "names.kind property 511\n",
      "Namespace controllers 325\n",
      "--namespace option 535, 537\n",
      "NamespaceLifecycle 317\n",
      "namespaces\n",
      "accessing resources in, using \n",
      "ClusterRoles 367–370\n",
      "creating\n",
      "from YAML files 78\n",
      "with kubectl create \n",
      "namespace 78–79\n",
      "deleting pods in while \n",
      "keeping 81–82\n",
      "discovering pods of 77–78\n",
      "enabling network isolation \n",
      "in 399\n",
      "granting full control of, with \n",
      "admin ClusterRole 372\n",
      "grouping resources with 76–80\n",
      "host network, binding to host \n",
      "ports without using\n",
      "377–379\n",
      "host nodes, using in pods\n",
      "376–380\n",
      "including service accounts in \n",
      "RoleBindings 361\n",
      "IPC, using 379–380\n",
      "isolating networks between\n",
      "401–402\n",
      "isolation provided by 79–80\n",
      "limiting resources available \n",
      "in 425–429\n",
      "limiting objects that can be \n",
      "created 427–428\n",
      "ResourceQuota resources\n",
      "425–427\n",
      "specifying quotas for per-\n",
      "sistent storage 427\n",
      "specifying quotas for specific \n",
      "pod states 429\n",
      "specifying quotas for specific \n",
      "QoS classes 429\n",
      "Linux, isolating processes \n",
      "with 11\n",
      "managing objects in 79\n",
      "node network, using in \n",
      "pods 376–377\n",
      "node PID 379–380\n",
      "of pods 242\n",
      "pods in, allowing some to con-\n",
      "nect to server pods 400\n",
      "setting default limits for pods \n",
      "per 421–425\n",
      "applying default resource \n",
      "limits 424–425\n",
      "creating LimitRange \n",
      "objects 422–423\n",
      "enforcing limits 423–424\n",
      "LimitRange resources 421–422\n",
      "setting default requests for pods \n",
      "per 421–425\n",
      "applying default resource \n",
      "requests 424–425\n",
      "creating LimitRange \n",
      "objects 422–423\n",
      "enforcing limits 423–424\n",
      "LimitRange resources\n",
      "421–422\n",
      "using kubectl with multiple\n",
      "535–538\n",
      "adding kube config entries\n",
      "536–537\n",
      "configuring location of \n",
      "kubeconfig files 535\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 611, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "579\n",
      "namespaces (continued)\n",
      "contents of kubeconfig \n",
      "files 535–536\n",
      "deleting clusters 538\n",
      "deleting contexts 538\n",
      "listing clusters 538\n",
      "listing contexts 538\n",
      "listing kube config \n",
      "entries 536–537\n",
      "modifying kube config \n",
      "entries 536–537\n",
      "switching between \n",
      "contexts 538\n",
      "why needed 77\n",
      "NAS (network-attached \n",
      "storage) 171\n",
      "NAT (Network Address \n",
      "Translation) 58, 335\n",
      "net.bridge.bridge-nf-call-iptables \n",
      "Kernel option 545\n",
      "Network (net) namespace 11\n",
      "network hops, preventing 141\n",
      "networks\n",
      "between pods 335–338\n",
      "architecture of 335–336\n",
      "CNI (Container Network \n",
      "Interface) 338\n",
      "enabling communication \n",
      "between pods on differ-\n",
      "ent nodes 337–338\n",
      "enabling communication \n",
      "between pods on same \n",
      "node 336–337\n",
      "overview 336–338\n",
      "configuring adapters for \n",
      "VMs 540\n",
      "enabling isolation in \n",
      "namespaces 399\n",
      "isolating between Kubernetes \n",
      "namespaces 401–402\n",
      "local ports, forwarding port in \n",
      "pod 67\n",
      "node namespaces, using in \n",
      "pods 376–377\n",
      "node, shutting down \n",
      "adapters 304–305\n",
      "of containers, setting up 550\n",
      "providing stable identities\n",
      "285–287\n",
      "governing service, \n",
      "overview 285–287\n",
      "scaling StatefulSets 287\n",
      "securing 375–403\n",
      "configuring container \n",
      "security contexts\n",
      "380–389\n",
      "isolating pod networks\n",
      "399–402\n",
      "restricting use of security-\n",
      "related features in \n",
      "pods 389–399\n",
      "using host node namespaces \n",
      "in pods 376–380\n",
      "simulating node disconnection \n",
      "from 304–306\n",
      "checking node status as seen \n",
      "by Kubernetes \n",
      "master 305\n",
      "pods with unknown \n",
      "status 305–306\n",
      "See also flat networks; host net-\n",
      "works\n",
      "NFS (Network File System) 175\n",
      "nfs volume 162\n",
      "nginx container 334, 514\n",
      "Nginx software\n",
      "signaling to reload config 212\n",
      "using cert files from Secrets\n",
      "221\n",
      "using key files from Secrets 221\n",
      "verifying use of mounted config \n",
      "files 208\n",
      "node affinity\n",
      "specifying hard rules 463–465\n",
      "nodeAffinity attribute \n",
      "names 464–465\n",
      "nodeSelectorTerms 465\n",
      "specifying preferential \n",
      "rules 466–467\n",
      "using to attract pods to \n",
      "nodes 462–468\n",
      "examining default node \n",
      "labels 462–463\n",
      "prioritizing nodes when \n",
      "scheduling 465–468\n",
      "vs. node selectors 462–463\n",
      "Node controllers 324\n",
      "node failures 304–307\n",
      "deleting pods manually 306–307\n",
      "simulating node disconnection \n",
      "from network 304–306\n",
      "checking node status as seen \n",
      "by Kubernetes \n",
      "master 305\n",
      "pods with unknown \n",
      "status 305–306\n",
      "shutting down node network \n",
      "adapters 304–305\n",
      "nodeAffinity attribute names\n",
      "464–465\n",
      "Node.js\n",
      "creating applications 28–29\n",
      "deploying applications 42–44\n",
      "behind the scenes 44\n",
      "listing pods 43–44\n",
      "NodeLost 306\n",
      "NODE_NAME variable 228\n",
      "NodePort services\n",
      "changing firewall rules to let \n",
      "external clients access\n",
      "137–138\n",
      "creating 135–136\n",
      "examining 136–137\n",
      "using 135–138\n",
      "nodes\n",
      "adding custom taints to 460\n",
      "applications running on, \n",
      "examining 51–52\n",
      "assigning to newly created pods \n",
      "with Scheduler 332\n",
      "checking status as seen by \n",
      "Kubernetes master 305\n",
      "configuring pod rescheduling \n",
      "after failures 462\n",
      "CPU cores, as seen by \n",
      "containers 416\n",
      "creating clusters with three 39\n",
      "creating pods that don’t fit on \n",
      "any 408–409\n",
      "displaying taints 458\n",
      "enabling communication \n",
      "between pods on \n",
      "different 337–338\n",
      "enabling communication \n",
      "between pods on same\n",
      "336–337\n",
      "examining default labels\n",
      "462–463\n",
      "finding acceptable 320\n",
      "inspecting capacity of 407–408\n",
      "labeling 466\n",
      "listing 40, 548–549\n",
      "memory, as seen by \n",
      "containers 415–416\n",
      "network namespaces, using in \n",
      "pods 376–377\n",
      "PID namespaces 379–380\n",
      "preferences 467\n",
      "prioritizing when scheduling \n",
      "pods 465–468\n",
      "deploying pods in two-node \n",
      "clusters 468\n",
      "specifying preferential node \n",
      "affinity rules 466–467\n",
      "relinquishing 453\n",
      "ReplicationControllers respond-\n",
      "ing to failures 97–98\n",
      "requesting from Cloud \n",
      "infrastructure 452–453\n",
      "running one pod on each, with \n",
      "DaemonSets 108–112\n",
      "running pods on\n",
      "adding required label to \n",
      "nodes 111\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 612, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "580\n",
      "nodes (continued)\n",
      "removing required label from \n",
      "nodes 111–112\n",
      "with DaemonSets 109–112\n",
      "running pods on every, with \n",
      "DaemonSets 109\n",
      "Scheduler using pod requests \n",
      "when selecting 407\n",
      "scheduling pods to specific\n",
      "74–75\n",
      "selecting best for pods 320–321\n",
      "selectors vs. affinity 462–463\n",
      "simulating disconnection from \n",
      "network 304–306\n",
      "checking node status as seen by \n",
      "Kubernetes master 305\n",
      "pods with unknown \n",
      "status 305–306\n",
      "shutting down node network \n",
      "adapters 304–305\n",
      "using inter-pod affinity to \n",
      "deploy pods on 468–471\n",
      "deploying pods with pod \n",
      "affinity 470\n",
      "specifying pod affinity in pod \n",
      "definitions 469\n",
      "using pod affinity rules with \n",
      "Scheduler 470–471\n",
      "using node affinity to attract \n",
      "pods to 462–468\n",
      "comparing node affinity to \n",
      "node selectors 462–463\n",
      "specifying hard node affinity \n",
      "rules 463–465\n",
      "using Scheduler to determine \n",
      "whether pods can fit on 406\n",
      "using taints to repel pods \n",
      "from 457–462\n",
      "taints, overview 458–460\n",
      "using taints 461–462\n",
      "using tolerations to repel pods \n",
      "from 457–462\n",
      "adding tolerations to \n",
      "pods 460–461\n",
      "tolerations, overview 458–460\n",
      "using tolerations 461–462\n",
      "worker\n",
      "categorizing with labels 74\n",
      "components running on 310\n",
      "See also cluster nodes; node \n",
      "affinity; node failures; \n",
      "worker node filesystems; \n",
      "worker nodes\n",
      "nodeSelector field 75, 462–464\n",
      "nodeSelectorTerms 465\n",
      "NoExecute 460\n",
      "non-resource URLs, allowing \n",
      "access to 365–367\n",
      "NoOps 7\n",
      "NoSchedule 460\n",
      "NotIn operator 107\n",
      "NotReady status 97, 305\n",
      "NotTerminating scope 429\n",
      "NTP (Network Time Protocol) \n",
      "daemon 385\n",
      "NTP daemon. See NTP\n",
      "O\n",
      "-o custom-columns option 312\n",
      "object fields 64\n",
      "Object metric types, scaling based \n",
      "on 449–450\n",
      "OCI (Open Container \n",
      "Initiative) 15, 554\n",
      "Omega 16\n",
      "OnlyLocal annotation 142\n",
      "OOM (OutOfMemory) score 420\n",
      "OpenAPI interface, building \n",
      "libraries with 248\n",
      "OpenServiceBroker API 522–523\n",
      "listing available services in \n",
      "clusters 523\n",
      "overview 522\n",
      "registering brokers in Service \n",
      "Catalog 522–523\n",
      "OpenShift. See Red Hat OpenShift \n",
      "Container platform\n",
      "operations (ops) team 2\n",
      "optimistic concurrency \n",
      "control 313\n",
      "OS (operating systems), \n",
      "installing 541–544\n",
      "initiating install 542\n",
      "running install 543–544\n",
      "selecting start-up disks 541\n",
      "setting installation options\n",
      "542–543\n",
      "OutOfMemoryErrors 85\n",
      "out-of-range user IDs, container \n",
      "images with 393\n",
      "overcommitting limits 413\n",
      "overriding arguments and \n",
      "commands 195–196\n",
      "--overwrite option 70, 99\n",
      "P\n",
      "parallelism property 114\n",
      "path traversa attack 353\n",
      "paths, mapping services to 146\n",
      "patterns, of ambassador \n",
      "containers 244\n",
      "pausing rollouts 273–274\n",
      "PD (Persistent Disk) 185\n",
      "PDB (PodDisruptionBudget) 455\n",
      "peers\n",
      "discovering in StatefulSets\n",
      "299–304\n",
      "clustered data store 303–304\n",
      "implementing peer discovery \n",
      "through DNS 301–302\n",
      "SRV records, overview 300\n",
      "updating StatefulSets\n",
      "302–303\n",
      "implementing discovery \n",
      "through DNS 301–302\n",
      "Pending status 44\n",
      "permissions 373\n",
      "persistent disks, GCE 171–172\n",
      "persistent storage 171–175\n",
      "specifying quotas for 427\n",
      "using GCE Persistent Disk in \n",
      "pod volumes 171–174\n",
      "creating GCE persistent \n",
      "disks 171–172\n",
      "creating pods using gcePer-\n",
      "sistentDisk volumes 172\n",
      "re-creating pods 173–174\n",
      "verifying pod can read data \n",
      "persisted by previous \n",
      "pod 173–174\n",
      "using volumes with underlying \n",
      "persistent storage 174–175\n",
      "using AWS elastic block store \n",
      "volume 174\n",
      "using NFS volume 175\n",
      "using storage technologies\n",
      "175\n",
      "writing data to, by adding docu-\n",
      "ments to MongoDB \n",
      "database 173\n",
      "PersistentVolume \n",
      "controllers 325–326\n",
      "PersistentVolumeClaims. See PVC\n",
      "PersistentVolumes 176–177, 288\n",
      "benefits of using 182\n",
      "claiming by creating Persistent-\n",
      "VolumeClaims 179–181\n",
      "creating PersistentVolume-\n",
      "Claims 179–180\n",
      "listing PersistentVolume-\n",
      "Claims 180\n",
      "creating 177–178\n",
      "dynamic provisioning of\n",
      "184–189\n",
      "defining available storage \n",
      "types through Storage-\n",
      "Class resources 185\n",
      "requesting storage class in \n",
      "PersistentVolumeClaims\n",
      "185–187\n",
      "without specifying storage \n",
      "class 187–189\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 613, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "581\n",
      "PersistentVolumes (continued)\n",
      "dynamically provisioned 186–187\n",
      "listing 180–181\n",
      "pre-provisioned, Persistent-\n",
      "VolumeClaims bound \n",
      "to 189\n",
      "reclaiming automatically\n",
      "183–184\n",
      "reclaiming manually 183\n",
      "recycling 183–184\n",
      "Pet pods 295–299\n",
      "communicating with pods \n",
      "through API servers\n",
      "295–297\n",
      "connecting to cluster-internal \n",
      "services through API \n",
      "servers 299\n",
      "deleting 297–298\n",
      "exposing through services\n",
      "298–299\n",
      "scaling StatefulSets 298\n",
      "PetSets 284\n",
      "photonPersistentDisk volume 163\n",
      "PID namespaces 379–380\n",
      "pinging service IP 131\n",
      "platforms 527–533\n",
      "Deis Workflow 530–533\n",
      "Helm 530–533\n",
      "Red Hat OpenShift Container \n",
      "platform 527–530\n",
      "application templates\n",
      "528–529\n",
      "automatically deploying \n",
      "newly built images with \n",
      "DeploymentConfigs\n",
      "529–530\n",
      "building images from source \n",
      "using BuildConfigs 529\n",
      "exposing services externally \n",
      "using Routes 530\n",
      "groups 528\n",
      "projects 528\n",
      "resources available in 527–528\n",
      "users 528\n",
      "using 530\n",
      "pod affinity\n",
      "co-locating pods with 468–476\n",
      "deploying pods in same avail-\n",
      "ability zone 471–472\n",
      "deploying pods in same geo-\n",
      "graphical regions 471–\n",
      "472\n",
      "deploying pods in same \n",
      "rack 471–472\n",
      "expressing podAffinity pref-\n",
      "erences instead of hard \n",
      "requirements 472–473\n",
      "deploying pods with 470\n",
      "specifying in pod \n",
      "definitions 469\n",
      "using inter-pod affinity to \n",
      "deploy pods on same \n",
      "node 468–471\n",
      "using rules with \n",
      "Scheduler 470–471\n",
      "See also inter-pod affinity\n",
      "podAffinity, preferences vs. \n",
      "hard requirements\n",
      "472–473\n",
      "podAntiAffinity property 474\n",
      "PodDisruptionBudget \n",
      "resource 455\n",
      "PodDisruptionBudget. See PDB\n",
      "POD_IP variable 228\n",
      "POD_NAME variable 228\n",
      "POD_NAMESPACE variable 228\n",
      "pods 20, 47–55, 83\n",
      "accessing through Ingress 145\n",
      "accessing through services 264\n",
      "adding Init containers to\n",
      "484–485\n",
      "adding readiness probes \n",
      "to 151–153\n",
      "adding readiness probe \n",
      "to pod template\n",
      "151–152\n",
      "hitting service with single \n",
      "ready pod 153\n",
      "modifying pod readiness \n",
      "status 152\n",
      "observing pod readiness \n",
      "status 152\n",
      "adding tolerations to 460–461\n",
      "advanced scheduling of 321\n",
      "annotating 75–76\n",
      "adding annotations 76\n",
      "looking up object \n",
      "annotations 75–76\n",
      "modifying annotations 76\n",
      "assigning Burstable QoS class \n",
      "to 418\n",
      "assigning nodes to, with \n",
      "Scheduler 332\n",
      "assigning service accounts \n",
      "to 351–353\n",
      "assigning to BestEffort class 417\n",
      "assigning to Guaranteed \n",
      "class 417–418\n",
      "attracting to nodes with node \n",
      "affinity 462–468\n",
      "comparing node affinity to \n",
      "node selectors 462–463\n",
      "examining default node \n",
      "labels 462–463\n",
      "specifying hard node affinity \n",
      "rules 463–465\n",
      "calculating required number \n",
      "of 439\n",
      "co-locating 468–476\n",
      "expressing podAffinity pref-\n",
      "erences instead of hard \n",
      "requirements 472–473\n",
      "using inter-pod affinity to \n",
      "deploy pods on same \n",
      "node 468–471\n",
      "with pod anti-affinity 468–476\n",
      "communicating with API \n",
      "servers 238–243\n",
      "authenticating with API \n",
      "servers 241–242\n",
      "finding API server \n",
      "addresses 239–240\n",
      "running pods to 239\n",
      "verifying server identity\n",
      "240–241\n",
      "communicating with \n",
      "Kubernetes 243\n",
      "configuring rescheduling after \n",
      "node failures 462\n",
      "connecting through port \n",
      "forwarders 67\n",
      "containers\n",
      "resources requests for\n",
      "411–412\n",
      "running with kubelet 332\n",
      "containers sharing IP 57\n",
      "containers sharing port \n",
      "space 57\n",
      "creating 164–165\n",
      "as different users 398–399\n",
      "from JSON descriptors 61–67\n",
      "from YAML descriptors\n",
      "61–67\n",
      "examining YAML descrip-\n",
      "tors of existing pods\n",
      "61–63\n",
      "viewing application \n",
      "logs 65–66\n",
      "manually 281\n",
      "new with Replication-\n",
      "Controllers 96\n",
      "specific service accounts for \n",
      "each 373\n",
      "using custom service \n",
      "accounts 351–352\n",
      "with gcePersistentDisk \n",
      "volumes 172\n",
      "with kubectl create 65\n",
      "with ReplicaSet \n",
      "controller 332\n",
      "with resource limits 412–413\n",
      "with resource requests\n",
      "405–406\n",
      "YAML descriptors for 63–65\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 614, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "582\n",
      "pods (continued)\n",
      "dead, rescheduling 482–483\n",
      "decoupling from underlying \n",
      "storage technologies\n",
      "176–184\n",
      "benefits of using claims 182\n",
      "using PersistentVolume-\n",
      "Claims 176–177, \n",
      "179–181\n",
      "using PersistentVolumes\n",
      "176–184\n",
      "definitions 62–63\n",
      "deleting 306\n",
      "by deleting whole \n",
      "namespace 80–81\n",
      "by name 80\n",
      "deleting resources in \n",
      "namespace 82\n",
      "forcibly 307\n",
      "in namespace while keeping \n",
      "namespace 81–82\n",
      "manually 306–307\n",
      "old 252–253\n",
      "using label selectors 80\n",
      "deploying\n",
      "in same availability \n",
      "zone 471–472\n",
      "in same geographical \n",
      "regions 471–472\n",
      "in same rack 471–472\n",
      "in two-node clusters 468\n",
      "managed 84–118\n",
      "with container images with \n",
      "out-of-range user \n",
      "IDs 393\n",
      "with runAsUser outside of \n",
      "policy ranges 393\n",
      "discovering all 156\n",
      "discovering namespaces 242\n",
      "discovering through DNS\n",
      "155–156\n",
      "displaying CPU usage for\n",
      "431–432\n",
      "displaying memory usage \n",
      "for 431–432\n",
      "displaying pod IP when \n",
      "listing 51\n",
      "displaying pod node when \n",
      "listing 51\n",
      "displaying tolerations 459\n",
      "flat inter-pod networks 58\n",
      "fortune, running with custom \n",
      "intervals 195–196\n",
      "freeing resources to \n",
      "schedule 410\n",
      "horizontal autoscaling of\n",
      "438–451\n",
      "autoscaling process 438–441\n",
      "metrics appropriate for \n",
      "autoscaling 450\n",
      "scaling based on CPU \n",
      "utilization 441–447\n",
      "scaling based on memory \n",
      "consumption 448\n",
      "scaling based on other and \n",
      "custom metrics 448–450\n",
      "scaling down to zero \n",
      "replicas 450–451\n",
      "horizontally scaling 102–103\n",
      "declarative approach to \n",
      "scaling 103\n",
      "scaling down with kubectl \n",
      "scale command 103\n",
      "scaling ReplicationController \n",
      "by editing definitions\n",
      "102–103\n",
      "scaling up Replication-\n",
      "Controller 102\n",
      "in action 165\n",
      "in namespaces, allowing some to \n",
      "connect to server pods 400\n",
      "inspecting details with kubectl \n",
      "describe 52\n",
      "isolating networks 399–402\n",
      "enabling network isolation in \n",
      "namespaces 399\n",
      "isolating networks between \n",
      "Kubernetes \n",
      "namespaces 401–402\n",
      "lifecycles of 479–491\n",
      "adding lifecycle hooks\n",
      "485–489\n",
      "killing applications 479–482\n",
      "relocating applications\n",
      "479–482\n",
      "listing 43–44, 548\n",
      "listing subsets through label \n",
      "selectors 71–72\n",
      "listing using label selectors\n",
      "71–72\n",
      "logs, retrieving with kubectl \n",
      "logs 66\n",
      "managed by ReplicationControl-\n",
      "lers, adding labels to 99\n",
      "managed, changing labels \n",
      "of 99–100\n",
      "marked for deletion 307\n",
      "modifying labels of existing\n",
      "70–71\n",
      "modifying resource requests \n",
      "while running 451–452\n",
      "monitoring resource \n",
      "usage 430–434\n",
      "analyzing historical resource \n",
      "consumption statistics\n",
      "432–434\n",
      "collecting resource \n",
      "usages 430–432\n",
      "retrieving resource \n",
      "usages 430–432\n",
      "storing historical resource \n",
      "consumption \n",
      "statistics 432–434\n",
      "moving in and out of scope of \n",
      "ReplicationControllers\n",
      "98–101\n",
      "adding labels to pods man-\n",
      "aged by Replication-\n",
      "Controllers 99\n",
      "changing label selector\n",
      "100–101\n",
      "multi-container, logs of 66\n",
      "networking between 335–338\n",
      "CNI (Container Network \n",
      "Interface) 338\n",
      "enabling communication \n",
      "between pods on differ-\n",
      "ent nodes 337–338\n",
      "enabling communication \n",
      "between pods on same \n",
      "node 336–337\n",
      "network architecture\n",
      "335–336\n",
      "networking overview 336–338\n",
      "not being scheduled 409–410\n",
      "of clients, using newly created \n",
      "Secrets in 525–526\n",
      "organizing containers \n",
      "across 58–60\n",
      "splitting into multiple pods \n",
      "for scaling 59\n",
      "splitting multi-tier applications \n",
      "into multiple pods 59\n",
      "organizing with labels 67–71\n",
      "overview 43, 48–58, 60\n",
      "partial isolation between con-\n",
      "tainers of same pod 57\n",
      "partially dead, rescheduling\n",
      "482–483\n",
      "performing single completable \n",
      "task 112–116\n",
      "defining jobs 113–114\n",
      "Job resource 112\n",
      "running multiple pod \n",
      "instances in Job 114–116\n",
      "seeing Job run pods 114\n",
      "preventing broken client \n",
      "connection 492–497\n",
      "prioritizing nodes when \n",
      "scheduling 465–468\n",
      "labeling nodes 466\n",
      "node preferences 467\n",
      "specifying preferential node \n",
      "affinity rules 466–467\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 615, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "583\n",
      "pods (continued)\n",
      "providing stable identity \n",
      "for 282–284\n",
      "QoS classes 417–421\n",
      "defining 417–419\n",
      "killing processes when mem-\n",
      "ory is low 420–421\n",
      "reading Secret entries in 218\n",
      "reattaching PersistentVolume-\n",
      "Claims to new instances \n",
      "of 289\n",
      "re-creating 173–174\n",
      "referencing non-existing config \n",
      "maps in 203\n",
      "removing from controllers 100\n",
      "repelling from nodes with \n",
      "taints 457–462\n",
      "adding custom taints to \n",
      "node 460\n",
      "taints, overview 458–460\n",
      "using taints 461–462\n",
      "repelling from nodes with \n",
      "tolerations 457–462\n",
      "adding tolerations to \n",
      "pods 460–461\n",
      "tolerations, overview\n",
      "458–460\n",
      "using tolerations 461–462\n",
      "replacing old 252, 259–260\n",
      "ReplicationControllers respond-\n",
      "ing to deleted 95\n",
      "requesting resources for \n",
      "containers 405–412\n",
      "defining custom \n",
      "resources 411–412\n",
      "effect of CPU requests on \n",
      "CPU time sharing 411\n",
      "effect of resource requests on \n",
      "scheduling 406–410\n",
      "requests hitting three when hit-\n",
      "ting services 50\n",
      "restricting use of security-related \n",
      "features in 389–399\n",
      "assigning different PodSecu-\n",
      "rityPolicies to different \n",
      "groups 396–399\n",
      "assigning different Pod-\n",
      "SecurityPolicies to differ-\n",
      "ent users 396–399\n",
      "configuring allowed \n",
      "capabilities 394–395\n",
      "configuring default \n",
      "capabilities 394–395\n",
      "configuring disallowed \n",
      "capabilities 394–395\n",
      "constraining types of vol-\n",
      "umes pods can use 395\n",
      "fsGroup policies 392–394\n",
      "PodSecurityPolicy \n",
      "resources 389–392\n",
      "runAsUser policies 392–394\n",
      "supplementalGroups \n",
      "policies 392–394\n",
      "retrieving whole definition \n",
      "of 65\n",
      "running 333–334, 357, 553\n",
      "in privileged mode 382–384\n",
      "on certain nodes\n",
      "adding required label to \n",
      "nodes 111\n",
      "DaemonSets 109–112\n",
      "removing required label \n",
      "from nodes 111–112\n",
      "on every node, \n",
      "DaemonSets 109\n",
      "one on each node, with \n",
      "DaemonSets 108–112\n",
      "shell in containers 130–131\n",
      "without specifying security \n",
      "contexts 381\n",
      "without writing YAML \n",
      "manifest 155\n",
      "running controllers as 515–516\n",
      "Scheduler using requests when \n",
      "selecting nodes 407\n",
      "scheduling to specific \n",
      "nodes 74–75\n",
      "seeing newly created in list \n",
      "of 65\n",
      "selecting best nodes for\n",
      "320–321\n",
      "sending requests to 66–67\n",
      "sequence of events at \n",
      "deletion 493–495\n",
      "setting default limits for, per \n",
      "namespace 421–425\n",
      "setting default requests for, per \n",
      "namespace 421–425\n",
      "shutdowns of 489–491\n",
      "implementing proper shut-\n",
      "down handler in \n",
      "applications 490–491\n",
      "replacing critical shut-down \n",
      "procedures with dedi-\n",
      "cated shut-down proce-\n",
      "dure pods 491\n",
      "specifying termination grace \n",
      "periods 490\n",
      "signaling when ready to accept \n",
      "connections 149–153\n",
      "specifying for quotas for specific \n",
      "states 429\n",
      "specifying image pull Secrets \n",
      "on 223\n",
      "specifying labels when \n",
      "creating 69–70\n",
      "specifying pod affinity in \n",
      "definitions 469\n",
      "spinning up new 252–253\n",
      "starting 483–484\n",
      "starting in specific order 483–485\n",
      "best practices for handling \n",
      "inter-pod dependencies\n",
      "485\n",
      "Init containers 484\n",
      "static, running without API \n",
      "servers 326–327\n",
      "stopping, deleting pods 80–82\n",
      "templates\n",
      "changing 101\n",
      "effect of changing 93\n",
      "using with volume claim \n",
      "templates 288\n",
      "updating applications running \n",
      "in 251–253\n",
      "using anti-affinity with pods of \n",
      "same Deployment 475\n",
      "using dedicated service for each \n",
      "instance 283–284\n",
      "using Docker-registry Secrets in \n",
      "definitions 223\n",
      "using emptyDir volume in\n",
      "163–164\n",
      "using GCE Persistent Disk in \n",
      "volumes 171–174\n",
      "creating GCE persistent \n",
      "disks 171–172\n",
      "creating pods using gce-\n",
      "PersistentDisk volumes\n",
      "172\n",
      "writing data to persistent stor-\n",
      "age by adding docu-\n",
      "ments to MongoDB \n",
      "database 173\n",
      "using headless services to \n",
      "discover 154–156\n",
      "using host node namespaces \n",
      "in 376–380\n",
      "binding to host ports without \n",
      "using host network \n",
      "namespaces 377–379\n",
      "using node IPC \n",
      "namespaces 379–380\n",
      "using node PID \n",
      "namespaces 379–380\n",
      "using labels to constrain \n",
      "scheduling 73–75\n",
      "categorizing worker nodes \n",
      "with labels 74\n",
      "scheduling pods to specific \n",
      "nodes 74–75\n",
      "using namespaces to group \n",
      "resources 76, 80\n",
      "creating namespaces 78–79\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 616, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "584\n",
      "pods (continued)\n",
      "discovering other namespaces \n",
      "and their pods 77–78\n",
      "isolation provided by 79–80\n",
      "managing objects in other \n",
      "namespaces 79\n",
      "namespaces, why needed 77\n",
      "using node network name-\n",
      "spaces in 376–377\n",
      "using one ReplicaSet per \n",
      "instance 281–282\n",
      "using PersistentVolumeClaims \n",
      "in 181\n",
      "using Scheduler to determine \n",
      "whether pod can fit on \n",
      "node 406\n",
      "using Secrets in 218, 222\n",
      "exposing Secret entries \n",
      "through environment \n",
      "variables 221–222\n",
      "modifying fortune-config \n",
      "config map to enable \n",
      "HTTPS 218–219\n",
      "mounting fortune-https \n",
      "Secret in pod 219–220\n",
      "Secret volumes stored in \n",
      "memory 221\n",
      "verifying Nginx is using cert \n",
      "files from Secrets 221\n",
      "verifying Nginx is using key \n",
      "files from Secrets 221\n",
      "using selectors to constrain \n",
      "scheduling\n",
      "categorizing worker nodes \n",
      "with labels 74\n",
      "scheduling pods to specific \n",
      "nodes 74–75\n",
      "verifying readability of data \n",
      "persisted by previous \n",
      "pods 173–174\n",
      "vertical autoscaling of 451–452\n",
      "automatically configuring \n",
      "resource requests 451\n",
      "modifying resource requests \n",
      "while pod is running\n",
      "451–452\n",
      "web server, serving files from \n",
      "cloned Git repository 167\n",
      "when to use multiple containers \n",
      "in 59–60\n",
      "why needed 56–57\n",
      "with multiple containers, deter-\n",
      "mining QoS classes of 419\n",
      "with pod affinity, deploying 470\n",
      "with unknown status 305–306\n",
      "See also curl pods; inter-pod \n",
      "affinity; Pet pods; pod \n",
      "affinity; server pods\n",
      "PodSecurityPolicy resources\n",
      "389–392\n",
      "assigning to different \n",
      "groups 396–399\n",
      "assigning to different \n",
      "users 396–399\n",
      "creating additional users for \n",
      "kubectl 398\n",
      "creating pods as different \n",
      "user 398–399\n",
      "with RBAC 397–398\n",
      "creating, allowing privileged \n",
      "containers to be \n",
      "deployed 396–397\n",
      "examining 391–392\n",
      "overview 390–391\n",
      "pod.spec.containers.args field 204\n",
      "policy ranges, deploying pods with \n",
      "runAsUser outside of 393\n",
      "port forwarding 66\n",
      "port spaces, containers sharing 57\n",
      "portability, limitations of Docker \n",
      "images 15\n",
      "ports\n",
      "exposing multiple in same \n",
      "service 126–127\n",
      "local network, forwarding port \n",
      "in pod 67\n",
      "named, using 127–128\n",
      "See also host ports\n",
      "Post-start hooks 485\n",
      "PreferNoSchedule 460\n",
      "preserving data, with volumes\n",
      "480–482\n",
      "Pre-stop hooks 485\n",
      "private image repositories, using \n",
      "on Docker Hub 222\n",
      "privilege escalation 372\n",
      "privileged containers\n",
      "deployment of 396–397\n",
      "overview 403\n",
      "privileged mode, running pods \n",
      "in 382–384\n",
      "privileged policy 397\n",
      "privileged property 383\n",
      "probes. See liveness probes; readi-\n",
      "ness probes\n",
      "procedures. See shut-down proce-\n",
      "dures\n",
      "Process ID (pid) Namespace 11\n",
      "processes\n",
      "isolating with Linux \n",
      "Namespaces 11\n",
      "killing when memory is \n",
      "low 420–421\n",
      "handling containers with \n",
      "same QoS class 420–421\n",
      "sequence of QoS classes 420\n",
      "multiple with one container vs. \n",
      "multiple containers 56–57\n",
      "preventing from writing to con-\n",
      "tainer filesystems 386–387\n",
      "projects, in Red Hat OpenShift \n",
      "Container platform 528\n",
      "provisioning. See dynamic provi-\n",
      "sioning\n",
      "ProvisioningFailed event 186\n",
      "proxies 327–328\n",
      "publicHtml volume 162\n",
      "pushing\n",
      "images to Docker Hub 36\n",
      "images to image registry 35–36\n",
      "running images on different \n",
      "machines 36\n",
      "tagging images under addi-\n",
      "tional tags 35\n",
      "PVC (PersistentVolumeClaims)\n",
      "176–177, 288, 423, 532\n",
      "bound to pre-provisioned \n",
      "PersistentVolumes 189\n",
      "creating 179–180, 288\n",
      "creating without specifying stor-\n",
      "age class 188–189\n",
      "deleting 288\n",
      "examining 295\n",
      "listing 180\n",
      "reattaching to new instances of \n",
      "pod 289\n",
      "requesting storage class in\n",
      "185–187\n",
      "by creating PersistentVolume-\n",
      "Claim definition\n",
      "185–186\n",
      "examining created Persistent-\n",
      "VolumeClaims 186–187\n",
      "examining dynamically provi-\n",
      "sioned Persistent-\n",
      "Volumes 186–187\n",
      "using storage classes 187\n",
      "using in pods 181\n",
      "Q\n",
      "QCOW2 image file format 555\n",
      "QEMU virtual machine tool 555\n",
      "QoS (Quality of Service) 417\n",
      "QoS classes 417–421\n",
      "defining for pods 417–419\n",
      "assigning Burstable QoS class \n",
      "to pods 418\n",
      "assigning pods to BestEffort \n",
      "class 417\n",
      "assigning pods to Guaranteed \n",
      "class 417–418\n",
      "handling containers with \n",
      "same 420–421\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 617, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "585\n",
      "QoS classes (continued)\n",
      "killing processes when memory \n",
      "is low 420–421\n",
      "handling containers with \n",
      "same QoS class 420–421\n",
      "sequence of QoS classes 420\n",
      "of containers, determining 418\n",
      "of pods with multiple contain-\n",
      "ers, determining 419\n",
      "sequence of 420\n",
      "specifying quotas for 429\n",
      "QPS (Queries-Per-Second) 439, \n",
      "449\n",
      "quobyte volume 163\n",
      "quorum 315\n",
      "quotas, specifying 427–429\n",
      "R\n",
      "racks, deploying pods in 471–472\n",
      "RBAC (role-based access \n",
      "control) 242\n",
      "authorization plugins 353–354\n",
      "plugins 354\n",
      "resources 355–357\n",
      "creating namespaces 357\n",
      "enabling in clusters 356\n",
      "listing services from pods 357\n",
      "running pods 357\n",
      "securing clusters with 353–373\n",
      "binding roles to service \n",
      "accounts 359–360\n",
      "default ClusterRoleBindings\n",
      "371–373\n",
      "default ClusterRoles\n",
      "371–373\n",
      "granting authorization \n",
      "permissions 373\n",
      "including service accounts \n",
      "from other namespaces \n",
      "in RoleBinding 361\n",
      "RBAC authorization \n",
      "plugins 353–354\n",
      "RBAC resources 355–357\n",
      "using ClusterRoleBindings\n",
      "362–371\n",
      "using ClusterRoles 362–371\n",
      "using RoleBindings 358–359\n",
      "using Roles 358–359\n",
      "using to assign different Pod-\n",
      "SecurityPolicies to differ-\n",
      "ent users 397–398\n",
      "rbd volume 163\n",
      "rc (replicationcontroller) 46, 95\n",
      "readiness probes\n",
      "adding to pods 151–153\n",
      "adding readiness probe to \n",
      "pod template 151–152\n",
      "hitting service with single \n",
      "ready pod 153\n",
      "modifying pod readiness \n",
      "status 152\n",
      "observing pod readiness \n",
      "status 152\n",
      "benefits of using 151\n",
      "defining 153\n",
      "defining to prevent rollouts 275\n",
      "including pod shutdown logic \n",
      "in 153\n",
      "operation of 150\n",
      "overview 149–151\n",
      "preventing rollouts with 277–278\n",
      "types of 150\n",
      "reading, Secret entries in \n",
      "pods 218\n",
      "read-only access, to resources with \n",
      "view ClusterRole 372\n",
      "ReadOnlyMany access mode 180\n",
      "readOnlyRootFilesystem \n",
      "property 387, 392\n",
      "ReadWriteMany access mode 180\n",
      "ReadWriteOnce access mode 180\n",
      "READY column 44, 152\n",
      "reconciliation loops 92\n",
      "records, DNS 155–156\n",
      "re-creating pods 173–174\n",
      "recycling PersistentVolumes\n",
      "183–184\n",
      "automatically 183–184\n",
      "manually 183\n",
      "Red Hat OpenShift Container \n",
      "platform 527–530\n",
      "application templates 528–529\n",
      "automatically deploying newly \n",
      "built images with \n",
      "DeploymentConfigs\n",
      "529–530\n",
      "building images from source \n",
      "using BuildConfigs 529\n",
      "exposing services externally \n",
      "using Routes 530\n",
      "groups 528\n",
      "projects 528\n",
      "resources available in 527–528\n",
      "users 528\n",
      "using 530\n",
      "referencing non-existing config \n",
      "maps in pods 203\n",
      "registering\n",
      "custom API servers 519\n",
      "service brokers in Service \n",
      "Catalog 522–523\n",
      "registries, creating Secrets for \n",
      "authenticating with 223\n",
      "rel=canary label 80\n",
      "relinquishing, nodes 453\n",
      "reloading config, signaling Nginx \n",
      "to 212\n",
      "relocating applications 479–482\n",
      "expecting data written to disk to \n",
      "disappear 480\n",
      "expecting hostnames to \n",
      "change 480\n",
      "expecting local IP to \n",
      "change 480\n",
      "using volumes to preserve data \n",
      "across container \n",
      "restarts 480–482\n",
      "removing\n",
      "containers 34–35\n",
      "labels from nodes 111–112\n",
      "pods from controllers 100\n",
      "repelling\n",
      "pods from nodes with \n",
      "taints 457–462\n",
      "adding custom taints to \n",
      "node 460\n",
      "taints, overview 458–460\n",
      "using taints 461–462\n",
      "pods from nodes with \n",
      "tolerations 457–462\n",
      "adding tolerations to \n",
      "pods 460–461\n",
      "tolerations, overview 458–460\n",
      "using tolerations 461–462\n",
      "replacing old pods\n",
      "by scaling two Replication-\n",
      "Controllers 259–260\n",
      "overview 252\n",
      "replica count 49, 92\n",
      "replicas\n",
      "running multiple with separate \n",
      "storage for each 281–282\n",
      "creating pods manually 281\n",
      "using multiple directories in \n",
      "same volume 282\n",
      "using one ReplicaSet per pod \n",
      "instance 281–282\n",
      "scaling down to zero 450–451\n",
      "updating count on scaled \n",
      "resources 440\n",
      "ReplicaSets 104–108, 324, 559\n",
      "controllers\n",
      "creating pods with 332\n",
      "creating with Deployment \n",
      "controller 331\n",
      "creating 106–107, 263–264\n",
      "defining 105–106\n",
      "examining 106–107\n",
      "using label selectors 107\n",
      "using one per pod \n",
      "instance 281–282\n",
      "vs. ReplicationControllers 105\n",
      "vs. StatefulSets 284–285\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 618, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "586\n",
      "replicating stateful pods 281–284\n",
      "providing stable identity for \n",
      "pods 282–284\n",
      "running multiple replicas with \n",
      "separate storage for \n",
      "each 281–282\n",
      "replication controllers 47–48\n",
      "replication managers 323–324\n",
      "ReplicationControllers 90–104\n",
      "benefits of using 93\n",
      "changing pod templates 101\n",
      "creating 93–94\n",
      "creating new pods with 96\n",
      "deleting 103–104\n",
      "getting information about\n",
      "95–96\n",
      "horizontally scaling pods\n",
      "102–103\n",
      "in action 94–98\n",
      "moving pods in and out of \n",
      "scope of 98–101\n",
      "adding labels to pods man-\n",
      "aged by 99\n",
      "changing label selectors\n",
      "100–101\n",
      "changing labels of managed \n",
      "pod 99–100\n",
      "removing pods from \n",
      "controllers 100\n",
      "operation of 91–93\n",
      "parts of 92–93\n",
      "performing automatic \n",
      "rolling updates with\n",
      "254–261\n",
      "obsolescence of kubectl \n",
      "rolling-update\n",
      "260–261\n",
      "performing rolling updates \n",
      "with kubectl 256–260\n",
      "running initial version of \n",
      "applications 254–255\n",
      "reconciliation loops 92\n",
      "replacing old pods with new \n",
      "pods by scaling 259–260\n",
      "responding to deleted pods 95\n",
      "responding to node \n",
      "failures 97–98\n",
      "scaling by editing definitions\n",
      "102–103\n",
      "scaling up 102\n",
      "vs. ReplicaSets 105\n",
      "vs. StatefulSets 284–285\n",
      "repo files, adding to yum package \n",
      "manager 544\n",
      "requests\n",
      "for pods per namespace, setting \n",
      "defaults 421–425\n",
      "from clients, handling 492–497\n",
      "preventing broken client con-\n",
      "nections when pod shuts \n",
      "down 493–497\n",
      "preventing broken client con-\n",
      "nections when pod starts \n",
      "up 492–493\n",
      "modifying resources in, with \n",
      "admission control \n",
      "plugins 317\n",
      "sending to pods 66–67\n",
      "connecting to pods through \n",
      "port forwarders 67\n",
      "forwarding local network \n",
      "port to port in pod 67\n",
      "See also CPU requests\n",
      "requiredDropCapabilities 394–395\n",
      "requiredDuringScheduling 464, \n",
      "475\n",
      "rescaling automatically 444–445\n",
      "rescheduling\n",
      "dead pods 482–483\n",
      "partially dead pods 482–483\n",
      "resource limits, creating pods \n",
      "with 412–413\n",
      "resource metric types, scaling \n",
      "based on 449\n",
      "resource requests\n",
      "automatically configuring 451\n",
      "creating pods with 405–406\n",
      "default, applying 424–425\n",
      "defining custom resources\n",
      "411–412\n",
      "effect on scheduling 406–410\n",
      "creating pods that don’t fit on \n",
      "any node 408–409\n",
      "freeing resources to schedule \n",
      "pods 410\n",
      "inspecting node capacity\n",
      "407–408\n",
      "pods not being scheduled\n",
      "409–410\n",
      "Scheduler determining \n",
      "whether pod can fit on \n",
      "node 406\n",
      "Scheduler using pod requests \n",
      "when selecting best \n",
      "nodes 407\n",
      "for pod containers 411–412\n",
      "modifying while pod is \n",
      "running 451–452\n",
      "resourceFieldRef 233\n",
      "resourceNames field 358\n",
      "ResourceQuota resources\n",
      "425–427\n",
      "creating for CPUs 425–426\n",
      "creating for memory 425–426\n",
      "creating LimitRange along with \n",
      "ResourceQuota 427\n",
      "inspecting Quota and Quota \n",
      "usage 426\n",
      "resources\n",
      "accessing in specific \n",
      "namespaces 367–370\n",
      "allowing modification of \n",
      "resources with edit \n",
      "ClusterRole 372\n",
      "analyzing statistics for historical \n",
      "consumption of 432–434\n",
      "Grafana 432\n",
      "InfluxDB 432\n",
      "running Grafana in \n",
      "clusters 433\n",
      "running InfluxDB in \n",
      "clusters 433\n",
      "using information shown in \n",
      "charts 434\n",
      "analyzing usage with \n",
      "Grafana 433–434\n",
      "auto-deploying manifests\n",
      "504–505\n",
      "available in namespaces, \n",
      "limiting 425–429\n",
      "limiting objects that can be \n",
      "created 427–428\n",
      "ResourceQuota \n",
      "resources 425–427\n",
      "specifying quotas for per-\n",
      "sistent storage 427\n",
      "specifying quotas for specific \n",
      "pod states 429\n",
      "specifying quotas for specific \n",
      "QoS classes 429\n",
      "available to containers, \n",
      "limiting 412–416\n",
      "exceeding limits 414–415\n",
      "limits as seen by applications \n",
      "in containers 415–416\n",
      "cluster-level, allowing access \n",
      "to 362–365\n",
      "collecting usage 430–432\n",
      "displaying CPU usage for \n",
      "cluster nodes 431\n",
      "displaying CPU usage for \n",
      "individual pods 431–432\n",
      "displaying memory usage for \n",
      "cluster nodes 431\n",
      "displaying memory usage for \n",
      "individual pods 431–432\n",
      "enabling Heapster 431\n",
      "custom\n",
      "automating with custom \n",
      "controllers 513–517\n",
      "creating instances of\n",
      "511–512\n",
      "retrieving instances of 512\n",
      "deleting in namespace 82\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 619, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "587\n",
      "resources (continued)\n",
      "deploying through Helm\n",
      "531–533\n",
      "describing through \n",
      "annotations 498\n",
      "federated\n",
      "functions of 559\n",
      "versions of 558\n",
      "freeing to schedule pods 410\n",
      "in Red Hat OpenShift Con-\n",
      "tainer platform 527–528\n",
      "limiting consumption of 11–12\n",
      "modifying in requests with \n",
      "admission control \n",
      "plugins 317\n",
      "notifying clients of \n",
      "changes 318–319\n",
      "of pods, monitoring usage \n",
      "of 430–434\n",
      "read-only access to 372\n",
      "requesting for pod \n",
      "containers 405–412\n",
      "creating pods with resource \n",
      "requests 405–406\n",
      "defining custom \n",
      "resources 411–412\n",
      "effect of CPU requests on \n",
      "CPU time sharing 411\n",
      "effect of resource requests on \n",
      "scheduling 406–410\n",
      "retrieving usage 430–432\n",
      "displaying CPU usage\n",
      "431–432\n",
      "displaying memory \n",
      "usage 431–432\n",
      "enabling Heapster 431\n",
      "setting hard limits for 412–413\n",
      "creating pod with resource \n",
      "limits 412–413\n",
      "overcommitting limits 413\n",
      "storing in etcd 313–314\n",
      "storing persistently 318\n",
      "storing statistics for historical \n",
      "consumption of 432–434\n",
      "Grafana 432\n",
      "InfluxDB 432\n",
      "running Grafana in \n",
      "clusters 433\n",
      "running InfluxDB in \n",
      "clusters 433\n",
      "using information shown in \n",
      "charts 434\n",
      "using namespaces to group\n",
      "76–80\n",
      "creating namespaces 78–79\n",
      "discovering other name-\n",
      "spaces and their pods\n",
      "77–78\n",
      "isolation provided by 79–80\n",
      "managing objects in other \n",
      "namespaces 79\n",
      "namespaces, why needed 77\n",
      "validating 318\n",
      "versioning manifests 504–505\n",
      "See also computational resources\n",
      "REST API 234–238\n",
      "accessing API server through \n",
      "kubectl proxy 234–235\n",
      "exploring batch API group \n",
      "REST endpoints 236–237\n",
      "exploring Kubernetes API \n",
      "through kubectl proxy\n",
      "235–236\n",
      "listing job instances in \n",
      "clusters 237–238\n",
      "retrieving job instances by \n",
      "name 238\n",
      "--restart=Never option 446\n",
      "restartPolicy property 113\n",
      "restarts, of containers 480–482\n",
      "RESTful (REpresentational State \n",
      "Transfer) APIs 4\n",
      "resuming rollouts 274\n",
      "retrieving\n",
      "instances of custom \n",
      "resources 512\n",
      "job instances by name 238\n",
      "resource usages 430–432\n",
      "displaying CPU usage for \n",
      "cluster nodes 431\n",
      "displaying CPU usage for \n",
      "individual pods 431–432\n",
      "displaying memory usage for \n",
      "cluster nodes 431\n",
      "displaying memory usage for \n",
      "individual pods 431–432\n",
      "enabling Heapster 431\n",
      "retry loops, implementing in live-\n",
      "ness probes 90\n",
      "revisionHistoryLimit property 270\n",
      "revisions, rolling back Deployment \n",
      "to 270\n",
      "RHEL (Red Hat Enterprise \n",
      "Linux) 12\n",
      "rkt container system\n",
      "configuring Kubernetes to \n",
      "use 552\n",
      "replacing Docker with 552–555\n",
      "using with Minikube 553–555\n",
      "inspecting running contain-\n",
      "ers in Minikube VM\n",
      "553–554\n",
      "listing container images\n",
      "554–555\n",
      "running pods 553\n",
      "--rm option 446\n",
      "Role resources\n",
      "binding to service \n",
      "accounts 359–360\n",
      "creating 357–359\n",
      "using 358–359\n",
      "role-based access control. See \n",
      "RBAC\n",
      "RoleBindings\n",
      "combining with ClusterRole-\n",
      "Bindings 370–371\n",
      "combining with ClusterRoles\n",
      "370–371\n",
      "combining with Roles 370–371\n",
      "including service accounts from \n",
      "other namespaces in 361\n",
      "using 358–359\n",
      "rolling back deployments\n",
      "268–270\n",
      "creating application \n",
      "versions 268\n",
      "deploying versions 269\n",
      "displaying deployment rollout \n",
      "history 270\n",
      "to specific Deployment \n",
      "revision 270\n",
      "undoing rollouts 269\n",
      "rolling updates 253\n",
      "performing automatically \n",
      "with Replication-\n",
      "Controller 254–261\n",
      "obsolescence of kubectl \n",
      "rolling-update 260–261\n",
      "running initial version of \n",
      "applications 254–255\n",
      "performing with kubectl\n",
      "256–260\n",
      "replacing old pods with new \n",
      "pods by scaling two \n",
      "ReplicationControllers\n",
      "259–260\n",
      "steps performed before roll-\n",
      "ing update commences\n",
      "258–259\n",
      "slowing 265\n",
      "triggering 265–267\n",
      "rolling-update command, in \n",
      "kubectl 260–261\n",
      "rollouts\n",
      "blocking 274–278\n",
      "configuring deadlines for \n",
      "rollouts 278\n",
      "defining readiness probes to \n",
      "prevent rollouts 275\n",
      "minReadySeconds 274–275\n",
      "preventing rollouts with read-\n",
      "iness probes 277–278\n",
      "updating deployments with \n",
      "kubectl apply 276–277\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 620, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "588\n",
      "rollouts (continued)\n",
      "configuring deadlines for 278\n",
      "controlling rate of 271–273\n",
      "maxSurge property 271–272\n",
      "maxUnavailable property\n",
      "271–273\n",
      "of deployments\n",
      "displaying history of 270\n",
      "displaying status of 263\n",
      "pausing 273–274\n",
      "preventing\n",
      "by using pause feature 274\n",
      "with readiness probes\n",
      "277–278\n",
      "resuming 274\n",
      "undoing 269\n",
      "root, preventing containers from \n",
      "running as 382\n",
      "Routes, exposing services exter-\n",
      "nally with 530\n",
      "rpi-cluster context 538\n",
      "rpi-foo context 538\n",
      "run command 27, 42–43, 47\n",
      "runAsUser fields, using mustRun-\n",
      "AsNonRoot rules in 394\n",
      "runAsUser policies 392–394\n",
      "deploying pods 393\n",
      "using MustRunAs rules 392–393\n",
      "using MustRunAsNonRoot rules \n",
      "in runAsUser fields 394\n",
      "runAsUser property 381, 393\n",
      "runtimes. See container runtimes\n",
      "S\n",
      "scaled resources, updating replica \n",
      "count on 440\n",
      "scale-downs 456\n",
      "of clusters, limiting service dis-\n",
      "ruption during 454–456\n",
      "to zero replicas 450–451\n",
      "scaleIO volume 163\n",
      "scale-out, results of 49–50\n",
      "scale-ups 456\n",
      "of Deployments, with \n",
      "Autoscaler 446–447\n",
      "triggering 445–446\n",
      "scaling\n",
      "applications, horizontally\n",
      "48–50\n",
      "automatic 23\n",
      "based on CPU utilization\n",
      "441–447\n",
      "automatic rescale events\n",
      "444–445\n",
      "creating Horizontal pod \n",
      "Autoscaler based on \n",
      "CPU usage 442–443\n",
      "maximum rate of scaling 447\n",
      "modifying target metric val-\n",
      "ues on existing HPA \n",
      "objects 447\n",
      "triggering scale-ups 445–446\n",
      "using Autoscaler to scale up \n",
      "Deployments 446–447\n",
      "based on custom metrics\n",
      "448–450\n",
      "Object metric types 449–450\n",
      "pods metric types 449\n",
      "resource metric types 449\n",
      "based on memory \n",
      "consumption 448\n",
      "cluster nodes horizontally\n",
      "452–456\n",
      "Cluster Autoscaler 452–453\n",
      "enabling Cluster Autoscaler\n",
      "454\n",
      "limiting service disruption \n",
      "during cluster scale-\n",
      "down 454–456\n",
      "down to zero replicas 450–451\n",
      "horizontal autoscaling of \n",
      "pods 438–451\n",
      "autoscaling process 438–441\n",
      "metrics appropriate for \n",
      "autoscaling 450\n",
      "Job resource 116\n",
      "maximum rate of 447\n",
      "microservices 4\n",
      "number of copies 21\n",
      "pods horizontally 102–103\n",
      "declarative approach to \n",
      "scaling 103\n",
      "scaling down with kubectl \n",
      "scale command 103\n",
      "scaling ReplicationControl-\n",
      "ler by editing \n",
      "definitions 102–103\n",
      "scaling up Replication-\n",
      "Controller 102\n",
      "ReplicationControllers\n",
      "259–260\n",
      "splitting into multiple pods \n",
      "for 59\n",
      "StatefulSets 287, 298\n",
      "vertical autoscaling of \n",
      "pods 451–452\n",
      "automatically configuring \n",
      "resource requests 451\n",
      "modifying resource requests \n",
      "while pod is running\n",
      "451–452\n",
      "See also autoscaling\n",
      "scaling out 3\n",
      "scaling up 3\n",
      "schedule, configuring 117\n",
      "Scheduler 319–321\n",
      "advanced scheduling of \n",
      "pods 321\n",
      "assigning nodes to newly cre-\n",
      "ated pods 332\n",
      "default scheduling algorithm\n",
      "319\n",
      "ensuring high availability \n",
      "of 343–344\n",
      "finding acceptable nodes 320\n",
      "selecting best node for \n",
      "pod 320–321\n",
      "using multiple schedulers 321\n",
      "using pod affinity rules \n",
      "with 470–471\n",
      "using pod requests when select-\n",
      "ing best nodes 407\n",
      "using to determine whether pod \n",
      "can fit on node 406\n",
      "scheduling 44, 457–476\n",
      "co-locating pods with pod \n",
      "affinity 468–476\n",
      "deploying pods 471–472\n",
      "expressing podAffinity pref-\n",
      "erences instead of hard \n",
      "requirements 472–473\n",
      "using inter-pod affinity to \n",
      "deploy pods on same \n",
      "node 468–471\n",
      "default 319\n",
      "effect of resource requests \n",
      "on 406–410\n",
      "creating pods that don’t fit on \n",
      "any node 408–409\n",
      "freeing resources to schedule \n",
      "pods 410\n",
      "inspecting node \n",
      "capacity 407–408\n",
      "pod not being \n",
      "scheduled 409–410\n",
      "Scheduler determining \n",
      "whether pod can fit on \n",
      "node 406\n",
      "Scheduler using pod requests \n",
      "when selecting best \n",
      "nodes 407\n",
      "jobs 116–118\n",
      "creating CronJob 116–117\n",
      "overview 117\n",
      "pods\n",
      "away from each other with \n",
      "pod anti-affinity 474–476\n",
      "to specific nodes 74–75\n",
      "using labels to constrain\n",
      "73–75\n",
      "using selectors to \n",
      "constrain 73–75\n",
      "using multiple 321\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 621, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "589\n",
      "scheduling (continued)\n",
      "using node affinity to attract \n",
      "pods to nodes 462–468\n",
      "comparing node affinity to \n",
      "node selectors 462–463\n",
      "examining default node \n",
      "labels 462–463\n",
      "prioritizing nodes when sched-\n",
      "uling pods 465–468\n",
      "specifying hard node affinity \n",
      "rules 463–465\n",
      "using taints during 461–462\n",
      "using taints to repel pods from \n",
      "nodes 457–462\n",
      "adding custom taints to \n",
      "node 460\n",
      "taints, overview 458–460\n",
      "using taints 461–462\n",
      "using tolerations during\n",
      "461–462\n",
      "using tolerations to repel pods \n",
      "from nodes 457–462\n",
      "adding tolerations to \n",
      "pods 460–461\n",
      "tolerations, overview\n",
      "458–460\n",
      "using tolerations 461–462\n",
      "See also scheduler\n",
      "secret volume 163\n",
      "Secrets\n",
      "creating 216, 223\n",
      "default token 214–215\n",
      "exposing entries through envi-\n",
      "ronment variables 221–222\n",
      "image pull 351\n",
      "mountable 350–351\n",
      "overview 214\n",
      "reading entries in pods 218\n",
      "using for binary data 217\n",
      "using in pods 218, 222\n",
      "modifying fortune-config \n",
      "config map to enable \n",
      "HTTPS 218–219\n",
      "mounting fortune-https \n",
      "Secret in pods 219–220\n",
      "Secret volumes stored in \n",
      "memory 221\n",
      "verifying Nginx is using cert \n",
      "and key from Secret 221\n",
      "using newly created in client \n",
      "pods 525–526\n",
      "using to pass sensitive data to \n",
      "containers 213, 223\n",
      "default token Secrets\n",
      "214–215\n",
      "image pull Secrets 222–223\n",
      "versus ConfigMaps 217–218\n",
      "See also image pull secrets\n",
      "securing\n",
      "cluster nodes 375–403\n",
      "configuring container secu-\n",
      "rity contexts 380–389\n",
      "isolating pod networks\n",
      "399–402\n",
      "restricting use of security-\n",
      "related features in \n",
      "pods 389–399\n",
      "using host node namespaces \n",
      "in pods 376–380\n",
      "clusters with RBAC 353–373\n",
      "binding roles to service \n",
      "accounts 359–360\n",
      "default ClusterRole-\n",
      "Bindings 371–373\n",
      "default ClusterRoles\n",
      "371–373\n",
      "granting authorization \n",
      "permissions 373\n",
      "including service accounts \n",
      "from other namespaces \n",
      "in RoleBinding 361\n",
      "RBAC authorization \n",
      "plugins 353–354\n",
      "RBAC resources 355–357\n",
      "using ClusterRole-\n",
      "Bindings 362–371\n",
      "using ClusterRoles 362–371\n",
      "using RoleBindings 358–359\n",
      "using Roles 358–359\n",
      "networks 375–403\n",
      "configuring container secu-\n",
      "rity contexts 380–389\n",
      "isolating pod networks\n",
      "399–402\n",
      "restricting use of security-\n",
      "related features in \n",
      "pods 389–399\n",
      "using host node namespaces \n",
      "in pods 376–380\n",
      "security contexts\n",
      "of containers 380–389\n",
      "running pods without \n",
      "specifying 381\n",
      "setting options at pod level 387\n",
      "securityContext property 380, \n",
      "383, 385, 387\n",
      "securityContext.capabilities \n",
      "field 394\n",
      "securityContext.capabilities.drop \n",
      "property 386\n",
      "securityContext.readOnlyRoot-\n",
      "Filesystem property 386\n",
      "securityContext.runAsUser \n",
      "property 381\n",
      "security-enhanced Linux. See \n",
      "SELinux\n",
      "security-related features, restrict-\n",
      "ing use of in pods 389–399\n",
      "assigning different PodSecurity-\n",
      "Policies to different \n",
      "groups 396–399\n",
      "assigning different PodSecurity-\n",
      "Policies to different \n",
      "users 396–399\n",
      "configuring allowed \n",
      "capabilities 394–395\n",
      "configuring default \n",
      "capabilities 394–395\n",
      "configuring disallowed \n",
      "capabilities 394–395\n",
      "constraining types of volumes \n",
      "pods can use 395\n",
      "fsGroup policies 392–394\n",
      "PodSecurityPolicy \n",
      "resources 389–392\n",
      "runAsUser policies 392–394\n",
      "supplementalGroups \n",
      "policies 392–394\n",
      "selecting start-up disks 541\n",
      "selector property 106\n",
      "selector.matchLabels 106\n",
      "selectors\n",
      "constraining pod scheduling \n",
      "with 73–75\n",
      "categorizing worker nodes \n",
      "with labels 74\n",
      "scheduling pods to specific \n",
      "nodes 74–75\n",
      "creating endpoints resource \n",
      "for services without\n",
      "133–134\n",
      "creating services without\n",
      "132–133\n",
      "SelectorSpreadPriority \n",
      "function 468\n",
      "self-healing 22–23\n",
      "SELinux (security-enhanced \n",
      "Linux)\n",
      "disabling 544\n",
      "overview 380\n",
      "semantics, at-most-one 290\n",
      "server pods, allowing some pods \n",
      "in namespaces to connect \n",
      "to 400\n",
      "servers 346–374\n",
      "API, connecting to 503\n",
      "authentication of 346–353\n",
      "groups 346–348\n",
      "service accounts 348–353\n",
      "users 347–348\n",
      "securing clusters with \n",
      "RBAC 353–373\n",
      "binding roles to service \n",
      "accounts 359–360\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 622, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "590\n",
      "servers (continued)\n",
      "default ClusterRoleBindings\n",
      "371–373\n",
      "default ClusterRoles 371–373\n",
      "granting authorization \n",
      "permissions 373\n",
      "including service accounts \n",
      "from other namespaces \n",
      "in RoleBinding 361\n",
      "RBAC authorization \n",
      "plugins 353–354\n",
      "RBAC resources 355–357\n",
      "using ClusterRoleBindings\n",
      "362–371\n",
      "using ClusterRoles 362–371\n",
      "using RoleBindings 358–359\n",
      "using Roles 358–359\n",
      "using custom service account \n",
      "tokens to communicate \n",
      "with 352–353\n",
      "service accounts 348–349\n",
      "assigning to pods 351–353\n",
      "binding Roles to 359–360\n",
      "creating 349–351\n",
      "creating for each pod 373\n",
      "custom\n",
      "creating pods that use\n",
      "351–352\n",
      "using tokens to communi-\n",
      "cate with API servers\n",
      "352–353\n",
      "image pull Secrets 351\n",
      "including from other name-\n",
      "spaces in RoleBinding 361\n",
      "mountable Secrets 350–351\n",
      "ServiceAccount resources\n",
      "348–349\n",
      "tying into authorizations 349\n",
      "service brokers 522–523\n",
      "listing available services in \n",
      "clusters 523\n",
      "registering in Service \n",
      "Catalog 522–523\n",
      "Service Catalog 519–527\n",
      "benefits of using 526–527\n",
      "Controller Manager 521\n",
      "deprovisioning instances 526\n",
      "OpenServiceBroker API\n",
      "522–523\n",
      "listing available services in \n",
      "clusters 523\n",
      "overview 522\n",
      "overview 520–521\n",
      "provisioning services 524–526\n",
      "binding service instances\n",
      "525\n",
      "provisioning service \n",
      "instances 524–525\n",
      "using newly created Secret in \n",
      "client pods 525–526\n",
      "registering service brokers \n",
      "in 522–523\n",
      "service brokers 522–523\n",
      "Service Catalog API server 521\n",
      "unbinding instances 526\n",
      "using services 524–526\n",
      "Service controllers 324\n",
      "service endpoints\n",
      "manually configuring 132–134\n",
      "creating endpoints resource \n",
      "for services without \n",
      "selectors 133–134\n",
      "creating services without \n",
      "selectors 132–133\n",
      "overview 131–132\n",
      "Service Proxy 327–328\n",
      "ServiceAccount resources 348–349\n",
      "SERVICE_ACCOUNT \n",
      "variable 228\n",
      "serviceAccountName \n",
      "property 373\n",
      "SERVICE_HOST variable 129\n",
      "SERVICE_PORT variable 129\n",
      "service-reader role 359\n",
      "services 47, 120–158\n",
      "accessing pods through 264\n",
      "accessing through external \n",
      "IP 46–47\n",
      "accessing through Ingress 145\n",
      "accessing pods through \n",
      "Ingress 145\n",
      "configuring host in Ingress \n",
      "pointing to Ingress IP \n",
      "addresses 145\n",
      "how Ingresses work 145\n",
      "obtaining IP address of \n",
      "Ingress 145\n",
      "available in clusters, listing 523\n",
      "backend, connecting to 502\n",
      "binding instances 525\n",
      "cluster-internal, connecting \n",
      "through API servers 299\n",
      "configuring for kubectl 41\n",
      "configuring session affinity \n",
      "on 126\n",
      "connecting to, through load \n",
      "balancers 139–141\n",
      "creating 45, 122–128\n",
      "remotely executing com-\n",
      "mands in running \n",
      "containers 124–126\n",
      "through kubectl expose \n",
      "command 123\n",
      "through YAML \n",
      "descriptors 123\n",
      "using named ports 127–128\n",
      "creating endpoints resource \n",
      "without selectors 133–134\n",
      "creating without selectors\n",
      "132–133\n",
      "dedicated, using for each pod \n",
      "instance 283–284\n",
      "discovering 128–131\n",
      "connecting to service \n",
      "through its FQDN 130\n",
      "pinging service IP 131\n",
      "running shell in pod \n",
      "containers 130–131\n",
      "through DNS 129\n",
      "through environment \n",
      "variables 128–129\n",
      "disruptions of, limiting during \n",
      "cluster scale-down 454–456\n",
      "examining new 124\n",
      "examples of 121–122\n",
      "exposing applications through, \n",
      "using YAML file 255\n",
      "exposing externally through \n",
      "Ingress resources 142–149\n",
      "benefits of using 142–143\n",
      "configuring Ingress to handle \n",
      "TLS traffic 147–149\n",
      "creating Ingress resources 144\n",
      "using Ingress controllers\n",
      "143–144\n",
      "exposing externally using \n",
      "Routes 530\n",
      "exposing multiple ports in \n",
      "same 126–127\n",
      "exposing multiple through \n",
      "same Ingress 146–147\n",
      "mapping different services to \n",
      "different hosts 147\n",
      "mapping different services to \n",
      "different paths of same \n",
      "host 146\n",
      "exposing Pet pods \n",
      "through 298–299\n",
      "exposing through external load \n",
      "balancers 138–141\n",
      "connecting to services \n",
      "through load balancers\n",
      "139–141\n",
      "creating LoadBalancer \n",
      "services 139\n",
      "exposing to external \n",
      "clients 134–142\n",
      "external connections 141–142\n",
      "using NodePort services\n",
      "135–138\n",
      "governing\n",
      "creating 292–294\n",
      "overview 285–287\n",
      "implementing 338–340\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 623, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "591\n",
      "services (continued)\n",
      "listing 46\n",
      "listing from pods 357\n",
      "living outside cluster, connect-\n",
      "ing to 131–134\n",
      "overview 121–131\n",
      "provisioning 524–526\n",
      "requests hitting three pods 50\n",
      "running applications through, \n",
      "using YAML file 255\n",
      "signaling when pods ready to \n",
      "accept connections\n",
      "149–153\n",
      "testing from within cluster 124\n",
      "troubleshooting 156\n",
      "using 524–526\n",
      "why needed 48\n",
      "See also external services\n",
      "sessionAffinity property 126\n",
      "sessions, configuring affinity on \n",
      "services 126\n",
      "set-context command 538\n",
      "share-type label 467\n",
      "sharing\n",
      "data between containers with \n",
      "volumes 163–169\n",
      "using emptyDir volume\n",
      "163–166\n",
      "using Git repository as start-\n",
      "ing point for volume\n",
      "166–169\n",
      "volumes when containers \n",
      "running as different \n",
      "users 387–389\n",
      "shell forms, versus exec forms\n",
      "193–194\n",
      "shells\n",
      "running in containers 130–131\n",
      "running inside existing \n",
      "containers 33\n",
      "shutdown handlers, implement-\n",
      "ing in applications 490–491\n",
      "shutdown logic, pods 153\n",
      "shutdowns\n",
      "critical, replacing dedicated \n",
      "shut-down procedure \n",
      "pods 491\n",
      "dedicated pods, replacing criti-\n",
      "cal shut-down procedures \n",
      "with 491\n",
      "of pods 489–491\n",
      "implementing proper shut-\n",
      "down handler in \n",
      "applications 490–491\n",
      "replacing critical shut-down \n",
      "procedures with dedi-\n",
      "cated shut-down proce-\n",
      "dure pods 491\n",
      "specifying termination grace \n",
      "periods 490\n",
      "shutting down VMs 545\n",
      "sidecar container 60, 168\n",
      "SIG (Special Interest Group) 448\n",
      "SIGKILL 89\n",
      "signaling, when pods ready to \n",
      "accept connections 149–153\n",
      "SIGTERM signal 487–491, 493, \n",
      "495\n",
      "simulating node disconnection \n",
      "from network 304–306\n",
      "checking node status as seen by \n",
      "Kubernetes master 305\n",
      "pods with unknown status\n",
      "305–306\n",
      "shutting down node network \n",
      "adapters 304–305\n",
      "single-node clusters, local 37–38\n",
      "sleep-interval entry 221\n",
      "slowing rolling updates 265\n",
      "SNAT (Source Network Address \n",
      "Translation) 142\n",
      "spec section 75\n",
      "spec.containers.ports field 377\n",
      "specifications, referring to \n",
      "container-level metadata \n",
      "in 233\n",
      "spec.initContainers field 484\n",
      "spec.replicas field 102–103\n",
      "spec.template.spec.contain-\n",
      "ers.image attribute 303\n",
      "spinning up, new pods 252–253\n",
      "splitting\n",
      "applications into \n",
      "microservices 3–4\n",
      "into multiple pods 59\n",
      "SRV records 300\n",
      "SSD (Solid-State Drive) 109\n",
      "ssd storage class 427\n",
      "startingDeadlineSeconds \n",
      "field 118\n",
      "start-up disks, selecting 541\n",
      "stateful pods 308\n",
      "examining 294–295\n",
      "overview 284\n",
      "replicating 281–284\n",
      "providing stable identity for \n",
      "pods 282–284\n",
      "running multiple replicas \n",
      "with separate storage for \n",
      "each 281–282\n",
      "StatefulSet controllers 324\n",
      "StatefulSet resources 280–308\n",
      "creating 294\n",
      "creating applications 290–291\n",
      "creating container images\n",
      "290–291\n",
      "deploying applications \n",
      "through 291–295\n",
      "creating governing \n",
      "services 292–294\n",
      "creating persistent \n",
      "volumes 291–292\n",
      "examining PersistentVolume-\n",
      "Claims 295\n",
      "examining stateful pods\n",
      "294–295\n",
      "discovering peers in 299–304\n",
      "clustered data store 303–304\n",
      "implementing peer discovery \n",
      "through DNS 301–302\n",
      "SRV records, overview 300\n",
      "guarantees 289–290\n",
      "at-most-one semantics 290\n",
      "implications of stable \n",
      "identity 289–290\n",
      "implications of stable \n",
      "storage 289–290\n",
      "node failures and 304–307\n",
      "deleting pods manually\n",
      "306–307\n",
      "simulating node disconnec-\n",
      "tion from network\n",
      "304–306\n",
      "overview 284–290\n",
      "providing stable dedicated \n",
      "storage to pets 287–289\n",
      "providing stable network \n",
      "identities 285–287\n",
      "replicating stateful pods\n",
      "281–284\n",
      "providing stable identity for \n",
      "pods 282–284\n",
      "running multiple replicas \n",
      "with separate storage for \n",
      "each 281–282\n",
      "scaling 287, 298\n",
      "updating 302–303\n",
      "using 290–295\n",
      "vs. ReplicaSets 284–285\n",
      "vs. ReplicationControllers\n",
      "284–285\n",
      "with Pet pods 295–299\n",
      "communicating with pods \n",
      "through API servers\n",
      "295–297\n",
      "connecting to cluster-inter-\n",
      "nal services through API \n",
      "servers 299\n",
      "deleting Pet pods 297–298\n",
      "exposing Pet pods through \n",
      "services 298–299\n",
      "states, visualizing new 50\n",
      "statistics, of historical resource \n",
      "consumption 432–434\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 624, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "592\n",
      "status\n",
      "of nodes 305\n",
      "pods with unknown 305–306\n",
      "status attribute 138\n",
      "STATUS column 183\n",
      "Status section 63\n",
      "status.certificate field 148\n",
      "status.qosClass field 419\n",
      "stopping\n",
      "containers 34–35\n",
      "pods 80, 82\n",
      "storage\n",
      "defining available types \n",
      "through StorageClass \n",
      "resources 185\n",
      "providing to pets 287–289\n",
      "creating PersistentVolume-\n",
      "Claims 288\n",
      "deleting PersistentVolume-\n",
      "Claims 288\n",
      "reattaching Persistent-\n",
      "VolumeClaims to new \n",
      "instances of same \n",
      "pod 289\n",
      "using pod templates with \n",
      "volume claim templates\n",
      "288\n",
      "running multiple replicas with \n",
      "separate storage for \n",
      "each 281–282\n",
      "creating pods manually 281\n",
      "using multiple directories in \n",
      "same volume 282\n",
      "using one ReplicaSet per pod \n",
      "instance 281–282\n",
      "stable, implications of\n",
      "289–290\n",
      "See also persistent storage\n",
      "storage classes\n",
      "creating PersistentVolume-\n",
      "Claims without specifying\n",
      "188–189\n",
      "default 188\n",
      "dynamic provisioning without \n",
      "specifying 187–189\n",
      "listing 187–188\n",
      "requesting in PersistentVolume-\n",
      "Claims 185–187\n",
      "creating PersistentVolume-\n",
      "Claim definition \n",
      "requesting specific stor-\n",
      "age class 185–186\n",
      "examining created Persistent-\n",
      "VolumeClaims 186–187\n",
      "examining dynamically provi-\n",
      "sioned Persistent-\n",
      "Volumes 186–187\n",
      "using 187\n",
      "storage technologies\n",
      "decoupling pods from 176–184\n",
      "benefits of using claims 182\n",
      "PersistentVolumeClaims\n",
      "176–177, 179–181\n",
      "PersistentVolumes 176–184\n",
      "using 175\n",
      "storage volumes 160\n",
      "StorageClass resources, defining \n",
      "available storage types \n",
      "through 185\n",
      "storageclass-fast-hostpath.yaml \n",
      "file 187\n",
      "storageClassName attribute\n",
      "188–189\n",
      "storing\n",
      "historical resource consump-\n",
      "tion statistics 432–434\n",
      "analyzing resource usage with \n",
      "Grafana 433–434\n",
      "Grafana 432\n",
      "InfluxDB 432\n",
      "running Grafana in \n",
      "clusters 433\n",
      "running InfluxDB \n",
      "clusters 433\n",
      "using information shown in \n",
      "charts 434\n",
      "resources persistently 318\n",
      "StringData field 218\n",
      "subPath property 210–211\n",
      "subsets, listing through label \n",
      "selectors 71–72\n",
      "listing pods using label \n",
      "selectors 71–72\n",
      "using multiple conditions in \n",
      "label selectors 72\n",
      "supplementalGroups \n",
      "policies 392–394\n",
      "deploying pod with container \n",
      "image with an out-of-range \n",
      "user ID 393\n",
      "using MustRunAs rule 392–393\n",
      "supplementalGroups property\n",
      "388–389\n",
      "Swagger framework 248\n",
      "symlink 213\n",
      "syncHandler field 323\n",
      "sysadmins, role in continuous \n",
      "delivery 7\n",
      "system:authenticated group 348, \n",
      "366\n",
      "system:discovery cluster role\n",
      "373\n",
      "system:serviceaccounts: 348\n",
      "system:unauthenticated \n",
      "group 348, 366\n",
      "SYS_TIME capability 394\n",
      "T\n",
      "tab completion, configuring for \n",
      "kubectl 41–42\n",
      "tagging images\n",
      "overview 497–498\n",
      "under additional tags 35\n",
      "tags 28\n",
      "taints\n",
      "custom, adding to nodes 460\n",
      "effects of 459–460\n",
      "of nodes, displaying 458\n",
      "overview 458–462\n",
      "repelling pods from nodes \n",
      "with 457–462\n",
      "using during scheduling\n",
      "461–462\n",
      "targeting containers 489\n",
      "TCP packets 126\n",
      "TCP Socket 86, 150\n",
      "templates\n",
      "for Job resources 117\n",
      "pods\n",
      "adding readiness probes \n",
      "to 151–152\n",
      "changing 101\n",
      "effect of changing 93\n",
      "using with volume claim \n",
      "templates 288\n",
      "volume claim, using with pod \n",
      "templates 288\n",
      "TERM variable 33\n",
      "terminating processes, providing \n",
      "information about 498–500\n",
      "Terminating scope 429\n",
      "termination grace periods, \n",
      "specifying 490\n",
      "terminationMessagePath field 499\n",
      "terminationMessagePolicy \n",
      "field 500\n",
      "testing services from within \n",
      "clusters 124\n",
      "ThirdPartyResource objects 509\n",
      "time sharing. See CPU time sharing\n",
      "TLS (Transport Layer \n",
      "Security) 147–149\n",
      "tmpfs filesystem 166\n",
      "TOKEN variable 241\n",
      "tokens, of custom service \n",
      "accounts 352–353\n",
      "tolerations\n",
      "adding to pods 460–461\n",
      "of pods, displaying 459\n",
      "overview 458–462\n",
      "repelling pods from nodes \n",
      "with 457–462\n",
      "using during scheduling\n",
      "461–462\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 625, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "593\n",
      "topologyKey 471–473\n",
      "triggering\n",
      "rolling updates 265–267\n",
      "scale-ups 445–446\n",
      "troubleshooting services 156\n",
      "U\n",
      "ubuntu:latest image 164\n",
      "UDP packets 126\n",
      "unbinding instances 526\n",
      "undoing rollouts 269\n",
      "universal resource locators. See \n",
      "URLs\n",
      "unschedulable nodes 109\n",
      "updates. See rolling updates\n",
      "updating\n",
      "annotations 232\n",
      "application config without \n",
      "restarting application\n",
      "211–213\n",
      "applications declaratively using \n",
      "Deployment 261–278\n",
      "blocking rollouts of bad \n",
      "versions 274–278\n",
      "controlling rate of \n",
      "rollout 271–273\n",
      "creating Deployments\n",
      "262–264\n",
      "pausing rollout process\n",
      "273–274\n",
      "rolling back \n",
      "deployments 268–270\n",
      "updating Deployments\n",
      "264–268\n",
      "applications running in \n",
      "pods 251–253\n",
      "deleting old pods 252–253\n",
      "replacing old pods 252\n",
      "spinning up new pods\n",
      "252–253\n",
      "ConfigMap 213\n",
      "Deployment resource 264–268\n",
      "Deployment strategies\n",
      "264–265\n",
      "slowing down rolling \n",
      "updates 265\n",
      "triggering rolling \n",
      "updates 265–267\n",
      "deployments with kubectl \n",
      "apply 276–277\n",
      "files automatically 212–213\n",
      "labels 232\n",
      "replica count on scaled \n",
      "resources 440\n",
      "StatefulSets 302–303\n",
      "URLs (universal resource \n",
      "locators) 365–367\n",
      "--user argument 359\n",
      "user credentials\n",
      "adding 536–537\n",
      "modifying 536–537\n",
      "tying with clusters 537\n",
      "USER directive 381\n",
      "users 347–348\n",
      "assigning PodSecurityPolicies \n",
      "to 396–399\n",
      "creating PodSecurityPolicy \n",
      "allowing privileged con-\n",
      "tainers to be deployed\n",
      "396–397\n",
      "with RBAC 397–398\n",
      "creating additional for \n",
      "kubectl 398\n",
      "creating pods as 398–399\n",
      "in kubeconfig files 536\n",
      "in Red Hat OpenShift Con-\n",
      "tainer platform 528\n",
      "out-of-range IDs 393\n",
      "running containers as \n",
      "specific 381–382\n",
      "sharing volumes when contain-\n",
      "ers running as different \n",
      "users 387–389\n",
      "using kubectl with 537–538\n",
      "UTS Namespace 11\n",
      "V\n",
      "V1 applications, creating 254\n",
      "validating\n",
      "custom objects 517–518\n",
      "resources 318\n",
      "valueFrom field 202\n",
      "values property 107\n",
      "values, referring to environment \n",
      "variables in 198\n",
      "variables. See environment vari-\n",
      "ables\n",
      "VCS (Version Control \n",
      "System) 505\n",
      "VDI (VirtualBox Disk Image)\n",
      "539\n",
      "verifying identity of API \n",
      "servers 240–241\n",
      "versioning\n",
      "container images 28\n",
      "resource manifests 504–505\n",
      "versions\n",
      "creating 268\n",
      "deploying 269\n",
      "of resources, federated 558\n",
      "vertical autoscaling, of pods\n",
      "451–452\n",
      "automatically configuring \n",
      "resource requests 451\n",
      "modifying resource requests \n",
      "while pod is running\n",
      "451–452\n",
      "veth pair 336–337\n",
      "view ClusterRole, allowing read-\n",
      "only access to resources \n",
      "with 372\n",
      "visualizing new states 50\n",
      "VMs (virtual machines) 8, 26\n",
      "cloning 545–547\n",
      "changing hostname on 546\n",
      "configuring name resolution \n",
      "for hosts 546–547\n",
      "comparing to containers 8–10\n",
      "comparing to Docker 14–15\n",
      "configuring network adapters \n",
      "for 540\n",
      "creating 539\n",
      "instead of containers 555\n",
      "shutting down 545\n",
      "See also Minikube VM\n",
      "volume claim templates, using \n",
      "with pod templates 288\n",
      "VOLUME column 186\n",
      "volumeClaimTemplates 293\n",
      "volumes 159–177, 190\n",
      "accessing files on worker node \n",
      "filesystems 169–170\n",
      "examining system pods that \n",
      "use hostPath \n",
      "volumes 170\n",
      "hostPath volume 169–170\n",
      "constraining types of 395\n",
      "decoupling pods from underly-\n",
      "ing storage technologies\n",
      "176–184\n",
      "claims 182\n",
      "PersistentVolumeClaims\n",
      "176–181\n",
      "PersistentVolumes 176–184\n",
      "dynamic provisioning of \n",
      "PersistentVolumes 184–189\n",
      "defining available storage \n",
      "types through Storage-\n",
      "Class resources 185\n",
      "requesting storage class in \n",
      "PersistentVolume-\n",
      "Claims 185–187\n",
      "without specifying storage \n",
      "class 187–189\n",
      "overview 160–163\n",
      "available volumes types\n",
      "162–163\n",
      "examples of 160–162\n",
      "sharing data between containers \n",
      "with 163–169\n",
      "sharing when containers run as \n",
      "different users 387–389\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 626, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "INDEX\n",
      "594\n",
      "volumes (continued)\n",
      "specifications, referring to \n",
      "container-level metadata \n",
      "in 233\n",
      "using AWS elastic block store 174\n",
      "using ConfigMap entries \n",
      "in 207–208\n",
      "using emptyDir 163–166\n",
      "creating pods 164–165\n",
      "in pods 163–164\n",
      "seeing pods in action 165\n",
      "specifying medium for 166\n",
      "using Git repository as starting \n",
      "point for 166–169\n",
      "files in sync with gitRepo \n",
      "volume 168\n",
      "gitRepo volume 169\n",
      "running web server pod serv-\n",
      "ing files from \n",
      "cloned 167\n",
      "sidecar containers 168\n",
      "using gitRepo volume with \n",
      "private Git repositories\n",
      "168\n",
      "using multiple directories in 282\n",
      "using NFS 175\n",
      "using persistent storage 171–175\n",
      "underlying storage 174–175\n",
      "using GCE Persistent Disk in \n",
      "pod volumes 171–174\n",
      "using to preserve data across \n",
      "container restarts 480–482\n",
      "See also gitRepo volume; per-\n",
      "sistent volumes\n",
      "vsphereVolume volume 163\n",
      "W\n",
      "watch command 445\n",
      "web applications, accessing 45–47\n",
      "accessing services through \n",
      "external IP 46–47\n",
      "creating services 45\n",
      "listing services 46\n",
      "web browsers 140\n",
      "web server pods, serving files from \n",
      "cloned Git repository 167\n",
      "WebHook plugin 353\n",
      "WebServer container 162\n",
      "Website controllers 514–515\n",
      "website-crd.yaml file 511\n",
      "worker nodes 18, 43\n",
      "accessing files on 169–170\n",
      "examining system pods that \n",
      "use hostPath \n",
      "volumes 170\n",
      "hostPath volume 169–170\n",
      "categorizing with labels 74\n",
      "components running on 310\n",
      "configuring with \n",
      "kubeadm 549–550\n",
      "worker() method 323\n",
      "Workflow. See Deis Workflow plat-\n",
      "form\n",
      "X\n",
      "-Xmx JVM option 416\n",
      "Y\n",
      "YAML file format\n",
      "creating definitions 110\n",
      "creating descriptors for \n",
      "pods 63–65\n",
      "creating namespaces 78\n",
      "creating pods from \n",
      "descriptors 61–67\n",
      "sending requests to pods\n",
      "66–67\n",
      "using kubectl create to create \n",
      "pods 65\n",
      "viewing application logs\n",
      "65–66\n",
      "creating services through 123\n",
      "examining descriptors of exist-\n",
      "ing pods 61–63\n",
      "exposing applications through \n",
      "services using 255\n",
      "manifests, writing 505–506\n",
      "running applications through \n",
      "services using 255\n",
      "running pods without writing \n",
      "manifest 155\n",
      "yum package manager, adding \n",
      "Kubernetes repo files 544\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n",
      "IN extract_images_per_page\n",
      "page_image_dict\n",
      "{'page': 627, 'img_cnt': 0, 'img_npy_lst': []}\n",
      "Kubernetes resources covered in the book (continued)\n",
      "* Cluster-level resource (not namespaced)\n",
      "** Also in other API versions; listed version is the one used in this book\n",
      "Resource (abbr.) [API version]\n",
      "Description\n",
      "Section\n",
      "Scaling\n",
      "HorizontalPodAutoscaler (hpa) \n",
      "[autoscaling/v2beta1**]\n",
      "Automatically scales number of pod replicas \n",
      "based on CPU usage or another metric\n",
      "15.1\n",
      "PodDisruptionBudget (pdb) \n",
      "[policy/v1beta1]\n",
      "Defines the minimum number of pods that must \n",
      "remain running when evacuating nodes\n",
      "15.3.3\n",
      "Resources\n",
      "LimitRange (limits) [v1]\n",
      "Defines the min, max, default limits, and default \n",
      "requests for pods in a namespace\n",
      "14.4\n",
      "ResourceQuota (quota) [v1]\n",
      "Defines the amount of computational resources \n",
      "available to pods in the namespace\n",
      "14.5\n",
      "Cluster state\n",
      "Node* (no) [v1]\n",
      "Represents a Kubernetes worker node\n",
      "2.2.2\n",
      "Cluster* [federation/v1beta1]\n",
      "A Kubernetes cluster (used in cluster federation)\n",
      "App. D\n",
      "ComponentStatus* (cs) [v1]\n",
      "Status of a Control Plane component\n",
      "11.1.1\n",
      "Event (ev) [v1]\n",
      "A report of something that occurred in the cluster\n",
      "11.2.3\n",
      "Security\n",
      "ServiceAccount (sa) [v1]\n",
      "An account used by apps running in pods\n",
      "12.1.2\n",
      "Role [rbac.authorization.k8s.io/v1]\n",
      "Defines which actions a subject may perform on \n",
      "which resources (per namespace)\n",
      "12.2.3\n",
      "ClusterRole* \n",
      "[rbac.authorization.k8s.io/v1]\n",
      "Like Role, but for cluster-level resources or to \n",
      "grant access to resources across all namespaces\n",
      "12.2.4\n",
      "RoleBinding \n",
      "[rbac.authorization.k8s.io/v1]\n",
      "Defines who can perform the actions defined in a \n",
      "Role or ClusterRole (within a namespace) \n",
      "12.2.3\n",
      "ClusterRoleBinding* \n",
      "[rbac.authorization.k8s.io/v1]\n",
      "Like RoleBinding, but across all namespaces\n",
      "12.2.4\n",
      "PodSecurityPolicy* (psp) \n",
      "[extensions/v1beta1]\n",
      "A cluster-level resource that defines which security-\n",
      "sensitive features pods can use\n",
      "13.3.1\n",
      "NetworkPolicy (netpol) \n",
      "[networking.k8s.io/v1]\n",
      "Isolates the network between pods by specifying \n",
      "which pods can connect to each other\n",
      "13.4\n",
      "CertificateSigningRequest* (csr) \n",
      "[certificates.k8s.io/v1beta1]\n",
      "A request for signing a public key certificate\n",
      "5.4.4\n",
      "Ext.\n",
      "CustomResourceDefinition* (crd) \n",
      "[apiextensions.k8s.io/v1beta1]\n",
      "Defines a custom resource, allowing users to cre-\n",
      "ate instances of the custom resource \n",
      "18.1\n",
      " \n",
      "\n",
      "IN extract_text_tables_images_per_page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN extract_images_per_page\n",
      "width\n",
      "527\n",
      "height\n",
      "879\n",
      "PIX BUFFER SIZE\n",
      "1389699\n",
      "Original IMG_BUFFER_SIZE\n",
      "1389699\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c60140113c0>\n",
      "width\n",
      "527\n",
      "height\n",
      "1902\n",
      "PIX BUFFER SIZE\n",
      "3007062\n",
      "Original IMG_BUFFER_SIZE\n",
      "3007062\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014012bc0>\n",
      "width\n",
      "786\n",
      "height\n",
      "601\n",
      "PIX BUFFER SIZE\n",
      "472386\n",
      "Original IMG_BUFFER_SIZE\n",
      "1417158\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014012bc0>\n",
      "Image too large for Picture\n",
      "width\n",
      "715\n",
      "height\n",
      "448\n",
      "PIX BUFFER SIZE\n",
      "320320\n",
      "Original IMG_BUFFER_SIZE\n",
      "960960\n",
      "Page Image Buffer Size\n",
      "<memory at 0x7c6014012bc0>\n",
      "Image too large for Picture\n",
      "page_image_dict\n",
      "{'page': 628, 'img_cnt': 4, 'img_npy_lst': []}\n",
      "Marko Lukša\n",
      "K\n",
      "ubernetes is Greek for “helmsman,” your guide through \n",
      "unknown waters. The Kubernetes container orchestra-\n",
      "tion system safely manages the structure and ﬂ\n",
      " ow of a \n",
      "distributed application, organizing containers and services for \n",
      "maximum efﬁ\n",
      " ciency. Kubernetes serves as an operating system \n",
      "for your clusters, eliminating the need to factor the underlying \n",
      "network and server infrastructure into your designs.\n",
      "Kubernetes in Action teaches you to use Kubernetes to deploy \n",
      "container-based distributed applications. You’ll start with an \n",
      "overview of Docker and Kubernetes before building your ﬁ\n",
      " rst \n",
      "Kubernetes cluster. You’ll gradually expand your initial \n",
      "application, adding features and deepening your knowledge \n",
      "of Kubernetes architecture and operation. As you navigate \n",
      "this comprehensive guide, you’ll explore high-value topics \n",
      "like monitoring, tuning, and scaling. \n",
      "What’s Inside\n",
      "● Kubernetes’ internals\n",
      "● Deploying containers across a cluster\n",
      "● Securing clusters\n",
      "● Updating applications with zero downtime\n",
      "Written for intermediate software developers with little or no \n",
      "familiarity with Docker or container orchestration systems.\n",
      "Marko Lukša is an engineer at Red Hat working on Kubernetes \n",
      "and OpenShift.\n",
      "To download their free eBook in PDF, ePub, and Kindle formats, \n",
      "owners of this book should visit \n",
      "www.manning.com/books/kubernetes-in-action\n",
      "$59.99 / Can $79.99  [INCLUDING eBOOK]\n",
      "Kubernetes IN ACTION\n",
      "SOFTWARE DEVELOPMENT/OPERATIONS\n",
      "M A N N I N G\n",
      "“\n",
      "Authoritative and \n",
      "exhaustive. In a hands-on \n",
      "style, the author teaches how \n",
      "to manage the complete \n",
      "lifecycle of any distributed \n",
      " and scalable application.”\n",
      " \n",
      "—Antonio Magnaghi, System1\n",
      "“\n",
      "The best parts are the real-\n",
      "world examples. They don’t \n",
      "just apply the concepts, \n",
      "they road test them.”\n",
      " \n",
      "—Paolo Antinori, Red Hat\n",
      "“\n",
      "An in-depth discussion \n",
      "of Kubernetes and related \n",
      " technologies. A must-have!”\n",
      "—Al Krinker, USPTO \n",
      "“\n",
      "The full path to becoming \n",
      "a professional Kubernaut. \n",
      " Fundamental reading.”\n",
      " \n",
      "—Csaba Sári\n",
      "Chimera Entertainment\n",
      "SEE  INSERT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#################################Executing The Function################################################\n",
    "#This url Variable is the input to the program\n",
    "url=\"./PDF_SOURCE/Kubernetes_in_Action.pdf\"\n",
    "#Extraction if Document Name\n",
    "doc_name=url.split('/')[-1]\n",
    "\n",
    "document_dict=extract_text_images_tables(url,download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c1d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len=len(document_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1be155f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDFs That faced Error\n",
    "#url='./PDF_FOLDER/Narasimha_Karumanchi_DS_Algo.pdf' -- error\n",
    "#url=\"./PDF_FOLDER/Kubernetes_in_Action.pdf\"\n",
    "#url=\"./PDF_FOLDER/2022.acl-long.62.pdf\"\n",
    "#This url Variable is the input to the program\n",
    "url=\"./PDF_SOURCE/Kubernetes_in_Action.pdf\"\n",
    "#Extraction if Document Name\n",
    "doc_name=url.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b96a3320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kubernetes_in_Action.pdf'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f27e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Required Libraries Used for Graph Visualization##################################\n",
    "from pyvis.network import Network\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8ece127",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################Initializing the Graph Network##############################\n",
    "G = nx.DiGraph()\n",
    "##########Adding the root Node.Root Node is te Name of the PDF#########################################\n",
    "G.add_node(1, label=doc_name , title=doc_name , color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e2afdf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 3,\n",
       " 'img_cnt': 0,\n",
       " 'img_flag': 0,\n",
       " 'img_npy_lst': [],\n",
       " 'text': 'Kubernetes in Action\\n \\nwww.allitebooks.com\\n',\n",
       " 'tables_flag': 0,\n",
       " 'tables': []}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6958a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def serialize_to_json(lst_dict):\n",
    "    # convert into json\n",
    "    json_serialized_output = json.dumps(lst_dict, indent=2)\n",
    "    return json_serialized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4bf984b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_with_no_tables=[]\n",
    "pages_with_no_images=[]\n",
    "\n",
    "############Using for loop for each page the operations will go on till the final page############################\n",
    "for i in range(2,doc_len):\n",
    "    \n",
    "    #Adding the Page Number Nodes and connecting then to the root Node that is the document_name name node\n",
    "    #Form the page number from the dictionary metadata 'page'\n",
    "    page_idx=document_dict[i]['page']\n",
    "    page_num=\"Page_\"+ str(document_dict[i]['page'])\n",
    "    G.add_node(page_idx, label=page_num , title=page_num,color=\"blue\")\n",
    "    G.add_edge(1,page_idx)\n",
    "    \n",
    "    ##Under each Page we will add the different types of extracted entities like tables,text,images\n",
    "    #We will Create and add the text node and attach it with the page_num node.\n",
    "    text_idx='text_'+str(i)\n",
    "    G.add_node(text_idx, label=document_dict[i]['text'] , title=document_dict[i]['text'],color=\"violet\")\n",
    "    G.add_edge(page_idx,text_idx)\n",
    "    \n",
    "    #We will Create and add the table_flag node which will indicate if a page has tables\n",
    "    #If a Page has Tables it will be labelled as 1 else as 0.This ca be used a fileter to make search faster \n",
    "    #Through PDf.Hence this metadata is added. \n",
    "    table_flag_idx='table_flag'+str(i)\n",
    "    G.add_node(table_flag_idx, label= document_dict[i]['tables_flag'] , title=str(document_dict[i]['tables_flag']),color=\"green\")\n",
    "    G.add_edge(page_idx,table_flag_idx)\n",
    "    \n",
    "\n",
    "    if document_dict[i]['tables_flag']==0:\n",
    "        pages_with_no_tables.append(page_idx)\n",
    "    else:\n",
    "        table_flag='table_flag'+str(i)\n",
    "        \n",
    "        ###Serialize DataFrame Object to json before we can visualize on pyviz###### \n",
    "        serialized_json=serialize_to_json(document_dict[i]['tables'])\n",
    "        \n",
    "        G.add_node(table_flag, label= serialized_json , title=serialized_json,color=\"green\")\n",
    "        G.add_edge(table_flag_idx,table_flag)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f51ca0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_dict[i]['tables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3a09915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "nx1.html\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type DataFrame is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m nt\u001b[38;5;241m.\u001b[39mfrom_nx(G)\n\u001b[1;32m      4\u001b[0m nt\u001b[38;5;241m.\u001b[39mtoggle_physics(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mnt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnx1.html\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/pyvis/network.py:546\u001b[0m, in \u001b[0;36mNetwork.show\u001b[0;34m(self, name, local, notebook)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28mprint\u001b[39m(name)\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m notebook:\n\u001b[0;32m--> 546\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopen_browser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mnotebook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_html(name, open_browser\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/pyvis/network.py:515\u001b[0m, in \u001b[0;36mNetwork.write_html\u001b[0;34m(self, name, local, notebook, open_browser)\u001b[0m\n\u001b[1;32m    513\u001b[0m getcwd_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    514\u001b[0m check_html(getcwd_name)\n\u001b[0;32m--> 515\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhtml \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnotebook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnotebook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcdn_resources \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/pyvis/network.py:479\u001b[0m, in \u001b[0;36mNetwork.generate_html\u001b[0;34m(self, name, local, notebook)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    477\u001b[0m     physics_enabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mphysics\u001b[38;5;241m.\u001b[39menabled\n\u001b[0;32m--> 479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhtml \u001b[38;5;241m=\u001b[39m \u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m                            \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mheading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m                            \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mphysics_enabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphysics_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m                            \u001b[49m\u001b[43muse_DOT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_DOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdot_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mwidget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbgcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbgcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtooltip_link\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_link_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mneighborhood_highlight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneighborhood_highlight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mselect_menu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_menu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mfilter_menu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_menu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnotebook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnotebook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcdn_resources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdn_resources\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhtml\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/jinja2/environment.py:1301\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1301\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/jinja2/environment.py:936\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 936\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/pyvis/templates/template.html:434\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m                   // parsing and collecting nodes and edges from the python\n\u001b[0;32m--> 434\u001b[0m                   nodes = new vis.DataSet({{nodes|tojson}});\n\u001b[1;32m    435\u001b[0m                   edges = new vis.DataSet({{edges|tojson}});\n\u001b[1;32m    436\u001b[0m \n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/jinja2/filters.py:1702\u001b[0m, in \u001b[0;36mdo_tojson\u001b[0;34m(eval_ctx, value, indent)\u001b[0m\n\u001b[1;32m   1699\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   1700\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m indent\n\u001b[0;32m-> 1702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhtmlsafe_json_dumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdumps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/site-packages/jinja2/utils.py:658\u001b[0m, in \u001b[0;36mhtmlsafe_json_dumps\u001b[0;34m(obj, dumps, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dumps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    655\u001b[0m     dumps \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m markupsafe\u001b[38;5;241m.\u001b[39mMarkup(\n\u001b[0;32m--> 658\u001b[0m     \u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mu003c\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mu003e\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mu0026\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mu0027\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    663\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type DataFrame is not JSON serializable"
     ]
    }
   ],
   "source": [
    "nt = Network('800px', '2000px',notebook=True)\n",
    "# populates the nodes and edges data structures\n",
    "nt.from_nx(G)\n",
    "nt.toggle_physics(True)\n",
    "nt.show('nx1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a96fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############Pickle the list of dictionaries ############\n",
    "import pickle\n",
    "\n",
    "# Store data (serialize in a pickle)\n",
    "with open('pdf_content_dict_stage1.pickle', 'wb') as handle:\n",
    "    pickle.dump(document_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_agent_poc",
   "language": "python",
   "name": "search_agent_poc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
