{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Python venv: search_agent_poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4375a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This module will take the serialized dictionary got out of PDF Parsing ann try to extract\n",
    "##Semantic Knowldge like identifying \n",
    "## 1.Important Objects/Entities\n",
    "## 2.Deduplicate Entities\n",
    "## 3.Extracting Relations\n",
    "## 4.Extract the main Ideas/Topics around Each Page\n",
    "## 5.Link the different topics via diffrent entities/Objects\n",
    "## 6.Break down the document by pages instead of Chunks .\n",
    "## 7.If a page does not fit a chunk then chunk them extract information and then deduplicate the information across\n",
    "## the page.\n",
    "\n",
    "#Next Steps:\n",
    "## 5.Try to Seggregate the BigPDF on Sections.\n",
    "## 8.Try To Find Common Objects or ideas that link these sections.\n",
    "## 9.Try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15191ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "#rag_test_input_path='/home/matrix4284/MY_GEN_AI_PROJECTS/RAG/GraphRAG/graphrag-local-ollama/ragtest/input/'+file_name\n",
    "import os\n",
    "# importing shutil module\n",
    "import shutil\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import XMLOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01aaca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_enrichment_output_dir = '../pdf_enrichment/pdf_enriched_output/'\n",
    "graph_qa_output_dir = './graph_qa_output_networkx/'\n",
    "graph_builder_output_dir = '../graph_builder/graph_output_networkx/'\n",
    "#full_filename=file_name+'_'+str(page_idx)+extension\n",
    "#full_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbcded8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Model for Prompt Tuning\n",
    "model_name=\"llama3.1\"\n",
    "llm = ChatOllama(base_url=\"http://192.168.50.100:11434\",model=model_name,temperature=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a648bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Initial Load data (deserialized data).The input data is the enitity extraction as below:\n",
    "#with open(pdf_enrichment_output_dir+'pdf_enriched_content_dict_phase1_entity_extraction_478_final.pickle', 'rb') as handle:\n",
    "#    document_dict_deserialized = pickle.load(handle)\n",
    "\n",
    "####If error happens start the load with intermediate index\n",
    "with open(pdf_enrichment_output_dir+'pdf_enriched_content_dict_phase5_extract_highligts_478_final.pickle', 'rb') as handle:\n",
    "    document_dict_deserialized = pickle.load(handle)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a3fad63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_len=len(document_dict_deserialized)\n",
    "doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5839dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker\\n\n",
    "            Machine Larning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study a user query and extract the main entities or objects from the user's query.\\n\n",
    "            The entities or objects can be anything surrounding \\n\n",
    "            technology in which your expetise lies.By Objects or entities i mean it can be a programming Language,\\n\n",
    "            command,framework,database,process,cache,controller,qeueues,schedulers,errors,exceptions or any \\n\n",
    "            components related to technology. \\n\n",
    "            Never extract page number or any kind of number as entity. \\n\n",
    "            surrounding the technology.Please output in form of list of json where each extracted entity will be \\n\n",
    "            represented by a json along with description and category.\\n\n",
    "            The category name can be like \\n\n",
    "            hardware,software,network,application,database,process,thread,container etc.Mention the category name \\n\n",
    "            in third bracket. \\n\n",
    "            After extracting all the entities along with their description and category format as per rules below: \\n\n",
    "            1.For each entity create a json with 3 keys \"entity\",\"description\" and \"category\". \\n\n",
    "            2.Each extracted entity must be enlosed within double quotes. \\n\n",
    "            3.There should not be any unterimnated string for \"entity\",\"description\" and \"category\".In case it is \\n\n",
    "            unterminated due to any reason put the ending double quote to it.\\n\n",
    "            3.Each extracted entity must be a valid word,phrase or a complete command.\\n\n",
    "            4.Extracted entity should never be a character,part of a word,alphabet,number or any alphaneumeric.\\n\n",
    "            After all the entities of the page are extracted where each entity is a json then only collate all the entity json \\m\n",
    "            into a list of json.\n",
    "            5.If any of the extracted entity,description value contain character ':',handle it as a part of entity \\n\n",
    "            or decription ,dont use the character ':' found in entity or description as  a delimiter.\\n\n",
    "            Output the collated list of json where each entity is represented by json. \\n\n",
    "            Do not output anything other than the list of json neither heading/decsription before the list of json\\n\n",
    "            nor any decsription/footer below the json \\n\n",
    "            Here is the user query: \\n\\n {query} \\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50b57233",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker\\n\n",
    "            Machine Larning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study a user query and extract the main entities or objects from the user's query.\\n\n",
    "            The entities or objects can be anything surrounding \\n\n",
    "            technology.By Objects or entities i mean it can be a programming Language,\\n\n",
    "            command,framework,database,process,cache,controller,qeueues,schedulers,errors,exceptions or any \\n\n",
    "            components related to technology. \\n\n",
    "            The entity words or phrases extracted needs to be a part of user query,\\n\n",
    "            you must not add extra entities which are not there in the query text.\\n\n",
    "            Please output in form of list of json where each extracted entity will be \\n\n",
    "            represented by a json along with description.\\n\n",
    "            After extracting all the entities along with their description format as per rules below: \\n\n",
    "            1.For each entity create a json with 2 keys \"entity\" and \"description\". \\n\n",
    "            2.Each extracted entity must be enlosed within double quotes. \\n\n",
    "            3.There should not be any unterimnated string for entity and description.\n",
    "            4.Each extracted entity must be a valid word,phrase or a complete command.\\n\n",
    "            5.Extracted entity should never be a character,part of a word,alphabet,number or any alphaneumeric.\\n\n",
    "            After all the entities of the page are extracted where each entity is a json then only collate all the entity json \\n\n",
    "            into a list of json.\\n\n",
    "            Do not output anything other than the list of json neither heading/decsription before the list of json\\n\n",
    "            nor any decsription/footer below the json \\n\n",
    "            Here is the user query: \\n\\n {query} \\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "778e87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_query(query):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #parser = XMLOutputParser()\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=template2,\n",
    "            input_variables=[\"query\"],\n",
    "            #partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    #Commented out By Kaustav 20/08/2024 as str output parser is giving some unwanted text #####\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Trying out XML Output parser by Kautsva added 20/08/2024####\n",
    "    #chain = prompt | llm | parser\n",
    "    \n",
    "    \n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "    \n",
    "    #print(\"Page Text\")\n",
    "    #print(page_text)\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"query\": query,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    #print('Relationship Output from LLM\"')\n",
    "    #print(output)\n",
    "    \n",
    "    if '[' in output:\n",
    "        json_output=output.split('[')[1]\n",
    "        json_output='['+json_output\n",
    "    else:    \n",
    "        json_output='['+output\n",
    "    \n",
    "    #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "    \n",
    "    \n",
    "\n",
    "    json_output=reverse(json_output)\n",
    "    \n",
    "    if ']' in json_output:\n",
    "        json_output=json_output.split(']')[1]\n",
    "        json_output=reverse(json_output)\n",
    "        json_output= json_output + ']'\n",
    "    else:\n",
    "        json_output=reverse(json_output)\n",
    "        json_output= json_output + ']'\n",
    "    \n",
    "    entities_json=json.loads(json_output)\n",
    "    \n",
    "    return entities_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d155a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "######This code enrcihes the PDF page by page and while doing so it fails after processing 20 to 50 pages####\n",
    "######currently we are manually tackling this problem by making the pdf dict list into aglobal variable######\n",
    "######After the llm fails we take the index of the last page done and save the pdf dict list uptil that ####\n",
    "#####Then as of now i am manually restarting the by loading the saved pdf dict list as the source pdf and ###\n",
    "#####Using the last done pdf page index +1 as the starting index.So (last Done page Index + 1) is the ######\n",
    "###starting index###\n",
    "####The maximum page has been given as 565 as that is the last page where the main content of the document###\n",
    "####resides.In case of the book Kubernetes in action it is 565.##############################################\n",
    "####If this is batch process then the last page can be manually seen and used as the variable else one can ####\n",
    "#####also go for the pdf enrichment of all the pages of the book#############################################\n",
    "\n",
    "###This manual thing needs to be automated by either langraph or by using agents framework##################\n",
    "\n",
    "def entity_collector(page_idx):\n",
    "    \n",
    "    #print(\"Page Number\")\n",
    "    #print(page_idx)\n",
    "    \n",
    "    entities_jsonl=document_dict_deserialized[page_idx]['entities']\n",
    "    \n",
    "    entity_lst=entity_collector_per_page(entities_jsonl)\n",
    "    \n",
    "    \n",
    "    return entity_lst\n",
    "    \n",
    "    \n",
    "    \n",
    "def parse_query(query,entity_lst):    \n",
    "    \n",
    "    ##Relationship Extraction Enrichment\n",
    "    if len(entity_lst)>0:\n",
    "        page_relationship_lst_dict=extract_relationship_per_query(query,entity_lst)\n",
    "\n",
    "        \n",
    "        print(page_relationship_lst_dict)\n",
    "        \n",
    "def parse_query(query,entity_lst):    \n",
    "    \n",
    "    ##Relationship Extraction Enrichment\n",
    "    if len(entity_lst)>0:\n",
    "        page_relationship_lst_dict=extract_relationship_per_query(query,entity_lst)\n",
    "\n",
    "        \n",
    "        print(page_relationship_lst_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a741fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_dict_deserialized[160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0a6784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"how to test whether a pod is running?\"\n",
    "entities=extract_entities_from_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99603263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"entity\": \"\"Pod\"\", \"description\": \"A container that runs as one or more processes.\"}',\n",
       " '{\"entity\": \"Kubernetes\"\", \"description\": \"An open-source container orchestration system for automating the deployment, scaling, and management of containers.\"}',\n",
       " '{\"entity\": \"test\"\", \"description\": \"To check if a pod is running, you can use the command \\'kubectl get pods\\' to list all running pods.\"}']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9cb6402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize in a pickle) upto page 102\n",
    "with open(graph_qa_output_dir+'graph_qa_phase1_entity_extraction.pickle', 'wb') as handle:\n",
    "    pickle.dump(entities, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54343336",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Definitions of Individual Enrichment Modules######\n",
    "\n",
    "# Function to reverse a string\n",
    "def reverse(string):\n",
    "    string = string[::-1]\n",
    "    return string\n",
    "\n",
    "def entity_collector_per_query(entity_lst):\n",
    "\n",
    "    entities=[]\n",
    "    \n",
    "    for entity in entity_lst:\n",
    "        print(\"Entity:\")\n",
    "        print(entity.replace('\"\"','\"'))\n",
    "        entity_json=json.loads(entity.replace('\"\"','\"\"'))\n",
    "        print(type(entity))\n",
    "        \n",
    "        entity_name=entity_json['entity']\n",
    "        print(entity_name)\n",
    "        entities.append(entity_name)\n",
    "    return list(set(entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09d3f7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity:\n",
      "{\"entity\": \"\"Pod\"\", \"description\": \"A container that runs as one or more processes.\"}\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 14 (char 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m entity_list\u001b[38;5;241m=\u001b[39m\u001b[43mentity_collector_per_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentities\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 15\u001b[0m, in \u001b[0;36mentity_collector_per_query\u001b[0;34m(entity_lst)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntity:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(entity)\n\u001b[0;32m---> 15\u001b[0m entity_json\u001b[38;5;241m=\u001b[39m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(entity))\n\u001b[1;32m     18\u001b[0m entity_name\u001b[38;5;241m=\u001b[39mentity_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/mambaforge-pypy3/envs/search_agent_poc/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 14 (char 13)"
     ]
    }
   ],
   "source": [
    "entity_list=entity_collector_per_query(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ff840b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relationship_per_query(query,entity_lst):\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker\\n\n",
    "            Machine Learning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study an user query/question related to a particular technology and extract all \\n\n",
    "            the closest relations between the list of entities provided as input along with the query.\\n\n",
    "            While extracting relations carry out the following steps as per the below rules for each : \\n\n",
    "            1.You must define each relation by means of a json which must have values for three keys source_entity \\n\n",
    "            ,description,destination_entity.\n",
    "                a.source_entity should have that entity who is the one performing some action that is the subject. \\n\n",
    "                b.destination_entity should have the entity who is the object on which the source entity \\n\n",
    "                carries out \\n\n",
    "                one more more actions.\\n\n",
    "                c.description of the relation will contain a brief summary about what action was being carried \\n\n",
    "                by the source_entity on the destination_entity.\\n\n",
    "            2.Make Sure the extracted source_entity,destination_entity and description are always \\n\n",
    "            within \"\" and never within''.\\n\n",
    "            3.There should not be any unterimnated string for \"source_entity\",\"destination_entity\" and \\n\n",
    "            \"description\"values.In case it is unterminated due to any reason put the ending double quote to it.\\n\n",
    "            4.Each source_entity and destination_entity must be a valid word,phrase or a complete command.\\n\n",
    "            5.source_entity and destination_entity should never be a character,part of a word,alphabet,number \\n\n",
    "            or any alphaneumeric.\\n\n",
    "            6.description of the relation field must be valid sentence and must never be character,\\n\n",
    "            part of a word,alphabet,number. \\n\n",
    "            6.After all the relations have been extracted collate them into a list of json.\n",
    "            7.Make sure the json is terminated properly.\n",
    "            Output should only contain the list of json and no other words or character or sentences. \\n\n",
    "            The out json list must be terminated properly.\n",
    "            Here is the user query: \\n\\n {query} \\n\\n and the entities: \\n\\n {entities} \\n\\n\"\"\",\n",
    "            input_variables=[\"query\",\"entities\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"query\": query,\"entities\":entity_lst\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    #print('Relationship Output from LLM\"')\n",
    "    #print(output)\n",
    "    \n",
    "    if '[' in output:\n",
    "        json_output=output.split('[')[1]\n",
    "        json_output='['+json_output\n",
    "    else:    \n",
    "        json_output='['+output\n",
    "    \n",
    "    #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "    \n",
    "    \n",
    "\n",
    "    json_output=reverse(json_output)\n",
    "    \n",
    "    if ']' in json_output:\n",
    "        json_output=json_output.split(']')[1]\n",
    "        json_output=reverse(json_output)\n",
    "        json_output= json_output + ']'\n",
    "    else:\n",
    "        json_output=reverse(json_output)\n",
    "        json_output= json_output + ']'\n",
    "        \n",
    "    \n",
    "    \n",
    "    #page_output_json=json.loads(output)\n",
    "    #return page_output_json\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a152a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_agent_poc",
   "language": "python",
   "name": "search_agent_poc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
