{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Python venv: search_agent_poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4375a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This module will take the serialized dictionary got out of PDF Parsing ann try to extract\n",
    "##Semantic Knowldge like identifying \n",
    "## 1.Important Objects/Entities\n",
    "## 2.Deduplicate Entities\n",
    "## 3.Extracting Relations\n",
    "## 4.Extract the main Ideas/Topics around Each Page\n",
    "## 5.Link the different topics via diffrent entities/Objects\n",
    "## 6.Break down the document by pages instead of Chunks .\n",
    "## 7.If a page does not fit a chunk then chunk them extract information and then deduplicate the information across\n",
    "## the page.\n",
    "\n",
    "#Next Steps:\n",
    "## 5.Try to Seggregate the BigPDF on Sections.\n",
    "## 8.Try To Find Common Objects or ideas that link these sections.\n",
    "## 9.Try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15191ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "#rag_test_input_path='/home/matrix4284/MY_GEN_AI_PROJECTS/RAG/GraphRAG/graphrag-local-ollama/ragtest/input/'+file_name\n",
    "import os\n",
    "# importing shutil module\n",
    "import shutil\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#embeddings = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.messages import BaseMessage, FunctionMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.output_parsers import XMLOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01aaca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_enrichment_output_dir = '../pdf_enrichment/pdf_enriched_output/'\n",
    "graph_qa_output_dir = './graph_qa_output_networkx/'\n",
    "graph_builder_output_dir = '../graph_builder/graph_output_networkx/'\n",
    "#full_filename=file_name+'_'+str(page_idx)+extension\n",
    "#full_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbcded8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Model for Prompt Tuning\n",
    "model_name=\"llama3.1\"\n",
    "llm = ChatOllama(base_url=\"http://192.168.50.100:11434\",model=model_name,temperature=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a648bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Initial Load data (deserialized data).The input data is the enitity extraction as below:\n",
    "#with open(pdf_enrichment_output_dir+'pdf_enriched_content_dict_phase1_entity_extraction_478_final.pickle', 'rb') as handle:\n",
    "#    document_dict_deserialized = pickle.load(handle)\n",
    "\n",
    "####If error happens start the load with intermediate index\n",
    "with open(pdf_enrichment_output_dir+'pdf_enriched_content_dict_phase5_extract_highligts_478_final.pickle', 'rb') as handle:\n",
    "    document_dict_deserialized = pickle.load(handle)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b57c8c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_len=len(document_dict_deserialized)\n",
    "doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26cd9e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker\\n\n",
    "            Machine Larning,Generative AI,Natural Language Understanding and Computer Vision.\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac2612d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker\\n\n",
    "            Machine Larning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study a user query and extract the main entities or objects from the user's query.\\n\n",
    "            The entities or objects can be anything surrounding \\n\n",
    "            technology in which your expetise lies.By Objects or entities i mean it can be a programming Language,\\n\n",
    "            command,framework,database,process,cache,controller,qeueues,schedulers,errors,exceptions or any \\n\n",
    "            components related to technology. \\n\n",
    "            Never extract page number or any kind of number as entity. \\n\n",
    "            surrounding the technology.Please output in form of list of json where each extracted entity will be \\n\n",
    "            represented by a json along with description and category.\\n\n",
    "            The category name can be like \\n\n",
    "            hardware,software,network,application,database,process,thread,container etc.Mention the category name \\n\n",
    "            in third bracket. \\n\n",
    "            After extracting all the entities along with their description and category format as per rules below: \\n\n",
    "            1.For each entity create a json with 3 keys \"entity\",\"description\" and \"category\". \\n\n",
    "            2.Each extracted entity must be enlosed within double quotes. \\n\n",
    "            3.There should not be any unterimnated string for \"entity\",\"description\" and \"category\".In case it is \\n\n",
    "            unterminated due to any reason put the ending double quote to it.\\n\n",
    "            3.Each extracted entity must be a valid word,phrase or a complete command.\\n\n",
    "            4.Extracted entity should never be a character,part of a word,alphabet,number or any alphaneumeric.\\n\n",
    "            After all the entities of the page are extracted where each entity is a json then only collate all the entity json \\m\n",
    "            into a list of json.\n",
    "            5.If any of the extracted entity,description value contain character ':',handle it as a part of entity \\n\n",
    "            or decription ,dont use the character ':' found in entity or description as  a delimiter.\\n\n",
    "            Output the collated list of json where each entity is represented by json. \\n\n",
    "            Do not output anything other than the list of json neither heading/decsription before the list of json\\n\n",
    "            nor any decsription/footer below the json \\n\n",
    "            Here is the user query: \\n\\n {query} \\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0204d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2=\"\"\"You have to deeply study a user query and extract the main entities or objects from the user's query.\\n\n",
    "            The entities or objects can be anything surrounding \\n\n",
    "            technology.By Objects or entities i mean it can be a programming Language,\\n\n",
    "            command,framework,database,process,cache,controller,qeueues,schedulers,errors,exceptions or any \\n\n",
    "            components related to technology. \\n\n",
    "            The entity words or phrases extracted needs to be a part of user query,\\n\n",
    "            you must not add extra entities which are not there in the query text.\\n\n",
    "            Please output in form of list of json where each extracted entity will be \\n\n",
    "            represented by a json along with description.\\n\n",
    "            After extracting all the entities along with their description format as per rules below: \\n\n",
    "            1.For each entity create a json with 2 keys \"entity\" and \"description\". \\n\n",
    "            2.Each extracted entity must be enlosed within double quotes. \\n\n",
    "            3.There should not be any unterimnated string for entity and description.\n",
    "            4.Each extracted entity must be a valid word,phrase or a complete command.\\n\n",
    "            5.Extracted entity should never be a character,part of a word,alphabet,number or any alphaneumeric.\\n\n",
    "            After all the entities of the page are extracted where each entity is a json then only collate all the entity json \\n\n",
    "            into a list of json.\\n\n",
    "            Do not output anything other than the list of json neither heading/decsription before the list of json\\n\n",
    "            nor any decsription/footer below the json \\n\n",
    "            Here is the user query: \\n\\n {query} \\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a0bb847",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_template=persona+template2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2303adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_query(query):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #parser = XMLOutputParser()\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=final_template,\n",
    "            input_variables=[\"query\"],\n",
    "            #partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    #Commented out By Kaustav 20/08/2024 as str output parser is giving some unwanted text #####\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Trying out XML Output parser by Kautsva added 20/08/2024####\n",
    "    #chain = prompt | llm | parser\n",
    "    \n",
    "    \n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "    \n",
    "    #print(\"Page Text\")\n",
    "    #print(page_text)\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"query\": query,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    #print('Relationship Output from LLM\"')\n",
    "    #print(output)\n",
    "    \n",
    "    if '[' in output:\n",
    "        json_output=output.split('[')[1]\n",
    "        json_output='['+json_output\n",
    "    else:    \n",
    "        json_output='['+output\n",
    "    \n",
    "    #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "    \n",
    "    \n",
    "\n",
    "    json_output=reverse(json_output)\n",
    "    \n",
    "    if ']' in json_output:\n",
    "        json_output=json_output.split(']')[1]\n",
    "        json_output=reverse(json_output)\n",
    "        json_output= json_output + ']'\n",
    "    else:\n",
    "        json_output=reverse(json_output)\n",
    "        json_output= json_output + ']'\n",
    "    \n",
    "    entities_json=json.loads(json_output)\n",
    "    \n",
    "    return entities_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d155a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "######This code enrcihes the PDF page by page and while doing so it fails after processing 20 to 50 pages####\n",
    "######currently we are manually tackling this problem by making the pdf dict list into aglobal variable######\n",
    "######After the llm fails we take the index of the last page done and save the pdf dict list uptil that ####\n",
    "#####Then as of now i am manually restarting the by loading the saved pdf dict list as the source pdf and ###\n",
    "#####Using the last done pdf page index +1 as the starting index.So (last Done page Index + 1) is the ######\n",
    "###starting index###\n",
    "####The maximum page has been given as 565 as that is the last page where the main content of the document###\n",
    "####resides.In case of the book Kubernetes in action it is 565.##############################################\n",
    "####If this is batch process then the last page can be manually seen and used as the variable else one can ####\n",
    "#####also go for the pdf enrichment of all the pages of the book#############################################\n",
    "\n",
    "###This manual thing needs to be automated by either langraph or by using agents framework##################\n",
    "##Definitions of Individual Enrichment Modules######\n",
    "\n",
    "# Function to reverse a string\n",
    "def reverse(string):\n",
    "    string = string[::-1]\n",
    "    return string\n",
    "\n",
    "def entity_collector(page_idx):\n",
    "    \n",
    "    #print(\"Page Number\")\n",
    "    #print(page_idx)\n",
    "    \n",
    "    entities_jsonl=document_dict_deserialized[page_idx]['entities']\n",
    "    \n",
    "    entity_lst=entity_collector_per_page(entities_jsonl)\n",
    "    \n",
    "    \n",
    "    return entity_lst\n",
    "    \n",
    "    \n",
    "    \n",
    "def parse_query(query,entity_lst):    \n",
    "    \n",
    "    ##Relationship Extraction Enrichment\n",
    "    if len(entity_lst)>0:\n",
    "        page_relationship_lst_dict=extract_relationship_per_query(query,entity_lst)\n",
    "\n",
    "        \n",
    "        print(page_relationship_lst_dict)\n",
    "        \n",
    "def parse_query(query,entity_lst):    \n",
    "    \n",
    "    ##Relationship Extraction Enrichment\n",
    "    if len(entity_lst)>0:\n",
    "        page_relationship_lst_dict=extract_relationship_per_query(query,entity_lst)\n",
    "\n",
    "        \n",
    "        print(page_relationship_lst_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a741fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_dict_deserialized[160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0a6784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"how to test whether a pod is running?\"\n",
    "entities=extract_entities_from_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9c6dd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'pod', 'description': 'a container-based instance in Kubernetes'},\n",
       " {'entity': 'Kubernetes',\n",
       "  'description': 'an open-source container orchestration system'},\n",
       " {'entity': 'running',\n",
       "  'description': 'the state of a process or application being executed'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47cc9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize in a pickle) upto page 102\n",
    "with open(graph_qa_output_dir+'graph_qa_phase1_entity_extraction.pickle', 'wb') as handle:\n",
    "    pickle.dump(entities, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d562176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def entity_collector_per_query(entity_lst):\n",
    "\n",
    "    entities=[]\n",
    "    \n",
    "    for entity in entity_lst:\n",
    "        #print(\"Entity:\")\n",
    "        #print(entity)\n",
    "        \n",
    "    \n",
    "        entity_json=json.loads(json.dumps(entity))\n",
    "        \n",
    "        \n",
    "        #print(\"Entity:\")\n",
    "        #print(type(entity_json))\n",
    "        \n",
    "        entity_name=entity_json['entity']\n",
    "        print(entity_name)\n",
    "        entities.append(entity_name)\n",
    "    return list(set(entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bdfb544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity:\n",
      "<class 'dict'>\n",
      "pod\n",
      "Entity:\n",
      "<class 'dict'>\n",
      "Kubernetes\n",
      "Entity:\n",
      "<class 'dict'>\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "entity_list=entity_collector_per_query(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e61a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9971d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pod', 'running', 'Kubernetes']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b09afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relationship_per_query(query,entity_lst):\n",
    "\n",
    "# Prompt\n",
    "    prompt = PromptTemplate(\n",
    "            template=\"\"\"You are a computer technology expert who has mastery in Kubernetes,Docker\\n\n",
    "            Machine Learning,Generative AI,Natural Language Understanding and Computer Vision.\\n\n",
    "            You have to deeply study an user query/question related to a particular technology and extract all \\n\n",
    "            the closest relations between the list of entities provided as input along with the query.\\n\n",
    "            While extracting relations carry out the following steps as per the below rules for each : \\n\n",
    "            1.You must define each relation by means of a json which must have values for three keys source_entity \\n\n",
    "            ,description,destination_entity.\n",
    "                a.source_entity should have that entity who is the one performing some action that is the subject. \\n\n",
    "                b.destination_entity should have the entity who is the object on which the source entity \\n\n",
    "                carries out \\n\n",
    "                one more more actions.\\n\n",
    "                c.description of the relation will contain a brief summary about what action was being carried \\n\n",
    "                by the source_entity on the destination_entity.\\n\n",
    "            2.Make Sure the extracted source_entity,destination_entity and description are always \\n\n",
    "            within \"\" and never within''.\\n\n",
    "            3.There should not be any unterimnated string for \"source_entity\",\"destination_entity\" and \\n\n",
    "            \"description\"values.In case it is unterminated due to any reason put the ending double quote to it.\\n\n",
    "            4.Each source_entity and destination_entity must be a valid word,phrase or a complete command.\\n\n",
    "            5.source_entity and destination_entity should never be a character,part of a word,alphabet,number \\n\n",
    "            or any alphaneumeric.\\n\n",
    "            6.description of the relation field must be valid sentence and must never be character,\\n\n",
    "            part of a word,alphabet,number. \\n\n",
    "            6.After all the relations have been extracted collate them into a list of json.\n",
    "            7.Make sure the json is terminated properly.\n",
    "            Output should only contain the list of json and no other words or character or sentences. \\n\n",
    "            The out json list must be terminated properly.\n",
    "            Here is the user query: \\n\\n {query} \\n\\n and the entities: \\n\\n {entities} \\n\\n\"\"\",\n",
    "            input_variables=[\"query\",\"entities\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Score\n",
    "    #filtered_docs = []\n",
    "\n",
    "    output = chain.invoke(\n",
    "        {\n",
    "                \"query\": query,\"entities\":entity_list\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    #print('Relationship Output from LLM\"')\n",
    "    #print(output)\n",
    "    \n",
    "    if '[' in output:\n",
    "        json_output=output.split('[')[1]\n",
    "        json_output='['+json_output\n",
    "    else:    \n",
    "        json_output='['+output\n",
    "    \n",
    "    #print('JSON OUTPUT:')\n",
    "    #print(json_output)\n",
    "    \n",
    "    \n",
    "\n",
    "    json_output=reverse(json_output)\n",
    "    \n",
    "    if ']' in json_output:\n",
    "        json_output=json_output.split(']')[1]\n",
    "        json_output=reverse(json_output)\n",
    "        json_output= json_output + ']'\n",
    "    else:\n",
    "        json_output=reverse(json_output)\n",
    "        json_output= json_output + ']'\n",
    "        \n",
    "    \n",
    "    \n",
    "    #page_output_json=json.loads(output)\n",
    "    #return page_output_json\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4b7c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output=extract_relationship_per_query(query,entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e671e790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\n  {\\n    \"source_entity\": \"kubectl\",\\n    \"description\": \"check if a pod is running using kubectl command\",\\n    \"destination_entity\": \"pod\"\\n  },\\n  {\\n    \"source_entity\": \"Kubernetes\",\\n    \"description\": \"use Kubernetes to manage and run pods\",\\n    \"destination_entity\": \"pod\"\\n  }\\n]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85dd591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_agent_poc",
   "language": "python",
   "name": "search_agent_poc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
